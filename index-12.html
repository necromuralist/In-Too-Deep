<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neurotic Networking (old posts, page 12) | Neurotic Networking</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/index-12.html" rel="canonical">
<link href="index-13.html" rel="prev" type="text/html">
<link href="index-11.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="."><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/machine-translation/">Machine Translation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/machine-translation/" rel="bookmark"><time class="published dt-published" datetime="2020-10-21T16:51:39-07:00" itemprop="datePublished" title="2020-10-21 16:51">2020-10-21 16:51</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<p>Posts that make up the project:</p>
<ul class="org-ul">
<li><a href="posts/nlp/english-to-french-data/">Loading the English and French Word Embeddings</a></li>
<li><a href="posts/nlp/building-the-machine-translation-data-set/">Building the Machine Translation Training Set</a></li>
<li><a href="posts/nlp/machine-translation-transformation-matrix/">Finding the Machine Translation Transformation Matrix</a></li>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/">Implementing k-Nearest Neighbors for Machine Translation</a></li>
<li><a href="posts/nlp/machine-translation-with-locality-sensitive-hashing/">Locality-Sensitive Hashing (LSH) for Machine Translation</a></li>
<li><a href="posts/nlp/machine-translation-with-approximate-knn/">Approximate kNN for Machine Translation</a></li>
</ul>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/building-the-machine-translation-data-set/">Building the Machine Translation Training Set</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/building-the-machine-translation-data-set/" rel="bookmark"><time class="published dt-published" datetime="2020-10-20T19:10:34-07:00" itemprop="datePublished" title="2020-10-20 19:10">2020-10-20 19:10</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/building-the-machine-translation-data-set/#org2a06046">Beginning</a>
<ul>
<li><a href="posts/nlp/building-the-machine-translation-data-set/#orgabc54a6">Imports</a></li>
<li><a href="posts/nlp/building-the-machine-translation-data-set/#org559ed4c">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/building-the-machine-translation-data-set/#org032a0f0">Middle</a>
<ul>
<li><a href="posts/nlp/building-the-machine-translation-data-set/#org2284d45">Generate Embedding and Transform Matrices</a></li>
<li><a href="posts/nlp/building-the-machine-translation-data-set/#org137f074">Getting the Training Sets</a></li>
</ul>
</li>
<li><a href="posts/nlp/building-the-machine-translation-data-set/#org802966a">End</a></li>
</ul>
</div>
</div>
<div class="highlight">
<pre><span></span><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">dotenv</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">embeddings</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">get</span><span class="o">-</span><span class="n">matrices</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">training</span><span class="o">-</span><span class="n">testing</span><span class="o">-</span><span class="n">data</span><span class="o">&gt;&gt;</span>
</pre></div>
<div class="outline-2" id="outline-container-org2a06046">
<h2 id="org2a06046">Beginning</h2>
<div class="outline-text-2" id="text-org2a06046">
<p>This continues from a prior post where we <a href="posts/nlp/english-to-french-data/">built the EmbeddingsLoader</a> to gather the word-embeddings that match our French and English dictionaries.</p>
</div>
<div class="outline-3" id="outline-container-orgabc54a6">
<h3 id="orgabc54a6">Imports</h3>
<div class="outline-text-3" id="text-orgabc54a6">
<div class="highlight">
<pre><span></span><span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.embeddings</span> <span class="kn">import</span> <span class="n">EmbeddingsLoader</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org559ed4c">
<h3 id="org559ed4c">Set Up</h3>
<div class="outline-text-3" id="text-org559ed4c"></div>
<div class="outline-4" id="outline-container-orga70b3df">
<h4 id="orga70b3df">The Dotenv</h4>
<div class="outline-text-4" id="text-orga70b3df">
<p>This loads the paths to the files.</p>
<div class="highlight">
<pre><span></span> <span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6d0626b">
<h4 id="org6d0626b">The Embeddings</h4>
<div class="outline-text-4" id="text-org6d0626b">
<p>Instead of using the subset word-embeddings that the course created I'm going to try and load the whole word embeddings from scratch. I defined the EmbeddingsLoader in {% lancelot title="this post" %}english-to-french-data{% /lancelot %} so I'll just load it here.</p>
<div class="highlight">
<pre><span></span> <span class="n">loader</span> <span class="o">=</span> <span class="n">EmbeddingsLoader</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org032a0f0">
<h2 id="org032a0f0">Middle</h2>
<div class="outline-text-2" id="text-org032a0f0"></div>
<div class="outline-3" id="outline-container-org2284d45">
<h3 id="org2284d45">Generate Embedding and Transform Matrices</h3>
<div class="outline-text-3" id="text-org2284d45">
<p>Our English and French Embeddings are stored as word:vector dictionaries. To work with the embeddings we're going to need to convert them to matrices. At the same time we need to filter out words that are in one set but not the other so we're going to brute force it a little.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_matrices</span><span class="p">(</span><span class="n">en_fr</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">french_vecs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">english_vecs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Args:</span>
<span class="sd">       en_fr: English to French dictionary</span>
<span class="sd">       french_vecs: French words to their corresponding word embeddings.</span>
<span class="sd">       english_vecs: English words to their corresponding word embeddings.</span>

<span class="sd">    Return: </span>
<span class="sd">       X: a matrix where the columns are the English embeddings.</span>
<span class="sd">       Y: a matrix where the columns correspond to the French embeddings.</span>
<span class="sd">    """</span>

    <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>

    <span class="c1"># X_l and Y_l are lists of the english and french word embeddings</span>
    <span class="c1"># X_l = list()</span>
    <span class="c1"># Y_l = list()</span>

    <span class="c1"># get the english words (the keys in the dictionary) and store in a set()</span>
    <span class="n">english_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">english_vecs</span><span class="p">)</span>

    <span class="c1"># get the french words (keys in the dictionary) and store in a set()</span>
    <span class="n">french_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">french_vecs</span><span class="p">)</span>

    <span class="c1"># store the french words that are part of the english-french dictionary (these are the values of the dictionary)</span>
    <span class="c1"># french_words = set(en_fr.values())</span>
    <span class="n">filtered</span> <span class="o">=</span> <span class="p">{</span><span class="n">english_word</span><span class="p">:</span> <span class="n">french_word</span>
                <span class="k">for</span> <span class="n">english_word</span><span class="p">,</span> <span class="n">french_word</span> <span class="ow">in</span> <span class="n">en_fr</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">english_word</span> <span class="ow">in</span> <span class="n">english_set</span> <span class="ow">and</span> <span class="n">french_word</span> <span class="ow">in</span> <span class="n">french_set</span><span class="p">}</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">english_vecs</span><span class="p">[</span><span class="n">english_word</span><span class="p">]</span> <span class="k">for</span> <span class="n">english_word</span> <span class="ow">in</span> <span class="n">filtered</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="n">french_vecs</span><span class="p">[</span><span class="n">french_word</span><span class="p">]</span> <span class="k">for</span> <span class="n">french_word</span> <span class="ow">in</span> <span class="n">filtered</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

    <span class="c1"># loop through all english, french word pairs in the english french dictionary</span>

    <span class="c1"># for en_word, fr_word in en_fr.items():</span>
    <span class="c1"># </span>
    <span class="c1">#     # check that the french word has an embedding and that the english word has an embedding</span>
    <span class="c1">#     if fr_word in french_set and en_word in english_set:</span>
    <span class="c1"># </span>
    <span class="c1">#         # get the english embedding</span>
    <span class="c1">#         en_vec = english_vecs[en_word]</span>
    <span class="c1"># </span>
    <span class="c1">#         # get the french embedding</span>
    <span class="c1">#         fr_vec = french_vecs[fr_word]</span>
    <span class="c1"># </span>
    <span class="c1">#         # add the english embedding to the list</span>
    <span class="c1">#         X_l.append(en_vec)</span>
    <span class="c1"># </span>
    <span class="c1">#         # add the french embedding to the list</span>
    <span class="c1">#         Y_l.append(fr_vec)</span>
    <span class="c1"># </span>
    <span class="c1"># # stack the vectors of X_l into a matrix X</span>
    <span class="c1"># X = numpy.vstack(X_l)</span>
    <span class="c1"># </span>
    <span class="c1"># # stack the vectors of Y_l into a matrix Y</span>
    <span class="c1"># Y = numpy.vstack(Y_l)</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># return X, Y</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org137f074">
<h3 id="org137f074">Getting the Training Sets</h3>
<div class="outline-text-3" id="text-org137f074">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TrainingData</span><span class="p">:</span>
    <span class="sd">"""Converts the embeddings into a test set</span>

<span class="sd">    Args:</span>
<span class="sd">     loader: EmbeddingsLoader instance</span>
<span class="sd">    """</span>
    <span class="n">_loader</span><span class="p">:</span> <span class="n">EmbeddingsLoader</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_english_vocabulary</span><span class="p">:</span> <span class="nb">set</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_french_vocabulary</span><span class="p">:</span> <span class="nb">set</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_filtered</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_x_train</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_y_train</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">loader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EmbeddingsLoader</span><span class="p">:</span>
        <span class="sd">"""A loader for the embeddings subsets"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_loader</span> <span class="o">=</span> <span class="n">EmbeddingsLoader</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loader</span>

    <span class="nd">@loader</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">loader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_loader</span><span class="p">:</span> <span class="n">EmbeddingsLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""Sets the embeddings loader"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loader</span> <span class="o">=</span> <span class="n">new_loader</span>
        <span class="k">return</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">english_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">set</span><span class="p">:</span>
        <span class="sd">"""The english embeddings subset words"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english_vocabulary</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_english_vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">english_subset</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english_vocabulary</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">french_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">set</span><span class="p">:</span>
        <span class="sd">"""The french embeddings subset words"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french_vocabulary</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_french_vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">french_subset</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french_vocabulary</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">filtered</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">"""A {enlish:french} dict filtered down</span>

<span class="sd">       This is a dict made of the original english-french dictionary created</span>
<span class="sd">       by the embeddings loader but filtered down so that the key is in the</span>
<span class="sd">       ``english_vocabulary`` and the value is in the ``french_vocabulary``</span>

<span class="sd">       This is used to ensure that the training set is created it will only</span>
<span class="sd">       contain terms that have entries in both embeddings subsets</span>
<span class="sd">       """</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_filtered</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_filtered</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">english_word</span><span class="p">:</span> <span class="n">french_word</span>
                <span class="k">for</span> <span class="n">english_word</span><span class="p">,</span> <span class="n">french_word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">english_word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">english_vocabulary</span> <span class="ow">and</span>
                    <span class="n">french_word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">french_vocabulary</span><span class="p">)}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_filtered</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">x_train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""The english-language embeddings as row-vectors"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_train</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_x_train</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span>
                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">english_subset</span><span class="p">[</span><span class="n">english_word</span><span class="p">]</span>
                 <span class="k">for</span> <span class="n">english_word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">filtered</span><span class="p">]</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_train</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">y_train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""The french-language embeddings as row-vectors"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_train</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_y_train</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span>
                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">french_subset</span><span class="p">[</span><span class="n">french_word</span><span class="p">]</span>
                 <span class="k">for</span> <span class="n">french_word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">filtered</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_train</span>

    <span class="k">def</span> <span class="nf">check_rep</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""Checks the shape of the training data</span>


<span class="sd">       Note:</span>
<span class="sd">        since this checks those attributes they will be built if they don't</span>
<span class="sd">        already exist</span>

<span class="sd">       Raises:</span>
<span class="sd">        AttributeError - there'se something unexpected about the shape of the data</span>
<span class="sd">       """</span>
        <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">rows</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filtered</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">columns</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">english_subset</span><span class="o">.</span><span class="n">values</span><span class="p">())))</span>

        <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">rows</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">filtered</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">columns</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">french_subset</span><span class="o">.</span><span class="n">values</span><span class="p">())))</span>            
        <span class="k">return</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org802966a">
<h2 id="org802966a">End</h2>
<div class="outline-text-2" id="text-org802966a">
<ul class="org-ul">
<li>The post that collects all the posts for this project is <a href="posts/nlp/machine-translation/">Machine Translation</a>.</li>
<li>The next post in this series is <a href="posts/nlp/machine-translation-transformation-matrix/">Training the Machine Translation Transformation Matrix</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/english-to-french-data/">Loading the English and French Word Embeddings</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/english-to-french-data/" rel="bookmark"><time class="published dt-published" datetime="2020-10-13T17:04:25-07:00" itemprop="datePublished" title="2020-10-13 17:04">2020-10-13 17:04</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/english-to-french-data/#org4424df7">Beginning</a>
<ul>
<li><a href="posts/nlp/english-to-french-data/#org141e3f7">Imports</a></li>
<li><a href="posts/nlp/english-to-french-data/#org83bb0fc">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/english-to-french-data/#orgd1a04f0">Middle</a>
<ul>
<li><a href="posts/nlp/english-to-french-data/#org416320c">The Embeddings</a></li>
<li><a href="posts/nlp/english-to-french-data/#orge5ffa80">Dict Loader</a></li>
<li><a href="posts/nlp/english-to-french-data/#org228944d">More Builders</a></li>
<li><a href="posts/nlp/english-to-french-data/#orge3aff3b">Load And Build</a></li>
<li><a href="posts/nlp/english-to-french-data/#org2475519">A Loader</a></li>
</ul>
</li>
<li><a href="posts/nlp/english-to-french-data/#org1856eca">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org4424df7">
<h2 id="org4424df7">Beginning</h2>
<div class="outline-text-2" id="text-org4424df7">
<p>This is the first post in a series - the document with links to all the posts in the series is <a href="posts/nlp/machine-translation/">this post</a>.</p>
<p>The Machine Translation exercise uses word embeddings that are subsets of <a href="https://code.google.com/archive/p/word2vec/%20">prebuilt Word2Vec</a> (English) embeddings (<code>GoogleNews-vectors-negative300.bin.gz</code>) and <a href="https://github.com/vjstark/crosslingual_text_classification">prebuilt French Embeddings</a> (<code>wiki.multi.fr.vec</code>). Coursera provides them but I thought it would be a good exercise to look at how they're built.</p>
</div>
<div class="outline-3" id="outline-container-org141e3f7">
<h3 id="org141e3f7">Imports</h3>
<div class="outline-text-3" id="text-org141e3f7">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org83bb0fc">
<h3 id="org83bb0fc">Set Up</h3>
<div class="outline-text-3" id="text-org83bb0fc"></div>
<div class="outline-4" id="outline-container-orgb34411a">
<h4 id="orgb34411a">The Dotenv</h4>
<div class="outline-text-4" id="text-orgb34411a">
<p>To make loading files more or less portable I'm using a <code>.env</code> file with the paths to the data sets. This loads it into the environment so the values are accessible using <code>os.environ</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd1a04f0">
<h2 id="orgd1a04f0">Middle</h2>
<div class="outline-text-2" id="text-orgd1a04f0"></div>
<div class="outline-3" id="outline-container-org416320c">
<h3 id="org416320c">The Embeddings</h3>
<div class="outline-text-3" id="text-org416320c">
<p>As I noted the English and French embeddings are available from the web. I was thinking of making a download if the files don't exist but the Google News embeddings file is pretty big so the download takes a while on my internet connection so I thought it'd be better to download it from a browser anyway. I'm going to assume the files are downloaded and the Google News embeddings are un-zipped (probably using <a href="https://linux.die.net/man/1/gunzip">gunzip</a> or <a href="https://www.zlib.net/pigz/">pigz</a>, both of which are installed by default on Ubuntu 20.04).</p>
</div>
<div class="outline-4" id="outline-container-org724e1fa">
<h4 id="org724e1fa">Notes</h4>
<div class="outline-text-4" id="text-org724e1fa">
<div class="highlight">
<pre><span></span><span class="sd">"""This is a module for word embeddings loaders.</span>
<span class="sd">"""</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga4c3465">
<h4 id="orga4c3465">Imports</h4>
<div class="outline-text-4" id="text-orga4c3465">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">BaseKeyedVectors</span><span class="p">,</span> <span class="n">KeyedVectors</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">pandas</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf60c7eb">
<h4 id="orgf60c7eb">The Raw Loader</h4>
<div class="outline-text-4" id="text-orgf60c7eb">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Embeddings</span><span class="p">:</span>
    <span class="sd">"""Embeddings Loader"""</span>
    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">binary</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">_embeddings</span><span class="p">:</span> <span class="n">BaseKeyedVectors</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgef6a7af"></a>The Embeddings<br>
<div class="outline-text-5" id="text-orgef6a7af">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseKeyedVectors</span><span class="p">:</span>
    <span class="sd">"""The loaded embeddings"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embeddings</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">,</span>
                                                             <span class="n">binary</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embeddings</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org2fe416f">
<h4 id="org2fe416f">The Subset Builder</h4>
<div class="outline-text-4" id="text-org2fe416f">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SubsetBuilder</span><span class="p">:</span>
    <span class="sd">"""Create subset of embeddings that matches sets</span>

<span class="sd">    Args:</span>
<span class="sd">     embeddings_1: word embeddings</span>
<span class="sd">     embeddings_2: word embeddings</span>
<span class="sd">     subset_dict: dict whose keys and values to pull out of the embeddings</span>
<span class="sd">     output_1: path to save the first subset to</span>
<span class="sd">     output_2: path to save the second subset to</span>
<span class="sd">    """</span>
    <span class="n">embeddings_1</span><span class="p">:</span> <span class="n">KeyedVectors</span>
    <span class="n">embeddings_2</span><span class="p">:</span> <span class="n">KeyedVectors</span>
    <span class="n">subset_dict</span><span class="p">:</span> <span class="nb">dict</span>
    <span class="n">output_1</span><span class="p">:</span> <span class="n">Path</span>
    <span class="n">output_2</span><span class="p">:</span> <span class="n">Path</span>

    <span class="n">_vocabulary_1</span><span class="p">:</span> <span class="nb">set</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vocabulary_2</span><span class="p">:</span> <span class="nb">set</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_subset_1</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_subset_2</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org30150c7"></a>Subset 1<br>
<div class="outline-text-5" id="text-org30150c7">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">subset_1</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""Subset of embeddings 1"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subset_1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_1</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>        
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_1</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_subset_1</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subset_1</span>
</pre></div>
</div>
</li>
<li><a id="org699e287"></a>Subset 2<br>
<div class="outline-text-5" id="text-org699e287">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">subset_2</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""subset of embeddings 2"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subset_2</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_2</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_2</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_subset_2</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subset_2</span>
</pre></div>
</div>
</li>
<li><a id="org50d1a7d"></a>Save<br>
<div class="outline-text-5" id="text-org50d1a7d">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">pickle_it</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Save the subsets"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subset_1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_1</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subset_1</span><span class="p">,</span> <span class="n">writer</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subset_2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_2</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">subset_2</span><span class="p">,</span> <span class="n">writer</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="orgffab113"></a>Clean it<br>
<div class="outline-text-5" id="text-orgffab113">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">clean</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Remove any pickled subsets</span>

<span class="sd">    Also removes any subset dictionaries</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_2</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">path</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
            <span class="n">path</span><span class="o">.</span><span class="n">unlink</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_subset_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subset_2</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="orgd06cd9a"></a>Call the Subset Builder<br>
<div class="outline-text-5" id="text-orgd06cd9a">
<div class="highlight">
<pre><span></span><span class="k">def</span>  <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pickle_it</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Builds or loads the subsets and saves them as pickles</span>

<span class="sd">    Args:</span>
<span class="sd">     pickle_it: whether to save the subsets</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">subset_1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">subset_2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_subset_1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subset_2</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">subset_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_1</span> <span class="ow">and</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_2</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_subset_1</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_1</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_subset_2</span><span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_2</span><span class="p">[</span><span class="n">value</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">pickle_it</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pickle_it</span><span class="p">()</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orge5ffa80">
<h3 id="orge5ffa80">Dict Loader</h3>
<div class="outline-text-3" id="text-orge5ffa80">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">DictLoader</span><span class="p">:</span>
    <span class="sd">"""Loader for the english and french dictionaries</span>

<span class="sd">    This is specifically for the training and testing files</span>
<span class="sd">     - CSV-ish (separated by spaces instead of commas)</span>
<span class="sd">     - No header: column 1 = English, column 2 = English</span>

<span class="sd">    Args:</span>
<span class="sd">     path: path to the file</span>
<span class="sd">     columns: list of strings</span>
<span class="sd">     delimiter: separator for the columns in the source file</span>
<span class="sd">    """</span>
    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">columns</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="p">[</span><span class="s2">"English"</span><span class="p">,</span> <span class="s2">"French"</span><span class="p">]</span>
    <span class="n">delimiter</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">" "</span>

    <span class="n">_dataframe</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_dictionary</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgcf94645"></a>Data Frame<br>
<div class="outline-text-5" id="text-orgcf94645">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">dataframe</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">"""Loads the space-separated file as a dataframe"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataframe</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dataframe</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">,</span>
                                          <span class="n">names</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
                                          <span class="n">delimiter</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">delimiter</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataframe</span>
</pre></div>
</div>
</li>
<li><a id="org6863c88"></a>Dictionary<br>
<div class="outline-text-5" id="text-org6863c88">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">dictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""english to french dictionary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dictionary</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataframe</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">dataframe</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dictionary</span>
</pre></div>
</div>
</li>
</ul>
<div class="outline-4" id="outline-container-orgc49ae49">
<h4 id="orgc49ae49">Loading It</h4>
<div class="outline-text-4" id="text-orgc49ae49">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.embeddings</span> <span class="kn">import</span> <span class="n">Embeddings</span>

<span class="n">english_embeddings</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"GOOGLE_EMBEDDINGS"</span><span class="p">],</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">french_embeddings</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"FRENCH_EMBEDDINGS"</span><span class="p">],</span> <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">english_embeddings</span><span class="o">.</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.embeddings</span> <span class="kn">import</span> <span class="n">DictLoader</span>

<span class="n">training</span> <span class="o">=</span> <span class="n">DictLoader</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"ENGLISH_FRENCH_TRAINING"</span><span class="p">])</span>
<span class="n">testing</span> <span class="o">=</span> <span class="n">DictLoader</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"ENGLISH_FRENCH_TESTING"</span><span class="p">])</span>

<span class="n">train_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">dictionary</span><span class="p">)</span>
<span class="n">test_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">testing</span><span class="o">.</span><span class="n">dictionary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_keys</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">test_keys</span><span class="p">))</span>
</pre></div>
<pre class="example">
set()
</pre>
<p>After I made the subset builder it occured to me that if there was overlap between the testing and training sets but they mapped to different definitions then the way I was going to build them would require two separated dictionaries, but as you can see, the training and testing sets don't have English terms in common.</p>
<div class="highlight">
<pre><span></span><span class="n">merged</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">dictionary</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">merged</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">testing</span><span class="o">.</span><span class="n">dictionary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training</span><span class="o">.</span><span class="n">dictionary</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">testing</span><span class="o">.</span><span class="n">dictionary</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">merged</span><span class="p">))</span>
</pre></div>
<pre class="example">
5000
1500
6500
</pre>
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.embeddings</span> <span class="kn">import</span> <span class="n">SubsetBuilder</span>

<span class="n">english_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"ENGLISH_EMBEDDINGS_SUBSET"</span><span class="p">])</span>
<span class="n">french_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"FRENCH_EMBEDDINGS_SUBSET"</span><span class="p">])</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">SubsetBuilder</span><span class="p">(</span><span class="n">embeddings_1</span><span class="o">=</span><span class="n">english_embeddings</span><span class="o">.</span><span class="n">embeddings</span><span class="p">,</span>
                        <span class="n">embeddings_2</span><span class="o">=</span><span class="n">french_embeddings</span><span class="o">.</span><span class="n">embeddings</span><span class="p">,</span>
                        <span class="n">subset_dict</span><span class="o">=</span><span class="n">merged</span><span class="p">,</span>
                        <span class="n">output_1</span><span class="o">=</span><span class="n">english_path</span><span class="p">,</span> <span class="n">output_2</span><span class="o">=</span><span class="n">french_path</span><span class="p">)</span>
<span class="n">builder</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">builder</span> <span class="o">=</span> <span class="n">SubsetBuilder</span><span class="p">(</span><span class="n">embeddings_1</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">embeddings_2</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">subset_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">output_1</span><span class="o">=</span><span class="n">english_path</span><span class="p">,</span> <span class="n">output_2</span><span class="o">=</span><span class="n">french_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org228944d">
<h3 id="org228944d">More Builders</h3>
<div class="outline-text-3" id="text-org228944d">
<p>After I tried using the <code>EmbeddingsLoader</code> on a different computer I realized that I didn't really simplify the creation of the embeddings all that much so I'm going to make an overall builder that maybe hides it from the end-user (although not entirely since I use environment variables that have to be set).</p>
</div>
<div class="outline-4" id="outline-container-orgb32bdbd">
<h4 id="orgb32bdbd">Source Keys</h4>
<div class="outline-text-4" id="text-orgb32bdbd">
<div class="highlight">
<pre><span></span><span class="n">SourceKeys</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">english</span><span class="o">=</span><span class="s2">"GOOGLE_EMBEDDINGS"</span><span class="p">,</span>
    <span class="n">french</span><span class="o">=</span><span class="s2">"FRENCH_EMBEDDINGS"</span><span class="p">,</span>
    <span class="n">training</span><span class="o">=</span><span class="s2">"ENGLISH_FRENCH_TRAINING"</span><span class="p">,</span>
    <span class="n">testing</span><span class="o">=</span><span class="s2">"ENGLISH_FRENCH_TESTING"</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgaf3bfb5">
<h4 id="orgaf3bfb5">Target Keys</h4>
<div class="outline-text-4" id="text-orgaf3bfb5">
<div class="highlight">
<pre><span></span><span class="n">TargetKeys</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">english</span><span class="o">=</span><span class="s2">"ENGLISH_EMBEDDINGS_SUBSET"</span><span class="p">,</span>
    <span class="n">french</span><span class="o">=</span><span class="s2">"FRENCH_EMBEDDINGS_SUBSET"</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org37b36b9">
<h4 id="org37b36b9">Keys</h4>
<div class="outline-text-4" id="text-org37b36b9">
<div class="highlight">
<pre><span></span><span class="n">Keys</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">source</span><span class="o">=</span><span class="n">SourceKeys</span><span class="p">,</span>
    <span class="n">target</span><span class="o">=</span><span class="n">TargetKeys</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1f43f55">
<h4 id="org1f43f55">Source Paths</h4>
<div class="outline-text-4" id="text-org1f43f55">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SourcePaths</span><span class="p">:</span>
    <span class="sd">"""Paths to the source files</span>

<span class="sd">    These are files provided from other sources</span>
<span class="sd">    """</span>
    <span class="n">keys</span><span class="p">:</span> <span class="n">Namespace</span><span class="o">=</span><span class="n">Keys</span>
    <span class="n">_english</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_french</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_training</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_testing</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">english</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
        <span class="sd">"""Path to the english word-embeddings"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_english</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">english</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">french</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
        <span class="sd">"""Path to the french word-embeddings"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_french</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">french</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">training</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
        <span class="sd">"""Path to the training dictionary"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">training</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">testing</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
        <span class="sd">"""Path to the testing dictionary"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">testing</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc38e88a">
<h4 id="orgc38e88a">Target Paths</h4>
<div class="outline-text-4" id="text-orgc38e88a">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TargetPaths</span><span class="p">:</span>
    <span class="sd">"""Paths to save derived files"""</span>
    <span class="n">keys</span><span class="p">:</span> <span class="n">Namespace</span><span class="o">=</span><span class="n">Keys</span>
    <span class="n">_english</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_french</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">english</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
        <span class="sd">"""Path to derived subset of english embeddings"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_english</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">english</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">french</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
        <span class="sd">"""Path to derived subset of french embeddings"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_french</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">french</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgde07823">
<h4 id="orgde07823">Paths</h4>
<div class="outline-text-4" id="text-orgde07823">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Paths</span><span class="p">:</span>
    <span class="sd">"""Class to build and hold the source and target file paths"""</span>
    <span class="n">_target</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_source</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">target</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TargetPaths</span><span class="p">:</span>
        <span class="sd">"""Holds object with paths to created embeddings subsets"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_target</span> <span class="o">=</span> <span class="n">TargetPaths</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">source</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SourcePaths</span><span class="p">:</span>
        <span class="sd">"""Holds objetw with paths to original source files"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_source</span> <span class="o">=</span> <span class="n">SourcePaths</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_source</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orge3aff3b">
<h3 id="orge3aff3b">Load And Build</h3>
<div class="outline-text-3" id="text-orge3aff3b">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LoadAndBuild</span><span class="p">:</span>
    <span class="sd">"""Loads embeddings and dictionaries and builds subsets"""</span>
    <span class="n">_paths</span><span class="p">:</span> <span class="n">Paths</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_english_embeddings</span><span class="p">:</span> <span class="n">BaseKeyedVectors</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_french_embeddings</span><span class="p">:</span> <span class="n">BaseKeyedVectors</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_training</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_testing</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_merged_dicts</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_subset_builder</span><span class="p">:</span> <span class="n">SubsetBuilder</span><span class="o">=</span><span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">paths</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Paths</span><span class="p">:</span>
        <span class="sd">"""Object with paths to files"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_paths</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_paths</span> <span class="o">=</span> <span class="n">Paths</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_paths</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">english_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseKeyedVectors</span><span class="p">:</span>
        <span class="sd">"""Word embeddings for English"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_english_embeddings</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">english</span><span class="p">,</span>
                                                  <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">embeddings</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english_embeddings</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">french_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseKeyedVectors</span><span class="p">:</span>
        <span class="sd">"""Word embeddings for French"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french_embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_french_embeddings</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">french</span><span class="p">,</span>
                                                 <span class="n">binary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">embeddings</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french_embeddings</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">training</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">"""training dictionary"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="o">=</span> <span class="n">DictLoader</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">training</span><span class="p">)</span><span class="o">.</span><span class="n">dictionary</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">testing</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">"""Testing dictionary"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span> <span class="o">=</span> <span class="n">DictLoader</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">testing</span><span class="p">)</span><span class="o">.</span><span class="n">dictionary</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">merged_dicts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="sd">"""Testing and training merged"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_dicts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_merged_dicts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_merged_dicts</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">testing</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_merged_dicts</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">testing</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merged_dicts</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">subset_builder</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SubsetBuilder</span><span class="p">:</span>
        <span class="sd">"""Builder of the subset dictionaries"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subset_builder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_subset_builder</span> <span class="o">=</span> <span class="n">SubsetBuilder</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">english_embeddings</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">french_embeddings</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">merged_dicts</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">english</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">french</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_subset_builder</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""Calls the subset builder"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">subset_builder</span><span class="p">()</span>
        <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2475519">
<h3 id="org2475519">A Loader</h3>
<div class="outline-text-3" id="text-org2475519">
<p>As a convenience I'm going to make a loader for all the parts.</p>
<div class="highlight">
<pre><span></span><span class="n">EmbeddingsKeys</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">english_subset</span><span class="o">=</span><span class="s2">"ENGLISH_EMBEDDINGS_SUBSET"</span><span class="p">,</span>
    <span class="n">french_subset</span><span class="o">=</span><span class="s2">"FRENCH_EMBEDDINGS_SUBSET"</span><span class="p">,</span>
    <span class="n">training</span><span class="o">=</span><span class="s2">"ENGLISH_FRENCH_TRAINING"</span><span class="p">,</span>
    <span class="n">testing</span><span class="o">=</span><span class="s2">"ENGLISH_FRENCH_TESTING"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">EmbeddingsLoader</span><span class="p">:</span>
    <span class="sd">"""Loads the embeddings and dictionaries</span>

<span class="sd">    Warning:</span>
<span class="sd">     this assumes that you've loaded the proper environment variables to</span>
<span class="sd">    find the files - it doesn't call ``load_dotenv``</span>

<span class="sd">    """</span>
    <span class="n">_loader_builder</span><span class="p">:</span> <span class="n">LoadAndBuild</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_english_subset</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_french_subset</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_training</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_testing</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">loader_builder</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LoadAndBuild</span><span class="p">:</span>
    <span class="sd">"""Object to load sources and build subsets"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loader_builder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_loader_builder</span> <span class="o">=</span> <span class="n">LoadAndBuild</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loader_builder</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">english_subset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""The english embeddings subset</span>

<span class="sd">    This is a subset of the Google News embeddings that matches the keys in </span>
<span class="sd">    the english to french dictionaries</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english_subset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">english</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_english_subset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="o">.</span><span class="n">subset_builder</span><span class="o">.</span><span class="n">subset_1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">english</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_english_subset</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_english_subset</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">french_subset</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""Subset of the MUSE French embeddings"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french_subset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">french</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">french</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_french_subset</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_french_subset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="o">.</span><span class="n">subset_builder</span><span class="o">.</span><span class="n">subset_2</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_french_subset</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">training</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""The english to french dictionary training set"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training</span> <span class="o">=</span> <span class="n">DictLoader</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">training</span><span class="p">)</span><span class="o">.</span><span class="n">dictionary</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">testing</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""testing english to french dictionary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span> <span class="o">=</span> <span class="n">DictLoader</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loader_builder</span><span class="o">.</span><span class="n">paths</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">testing</span><span class="p">)</span><span class="o">.</span><span class="n">dictionary</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_testing</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1856eca">
<h2 id="org1856eca">End</h2>
<div class="outline-text-2" id="text-org1856eca">
<ul class="org-ul">
<li>The next step is to <a href="posts/nlp/building-the-machine-translation-data-set/">convert the embeddings to a data set</a>.</li>
<li>The page that collects all the pages for this project is the <a href="posts/nlp/machine-translation/">Machine Translation</a> page.</li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/machine-translation-with-approximate-knn/">Approximate kNN for Machine Translation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/machine-translation-with-approximate-knn/" rel="bookmark"><time class="published dt-published" datetime="2020-10-12T13:39:45-07:00" itemprop="datePublished" title="2020-10-12 13:39">2020-10-12 13:39</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/machine-translation-with-approximate-knn/#orga40965f">Beginning</a>
<ul>
<li><a href="posts/nlp/machine-translation-with-approximate-knn/#orgc861d32">Imports</a></li>
<li><a href="posts/nlp/machine-translation-with-approximate-knn/#org7ce4e42">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/machine-translation-with-approximate-knn/#org133b532">Middle</a>
<ul>
<li><a href="posts/nlp/machine-translation-with-approximate-knn/#org412fa91">Approximate K-NN</a></li>
</ul>
</li>
<li><a href="posts/nlp/machine-translation-with-approximate-knn/#org1874a21">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orga40965f">
<h2 id="orga40965f">Beginning</h2>
<div class="outline-text-2" id="text-orga40965f">
<p>In the <a href="posts/nlp/machine-translation-with-locality-sensitive-hashing/">previous post</a> we implemented Locality Sensitive Hashing. It's part of a series of posts building an English to French translator whose links are gathered in <a href="posts/nlp/machine-translation/">this post</a>.</p>
</div>
<div class="outline-3" id="outline-container-orgc861d32">
<h3 id="orgc861d32">Imports</h3>
<div class="outline-text-3" id="text-orgc861d32">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">twitter_samples</span>

<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># this repo</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.embeddings</span> <span class="kn">import</span> <span class="n">EmbeddingsLoader</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.english_french</span> <span class="kn">import</span> <span class="n">TrainingData</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.hashing</span> <span class="kn">import</span> <span class="p">(</span><span class="n">DocumentsEmbeddings</span><span class="p">,</span>
                                                  <span class="n">PlanesUniverse</span><span class="p">,</span>
                                                  <span class="n">HashTable</span><span class="p">,</span>
                                                  <span class="n">HashTables</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.nearest_neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.processor</span> <span class="kn">import</span> <span class="n">TwitterProcessor</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.training</span> <span class="kn">import</span> <span class="n">TheTrainer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7ce4e42">
<h3 id="org7ce4e42">Set Up</h3>
<div class="outline-text-3" id="text-org7ce4e42"></div>
<div class="outline-4" id="outline-container-org78f22e6">
<h4 id="org78f22e6">The Environment</h4>
<div class="outline-text-4" id="text-org78f22e6">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb6645e4">
<h4 id="orgb6645e4">The Tweets</h4>
<div class="outline-text-4" id="text-orgb6645e4">
<div class="highlight">
<pre><span></span><span class="n">positive_tweets</span> <span class="o">=</span> <span class="n">twitter_samples</span><span class="o">.</span><span class="n">strings</span><span class="p">(</span><span class="s2">"positive_tweets.json"</span><span class="p">)</span>
<span class="n">negative_tweets</span> <span class="o">=</span> <span class="n">twitter_samples</span><span class="o">.</span><span class="n">strings</span><span class="p">(</span><span class="s2">"negative_tweets.json"</span><span class="p">)</span>
<span class="n">tweets</span> <span class="o">=</span> <span class="n">positive_tweets</span> <span class="o">+</span> <span class="n">negative_tweets</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org42e0e49">
<h4 id="org42e0e49">The Twitter Processor</h4>
<div class="outline-text-4" id="text-org42e0e49">
<div class="highlight">
<pre><span></span><span class="n">process_tweet</span> <span class="o">=</span> <span class="n">TwitterProcessor</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org288808e">
<h4 id="org288808e">The Embeddings</h4>
<div class="outline-text-4" id="text-org288808e">
<div class="highlight">
<pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">EmbeddingsLoader</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">documents</span> <span class="o">=</span> <span class="n">DocumentsEmbeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings</span><span class="o">.</span><span class="n">english_subset</span><span class="p">,</span>
                                <span class="n">process</span><span class="o">=</span><span class="n">process_tweet</span><span class="p">,</span> <span class="n">documents</span><span class="o">=</span><span class="n">tweets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org00cdfc6">
<h4 id="org00cdfc6">Some Constants</h4>
<div class="outline-text-4" id="text-org00cdfc6">
<div class="highlight">
<pre><span></span><span class="n">TWEET</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">vectors</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">tweets</span><span class="p">),</span>
    <span class="n">dimensions</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">english_subset</span><span class="o">.</span><span class="n">values</span><span class="p">()))),</span>
    <span class="n">universes</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">vectors_per_bucket</span><span class="o">=</span><span class="mi">16</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org68e6d73">
<h4 id="org68e6d73">The Planes</h4>
<div class="outline-text-4" id="text-org68e6d73">
<div class="highlight">
<pre><span></span><span class="n">universes</span> <span class="o">=</span> <span class="n">PlanesUniverse</span><span class="p">(</span><span class="n">vector_count</span><span class="o">=</span><span class="n">TWEET</span><span class="o">.</span><span class="n">vectors</span><span class="p">,</span>
                           <span class="n">dimensions</span><span class="o">=</span><span class="n">TWEET</span><span class="o">.</span><span class="n">dimensions</span><span class="p">,</span>
                           <span class="n">universes</span><span class="o">=</span><span class="n">TWEET</span><span class="o">.</span><span class="n">universes</span><span class="p">,</span>
                           <span class="n">vectors_per_bucket</span><span class="o">=</span><span class="n">TWEET</span><span class="o">.</span><span class="n">vectors_per_bucket</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">universes</span><span class="o">.</span><span class="n">plane_count</span> <span class="o">==</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge7e3fb0">
<h4 id="orge7e3fb0">The Hash Tables</h4>
<div class="outline-text-4" id="text-orge7e3fb0">
<div class="highlight">
<pre><span></span><span class="n">hasher</span> <span class="o">=</span> <span class="n">HashTables</span><span class="p">(</span><span class="n">planes</span><span class="o">=</span><span class="n">universes</span><span class="o">.</span><span class="n">planes</span><span class="p">,</span>
                    <span class="n">universes</span><span class="o">=</span><span class="n">TWEET</span><span class="o">.</span><span class="n">universes</span><span class="p">,</span>
                    <span class="n">vectors</span><span class="o">=</span><span class="n">documents</span><span class="o">.</span><span class="n">documents_embeddings</span><span class="p">)</span>
<span class="n">hash_tables</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">hash_tables</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org452c687">
<h4 id="org452c687">The ID Tables</h4>
<div class="outline-text-4" id="text-org452c687">
<div class="highlight">
<pre><span></span><span class="n">id_tables</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">id_tables</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga575e6d">
<h4 id="orga575e6d">The Training Data</h4>
<div class="outline-text-4" id="text-orga575e6d">
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">TrainingData</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org133b532">
<h2 id="org133b532">Middle</h2>
<div class="outline-text-2" id="text-org133b532"></div>
<div class="outline-3" id="outline-container-org412fa91">
<h3 id="org412fa91">Approximate K-NN</h3>
<div class="outline-text-3" id="text-org412fa91">
<p>Implement approximate K nearest neighbors using locality sensitive hashing, to search for documents that are similar to a given document at the index <code>doc_id</code>.</p>
</div>
<div class="outline-4" id="outline-container-org812aa2a">
<h4 id="org812aa2a">Arguments</h4>
<div class="outline-text-4" id="text-org812aa2a">
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Variable</th>
<th class="org-left" scope="col">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><code>doc_id</code></td>
<td class="org-left">index into the document list <code>all_tweets</code></td>
</tr>
<tr>
<td class="org-left"><code>v</code></td>
<td class="org-left">document vector for the tweet in <code>all_tweets</code> at index <code>doc_id</code></td>
</tr>
<tr>
<td class="org-left"><code>planes_l</code></td>
<td class="org-left">list of planes (the global variable created earlier)</td>
</tr>
<tr>
<td class="org-left"><code>k</code></td>
<td class="org-left">number of nearest neighbors to search for</td>
</tr>
<tr>
<td class="org-left"><code>num_universes_to_use</code></td>
<td class="org-left">Number of available universes to use (25 by default)</td>
</tr>
</tbody>
</table>
<p>The <code>approximate_knn</code> function finds a subset of candidate vectors that are in the same "hash bucket" as the input vector 'v'. Then it performs the usual k-nearest neighbors search on this subset (instead of searching through all 10,000 tweets).</p>
</div>
</div>
<div class="outline-4" id="outline-container-org48074f7">
<h4 id="org48074f7">Hints</h4>
<div class="outline-text-4" id="text-org48074f7">
<ul class="org-ul">
<li>There are many dictionaries used in this function. Try to print out <code>planes_l</code>, <code>hash_tables</code>, <code>id_tables</code> to understand how they are structured, what the keys represent, and what the values contain.</li>
<li>To remove an item from a list, use <code>.remove()</code></li>
<li>To append to a list, use <code>.append()</code></li>
<li>To add to a set, use <code>.add()</code></li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C21 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># This is the code used to do the fast nearest neighbor search. Feel free to go over it</span>
<span class="k">def</span> <span class="nf">approximate_knn</span><span class="p">(</span><span class="n">document_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">document_embedding</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                    <span class="n">multiverse_planes</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">universes</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">TWEET</span><span class="o">.</span><span class="n">universes</span><span class="p">):</span>
    <span class="sd">"""Search for k-NN using hashes</span>

<span class="sd">    Args:</span>
<span class="sd">     document_id: index for the document in the lists</span>
<span class="sd">     document_embedding: vector representing a documents word embeddings</span>
<span class="sd">     multiverse_planes: dictionary of planes for the hash-tables</span>
<span class="sd">     k: number of neighbors to find</span>
<span class="sd">     universes: number of times to repeat the search</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of indexes for neighbor documents</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="n">universes</span> <span class="o">&lt;=</span> <span class="n">TWEET</span><span class="o">.</span><span class="n">universes</span>

    <span class="c1"># Vectors that will be checked as possible nearest neighbor</span>
    <span class="n">possible_neighbors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="c1"># list of document IDs</span>
    <span class="n">ids_of_possible_neighbors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="c1"># create a set for ids to consider, for faster checking if a document ID already exists in the set</span>
    <span class="n">set_of_ids_of_possible_neighbors</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">hasher</span> <span class="o">=</span> <span class="n">HashTable</span><span class="p">(</span><span class="n">planes</span><span class="o">=</span><span class="n">multiverse_planes</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="c1"># loop through the universes of planes</span>
    <span class="k">for</span> <span class="n">universe</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">universes</span><span class="p">):</span>

        <span class="c1"># get the set of planes from the planes_l list, for this particular universe_id</span>
        <span class="n">planes</span> <span class="o">=</span> <span class="n">multiverse_planes</span><span class="p">[</span><span class="n">universe</span><span class="p">]</span>

        <span class="c1"># get the hash value of the vector for this set of planes</span>
        <span class="c1"># hash_value = hash_value_of_vector(v, planes)</span>
        <span class="n">hash_value</span> <span class="o">=</span> <span class="n">HashTable</span><span class="p">(</span><span class="n">planes</span><span class="o">=</span><span class="n">planes</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">hash_value</span><span class="p">(</span><span class="n">document_embedding</span><span class="p">)</span>

        <span class="c1"># get the hash table for this particular universe_id</span>
        <span class="n">hash_table</span> <span class="o">=</span> <span class="n">hash_tables</span><span class="p">[</span><span class="n">universe</span><span class="p">]</span>

        <span class="c1"># get the list of document vectors for this hash table, where the key is the hash_value</span>
        <span class="n">document_vectors</span> <span class="o">=</span> <span class="n">hash_table</span><span class="p">[</span><span class="n">hash_value</span><span class="p">]</span>

        <span class="c1"># get the id_table for this particular universe_id</span>
        <span class="n">id_table</span> <span class="o">=</span> <span class="n">id_tables</span><span class="p">[</span><span class="n">universe</span><span class="p">]</span>

        <span class="c1"># get the subset of documents to consider as nearest neighbors from this id_table dictionary</span>
        <span class="n">new_ids_to_consider</span> <span class="o">=</span> <span class="n">id_table</span><span class="p">[</span><span class="n">hash_value</span><span class="p">]</span>

        <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>

        <span class="c1"># remove the id of the document that we're searching</span>
        <span class="k">if</span> <span class="n">document_id</span> <span class="ow">in</span> <span class="n">new_ids_to_consider</span><span class="p">:</span>
            <span class="n">new_ids_to_consider</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">document_id</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"removed document_id </span><span class="si">{</span><span class="n">document_id</span><span class="si">}</span><span class="s2"> of input vector from new_ids_to_search"</span><span class="p">)</span>

        <span class="c1"># loop through the subset of document vectors to consider</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">new_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">new_ids_to_consider</span><span class="p">):</span>

            <span class="c1"># if the document ID is not yet in the set ids_to_consider...</span>
            <span class="k">if</span> <span class="n">new_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">set_of_ids_of_possible_neighbors</span><span class="p">:</span>
                <span class="c1"># access document_vectors_l list at index i to get the embedding</span>
                <span class="c1"># then append it to the list of vectors to consider as possible nearest neighbors</span>
                <span class="n">document_vector</span> <span class="o">=</span> <span class="n">document_vectors</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
                <span class="n">possible_neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">document_vector</span><span class="p">)</span>

                <span class="c1"># append the new_id (the index for the document) to the list of ids to consider</span>
                <span class="n">ids_of_possible_neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_id</span><span class="p">)</span>

                <span class="c1"># also add the new_id to the set of ids to consider</span>
                <span class="c1"># (use this to check if new_id is not already in the IDs to consider)</span>
                <span class="n">set_of_ids_of_possible_neighbors</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">new_id</span><span class="p">)</span>

        <span class="c1">### END CODE HERE ###</span>

    <span class="c1"># Now run k-NN on the smaller set of vecs-to-consider.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Fast considering </span><span class="si">%d</span><span class="s2"> vecs"</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">possible_neighbors</span><span class="p">))</span>

    <span class="c1"># convert the vecs to consider set to a list, then to a numpy array</span>
    <span class="n">vecs_to_consider_arr</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">possible_neighbors</span><span class="p">)</span>

    <span class="c1"># call nearest neighbors on the reduced list of candidate vectors</span>
    <span class="n">nearest_neighbors</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">candidates</span><span class="o">=</span><span class="n">possible_neighbors</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">nearest_neighbor_ids</span> <span class="o">=</span> <span class="n">nearest_neighbors</span><span class="p">(</span><span class="n">document_embedding</span><span class="p">)</span>

    <span class="c1"># Use the nearest neighbor index list as indices into the ids to consider</span>
    <span class="c1"># create a list of nearest neighbors by the document ids</span>
    <span class="n">nearest_neighbor_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">ids_of_possible_neighbors</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
                            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">nearest_neighbor_ids</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">nearest_neighbor_ids</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">doc_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">doc_to_search</span> <span class="o">=</span> <span class="n">tweets</span><span class="p">[</span><span class="n">doc_id</span><span class="p">]</span>
<span class="n">vec_to_search</span> <span class="o">=</span> <span class="n">documents</span><span class="o">.</span><span class="n">documents_embeddings</span><span class="p">[</span><span class="n">doc_id</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">doc_to_search</span><span class="p">)</span>
</pre></div>
<pre class="example">
#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)
</pre>
<div class="highlight">
<pre><span></span><span class="n">nearest_neighbor_ids</span> <span class="o">=</span> <span class="n">approximate_knn</span><span class="p">(</span>
    <span class="n">document_id</span><span class="o">=</span><span class="n">doc_id</span><span class="p">,</span>
    <span class="n">document_embedding</span><span class="o">=</span><span class="n">vec_to_search</span><span class="p">,</span>
    <span class="n">multiverse_planes</span><span class="o">=</span><span class="n">universes</span><span class="o">.</span><span class="n">planes</span><span class="p">,</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">universes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Nearest neighbors for document </span><span class="si">{</span><span class="n">doc_id</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Document contents: </span><span class="si">{</span><span class="n">doc_to_search</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">""</span><span class="p">)</span>

<span class="k">for</span> <span class="n">neighbor_id</span> <span class="ow">in</span> <span class="n">nearest_neighbor_ids</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Nearest neighbor at document id </span><span class="si">{</span><span class="n">neighbor_id</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"document contents: </span><span class="si">{</span><span class="n">tweets</span><span class="p">[</span><span class="n">neighbor_id</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Fast considering 79 vecs
Nearest neighbors for document 0
Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)

Nearest neighbor at document id 254
document contents: Something to get your #Friday off to a great start :) Have a great day all! #Mclaren #FridayFeeling #TGIF http://t.co/LshgwcXsSv
Nearest neighbor at document id 2714
document contents: Current playlist :D http://t.co/PYKQLD4KHr
Nearest neighbor at document id 51
document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)
</pre>
<p>The first and third neighbors seem reasonable, although the third looks like it's just a re-working of our source tweet.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1874a21">
<h2 id="org1874a21">End</h2>
<div class="outline-text-2" id="text-org1874a21">
<ul class="org-ul">
<li>The post that collects all the posts in this project is <a href="posts/nlp/machine-translation/">Machine Translation</a>.</li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/hash-tables/">Hash Tables</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/hash-tables/" rel="bookmark"><time class="published dt-published" datetime="2020-10-07T19:37:18-07:00" itemprop="datePublished" title="2020-10-07 19:37">2020-10-07 19:37</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/hash-tables/#org445a980">Beginning</a>
<ul>
<li><a href="posts/nlp/hash-tables/#org48ec381">Imports</a></li>
<li><a href="posts/nlp/hash-tables/#orgc730435">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/hash-tables/#org49b2244">Middle</a>
<ul>
<li><a href="posts/nlp/hash-tables/#org7dfdb8d">A Basic Hash Table</a></li>
<li><a href="posts/nlp/hash-tables/#orgfb440fa">Multiplane Hash Functions</a></li>
<li><a href="posts/nlp/hash-tables/#org6f8bf66">Multiple PLanes</a></li>
<li><a href="posts/nlp/hash-tables/#org74c21d4">Random Planes</a></li>
<li><a href="posts/nlp/hash-tables/#org20420a0">Document Vectors</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org445a980">
<h2 id="org445a980">Beginning</h2>
<div class="outline-text-2" id="text-org445a980"></div>
<div class="outline-3" id="outline-container-org48ec381">
<h3 id="org48ec381">Imports</h3>
<div class="outline-text-3" id="text-org48ec381">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">pprint</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">default_rng</span>

<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc730435">
<h3 id="orgc730435">Set Up</h3>
<div class="outline-text-3" id="text-orgc730435"></div>
<div class="outline-4" id="outline-container-org7e19ffb">
<h4 id="org7e19ffb">Plotting</h4>
<div class="outline-text-4" id="text-org7e19ffb">
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"hash-tables"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1f60126">
<h4 id="org1f60126">Random Number Generator</h4>
<div class="outline-text-4" id="text-org1f60126">
<div class="highlight">
<pre><span></span><span class="n">numpy_random</span> <span class="o">=</span> <span class="n">default_rng</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgfdb1b08">
<h4 id="orgfdb1b08">Pretty Printer</h4>
<div class="outline-text-4" id="text-orgfdb1b08">
<div class="highlight">
<pre><span></span><span class="n">pretty</span> <span class="o">=</span> <span class="n">pprint</span><span class="o">.</span><span class="n">PrettyPrinter</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org49b2244">
<h2 id="org49b2244">Middle</h2>
<div class="outline-text-2" id="text-org49b2244"></div>
<div class="outline-3" id="outline-container-org7dfdb8d">
<h3 id="org7dfdb8d">A Basic Hash Table</h3>
<div class="outline-text-3" id="text-org7dfdb8d">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">basic_hash_table</span><span class="p">(</span><span class="n">things_to_hash</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">buckets</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""Create a basic hash table</span>

<span class="sd">    Args :</span>
<span class="sd">     things_to_hash: list of integers to hash</span>
<span class="sd">     buckets: number of buckets in the table</span>

<span class="sd">    Returns:</span>
<span class="sd">     hash_table: the things to hash sorted into their buckets</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="nf">hash_function</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">buckets</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sd">"""Maps the value to an integer</span>

<span class="sd">       Args:</span>
<span class="sd">        value: what to hash</span>
<span class="sd">        n_buckets: number of buckets in the hash table</span>

<span class="sd">       Returns:</span>
<span class="sd">        remainder of value//n_buckets</span>
<span class="sd">       """</span>        
        <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">%</span> <span class="n">buckets</span>

     <span class="c1"># Initialize all the buckets in the hash table as empty lists</span>
    <span class="n">hash_table</span> <span class="o">=</span> <span class="p">{</span><span class="n">bucket</span><span class="p">:[]</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">buckets</span><span class="p">)}</span>

    <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">things_to_hash</span><span class="p">:</span>
         <span class="c1"># Get the hash key for the given value</span>
        <span class="n">hash_value</span> <span class="o">=</span> <span class="n">hash_function</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">buckets</span><span class="p">)</span>

         <span class="c1"># Add the element to the corresponding bucket</span>
        <span class="n">hash_table</span><span class="p">[</span><span class="n">hash_value</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hash_table</span>
</pre></div>
<p>The <code>basic_hash_table</code> maps values that can be cast to integers to a dictionary of lists. Let's see what it does.</p>
<div class="highlight">
<pre><span></span><span class="n">examples</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">97</span><span class="p">]</span>
<span class="n">hash_table_example</span> <span class="o">=</span> <span class="n">basic_hash_table</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">buckets</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">pretty</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">hash_table_example</span><span class="p">)</span>
</pre></div>
<pre class="example">
{0: [100, 10],
 1: [],
 2: [],
 3: [],
 4: [14],
 5: [],
 6: [],
 7: [17, 97],
 8: [],
 9: []}
</pre>
<p>This Basic Hash Table maps the values based on their remainder after dividing the value by the number of buckets. In this case there are ten buckets so the value gets mapped to the value in its ones column.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgfb440fa">
<h3 id="orgfb440fa">Multiplane Hash Functions</h3>
<div class="outline-text-3" id="text-orgfb440fa">
<p>To visualize it we'll start with a single plane and color some points based on which side of the plane they fall.</p>
<p>I'll start by defining the vector that we'll use to decide which side of the plane a vector is on (by taking the dot product and checking the sign of the result).</p>
<div class="highlight">
<pre><span></span><span class="n">decider</span>  <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</pre></div>
<p>This isn't the separating plane but rather a vector perpendicular to the separating plane. You don't need the separating plane to make the categorizations of the vectors, but for the sake of visualization it might be useful to see it. We can create it by creating a rotation matrix that rotates our originar vector 90 degrees.</p>
<div class="highlight">
<pre><span></span><span class="n">theta_1</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">90</span><span class="p">)</span>

<span class="n">rotation</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta_1</span><span class="p">),</span> <span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta_1</span><span class="p">)],</span>
                        <span class="p">[</span><span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta_1</span><span class="p">),</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta_1</span><span class="p">)]])</span>

<span class="n">plane</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rotation</span><span class="p">,</span> <span class="n">decider</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
<p>Now we can plot them along with some categorized points.</p>
<p>First plot the vector we use to decide what side of the plane the points are.</p>
<div class="highlight">
<pre><span></span><span class="c1"># so to plot it I'll add a starting point</span>
<span class="n">COLUMNS</span> <span class="o">=</span> <span class="s2">"X Y"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">decider_plotter</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">start</span><span class="p">,</span> <span class="n">plane</span><span class="p">])</span>
<span class="n">decider_plotter</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">COLUMNS</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">decider_plotter</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">)</span>
</pre></div>
<p>Now plot the plane that separates the categories. I'll scale it a little to move the plot back a little. Also the rotation gives us only the line segment rotated by 90 degrees so I'm going to negate it to get the -90 segment as well to complete the rendering of the plane.</p>
<div class="highlight">
<pre><span></span><span class="n">SCALE</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">plane_plotter</span> <span class="o">=</span> <span class="n">start</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">plane</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="n">SCALE</span>
<span class="n">plane_plotter</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">COLUMNS</span>
<span class="n">plot</span> <span class="o">*=</span> <span class="n">plane_plotter</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">tan</span><span class="p">,</span> <span class="n">line_dash</span><span class="o">=</span><span class="s2">"dashed"</span><span class="p">)</span>

<span class="n">plane_plotter</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">plot</span> <span class="o">*=</span> <span class="n">plane_plotter</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">tan</span><span class="p">,</span> <span class="n">line_dash</span><span class="o">=</span><span class="s2">"dashed"</span><span class="p">)</span>
</pre></div>
<p>Now we get to the points. The main lines to pay attention to are the calculation of the <code>side_of_plane</code> value and the conditional. The <code>side_of_plane</code> is an array but you can do boolean equality checks with integers as shown.</p>
<div class="highlight">
<pre><span></span><span class="c1">## Get a pair of random numbers between -4 and 4 </span>
<span class="n">POINTS</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">LIMIT</span> <span class="o">=</span> <span class="mi">4</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">POINTS</span><span class="p">):</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">numpy_random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">LIMIT</span><span class="p">,</span> <span class="n">LIMIT</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> 
                              <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"x"</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">])</span>
    <span class="n">side_of_plane</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">plane</span><span class="p">,</span> <span class="n">vector</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> 

    <span class="k">if</span> <span class="n">side_of_plane</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">plot</span> <span class="o">*=</span> <span class="n">vector</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plot</span> <span class="o">*=</span> <span class="n">vector</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Plane Hash Table"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
    <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">LIMIT</span><span class="p">,</span> <span class="n">LIMIT</span><span class="p">),</span>
    <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">LIMIT</span><span class="p">,</span> <span class="n">LIMIT</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"multiplane_hash"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/hash-tables/multiplane_hash.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>So the dashed tan line is our separation plane and the blue line segment is the vector we use to decide which side of the plane the dots are on. The blue dots have a positive dot product with the blue vector and the red dots have a negative dot product with the blue vector.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org6f8bf66">
<h3 id="org6f8bf66">Multiple PLanes</h3>
<div class="outline-text-3" id="text-org6f8bf66">
<div class="highlight">
<pre><span></span><span class="n">plane_1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">plane_2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">plane_3</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">multi_plane</span> <span class="o">=</span> <span class="p">[</span><span class="n">plane_1</span><span class="p">,</span> <span class="n">plane_2</span><span class="p">,</span> <span class="n">plane_3</span><span class="p">]</span>

<span class="n">search_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">side_of_plane</span><span class="p">(</span><span class="n">plane</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">vector</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Finds the side of the plane that the vector is</span>

<span class="sd">    Args:</span>
<span class="sd">     plane: separating plane</span>
<span class="sd">     vector: location to check </span>

<span class="sd">    Returns:</span>
<span class="sd">     sign of the dot product between the plane and the vector</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">plane</span><span class="p">,</span> <span class="n">vector</span><span class="o">.</span><span class="n">T</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">hash_multi_plane</span><span class="p">(</span><span class="n">planes</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">vector</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Creates hash value for set of planes</span>

<span class="sd">    Args:</span>
<span class="sd">     planes: list of arrays to hash</span>
<span class="sd">     vector: array to determine which side of the planes are positive</span>

<span class="sd">    Returns:</span>
<span class="sd">     hash_value: the hash for plane matching the vector</span>
<span class="sd">    """</span>
    <span class="n">hash_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">plane</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">planes</span><span class="p">):</span>
        <span class="n">sign</span> <span class="o">=</span> <span class="n">side_of_plane</span><span class="p">(</span><span class="n">plane</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span>

        <span class="c1"># increment the hash if the sign is non-negative</span>
        <span class="n">hash_i</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">sign</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">hash_value</span> <span class="o">+=</span> <span class="mi">2</span><span class="o">**</span><span class="n">index</span> <span class="o">*</span> <span class="n">hash_i</span>
    <span class="k">return</span> <span class="n">hash_value</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">hash_multi_plane</span><span class="p">(</span><span class="n">multi_plane</span><span class="p">,</span> <span class="n">search_vector</span><span class="p">))</span>
</pre></div>
<pre class="example">
3
</pre></div>
</div>
<div class="outline-3" id="outline-container-org74c21d4">
<h3 id="org74c21d4">Random Planes</h3>
<div class="outline-text-3" id="text-org74c21d4">
<div class="highlight">
<pre><span></span><span class="n">numpy_random</span> <span class="o">=</span> <span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_dimensions</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_planes</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">random_planes_matrix</span> <span class="o">=</span> <span class="n">numpy_random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
                       <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_planes</span><span class="p">,</span>
                             <span class="n">num_dimensions</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random_planes_matrix</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.12573022 -0.13210486]
 [ 0.64042265  0.10490012]
 [-0.53566937  0.36159505]]
</pre>
<div class="highlight">
<pre><span></span><span class="n">search_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">side_of_plane_matrix</span><span class="p">(</span><span class="n">planes</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">vector</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Decides which side of planes vector is on</span>

<span class="sd">    Returns:</span>
<span class="sd">     side-of-plane value for vector with respect to each plane</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">planes</span><span class="p">,</span> <span class="n">vector</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">side_of_plane_matrix</span><span class="p">(</span><span class="n">random_planes_matrix</span><span class="p">,</span> <span class="n">search_vector</span><span class="p">))</span>
</pre></div>
<pre class="example">
[[-1.]
 [ 1.]
 [-1.]]
</pre>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">hash_multi_plane_matrix</span><span class="p">(</span><span class="n">planes</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                            <span class="n">vector</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                            <span class="n">num_planes</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">"""calculates hash for vector with respect to planes"""</span>
    <span class="n">sides_matrix</span> <span class="o">=</span> <span class="n">side_of_plane_matrix</span><span class="p">(</span><span class="n">planes</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span>
    <span class="n">hash_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_planes</span><span class="p">):</span>
        <span class="n">sign</span> <span class="o">=</span> <span class="n">sides_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># Get the value inside the matrix cell</span>
        <span class="n">hash_i</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">sign</span> <span class="o">&gt;=</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">hash_value</span> <span class="o">+=</span> <span class="mi">2</span><span class="o">**</span><span class="n">i</span> <span class="o">*</span> <span class="n">hash_i</span> <span class="c1"># sum 2^i * hash_i</span>
    <span class="k">return</span> <span class="n">hash_value</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sm</span> <span class="o">=</span> <span class="n">side_of_plane_matrix</span><span class="p">(</span><span class="n">random_planes_matrix</span><span class="p">,</span> <span class="n">search_vector</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">hash_multi_plane_matrix</span><span class="p">(</span><span class="n">random_planes_matrix</span><span class="p">,</span> <span class="n">search_vector</span><span class="p">,</span> <span class="n">num_planes</span><span class="p">))</span>
</pre></div>
<pre class="example">
2
</pre></div>
</div>
<div class="outline-3" id="outline-container-org20420a0">
<h3 id="org20420a0">Document Vectors</h3>
<div class="outline-text-3" id="text-org20420a0">
<p>This is how you would convert a document to an embedding using word vectors (just add up all the vectors for the words in the document).</p>
<div class="highlight">
<pre><span></span><span class="n">word_embedding</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"I"</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span>
                  <span class="s2">"love"</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span>
                  <span class="s2">"learning"</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
                  <span class="p">}</span>
<span class="n">document</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'I'</span><span class="p">,</span> <span class="s1">'love'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">,</span> <span class="s1">'not_a_word'</span><span class="p">]</span>
<span class="n">document_embedding</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">document</span><span class="p">:</span>
    <span class="n">document_embedding</span> <span class="o">+=</span> <span class="n">word_embedding</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">document_embedding</span><span class="p">)</span>
</pre></div>
<pre class="example">
[1 0 3]
</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/more-matrix-math-in-python/">More Matrix Math in Python</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/more-matrix-math-in-python/" rel="bookmark"><time class="published dt-published" datetime="2020-10-06T21:10:51-07:00" itemprop="datePublished" title="2020-10-06 21:10">2020-10-06 21:10</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/more-matrix-math-in-python/#orge127904">Beginning</a>
<ul>
<li><a href="posts/nlp/more-matrix-math-in-python/#org203704d">Imports</a></li>
<li><a href="posts/nlp/more-matrix-math-in-python/#orge54ab78">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/more-matrix-math-in-python/#org3cb4423">Middle</a>
<ul>
<li><a href="posts/nlp/more-matrix-math-in-python/#orgb008cc6">More Rotations</a></li>
<li><a href="posts/nlp/more-matrix-math-in-python/#org9327257">The Frobenius Norm</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orge127904">
<h2 id="orge127904">Beginning</h2>
<div class="outline-text-2" id="text-orge127904">
<p>This is another lab from Coursera's NLP Specialization. This time it's about using numpy to perform vector operations.</p>
</div>
<div class="outline-3" id="outline-container-org203704d">
<h3 id="org203704d">Imports</h3>
<div class="outline-text-3" id="text-org203704d">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># from pypi</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge54ab78">
<h3 id="orge54ab78">Set Up</h3>
<div class="outline-text-3" id="text-orge54ab78"></div>
<div class="outline-4" id="outline-container-orgc139e27">
<h4 id="orgc139e27">Plotting</h4>
<div class="outline-text-4" id="text-orgc139e27">
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"more-matrix-math-in-python"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org3cb4423">
<h2 id="org3cb4423">Middle</h2>
<div class="outline-text-2" id="text-org3cb4423">
<p>Let's start with a simple matrix. We'll call it <code>R</code> because when we do our machine translation we'll need a <i>rotation matrix</i> which is named <code>R</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">R</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                 <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]])</span>
</pre></div>
<p>Now we'll create another matrix.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(1, 2)
</pre>
<p>Note the nested square brackets, this makes it a matrix and not a vector.</p>
</div>
<div class="outline-4" id="outline-container-org591fb04">
<h4 id="org591fb04">The Dot Product</h4>
<div class="outline-text-4" id="text-org591fb04">
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 2 -2]]
</pre>
<p>The rotation matrix (<code>R</code>) rotates and scales the matrix <code>x</code>. To see the effect we can plot the original vector <code>x</code> and the rotated version <code>y</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">Y</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]]))</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]],</span> <span class="n">Y</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]]))</span>

<span class="n">x_plot</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">y_plot</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_plot</span> <span class="o">*</span> <span class="n">y_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Original and Rotated Vectors"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
    <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"original_and_rotate_vectors"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/more-matrix-math-in-python/original_and_rotate_vectors.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>The blue segment is the original vector and the red is the rotated and scaled vector.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgb008cc6">
<h3 id="orgb008cc6">More Rotations</h3>
<div class="outline-text-3" id="text-orgb008cc6">
<p>In the previous section we rotated the vector using integer values, but if we wanted to rotate the vector a specific number of degrees then the way to do that is to use a rotation matrix.</p>
<p>\[ Ro = \begin{bmatrix} cos \theta & -sin \theta \\ sin \theta & cos \theta \end{bmatrix} \]</p>
<p>Let's start with a vector and rotate it \(100^o\).</p>
<div class="highlight">
<pre><span></span><span class="n">theta</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">Ro</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="n">numpy</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)],</span>
                  <span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">theta</span><span class="p">)]])</span>

<span class="n">x_2</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">x_2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ro</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"The Rotation Matrix"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Ro</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">The Rotated Vector"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1"> x2 norm </span><span class="si">{</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_2</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1"> y2 norm </span><span class="si">{</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y_2</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n</span><span class="s1"> Rotation matrix norm </span><span class="si">{</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Ro</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" Square Root of 2: </span><span class="si">{</span><span class="mi">2</span><span class="o">**</span><span class="mf">0.5</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
The Rotation Matrix
          0         1
0 -0.173648 -0.984808
1  0.984808 -0.173648

The Rotated Vector
0    1.622319
1   -2.316912
dtype: float64

 x2 norm 2.8284271247461903

 y2 norm 2.82842712474619

 Rotation matrix norm 1.414213562373095
 Square Root of 2: 1.4142135623730951
</pre>
<p>You can see that in this case our transformed vector (<code>y2</code>) didn't change in length the way it did in the previous example. Let's plot it and see what it looks like.</p>
<div class="highlight">
<pre><span></span><span class="n">origin</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">origin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_2</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">origin</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_2</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">COLUMNS</span> <span class="o">=</span> <span class="s2">"X Y"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="n">X</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">COLUMNS</span>
<span class="n">Y</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">COLUMNS</span>

<span class="n">x_plot</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">y_plot</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_plot</span> <span class="o">*</span> <span class="n">y_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"100 Degree rotation"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
    <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"one_hundred_degree_rotation"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/more-matrix-math-in-python/one_hundred_degree_rotation.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Rotation matrices rotate anti-clockwise, which makes that look like more than a 100 degree rotation. I'm going to have to figure that out.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org9327257">
<h3 id="org9327257">The Frobenius Norm</h3>
<div class="outline-text-3" id="text-org9327257">
<p>\[ \| \vec a \| = \sqrt {{\vec a} \cdot {\vec a}} \]</p>
<p>For an \(R_2\) matrix, the Frobenius Norm looks like this:</p>
<p>\[ \| \mathrm{A} \|_{F} \equiv \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}} \]</p>
<p>We can translate the second equation directly to numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">some_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                          <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">frobenius_norm</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">some_array</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The Frobenius Norm = </span><span class="si">{</span><span class="n">frobenius_norm</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
The Frobenius Norm = 4.0
</pre>
<p>So, you might be thinking, we've been using <code>numpy.linalg.norm</code> all this time, what's the difference?</p>
<div class="highlight">
<pre><span></span><span class="n">old_norm</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">some_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">old_norm</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">old_norm</span> <span class="o">==</span> <span class="n">frobenius_norm</span>
</pre></div>
<pre class="example">
4.0
</pre>
<p>It turns out that the default for <code>norm</code> is the Frobenius Norm so you can calculate it either way.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/pca-dimensionality-reduction-and-word-vectors/">PCA Dimensionality Reduction and Word Vectors</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/pca-dimensionality-reduction-and-word-vectors/" rel="bookmark"><time class="published dt-published" datetime="2020-10-03T19:48:52-07:00" itemprop="datePublished" title="2020-10-03 19:48">2020-10-03 19:48</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/pca-dimensionality-reduction-and-word-vectors/#org0271170">Beginning</a>
<ul>
<li><a href="posts/nlp/pca-dimensionality-reduction-and-word-vectors/#org0e62574">Imports</a></li>
<li><a href="posts/nlp/pca-dimensionality-reduction-and-word-vectors/#org85cad36">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/pca-dimensionality-reduction-and-word-vectors/#org0ce11f9">Middle</a>
<ul>
<li><a href="posts/nlp/pca-dimensionality-reduction-and-word-vectors/#orgb16ca70">Predicting Relationships Among Words</a></li>
<li><a href="posts/nlp/pca-dimensionality-reduction-and-word-vectors/#org2a66e18">Plotting With PCA</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org0271170">
<h2 id="org0271170">Beginning</h2>
<div class="outline-text-2" id="text-org0271170">
<p>This is an extension of the previous two posts about <a href="posts/nlp/word-embeddings/">Word Embeddings</a> and <a href="posts/nlp/pca-exploration/">Principal Component Analysis</a>. Once again we're going to start with pre-trained word embeddings rather than train our own and then take the embeddings and explore them to better understand them.</p>
</div>
<div class="outline-3" id="outline-container-org0e62574">
<h3 id="org0e62574">Imports</h3>
<div class="outline-text-3" id="text-org0e62574">
<div class="highlight">
<pre><span></span><span class="c1"># from python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">be_true</span><span class="p">,</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">default_rng</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org85cad36">
<h3 id="org85cad36">Set Up</h3>
<div class="outline-text-3" id="text-org85cad36"></div>
<div class="outline-4" id="outline-container-org91b022c">
<h4 id="org91b022c">The Timer</h4>
<div class="outline-text-4" id="text-org91b022c">
<p>Just something to tell how long some processes take.</p>
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org634fc1d">
<h4 id="org634fc1d">Plotting</h4>
<div class="outline-text-4" id="text-org634fc1d">
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"pca-dimensionality-reduction-and-word-vectors"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span>
                <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
    <span class="n">color_cycle</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">Cycle</span><span class="p">([</span><span class="s2">"#4687b7"</span><span class="p">,</span> <span class="s2">"#ce7b6d"</span><span class="p">])</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org28e0a3a">
<h4 id="org28e0a3a">Randomness</h4>
<div class="outline-text-4" id="text-org28e0a3a">
<div class="highlight">
<pre><span></span><span class="n">numpy_random</span> <span class="o">=</span> <span class="n">default_rng</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge616293">
<h4 id="orge616293">The Environment</h4>
<div class="outline-text-4" id="text-orge616293">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org39e2b52">
<h4 id="org39e2b52">The Embeddings</h4>
<div class="outline-text-4" id="text-org39e2b52">
<p>These are the same embeddings as in the <a href="posts/nlp/word-embeddings/">Word Embeddings</a> exploration. They're loaded a dictionary of arrays (vectors). The original source is the Google News pre-trained data set available from the <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a> archive, but it is 3.64 gigabytes so Coursera extracted a subset of it to work with.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"WORD_EMBEDDINGS"</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">path</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>

<span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">==</span> <span class="mi">243</span>
</pre></div>
<p>The instructors also provide some code to show you how to create a different subset and I'm assuming that what they're showing is the actual way that they built this dataset. For future reference, this is the code given.</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">'./GoogleNews-vectors-negative300.bin'</span><span class="p">,</span> <span class="n">binary</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'capitals.txt'</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">set_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
<span class="n">select_words</span> <span class="o">=</span> <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'king'</span><span class="p">,</span> <span class="s1">'queen'</span><span class="p">,</span> <span class="s1">'oil'</span><span class="p">,</span> <span class="s1">'gas'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'sad'</span><span class="p">,</span> <span class="s1">'city'</span><span class="p">,</span> <span class="s1">'town'</span><span class="p">,</span> <span class="s1">'village'</span><span class="p">,</span> <span class="s1">'country'</span><span class="p">,</span> <span class="s1">'continent'</span><span class="p">,</span> <span class="s1">'petroleum'</span><span class="p">,</span> <span class="s1">'joyful'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">select_words</span><span class="p">:</span>
    <span class="n">set_words</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_word_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">):</span>

    <span class="n">word_embeddings</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">set_words</span><span class="p">:</span>
            <span class="n">word_embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">word_embeddings</span>

<span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">get_word_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb247946">
<h4 id="orgb247946">The Data</h4>
<div class="outline-text-4" id="text-orgb247946">
<p>The data set is a space-separated-values file with no header.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CAPITALS"</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">path</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">" "</span><span class="p">,</span>
                       <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s2">"City 1"</span><span class="p">,</span> <span class="s2">"Country 1"</span><span class="p">,</span> <span class="s2">"City 2"</span><span class="p">,</span> <span class="s2">"Country 2"</span><span class="p">])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
<pre class="example">
   City 1 Country 1   City 2    Country 2
0  Athens    Greece  Baghdad         Iraq
1  Athens    Greece  Bangkok     Thailand
2  Athens    Greece  Beijing        China
3  Athens    Greece   Berlin      Germany
4  Athens    Greece     Bern  Switzerland
</pre>
<p>It looks odd because this is actually an evaluation set. The first three columns are used to predict the fourth (e.g. <i>Athens, Greece,</i> and <i>Baghdad</i> are used to predict that <i>Baghdad</i> is the capital of <i>Iraq</i>).</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0ce11f9">
<h2 id="org0ce11f9">Middle</h2>
<div class="outline-text-2" id="text-org0ce11f9"></div>
<div class="outline-3" id="outline-container-orgb16ca70">
<h3 id="orgb16ca70">Predicting Relationships Among Words</h3>
<div class="outline-text-3" id="text-orgb16ca70">
<p>This part is about writing a function that will use the word embeddings to predict relationships among words.</p>
</div>
<div class="outline-4" id="outline-container-org8ea791f">
<h4 id="org8ea791f">Requirements</h4>
<div class="outline-text-4" id="text-org8ea791f">
<ul class="org-ul">
<li>The arguments will be three words</li>
<li>The first two will be considered related to each other somehow</li>
<li>The function will then predict a fourth word that is related to the third word in a way that is similar to the relationship between the first two words.</li>
</ul>
<p>Another way to look at is it that if you are given three words - <i>Athens, Greece,</i> and <i>Bangkok</i> then the function will fill in the blank for "Athens is to Greece as Bangkok is to __".</p>
<p>Because of our input data set what the function will end up doing is finding the capital of a country. But first we need a distance function.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org514deb4">
<h4 id="org514deb4">Cosine Similarity</h4>
<div class="outline-text-4" id="text-org514deb4">\begin{align} \cos (\theta) &amp;=\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}\\ &amp;= \frac{\sum_{i=1}^{n} A_{i} B_{i}}{\sqrt{\sum_{i=1}^{n} A_{i}^{2}} \sqrt{\sum_{i=1}^{n} B_{i}^{2}}}\\ \end{align}
<ul class="org-ul">
<li><i>A</i> and <i>B</i> are the word vectors and \(A_i\) or \(B_i\) is the <i>ith</i> item of that vector</li>
<li>If the output is 0 then they are opposites and if the output is 1 then they are the same</li>
<li>If the number is between 0 and 1 then it is a similarity score</li>
<li>If the number is between 0 and -1 then it is a dissimilarity score</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">'''Calculates the cosine similarity between two arrays</span>

<span class="sd">    Args:</span>
<span class="sd">       A: a numpy array which corresponds to a word vector</span>
<span class="sd">       B: A numpy array which corresponds to a word vector</span>
<span class="sd">    Return:</span>
<span class="sd">       cos: numerical number representing the cosine similarity between A and B.</span>
<span class="sd">    '''</span>
    <span class="n">dot_product</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="n">norm_of_A</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">norm_of_B</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">dot_product</span><span class="o">/</span><span class="p">(</span><span class="n">norm_of_A</span> <span class="o">*</span> <span class="n">norm_of_B</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cos</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">king</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"king"</span><span class="p">]</span>
<span class="n">queen</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"queen"</span><span class="p">]</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">king</span><span class="p">,</span> <span class="n">queen</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The Cosine Similarity between 'king' and 'queen': </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">0.6510956</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">similarity</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
The Cosine Similarity between 'king' and 'queen': 0.65.
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgdb78612">
<h4 id="orgdb78612">Euclidean Distance</h4>
<div class="outline-text-4" id="text-orgdb78612">
<p>In addition to the Cosine Similarity we can use the (probably better known) Euclidean Distance.</p>
\begin{aligned} d(\mathbf{A}, \mathbf{B})=d(\mathbf{B}, \mathbf{A}) &amp;=\sqrt{\left(A_{1}-B_{1}\right)^{2}+\left(A_{2}-B_{2}\right)^{2}+\cdots+\left(A_{n}-B_{n}\right)^{2}} \\ &amp;=\sqrt{\sum_{i=1}^{n}\left(A_{i}-B_{i}\right)^{2}} \end{aligned}
<ul class="org-ul">
<li><i>n</i> is the number of elements in the vector</li>
<li><i>A</i> and <i>B</i> are the corresponding word vectors.</li>
<li>The more similar the words, the more likely the Euclidean distance will be close to 0 (and zero means they are the same).</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">euclidean</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Calculate the euclidean distance between two vectors</span>

<span class="sd">    Args:</span>
<span class="sd">       A: a numpy array which corresponds to a word vector</span>
<span class="sd">       B: A numpy array which corresponds to a word vector</span>
<span class="sd">    Return:</span>
<span class="sd">       d: numerical number representing the Euclidean distance between A and B.</span>
<span class="sd">    """</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">A</span> <span class="o">-</span> <span class="n">B</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">d</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">euclidean</span><span class="p">(</span><span class="n">king</span><span class="p">,</span> <span class="n">queen</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">2.4796925</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The Euclidean Distance between 'king' and 'queen' is </span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
The Euclidean Distance between 'king' and 'queen' is 2.48.
</pre></div>
</div>
<div class="outline-4" id="outline-container-org24e90cd">
<h4 id="org24e90cd">The Predictor</h4>
<div class="outline-text-4" id="text-org24e90cd">
<p>Here's whdere we make the function that tries to predict the Country for a given Capital City. This will use the cosine similarity. This first version will use brute-force.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_country</span><span class="p">(</span><span class="n">city1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">country1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">city2</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Find the country that has a particular capital city</span>

<span class="sd">    Args:</span>
<span class="sd">       city1: a string (the capital city of country1)</span>
<span class="sd">       country1: a string (the country of capital1)</span>
<span class="sd">       city2: a string (the capital city of country2)</span>
<span class="sd">       embeddings: a dictionary where the keys are words and values are their embeddings</span>
<span class="sd">    Return:</span>
<span class="sd">       countries: most likely country, similarity score</span>
<span class="sd">    """</span>
    <span class="n">group</span> <span class="o">=</span> <span class="nb">set</span><span class="p">((</span><span class="n">city1</span><span class="p">,</span> <span class="n">country1</span><span class="p">,</span> <span class="n">city2</span><span class="p">))</span>

    <span class="n">city1_emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">city1</span><span class="p">]</span>

    <span class="n">country1_emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">country1</span><span class="p">]</span>

    <span class="n">city2_emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">city2</span><span class="p">]</span>

    <span class="n">vec</span> <span class="o">=</span> <span class="n">country1_emb</span> <span class="o">-</span> <span class="n">city1_emb</span>  <span class="o">+</span> <span class="n">city2_emb</span>

    <span class="c1"># Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="c1"># initialize country to an empty string</span>
    <span class="n">country</span> <span class="o">=</span> <span class="s1">''</span>

    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">group</span><span class="p">:</span>
            <span class="n">word_emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
            <span class="c1"># calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary</span>
            <span class="n">cur_similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">word_emb</span><span class="p">)</span>

            <span class="c1"># if the cosine similarity is more similar than the previously best similarity...</span>
            <span class="k">if</span> <span class="n">cur_similarity</span> <span class="o">&gt;</span> <span class="n">similarity</span><span class="p">:</span>

                <span class="c1"># update the similarity to the new, better similarity</span>
                <span class="n">similarity</span> <span class="o">=</span> <span class="n">cur_similarity</span>

                <span class="c1"># store the country as a tuple, which contains the word and the similarity</span>
                <span class="n">country</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">similarity</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">country</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">actual_country</span><span class="p">,</span> <span class="n">actual_similarity</span> <span class="o">=</span> <span class="n">get_country</span><span class="p">(</span><span class="s2">"Athens"</span><span class="p">,</span> <span class="s2">"Greece"</span><span class="p">,</span> <span class="s2">"Cairo"</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Cairo is the capital of </span><span class="si">{</span><span class="n">actual_country</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>

<span class="n">expected_country</span><span class="p">,</span> <span class="n">expected_similarity</span> <span class="o">=</span> <span class="s2">"Egypt"</span><span class="p">,</span> <span class="mf">0.7626821</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual_country</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected_country</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">actual_similarity</span><span class="p">,</span> <span class="n">expected_similarity</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
Cairo is the capital of Egypt.
</pre></div>
</div>
<div class="outline-4" id="outline-container-org6186c9d">
<h4 id="org6186c9d">Checking the Model Accuracy</h4>
<div class="outline-text-4" id="text-org6186c9d">
<p>\[ \text{Accuracy}=\frac{\text{Correct # of predictions}}{\text{Total # of predictions}} \]</p>
<div class="highlight">
<pre><span></span><span class="n">country_getter</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">get_country</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_accuracy</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">'''Calculate the fraction of correct capitals</span>

<span class="sd">    Args:</span>
<span class="sd">       embeddings: a dictionary where the key is a word and the value is its embedding</span>

<span class="sd">    Return:</span>
<span class="sd">       accuracy: the accuracy of the model</span>
<span class="sd">    '''</span>
    <span class="n">num_correct</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># loop through the rows of the dataframe</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>

        <span class="c1"># get city1</span>
        <span class="n">city1</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">"City 1"</span><span class="p">]</span>

        <span class="c1"># get country1</span>
        <span class="n">country1</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">"Country 1"</span><span class="p">]</span>

        <span class="c1"># get city2</span>
        <span class="n">city2</span> <span class="o">=</span>  <span class="n">row</span><span class="p">[</span><span class="s2">"City 2"</span><span class="p">]</span>

        <span class="c1"># get country2</span>
        <span class="n">country2</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">"Country 2"</span><span class="p">]</span>

        <span class="c1"># use get_country to find the predicted country2</span>
        <span class="n">predicted_country2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">country_getter</span><span class="p">(</span><span class="n">city1</span><span class="o">=</span><span class="n">city1</span><span class="p">,</span> <span class="n">country1</span><span class="o">=</span><span class="n">country1</span><span class="p">,</span> <span class="n">city2</span><span class="o">=</span><span class="n">city2</span><span class="p">)</span>

        <span class="c1"># if the predicted country2 is the same as the actual country2...</span>
        <span class="k">if</span> <span class="n">predicted_country2</span> <span class="o">==</span> <span class="n">country2</span><span class="p">:</span>
            <span class="c1"># increment the number of correct by 1</span>
            <span class="n">num_correct</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># get the number of rows in the data dataframe (length of dataframe)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># calculate the accuracy by dividing the number correct by m</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">num_correct</span><span class="o">/</span><span class="n">m</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">get_accuracy</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
2020-10-07 17:50:28,897 graeae.timers.timer start: Started: 2020-10-07 17:50:28.897165
2020-10-07 17:50:50,755 graeae.timers.timer end: Ended: 2020-10-07 17:50:50.755424
2020-10-07 17:50:50,756 graeae.timers.timer end: Elapsed: 0:00:21.858259
Accuracy: 0.92
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2a66e18">
<h3 id="org2a66e18">Plotting With PCA</h3>
<div class="outline-text-3" id="text-org2a66e18"></div>
<div class="outline-4" id="outline-container-org115c862">
<h4 id="org115c862">Computing the PCA</h4>
<div class="outline-text-4" id="text-org115c862">
<p>Now we'll write a function to do the Principal Component Analysis for our embeddings.</p>
<ul class="org-ul">
<li>The word vectors are of dimension 300.</li>
<li>Use PCA to change the 300 dimensions to <code>n_components</code> dimensions.</li>
<li>The new matrix should be of dimension <code>m, n_components</code> (<code>m</code> being the number of rows).</li>
<li></li>
<li>First de-mean the data</li>
<li>Get the eigenvalues using `linalg.eigh`. Use `eigh` rather than `eig` since R is symmetric. The performance gain when using `eigh` instead of `eig` is substantial.</li>
<li>Sort the eigenvectors and eigenvalues by decreasing order of the eigenvalues.</li>
<li>Get a subset of the eigenvectors (choose how many principle components you want to use using `n_components`).</li>
<li>Return the new transformation of the data by multiplying the eigenvectors with the original data.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_pca</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n_components</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate the principal components for X</span>

<span class="sd">    Args:</span>
<span class="sd">       X: of dimension (m,n) where each row corresponds to a word vector</span>
<span class="sd">       n_components: Number of components you want to keep.</span>

<span class="sd">    Return:</span>
<span class="sd">       X_reduced: data transformed in 2 dims/columns + regenerated original data</span>
<span class="sd">    """</span>
    <span class="c1"># you need to set axis to 0 or it will calculate the mean of the entire matrix instead of one per row</span>
    <span class="n">X_demeaned</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate the covariance matrix</span>
    <span class="c1"># the default numpy.cov assumes the rows are variables, not columns so set rowvar to False</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_demeaned</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># calculate eigenvectors & eigenvalues of the covariance matrix</span>
    <span class="n">eigen_vals</span><span class="p">,</span> <span class="n">eigen_vecs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">covariance_matrix</span><span class="p">)</span>

    <span class="c1"># sort eigenvalue in increasing order (get the indices from the sort)</span>
    <span class="n">idx_sorted</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigen_vals</span><span class="p">)</span>

    <span class="c1"># reverse the order so that it's from highest to lowest.</span>
    <span class="n">idx_sorted_decreasing</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">idx_sorted</span><span class="p">))</span>

    <span class="c1"># sort the eigen values by idx_sorted_decreasing</span>
    <span class="n">eigen_vals_sorted</span> <span class="o">=</span> <span class="n">eigen_vals</span><span class="p">[</span><span class="n">idx_sorted_decreasing</span><span class="p">]</span>

    <span class="c1"># sort eigenvectors using the idx_sorted_decreasing indices</span>
    <span class="c1"># We're only sorting the columns so remember to get all the rows in the slice</span>
    <span class="n">eigen_vecs_sorted</span> <span class="o">=</span> <span class="n">eigen_vecs</span><span class="p">[:,</span> <span class="n">idx_sorted_decreasing</span><span class="p">]</span>

    <span class="c1"># select the first n eigenvectors (n is desired dimension</span>
    <span class="c1"># of rescaled data array, or dims_rescaled_data)</span>
    <span class="c1"># once again, make sure to get all the rows and only slice the columns</span>
    <span class="n">eigen_vecs_subset</span> <span class="o">=</span> <span class="n">eigen_vecs_sorted</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">]</span>

    <span class="c1"># transform the data by multiplying the transpose of the eigenvectors </span>
    <span class="c1"># with the transpose of the de-meaned data</span>
    <span class="c1"># Then take the transpose of that product.</span>
    <span class="n">X_reduced</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eigen_vecs_subset</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_demeaned</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">X_reduced</span>
</pre></div>
<p>I was getting the wrong values because for some reason so I decided to take out the call to random (since the seed was being set the values were always the same anyway) and just declare the test input array.</p>
<div class="highlight">
<pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">4.17022005e-01</span><span class="p">,</span> <span class="mf">7.20324493e-01</span><span class="p">,</span> <span class="mf">1.14374817e-04</span><span class="p">,</span> <span class="mf">3.02332573e-01</span><span class="p">,</span>
                  <span class="mf">1.46755891e-01</span><span class="p">,</span> <span class="mf">9.23385948e-02</span><span class="p">,</span> <span class="mf">1.86260211e-01</span><span class="p">,</span> <span class="mf">3.45560727e-01</span><span class="p">,</span>
                  <span class="mf">3.96767474e-01</span><span class="p">,</span> <span class="mf">5.38816734e-01</span><span class="p">],</span>
                 <span class="p">[</span><span class="mf">4.19194514e-01</span><span class="p">,</span> <span class="mf">6.85219500e-01</span><span class="p">,</span> <span class="mf">2.04452250e-01</span><span class="p">,</span> <span class="mf">8.78117436e-01</span><span class="p">,</span>
                  <span class="mf">2.73875932e-02</span><span class="p">,</span> <span class="mf">6.70467510e-01</span><span class="p">,</span> <span class="mf">4.17304802e-01</span><span class="p">,</span> <span class="mf">5.58689828e-01</span><span class="p">,</span>
                  <span class="mf">1.40386939e-01</span><span class="p">,</span> <span class="mf">1.98101489e-01</span><span class="p">],</span>
                 <span class="p">[</span><span class="mf">8.00744569e-01</span><span class="p">,</span> <span class="mf">9.68261576e-01</span><span class="p">,</span> <span class="mf">3.13424178e-01</span><span class="p">,</span> <span class="mf">6.92322616e-01</span><span class="p">,</span>
                  <span class="mf">8.76389152e-01</span><span class="p">,</span> <span class="mf">8.94606664e-01</span><span class="p">,</span> <span class="mf">8.50442114e-02</span><span class="p">,</span> <span class="mf">3.90547832e-02</span><span class="p">,</span>
                  <span class="mf">1.69830420e-01</span><span class="p">,</span> <span class="mf">8.78142503e-01</span><span class="p">]])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">X_reduced</span> <span class="o">=</span> <span class="n">compute_pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># eigen_vecs, eigen_subset, X_demeaned = compute_pca(X, n_components=2)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Your original matrix was "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="s2">" and it became:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_reduced</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mf">0.43437323</span><span class="p">,</span> <span class="mf">0.49820384</span><span class="p">],</span>
 <span class="p">[</span><span class="mf">0.42077249</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.50351448</span><span class="p">],</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.85514571</span><span class="p">,</span> <span class="mf">0.00531064</span><span class="p">],</span>
<span class="p">])</span>

<span class="n">numpy</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">X_reduced</span><span class="p">,</span> <span class="n">expected</span><span class="p">)</span>
</pre></div>
<pre class="example">
Your original matrix was (3, 10) and it became:
[[ 0.43437323  0.49820384]
 [ 0.42077249 -0.50351448]
 [-0.85514571  0.00531064]]
</pre></div>
</div>
<div class="outline-4" id="outline-container-org47c4d7c">
<h4 id="org47c4d7c">Plot It</h4>
<div class="outline-text-4" id="text-org47c4d7c">
<p>We'll use most of the non-country words to create a plot to see how well the PCA does.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'oil'</span><span class="p">,</span> <span class="s1">'gas'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'sad'</span><span class="p">,</span> <span class="s1">'city'</span><span class="p">,</span> <span class="s1">'town'</span><span class="p">,</span>
         <span class="s1">'village'</span><span class="p">,</span> <span class="s1">'country'</span><span class="p">,</span> <span class="s1">'continent'</span><span class="p">,</span> <span class="s1">'petroleum'</span><span class="p">,</span> <span class="s1">'joyful'</span><span class="p">]</span>
<span class="n">subset</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">compute_pca</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reduced</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">"X Y"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="n">reduced</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">,</span> <span class="n">text_baseline</span><span class="o">=</span><span class="s2">"top"</span><span class="p">)</span>

<span class="n">points</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span> <span class="o">*</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"PCA of Words"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"pca_words"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/pca-dimensionality-reduction-and-word-vectors/pca_words.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>It appears to have worked fairly well.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgeeec538">
<h4 id="orgeeec538">Sklearn Comparison</h4>
<div class="outline-text-4" id="text-orgeeec538">
<p>As a comparison here's what SKlearn's PCA does.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reduced</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">"X Y"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="n">reduced</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">,</span> <span class="n">text_baseline</span><span class="o">=</span><span class="s2">"top"</span><span class="p">)</span>

<span class="n">points</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span> <span class="o">*</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"PCA of Words (SKLearn)"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"sklearn_pca_words"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/pca-dimensionality-reduction-and-word-vectors/sklearn_pca_words.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>They look fairly comparable, I'll conclude that they are close (or close enough).</p>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/pca-exploration/">PCA Exploration</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/pca-exploration/" rel="bookmark"><time class="published dt-published" datetime="2020-10-01T17:53:17-07:00" itemprop="datePublished" title="2020-10-01 17:53">2020-10-01 17:53</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/pca-exploration/#org0ca9fbc">Beginning</a>
<ul>
<li><a href="posts/nlp/pca-exploration/#org818736d">Imports</a></li>
<li><a href="posts/nlp/pca-exploration/#org2ca98ec">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/pca-exploration/#orgafdbf88">Middle</a>
<ul>
<li><a href="posts/nlp/pca-exploration/#orgdd687bb">Example One: Random Uniform Data</a></li>
<li><a href="posts/nlp/pca-exploration/#org2456fe5">Example Two: Normal Random Data</a></li>
<li><a href="posts/nlp/pca-exploration/#org8a7a0c5">Dimensionality Reduction</a></li>
</ul>
</li>
<li><a href="posts/nlp/pca-exploration/#org24fdc85">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org0ca9fbc">
<h2 id="org0ca9fbc">Beginning</h2>
<div class="outline-text-2" id="text-org0ca9fbc">
<p>In this post I'm going to walk through the Lab for Coursera's NLP Specialization in which we take a look at <a href="https://www.wikiwand.com/en/Principal_component_analysis">Principal Component Analysis</a> which we're going to use for Dimensionality Reduction later on. While PCA can be used as a black box it's useful to get an intuitive understanding of what it's doing so we'll take a look at a couple of simplified examples and pick apart a little bit of what's going on.</p>
</div>
<div class="outline-3" id="outline-container-org818736d">
<h3 id="org818736d">Imports</h3>
<div class="outline-text-3" id="text-org818736d">
<p>Just the usual suspects.</p>
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">default_rng</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2ca98ec">
<h3 id="org2ca98ec">Set Up</h3>
<div class="outline-text-3" id="text-org2ca98ec"></div>
<div class="outline-4" id="outline-container-orgd7ca85f">
<h4 id="orgd7ca85f">Plotting</h4>
<div class="outline-text-4" id="text-orgd7ca85f">
<p>This is a little bit of convenience code for the HoloViews plotting.</p>
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"pca-exploration"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span>
                <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
    <span class="n">color_cycle</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">Cycle</span><span class="p">([</span><span class="s2">"#4687b7"</span><span class="p">,</span> <span class="s2">"#ce7b6d"</span><span class="p">])</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5fb7e4b">
<h4 id="org5fb7e4b">Randomness</h4>
<div class="outline-text-4" id="text-org5fb7e4b">
<p><code>numpy's</code> <a href="https://numpy.org/devdocs/reference/random/generator.html">default_rng</a> creates a random-number generator. The only argument it takes is the <code>seed</code> which I'll set to 0.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy_random</span> <span class="o">=</span> <span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgafdbf88">
<h2 id="orgafdbf88">Middle</h2>
<div class="outline-text-2" id="text-orgafdbf88"></div>
<div class="outline-3" id="outline-container-orgdd687bb">
<h3 id="orgdd687bb">Example One: Random Uniform Data</h3>
<div class="outline-text-3" id="text-orgdd687bb">
<p>This first example will use a set of data that's in a straight line so that we can see a really basic example of what the PCA does to a straight line.</p>
</div>
<div class="outline-4" id="outline-container-orgd9d4603">
<h4 id="orgd9d4603">The Dataset</h4>
<div class="outline-text-4" id="text-orgd9d4603">
<p>To start I'll create a dataset generated by numpy's <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html">uniform</a> function, which takes three arguments - <code>low</code>, <code>high</code>, and <code>size</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">correlation</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy_random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>
<p>Since \(x=y\) we're going to end up with a line segment at a 45 degree angle.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org58eeeae">
<h4 id="org58eeeae">Center It</h4>
<div class="outline-text-4" id="text-org58eeeae">
<p>For PCA they recommend that you center the data by subtracting the mean.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">-=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">-=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
<p><b>Note:</b> according to <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn's PCA documentation</a> they center it for you so this is probably an unnecessary step.</p>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org702d750">
<h4 id="org702d750">The PCA Transformation</h4>
<div class="outline-text-4" id="text-org702d750">
<p>We're going to use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn's PCA</a> for Principal Component Analysis. The <code>n_components</code> argument is the number of components it will keep - we'll keep 2.</p>
<div class="highlight">
<pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>Now fit it to the data.</p>
<div class="highlight">
<pre><span></span><span class="n">transformation_model</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
<p>And then transform it.</p>
<div class="highlight">
<pre><span></span><span class="n">pca_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">transformation_model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">),</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"Principal Component 1"</span><span class="p">,</span> <span class="s2">"Principal Component 2"</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga93467a">
<h4 id="orga93467a">Plot the Transformation</h4>
<div class="outline-text-4" id="text-orga93467a">
<div class="highlight">
<pre><span></span><span class="n">original</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">transformed</span> <span class="o">=</span> <span class="n">pca_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Principal Component 1"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Principal Component 2"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">*</span> <span class="n">transformed</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Correlated and PCA Data"</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"correlated_data"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/pca-exploration/correlated_data.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Our blue line is the original data and the red line is the transformed data. So it looks like the PCA transform rotates the line to a horizontal one.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orga2d704f">
<h4 id="orga2d704f">Understanding the Model</h4>
<div class="outline-text-4" id="text-orga2d704f">
<p>Now that we have the model we can look at the <a href="https://www.wikiwand.com/en/Eigenvalues_and_eigenvectors">Eigenvalues and Eigenvectors</a> that it created to do the transformation.</p>
</div>
<ul class="org-ul">
<li><a id="org8d850b7"></a>The Eigenvectors (principal component).<br>
<div class="outline-text-5" id="text-org8d850b7">
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">transformation_model</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.70710678  0.70710678]
 [-0.70710678  0.70710678]]
</pre>
<p>The numbers look a little inscrutable at first, but what you need to know that it's a <a href="https://www.wikiwand.com/en/Rotation_matrix">rotation matrix</a>.</p>
<p>\[ R = \begin{bmatrix} cos(45^o) & sin(45^o)\\ -sin(45^o) & cos(45^o)\\ \end{bmatrix} \]</p>
<p>And since our line is at a \(45^\circ\) angle, the values in the Eigenvectors are the sin and cos of \(45^\circ\) that are used to rotate the line flat.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">45</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="mi">45</span><span class="p">)))</span>
</pre></div>
<pre class="example">
0.7071067811865476
0.7071067811865475
</pre></div>
</li>
<li><a id="orgb9b5432"></a>The Eigenvalues (explained variance).<br>
<div class="outline-text-5" id="text-orgb9b5432">
<p>Also part of the model are the eigenvalues which give the amount of variance explained by each of the components.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">transformation_model</span><span class="o">.</span><span class="n">explained_variance_</span><span class="p">)</span>
</pre></div>
<pre class="example">
[1.59912782e-01 7.31437644e-33]
</pre>
<p>So, what does that mean? Start with the fact that the equation for <a href="https://www.dummies.com/education/math/business-statistics/how-to-calculate-the-variance-and-standard-deviation-in-the-uniform-distribution/">variance of a uniform distribution</a> is:</p>
<p>\[ Var = \frac{(b - a)^2}{12} \]</p>
<p>And remember that hen we called the <code>uniform</code> function we set <code>b</code> to 2, and <code>a</code> to 1, so we get.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">((</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">12</span><span class="p">)</span>
</pre></div>
<pre class="example">
0.08333333333333333
</pre>
<p>If you look at the Eigenvalues we got, the second term is \(7 \times 10^{-33}\) which is pretty much zero, and the first term is about <code>0.16</code>, so what we have here is.</p>
\begin{align} Var &amp;= \langle Var(x) + Var(y), 0\rangle\\ &amp;= \langle 0.083 + 0.083, 0 \rangle\\ &amp;= \langle 0.16, 0 \rangle\\ \end{align}
<p>It rounds more to 0.167, but close enough, the point is that the first component contributed all the variance and the second didn't contribute any.</p>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org2456fe5">
<h3 id="org2456fe5">Example Two: Normal Random Data</h3>
<div class="outline-text-3" id="text-org2456fe5">
<p>Now we'll move onto normally-distributed data so we can see something a little more interesting.</p>
</div>
<div class="outline-4" id="outline-container-org0b1881a">
<h4 id="org0b1881a">Generate the Data</h4>
<div class="outline-text-4" id="text-org0b1881a">
<p>Now we'll to use numpy's random <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html">normal</a> function to generate the data. The three arguments it takes are <code>loc</code> (the mean), <code>scale</code> (the standard deviation), and <code>size</code> (the number of numbers to generate).</p>
<div class="highlight">
<pre><span></span><span class="n">standard_deviation_1</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">standard_deviation_2</span> <span class="o">=</span> <span class="mf">0.333</span>
<span class="n">points</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">3</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">numpy_random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">standard_deviation_1</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy_random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">standard_deviation_2</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>
</pre></div>
<p>Even though we specify that the mean is 0, because it the data is generated randomly it isn't exactly zero so we'll center it.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"x mean start: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"y mean start: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">x mean: </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"y mean: </span><span class="si">{</span><span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
x mean start: -0.012000607736595292
y mean start: -0.01409218413437418

x mean: 3.552713678800501e-18
y mean: 2.6645352591003758e-18
</pre></div>
</div>
<div class="outline-4" id="outline-container-org7e0f803">
<h4 id="org7e0f803">Plot It</h4>
<div class="outline-text-4" id="text-org7e0f803">
<p>And now a plot to show the data.</p>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">))</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Random Normal Data"</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"random_normal_data"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/pca-exploration/random_normal_data.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>As you can see, the data is pretty uncorrelated so we're going to rotate it to make it a little less of a blob.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org3ba9620">
<h4 id="org3ba9620">Rotate The Data</h4>
<div class="outline-text-4" id="text-org3ba9620">
<p>Now we're going to put the <code>x</code> and <code>y</code> data into a matrix and rotate it.</p>
<div class="highlight">
<pre><span></span><span class="n">covariance</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">in_degrees</span> <span class="o">=</span> <span class="mi">45</span>
<span class="n">angle</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">in_degrees</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"angle: </span><span class="si">{</span><span class="n">math</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">angle</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">rotation_matrix</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">numpy</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">)],</span>
                               <span class="p">[</span><span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle</span><span class="p">)]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rotation_matrix</span><span class="p">)</span>
</pre></div>
<pre class="example">
angle: 45.0

[[ 0.70710678  0.70710678]
 [-0.70710678  0.70710678]]
</pre>
<p>You might notice that this is the same rotation matrix that we had before with the sklearn eigenvectors, so we could have used that, but this is how you would roll your own.</p>
<p>Now we can apply the rotation by taking the dot-product between the data array and the rotation-matrix.</p>
<div class="highlight">
<pre><span></span><span class="n">rotated</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rotation_matrix</span><span class="p">)</span>
<span class="n">rotated</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"x"</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgd9204a8">
<h4 id="orgd9204a8">Plot The Rotated Data</h4>
<div class="outline-text-4" id="text-orgd9204a8">
<p>To get a sense of what our transformation did we can plot it. In addition we'll plot the axes created by the rotation matrix so we can see how they're related. So first thing is to unpack the axes contained within the rotation matrix. In addition we'll scale the axes by the standard deviation we used along each of the original axes to see how that relates to the shape of the data.</p>
<div class="highlight">
<pre><span></span><span class="n">FIRST_ROW</span><span class="p">,</span> <span class="n">SECOND_ROW</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">FIRST_COLUMN</span><span class="p">,</span> <span class="n">SECOND_COLUMN</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">ORIGIN</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">SCALAR</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">FIRST_SPREAD</span><span class="p">,</span> <span class="n">SECOND_SPREAD</span> <span class="o">=</span> <span class="p">(</span><span class="n">standard_deviation_1</span> <span class="o">*</span> <span class="n">SCALAR</span><span class="p">,</span>
                               <span class="n">standard_deviation_2</span> <span class="o">*</span> <span class="n">SCALAR</span><span class="p">)</span>
<span class="n">COLUMNS</span> <span class="o">=</span> <span class="s2">"x y"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="n">first_axis</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="n">ORIGIN</span><span class="p">,</span>
    <span class="n">rotation_matrix</span><span class="p">[</span><span class="n">FIRST_ROW</span><span class="p">][:]],</span>
                              <span class="n">columns</span><span class="o">=</span><span class="n">COLUMNS</span><span class="p">)</span>
<span class="n">first_axis</span> <span class="o">*=</span> <span class="n">FIRST_SPREAD</span>


<span class="n">second_axis</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="n">ORIGIN</span><span class="p">,</span>
    <span class="n">rotation_matrix</span><span class="p">[</span><span class="n">SECOND_ROW</span><span class="p">][:]],</span>
                               <span class="n">columns</span><span class="o">=</span><span class="n">COLUMNS</span><span class="p">)</span>
<span class="n">second_axis</span> <span class="o">*=</span> <span class="n">SECOND_SPREAD</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">first_axis_plot</span> <span class="o">=</span> <span class="n">first_axis</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span>
<span class="n">second_axis_plot</span> <span class="o">=</span> <span class="n">second_axis</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"orange"</span><span class="p">)</span>
<span class="n">rotated_plot</span> <span class="o">=</span> <span class="n">rotated</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">rotated_plot</span> <span class="o">*</span> <span class="n">first_axis_plot</span> <span class="o">*</span> <span class="n">second_axis_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Rotated Normal Data"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"rotated_normal_data"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/pca-exploration/rotated_normal_data.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>So our data is now grouped around a 45-degree angle and spread further along the axis that had more variance.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org9b8b15d">
<h4 id="org9b8b15d">Apply the PCA</h4>
<div class="outline-text-4" id="text-org9b8b15d">
<div class="highlight">
<pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">fitted</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">rotated</span><span class="p">)</span>
</pre></div>
<p>Once again, the Eigenvectors (the transformation matirix).</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">fitted</span><span class="o">.</span><span class="n">components_</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[-0.70844626 -0.70576476]
 [-0.70576476  0.70844626]]
</pre>
<p>And then the Eigenvalues (the variance).</p>
<div class="highlight">
<pre><span></span><span class="n">variance</span> <span class="o">=</span> <span class="n">fitted</span><span class="o">.</span><span class="n">explained_variance_</span>
<span class="nb">print</span><span class="p">(</span><span class="n">variance</span><span class="p">)</span>
</pre></div>
<pre class="example">
[1.05270169 0.10604603]
</pre>
<p>Now we apply the PCA transformation.</p>
<div class="highlight">
<pre><span></span><span class="n">pca_data</span> <span class="o">=</span> <span class="n">fitted</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">rotated</span><span class="p">)</span>
<span class="n">pca_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca_data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">"x y"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1b087dc">
<h4 id="org1b087dc">Plot the PCA Transformed Data</h4>
<div class="outline-text-4" id="text-org1b087dc">
<p>We're going to plot the rotated and the transformed data along with the axes for the rotated data so the first</p>
<div class="highlight">
<pre><span></span><span class="n">transformed</span> <span class="o">=</span> <span class="n">pca_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">,</span> <span class="n">fill_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">rotated_plot</span> <span class="o">=</span> <span class="n">rotated</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">,</span> <span class="n">fill_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">first_axis_plot</span> <span class="o">=</span> <span class="n">first_axis</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span>
<span class="n">second_axis_plot</span> <span class="o">=</span> <span class="n">second_axis</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"orange"</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">transformed</span> <span class="o">*</span> <span class="n">rotated_plot</span> <span class="o">*</span> <span class="n">first_axis_plot</span> <span class="o">*</span> <span class="n">second_axis_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"PCA of Random Normal Data"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"pca_random_normal"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/pca-exploration/pca_random_normal.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
<div class="outline-4" id="outline-container-org95dd33d">
<h4 id="org95dd33d">Looking at the model</h4>
<div class="outline-text-4" id="text-org95dd33d">
<ul class="org-ul">
<li>The rotation matrix took the original uncorrelated variables and transformed them into correllated variables (the blue circles).</li>
<li>Fitting the PCA to our correlated data finds the rotation matrix that was used to create the blue points.</li>
<li>Applying the PCA transformation undoes the rotation (but the spread doesn't return).</li>
</ul>
<p>Our orginal standard deviations were 1 and 0.333 and if we look at the Explained Variance it is roughly our original standard deviations squared.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span><span class="p">))</span>
</pre></div>
<pre class="example">
[0.99140088 0.32958007]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8a7a0c5">
<h3 id="org8a7a0c5">Dimensionality Reduction</h3>
<div class="outline-text-3" id="text-org8a7a0c5">
<p>The previous sections were meant to understand what PCA is doing, but to use the PCA for visualization we will use it to reduce the number of dimensions of a data set so that it can be plotted. We can get a sense of how that works here by looking at our rotated data set with either the entire x-axis set to 0 or the entire y-axis set to 0.</p>
<div class="highlight">
<pre><span></span><span class="n">first_component</span> <span class="o">=</span> <span class="n">rotated</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">first_component</span><span class="p">[</span><span class="s2">"y"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">second_component</span> <span class="o">=</span> <span class="n">rotated</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">second_component</span><span class="p">[</span><span class="s2">"x"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">original</span> <span class="o">=</span> <span class="n">rotated</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">tan</span><span class="p">,</span>
                                  <span class="n">fill_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">first</span> <span class="o">=</span> <span class="n">first_component</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span>
                                       <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">,</span> <span class="n">fill_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">second</span> <span class="o">=</span> <span class="n">second_component</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span>
                                         <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">,</span> <span class="n">fill_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">original</span> <span class="o">*</span> <span class="n">first</span> <span class="o">*</span> <span class="n">second</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Data Decomposition"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span>
<span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"data_decomposed"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/pca-exploration/data_decomposed.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>This is only a teaser to doing an actual dimensionality reduction.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org24fdc85">
<h2 id="org24fdc85">End</h2>
<div class="outline-text-2" id="text-org24fdc85">
<p>This is a walk-through of a lab for Coursera's NLP Specialization.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings/">Word Embeddings</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings/" rel="bookmark"><time class="published dt-published" datetime="2020-09-29T19:25:16-07:00" itemprop="datePublished" title="2020-09-29 19:25">2020-09-29 19:25</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings/#orgb1248f0">Beginning</a>
<ul>
<li><a href="posts/nlp/word-embeddings/#orgb492efe">Set Up</a></li>
<li><a href="posts/nlp/word-embeddings/#org548d27a">The Embeddings</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings/#orge0519bd">Middle</a>
<ul>
<li><a href="posts/nlp/word-embeddings/#orgd6dcea9">Inspecting the Embeddings</a></li>
<li><a href="posts/nlp/word-embeddings/#org6a8b96e">Word Distance</a></li>
<li><a href="posts/nlp/word-embeddings/#orga2bfcf2">Linear Algebra on Word Embeddings</a></li>
<li><a href="posts/nlp/word-embeddings/#orgbdd6253">Predicting Capitals</a></li>
<li><a href="posts/nlp/word-embeddings/#org107a7b2">More Countries</a></li>
<li><a href="posts/nlp/word-embeddings/#org9d361bc">Sentence Vectors</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgb1248f0">
<h2 id="orgb1248f0">Beginning</h2>
<div class="outline-text-2" id="text-orgb1248f0">
<p>This is a walk through a lab for week 3 of Coursera's Natural Language Processing course. It's going to use some pretrained word embeddings to develop some sense of how to use them.</p>
</div>
<div class="outline-3" id="outline-container-orgb492efe">
<h3 id="orgb492efe">Set Up</h3>
<div class="outline-text-3" id="text-orgb492efe"></div>
<div class="outline-4" id="outline-container-org5ec160d">
<h4 id="org5ec160d">Imports</h4>
<div class="outline-text-4" id="text-org5ec160d">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgdfb5dc2">
<h4 id="orgdfb5dc2">Plotting</h4>
<div class="outline-text-4" id="text-orgdfb5dc2">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>
<span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"word-embeddings"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">plot_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_PLOT"</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">plot_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>
<span class="k">with</span> <span class="n">plot_path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">Plot</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org548d27a">
<h3 id="org548d27a">The Embeddings</h3>
<div class="outline-text-3" id="text-org548d27a">
<p>Like I mentioned abovve, I'm going to use pre-trained word embeddings that have been pickled so I'll load them here.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"WORD_EMBEDDINGS"</span><span class="p">])</span>
<span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="mi">243</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orge0519bd">
<h2 id="orge0519bd">Middle</h2>
<div class="outline-text-2" id="text-orge0519bd"></div>
<div class="outline-3" id="outline-container-orgd6dcea9">
<h3 id="orgd6dcea9">Inspecting the Embeddings</h3>
<div class="outline-text-3" id="text-orgd6dcea9">
<p>The <code>embeddings</code> is a dictionary of words to word-vectors that represent them. Here's the first 5 words.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">keys</span><span class="p">())[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
<pre class="example">
&lt;class 'dict'&gt;
['country', 'city', 'China', 'Iraq', 'oil']
</pre>
<div class="highlight">
<pre><span></span><span class="n">vector</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"country"</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">vector</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
&lt;class 'numpy.ndarray'&gt;
(300,)
</pre>
<p>Each word-embedding vector has 300 entries.</p>
</div>
<div class="outline-4" id="outline-container-orgbd4dbce">
<h4 id="orgbd4dbce">Plotting</h4>
<div class="outline-text-4" id="text-orgbd4dbce">
<p>Since there are 300 columns you can't easily visualize them without using PCA or some other method, but this is more about getting an intuition as to how the linear-algebra works, so instead we're going to reduce a subset of words to only two columns so that we can plot them.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'oil'</span><span class="p">,</span> <span class="s1">'gas'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'sad'</span><span class="p">,</span> <span class="s1">'city'</span><span class="p">,</span> <span class="s1">'town'</span><span class="p">,</span> <span class="s1">'village'</span><span class="p">,</span> <span class="s1">'country'</span><span class="p">,</span> <span class="s1">'continent'</span><span class="p">,</span> <span class="s1">'petroleum'</span><span class="p">,</span> <span class="s1">'joyful'</span><span class="p">]</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
<span class="n">plot_columns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">plot_data</span><span class="p">[</span><span class="n">plot_columns</span><span class="p">]</span>
<span class="n">plot_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"x"</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">]</span>
<span class="n">plot_data</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>
<span class="n">origins</span> <span class="o">=</span> <span class="n">plot_data</span> <span class="o">*</span> <span class="mi">0</span>
<span class="n">origins</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>
<span class="n">combined_plot_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">origins</span><span class="p">,</span> <span class="n">plot_data</span><span class="p">])</span>

<span class="n">segment_plot</span> <span class="o">=</span> <span class="n">combined_plot_data</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>
<span class="n">scatter_plot</span> <span class="o">=</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">segment_plot</span> <span class="o">*</span> <span class="n">scatter_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Embeddings Columns 3 and 2"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">font_scale</span>
<span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"embeddings_segments"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/word-embeddings/embeddings_segments.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>You can see that words like "village" and "town" are similar while "city" and "oil" are opposites for whatever reason. Oddly, "joyful" and "country" are also very similar (although I'm only looking at two out of three-hundred columns so that might not be the case once the other columns enter into place).</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org6a8b96e">
<h3 id="org6a8b96e">Word Distance</h3>
<div class="outline-text-3" id="text-org6a8b96e">
<p>This is supposed to be a visualization of the difference vectors between <i>sad</i> and <i>happy</i> and <i>town</i> and <i>village</i>, but as far as I can see holoviews doesn't have the equivalent of matplotlib's arrow which lets you use the base coordinate and distance in each dimension to draw arrows, so it's kind of a fake version where I use the points directly. Oh, well.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'sad'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'town'</span><span class="p">,</span> <span class="s1">'village'</span><span class="p">]</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">plot_data</span><span class="p">[</span><span class="n">plot_columns</span><span class="p">]</span>
<span class="n">plot_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"x"</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">]</span>
<span class="n">plot_data</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">words</span>
</pre></div>
<p>This is the fake part - when you take the difference between two "points" it gives you a vector with the base at the origin so you have to add the base point back in to move it from the origin, but then all you're doing is undoing the subtraction, giving you what you started with.</p>
<div class="highlight">
<pre><span></span><span class="n">difference</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"happy"</span><span class="p">]</span> <span class="o">-</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"sad"</span><span class="p">]</span> <span class="o">+</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"sad"</span><span class="p">],</span>
    <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"town"</span><span class="p">]</span> <span class="o">-</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"village"</span><span class="p">]</span> <span class="o">+</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"village"</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">difference</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"sad"</span><span class="p">,</span> <span class="s2">"village"</span><span class="p">]</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">))</span>

<span class="n">difference</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">difference</span><span class="p">,</span>
                            <span class="n">plot_data</span><span class="p">[</span><span class="n">plot_data</span><span class="o">.</span><span class="n">Word</span><span class="o">==</span><span class="s2">"sad"</span><span class="p">],</span>
                            <span class="n">plot_data</span><span class="p">[</span><span class="n">plot_data</span><span class="o">.</span><span class="n">Word</span><span class="o">==</span><span class="s2">"village"</span><span class="p">]])</span>


<span class="n">with_origin</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">origins</span><span class="p">[</span><span class="n">origins</span><span class="o">.</span><span class="n">Word</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">words</span><span class="p">)],</span> <span class="n">plot_data</span><span class="p">])</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>
<span class="n">segments</span> <span class="o">=</span> <span class="n">with_origin</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>
<span class="n">distances</span> <span class="o">=</span> <span class="n">difference</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">distances</span> <span class="o">*</span> <span class="n">segments</span> <span class="o">*</span> <span class="n">scatter</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Vector Differences"</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">font_scale</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"vector_differences"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/word-embeddings/vector_differences.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
<div class="outline-3" id="outline-container-orga2bfcf2">
<h3 id="orga2bfcf2">Linear Algebra on Word Embeddings</h3>
<div class="outline-text-3" id="text-orga2bfcf2"></div>
<div class="outline-4" id="outline-container-org94a6523">
<h4 id="org94a6523">The <b>norm</b></h4>
<div class="outline-text-4" id="text-org94a6523">
<p>First I'll check out the <a href="https://www.wikiwand.com/en/Norm_(mathematics)">norm</a> of some word vectors using <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html">numpy.linalg.norm</a>. This calculates the Euclidean Distance between vectors (but oddly we won't use it here).</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">"town"</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">"sad"</span><span class="p">]))</span>
</pre></div>
<pre class="example">
2.3858097
2.9004838
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgbdd6253">
<h3 id="orgbdd6253">Predicting Capitals</h3>
<div class="outline-text-3" id="text-orgbdd6253">
<p>Here we'll see how to use the embeddings to predict what country a city is the capital of. To encode the concept of "capital" into a vector we'll use the difference between a specific country and its real capital (in this case <i>France</i> and <i>Paris</i>).</p>
<div class="highlight">
<pre><span></span><span class="n">capital</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"France"</span><span class="p">]</span> <span class="o">-</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"Paris"</span><span class="p">]</span>
</pre></div>
<p>Now that we have the concept of a capital encoded as a word embedding we can add it to the embedding of "Madrid" to get a vector near where "Spain" would be. Note that although there is a "Spain" in the embeddings we're going to use this to see if we can find it without knowing that Madrid is the capital of Spain.</p>
<div class="highlight">
<pre><span></span><span class="n">country</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"Madrid"</span><span class="p">]</span> <span class="o">+</span> <span class="n">capital</span>
</pre></div>
<p>To make a prediction we have to find the embeddings that are closest to a country. We're going to convert the embeddings to a pandas DataFrame and since our embeddings are a dictionary of arrays we'll have to do a little unpacking first.</p>
<div class="highlight">
<pre><span></span><span class="n">keys</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">embeddings</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">keys</span><span class="p">)</span>
</pre></div>
<p>Now we'll make a function to find the closest embeddings for a word vector.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">closest_word</span><span class="p">(</span><span class="n">vector</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""Find the word closest to a given vector</span>

<span class="sd">    Args:</span>
<span class="sd">     vector: the vector to match</span>

<span class="sd">    Returns:</span>
<span class="sd">     name of the closest embedding</span>
<span class="sd">    """</span>
    <span class="n">differences</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">-</span> <span class="n">vector</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">differences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="p">(</span><span class="n">differences</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"columns"</span><span class="p">)</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">distances</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">differences</span><span class="p">),)))</span>

    <span class="k">return</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)]</span><span class="o">.</span><span class="n">name</span>
</pre></div>
<p>Now we can check what word most closesly matches <i>Madrid + (France - Paris)</i>.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">closest_word</span><span class="p">(</span><span class="n">country</span><span class="p">))</span>
</pre></div>
<pre class="example">
Spain
</pre>
<p>Like magic.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org107a7b2">
<h3 id="org107a7b2">More Countries</h3>
<div class="outline-text-3" id="text-org107a7b2">
<p>What happens if we use a different know country and its capital instead of France and Paris?</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">closest_word</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">'Italy'</span><span class="p">]</span> <span class="o">-</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">'Rome'</span><span class="p">]</span>
                   <span class="o">+</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">'Madrid'</span><span class="p">]))</span>
</pre></div>
<pre class="example">
Spain
</pre>
<p>So swapping the capital derivation didn't change the prediction. Now we'll go back to using <code>France - Paris</code> but try different cities.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="s2">"Tokyo Moscow"</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> is the capital of </span><span class="si">{</span><span class="n">closest_word</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+</span> <span class="n">capital</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Tokyo is the capital of Japan
Moscow is the capital of Russia
</pre>
<p>That seems to be working, but here's a case where our search fails.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">closest_word</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">'Lisbon'</span><span class="p">]</span> <span class="o">+</span> <span class="n">capital</span><span class="p">))</span>
</pre></div>
<pre class="example">
Lisbon
</pre>
<p>For some reason "Lisbon" is closer to itself than portugal. I tried it with Germany and Italy instead of France as the template capital but it still didn't work. If you try random cities from the embeddings you'll see that a fair amount of them fail.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org9d361bc">
<h3 id="org9d361bc">Sentence Vectors</h3>
<div class="outline-text-3" id="text-org9d361bc">
<p>To use this for sentences you construct a vector with all the vectors for each word and then sum up all the columns to get back to a single vector.</p>
<div class="highlight">
<pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">"Canada oil city town"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>
<span class="n">summed</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">closest_word</span><span class="p">(</span><span class="n">summed</span><span class="p">))</span>
</pre></div>
<pre class="example">
city
</pre>
<p>Not exciting, but that's how you do it.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/tweet-classifier-class/">Tweet Classifier Class</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/tweet-classifier-class/" rel="bookmark"><time class="published dt-published" datetime="2020-09-09T17:49:07-07:00" itemprop="datePublished" title="2020-09-09 17:49">2020-09-09 17:49</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/tweet-classifier-class/#org78e444e">Beginning</a></li>
<li><a href="posts/nlp/tweet-classifier-class/#orgf4f6d3f">Middle</a>
<ul>
<li><a href="posts/nlp/tweet-classifier-class/#orgf55973d">The Logistic Regression Class</a></li>
<li><a href="posts/nlp/tweet-classifier-class/#org7a0f679">Weights</a></li>
<li><a href="posts/nlp/tweet-classifier-class/#org30e783d">The Weights Setter</a></li>
<li><a href="posts/nlp/tweet-classifier-class/#orge22f5e6">Sigmoid</a></li>
<li><a href="posts/nlp/tweet-classifier-class/#org7dd4609">This is the training function</a></li>
<li><a href="posts/nlp/tweet-classifier-class/#org9a9d104">Fit</a></li>
<li><a href="posts/nlp/tweet-classifier-class/#org34e1532">Predict</a></li>
<li><a href="posts/nlp/tweet-classifier-class/#org196e2d5">Score</a></li>
</ul>
</li>
<li><a href="posts/nlp/tweet-classifier-class/#orgdd499a3">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org78e444e">
<h2 id="org78e444e">Beginning</h2>
<div class="outline-text-2" id="text-org78e444e">
<p>I implemented the Logistic Regression Tweet Sentiment Analysis classifier in <a href="posts/nlp/implementing-twitter-logistic-regression/">this post</a> but I'm going to re-use it later so this just gathers everything together. There's already a class called <code>TweetSentiment</code> but I'm going to add the training to this one as well as the tweet pre-processing and vectorization.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgf4f6d3f">
<h2 id="orgf4f6d3f">Middle</h2>
<div class="outline-text-2" id="text-orgf4f6d3f">
<p>We'll start with the imports.</p>
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">.counter</span> <span class="kn">import</span> <span class="n">WordCounter</span>
<span class="kn">from</span> <span class="nn">.sentiment</span> <span class="kn">import</span> <span class="n">TweetSentiment</span>
<span class="kn">from</span> <span class="nn">.vectorizer</span> <span class="kn">import</span> <span class="n">TweetVectorizer</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgf55973d">
<h3 id="orgf55973d">The Logistic Regression Class</h3>
<div class="outline-text-3" id="text-orgf55973d">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">:</span>
    <span class="sd">"""train and predict tweet sentiment</span>

<span class="sd">    Args:</span>
<span class="sd">     iterations: number of times to run gradient descent</span>
<span class="sd">     learning_rate: how fast to change the weights during training</span>
<span class="sd">    """</span>
    <span class="n">iterations</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">loss</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7a0f679">
<h3 id="org7a0f679">Weights</h3>
<div class="outline-text-3" id="text-org7a0f679">
<p>These are the weights for the regression function (\(\theta\)).</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
    <span class="sd">"""The weights for the regression</span>

<span class="sd">    Initially this will be an array of zeros.</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org30e783d">
<h3 id="org30e783d">The Weights Setter</h3>
<div class="outline-text-3" id="text-org30e783d">
<div class="highlight">
<pre><span></span><span class="nd">@weights</span><span class="o">.</span><span class="n">setter</span>
<span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Set the weights to a new value"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">new_weights</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge22f5e6">
<h3 id="orge22f5e6">Sigmoid</h3>
<div class="outline-text-3" id="text-orge22f5e6">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectors</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Calculates the logistic function value</span>

<span class="sd">    Args:</span>
<span class="sd">     vectors: a matrix of bias, positive, negative wordc ounts</span>

<span class="sd">    Returns:</span>
<span class="sd">     array of probabilities that the tweets are positive</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">vectors</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7dd4609">
<h3 id="org7dd4609">This is the training function</h3>
<div class="outline-text-3" id="text-org7dd4609">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="sd">"""Finds the weights for the model</span>

<span class="sd">    Args:</span>
<span class="sd">     x: the tweet vectors</span>
<span class="sd">     y: the positive/negative labels</span>
<span class="sd">    """</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">/=</span> <span class="n">rows</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">))</span>
        <span class="c1"># average loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)))</span> <span class="o">+</span>
                               <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">))))</span><span class="o">/</span><span class="n">rows</span>
        <span class="n">gradient</span> <span class="o">=</span> <span class="p">((</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9a9d104">
<h3 id="org9a9d104">Fit</h3>
<div class="outline-text-3" id="text-org9a9d104">
<p>This is mostly an alias to make it match (somewhat) sklearn's methods.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_train</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_train</span><span class="p">:</span><span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""fits the weights for the logistic regression</span>

<span class="sd">    Note:</span>
<span class="sd">     as a side effect this also sets counter, loss, and sentimenter attributes</span>

<span class="sd">    Args:</span>
<span class="sd">     x_train: the training tweets</span>
<span class="sd">     y_train: the training labels</span>

<span class="sd">    Returns:</span>
<span class="sd">     The final mean loss (which is also saved as the =.loss= attribute)</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="n">WordCounter</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TweetVectorizer</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">processed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_descent</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">vectors</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org34e1532">
<h3 id="org34e1532">Predict</h3>
<div class="outline-text-3" id="text-org34e1532">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Predict the labels for the inputs</span>

<span class="sd">    Args:</span>
<span class="sd">     x: a list or array of tweets</span>

<span class="sd">    Returns:</span>
<span class="sd">     array of predicted labels for the tweets</span>
<span class="sd">    """</span>
    <span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TweetVectorizer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">processed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">sentimenter</span> <span class="o">=</span> <span class="n">TweetSentiment</span><span class="p">(</span><span class="n">vectorizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sentimenter</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org196e2d5">
<h3 id="org196e2d5">Score</h3>
<div class="outline-text-3" id="text-org196e2d5">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Get the mean accuracy</span>

<span class="sd">    Args:</span>
<span class="sd">     x: arrray of tweets</span>
<span class="sd">     y: labels for the tweets</span>

<span class="sd">    Returns:</span>
<span class="sd">     mean accuracy</span>
<span class="sd">    """</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">correct</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgdd499a3">
<h2 id="orgdd499a3">End</h2>
<div class="outline-text-2" id="text-orgdd499a3">
<p>Testing it out.</p>
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">be_true</span><span class="p">,</span>
    <span class="n">expect</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.logistic_regression</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>

<span class="n">train_raw</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_feather</span><span class="p">(</span>
    <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_TRAINING_RAW"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">())</span>

<span class="n">test_raw</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_feather</span><span class="p">(</span>
    <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_TEST_RAW"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="p">)</span>


<span class="n">Settings</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="mf">1e-9</span><span class="p">,</span>
    <span class="n">iterations</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">iterations</span><span class="o">=</span><span class="n">Settings</span><span class="o">.</span><span class="n">iterations</span><span class="p">,</span>
                           <span class="n">learning_rate</span><span class="o">=</span><span class="n">Settings</span><span class="o">.</span><span class="n">eta</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="o">=</span><span class="n">train_raw</span><span class="o">.</span><span class="n">tweet</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">train_raw</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">expected</span> <span class="o">=</span> <span class="mf">0.22043072</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_raw</span><span class="o">.</span><span class="n">tweet</span><span class="p">,</span> <span class="n">test_raw</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Accuracy: 0.996
</pre></div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="index-13.html" rel="prev">Newer posts</a></li>
<li class="next"><a href="index-11.html" rel="next">Older posts</a></li>
</ul>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script><!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
