<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>In Too Deep (old posts, page 5) | In Too Deep</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/index-5.html" rel="canonical">
<link href="index-6.html" rel="prev" type="text/html">
<link href="index-4.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/cnn/mnist-mlp/">MNIST MLP</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/cnn/mnist-mlp/" rel="bookmark"><time class="published dt-published" datetime="2018-11-25T17:29:13-08:00" itemprop="datePublished" title="2018-11-25 17:29">2018-11-25 17:29</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/cnn/mnist-mlp/#org8aefa1e">Introduction</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org11e5ebb">Imports</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org3c62cd1">Setup the Plotting</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org9d2bf1e">The Data</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#orgcaa7f6c">Visualize a Batch of Training Data</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org5ddd997">Define the Network Architecture</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org2128ad9">Specify the Loss Function and Optimizer</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org39bd2cc">Train the Network</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org94441ec">Test the Trained Network</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org8aefa1e">
<h2 id="org8aefa1e">Introduction</h2>
<div class="outline-text-2" id="text-org8aefa1e">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>We are going to train a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multi-Layer Perceptron</a> to classify images from the <a href="http://yann.lecun.com/exdb/mnist/">MNIST database</a> of hand-written digits.</p>
<p>We're going to do it using the following steps.</p>
<ol class="org-ol">
<li>Load and visualize the data</li>
<li>Define a neural network</li>
<li>Train the model</li>
<li>Evaluate the performance of our trained model on a test dataset</li>
</ol>
</div>
</div>
<div class="outline-2" id="outline-container-org11e5ebb">
<h2 id="org11e5ebb">Imports</h2>
<div class="outline-text-2" id="text-org11e5ebb"></div>
<div class="outline-3" id="outline-container-org017bb68">
<h3 id="org017bb68">From Python</h3>
<div class="outline-text-3" id="text-org017bb68">
<div class="highlight">
<pre><span></span>from datetime import datetime
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8ede254">
<h3 id="org8ede254">From PyPi</h3>
<div class="outline-text-3" id="text-org8ede254">
<div class="highlight">
<pre><span></span>from dotenv import load_dotenv
from torchvision import datasets
import matplotlib.pyplot as pyplot
import seaborn
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torch
import numpy
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org83ef700">
<h3 id="org83ef700">This Project</h3>
<div class="outline-text-3" id="text-org83ef700">
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org3c62cd1">
<h2 id="org3c62cd1">Setup the Plotting</h2>
<div class="outline-text-2" id="text-org3c62cd1">
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=3)
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org9d2bf1e">
<h2 id="org9d2bf1e">The Data</h2>
<div class="outline-text-2" id="text-org9d2bf1e"></div>
<div class="outline-3" id="outline-container-orgca74ed6">
<h3 id="orgca74ed6">The Path To the Data</h3>
<div class="outline-text-3" id="text-orgca74ed6">
<div class="highlight">
<pre><span></span>load_dotenv()
path = DataPathTwo(folder_key="MNIST")
</pre></div>
<div class="highlight">
<pre><span></span>print(path.folder)
print(path.folder.exists())
</pre></div>
<pre class="example">
/home/hades/datasets/MNIST
True

</pre></div>
</div>
<div class="outline-3" id="outline-container-org1ec8eb0">
<h3 id="org1ec8eb0">Some Settings</h3>
<div class="outline-text-3" id="text-org1ec8eb0">
<p>Since I downloaded the data earlier for some other exercise forking sub-processes is probably unnecessary, and for the training and testing we'll use a relatively small batch-size of 20.</p>
<div class="highlight">
<pre><span></span>WORKERS = 0
BATCH_SIZE = 20
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7a40b16">
<h3 id="org7a40b16">A Transform</h3>
<div class="outline-text-3" id="text-org7a40b16">
<p>We're just going to convert the images to tensors.</p>
<div class="highlight">
<pre><span></span>transform = transforms.ToTensor()
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgeb1d0a9">
<h3 id="orgeb1d0a9">Split Up the Training and Testing Data</h3>
<div class="outline-text-3" id="text-orgeb1d0a9">
<div class="highlight">
<pre><span></span>train_data = datasets.MNIST(root=path.folder, train=True,
                            download=True, transform=transform)
test_data = datasets.MNIST(root=path.folder, train=False,
                           download=True, transform=transform)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org71a9cf6">
<h3 id="org71a9cf6">Create the Batch Loaders</h3>
<div class="outline-text-3" id="text-org71a9cf6">
<div class="highlight">
<pre><span></span>train_batches = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
                                            num_workers=WORKERS)
test_batches = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, 
                                           num_workers=WORKERS)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgcaa7f6c">
<h2 id="orgcaa7f6c">Visualize a Batch of Training Data</h2>
<div class="outline-text-2" id="text-orgcaa7f6c">
<p>The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data.</p>
</div>
<div class="outline-3" id="outline-container-org8a31a7b">
<h3 id="org8a31a7b">Grab a batch</h3>
<div class="outline-text-3" id="text-org8a31a7b">
<div class="highlight">
<pre><span></span>images, labels = iter(train_batches).next()
images = images.numpy()
</pre></div>
<p>Now that we have a batch we're going to plot the images in the batch, along with the corresponding labels.</p>
<div class="highlight">
<pre><span></span>figure = pyplot.figure(figsize=(25, 4))
figure.suptitle("First Batch", weight="bold")
for index in numpy.arange(BATCH_SIZE):
    ax = figure.add_subplot(2, BATCH_SIZE/2, index+1, xticks=[], yticks=[])
    ax.imshow(numpy.squeeze(images[index]), cmap='gray')
    # print out the correct label for each image
    # .item() gets the value contained in a Tensor
    ax.set_title(str(labels[index].item()))
</pre></div>
<div class="figure">
<p><img alt="batch.png" src="/posts/nano/cnn/mnist-mlp/batch.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7916fbb">
<h3 id="org7916fbb">View a Single Image</h3>
<div class="outline-text-3" id="text-org7916fbb">
<p>Now we're going to take a closer look at the second image in the batch.</p>
<div class="highlight">
<pre><span></span>image = numpy.squeeze(images[1])

figure = pyplot.figure(figsize = (12,12)) 
ax = figure.add_subplot(111)
ax.imshow(image, cmap='gray')
width, height = image.shape
threshold = image.max()/2.5
for x in range(width):
    for y in range(height):
        val = round(image[x][y],2) if image[x][y] !=0 else 0
        ax.annotate(str(val), xy=(y,x),
                    horizontalalignment='center',
                    verticalalignment='center',
                    color='white' if image[x][y]&lt;threshold else 'black')
</pre></div>
<div class="figure">
<p><img alt="image.png" src="/posts/nano/cnn/mnist-mlp/image.png"></p>
</div>
<p>We're looking at a single image with the normalized values for each pixel superimposed on it. It looks like black is 0 and white is 1, although for this image most of the 'white' pixels are just a little less than one.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5ddd997">
<h2 id="org5ddd997">Define the Network <a href="http://pytorch.org/docs/stable/nn.html">Architecture</a></h2>
<div class="outline-text-2" id="text-org5ddd997">
<p>The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting.</p>
<p>These values are based on the <a href="https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py">keras</a> example implementation.</p>
<div class="highlight">
<pre><span></span>INPUT_NODES = 28 * 28
HIDDEN_NODES = 512
DROPOUT = 0.2
CLASSES = 10
</pre></div>
<div class="highlight">
<pre><span></span>class Net(nn.Module):
    def __init__(self):
        super().__init__()        
        self.fully_connected_layer_1 = nn.Linear(INPUT_NODES, HIDDEN_NODES)
        self.fully_connected_layer_2 = nn.Linear(HIDDEN_NODES, HIDDEN_NODES)
        self.output = nn.Linear(HIDDEN_NODES, CLASSES)
        self.dropout = nn.Dropout(p=DROPOUT)
        return

    def forward(self, x):
        # flatten image input
        x = x.view(-1, 28 * 28)
        # add hidden layer, with relu activation function
        x = self.dropout(F.relu(self.fully_connected_layer_1(x)))
        x = self.dropout(F.relu(self.fully_connected_layer_2(x)))        
        return self.output(x)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org1fc2d24">
<h3 id="org1fc2d24">Initialize the NN</h3>
<div class="outline-text-3" id="text-org1fc2d24">
<div class="highlight">
<pre><span></span>model = Net()
print(model)
</pre></div>
<pre class="example">
Net(
  (fully_connected_layer_1): Linear(in_features=784, out_features=512, bias=True)
  (fully_connected_layer_2): Linear(in_features=512, out_features=512, bias=True)
  (output): Linear(in_features=512, out_features=10, bias=True)
  (dropout): Dropout(p=0.2)
)

</pre></div>
</div>
<div class="outline-3" id="outline-container-org0afb4f6">
<h3 id="org0afb4f6">A Little CUDA</h3>
<div class="outline-text-3" id="text-org0afb4f6">
<div class="highlight">
<pre><span></span>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org2128ad9">
<h2 id="org2128ad9">Specify the <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">Loss Function</a> and <a href="http://pytorch.org/docs/stable/optim.html">Optimizer</a></h2>
<div class="outline-text-2" id="text-org2128ad9">
<p>It's recommended that you use <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">cross-entropy loss</a> for classification. If you look at the documentation you can see that PyTorch's cross entropy function applies a softmax function to the output layer <b>and</b> then calculates the log loss (so you don't want to do softmax as part of the model output).</p>
<div class="highlight">
<pre><span></span>criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org39bd2cc">
<h2 id="org39bd2cc">Train the Network</h2>
<div class="outline-text-2" id="text-org39bd2cc">
<p>The steps for training/learning from a batch of data are:</p>
<ol class="org-ol">
<li>Clear the gradients of all optimized variables</li>
<li>Forward pass: compute predicted outputs by passing inputs to the model</li>
<li>Calculate the loss</li>
<li>Backward pass: compute gradient of the loss with respect to model parameters</li>
<li>Perform a single optimization step (parameter update)</li>
<li>Update average training loss</li>
</ol>
<p>The following loop trains for 30 epochs; feel free to change this number. For now, we suggest somewhere between 20-50 epochs. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data.</p>
<div class="highlight">
<pre><span></span>EPOCHS = 30
</pre></div>
<div class="highlight">
<pre><span></span>start = datetime.now()
model.train() # prep model for training

for epoch in range(EPOCHS):
    # monitor training loss
    train_loss = 0.0
    train_losses = []
    # train the model
    for data, target in train_batches:
        # move it to the GPU or CPU
        data, target = data.to(device), target.to(device)
        # clear the gradients of all optimized variables
        optimizer.zero_grad()
        # forward pass: compute predicted outputs by passing inputs to the model
        output = model(data)
        # calculate the loss
        loss = criterion(output, target)
        # backward pass: compute gradient of the loss with respect to model parameters
        loss.backward()
        # perform a single optimization step (parameter update)
        optimizer.step()
        # update running training loss
        train_loss += loss.item() * data.size(0)

        # print training statistics 
        # calculate average loss over an epoch
    train_loss = train_loss/len(train_batches.dataset)
    train_losses.append(train_loss)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(
        epoch+1, 
        train_loss
        ))
print("Training Time: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 0.826836
Epoch: 2        Training Loss: 0.324859
Epoch: 3        Training Loss: 0.251608
Epoch: 4        Training Loss: 0.202294
Epoch: 5        Training Loss: 0.170231
Epoch: 6        Training Loss: 0.146775
Epoch: 7        Training Loss: 0.127352
Epoch: 8        Training Loss: 0.115026
Epoch: 9        Training Loss: 0.104332
Epoch: 10       Training Loss: 0.093575
Epoch: 11       Training Loss: 0.084913
Epoch: 12       Training Loss: 0.077826
Epoch: 13       Training Loss: 0.071506
Epoch: 14       Training Loss: 0.067273
Epoch: 15       Training Loss: 0.063749
Epoch: 16       Training Loss: 0.058150
Epoch: 17       Training Loss: 0.054770
Epoch: 18       Training Loss: 0.051584
Epoch: 19       Training Loss: 0.047762
Epoch: 20       Training Loss: 0.045219
Epoch: 21       Training Loss: 0.041732
Epoch: 22       Training Loss: 0.040526
Epoch: 23       Training Loss: 0.038247
Epoch: 24       Training Loss: 0.035713
Epoch: 25       Training Loss: 0.033801
Epoch: 26       Training Loss: 0.031963
Epoch: 27       Training Loss: 0.031082
Epoch: 28       Training Loss: 0.028971
Epoch: 29       Training Loss: 0.027500
Epoch: 30       Training Loss: 0.026876
Training Time: 0:05:59.808071
</pre></div>
</div>
<div class="outline-2" id="outline-container-org94441ec">
<h2 id="org94441ec">Test the Trained Network</h2>
<div class="outline-text-2" id="text-org94441ec">
<p>Finally, we test our best model on previously unseen <b>test data</b> and evaluate it's performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy.</p>
</div>
<div class="outline-3" id="outline-container-org3ce8699">
<h3 id="org3ce8699"><code>model.eval()</code></h3>
<div class="outline-text-3" id="text-org3ce8699">
<p><code>model.eval(</code>) will set all the layers in your model to evaluation mode. This affects layers like dropout layers that turn "off" nodes during training with some probability, but should allow every node to be "on" for evaluation!</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgc4bfd6e">
<h3 id="orgc4bfd6e">Set Up the Testing</h3>
<div class="outline-text-3" id="text-orgc4bfd6e">
<div class="highlight">
<pre><span></span><span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">class_correct</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mf">0.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">class_total</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mf">0.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_batches</span><span class="p">:</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># forward pass: compute predicted outputs by passing inputs to the model</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="c1"># calculate the loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="c1"># update test loss </span>
    <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># convert output probabilities to predicted class</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># compare predictions to true label</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">prediction</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">prediction</span><span class="p">)))</span>
    <span class="c1"># calculate test accuracy for each object class</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">class_correct</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="n">correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">class_total</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Test Time: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<pre class="example">
Test Time: 0:00:01.860151

</pre></div>
</div>
<div class="outline-3" id="outline-container-org792b12a">
<h3 id="org792b12a">Calculate and Print Average Test Loss</h3>
<div class="outline-text-3" id="text-org792b12a">
<div class="highlight">
<pre><span></span><span class="n">test_loss</span> <span class="o">=</span> <span class="n">test_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_batches</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Test Loss: {:.6f}</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Test Accuracy of Batch {}: {:.2f} ({}/{})'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">class_correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_correct</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Test Accuracy of {}: N/A (no training examples)'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Test Accuracy (Overall): </span><span class="si">%2d%%</span><span class="s1"> (</span><span class="si">%2d</span><span class="s1">/</span><span class="si">%2d</span><span class="s1">)'</span> <span class="o">%</span> <span class="p">(</span>
    <span class="mf">100.</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_total</span><span class="p">),</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_correct</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_total</span><span class="p">)))</span>
</pre></div>
<pre class="example">
Test Loss: 0.056054

Test Accuracy of Batch 0: 99.18 (972.0/980.0)
Test Accuracy of Batch 1: 99.21 (1126.0/1135.0)
Test Accuracy of Batch 2: 98.16 (1013.0/1032.0)
Test Accuracy of Batch 3: 98.02 (990.0/1010.0)
Test Accuracy of Batch 4: 98.47 (967.0/982.0)
Test Accuracy of Batch 5: 98.43 (878.0/892.0)
Test Accuracy of Batch 6: 98.12 (940.0/958.0)
Test Accuracy of Batch 7: 97.47 (1002.0/1028.0)
Test Accuracy of Batch 8: 97.13 (946.0/974.0)
Test Accuracy of Batch 9: 98.12 (990.0/1009.0)

Test Accuracy (Overall): 98% (9824/10000)
</pre></div>
</div>
<div class="outline-3" id="outline-container-orga46f2ae">
<h3 id="orga46f2ae">Visualize Sample Test Results</h3>
<div class="outline-text-3" id="text-orga46f2ae">
<p>This cell displays test images and their labels in this format: <code>predicted (ground-truth)</code>. The text will be green for accurately classified examples and red for incorrect predictions.</p>
</div>
<div class="outline-4" id="outline-container-org60b86fe">
<h4 id="org60b86fe">Obtain One Batch of Test Images</h4>
<div class="outline-text-4" id="text-org60b86fe">
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_batches</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

<span class="c1"># get sample outputs</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="c1"># convert output probabilities to predicted class</span>
<span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># prep images for display</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1"># plot the images in the batch, along with predicted and true labels</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">idx</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"{} ({})"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span> <span class="nb">str</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())),</span>
                 <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="s2">"green"</span> <span class="k">if</span> <span class="n">preds</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">==</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">else</span> <span class="s2">"red"</span><span class="p">))</span>
</pre></div>
<div class="figure">
<p><img alt="test.png" src="/posts/nano/cnn/mnist-mlp/test.png"></p>
</div>
<p>This model is surprisingly accurate. I say surprising, even though we created a very accurate model previously, because in my original implementation I used <code>RMSprop</code> as the optimizer, because that's what the Keras implementation used, but then I only got 11%. I'm guessing that there's some extra tuning you need to do to the parameters for <code>RMSprop</code> but I just naively used the defaults. In any case, it semms that SGD is still the champ.</p>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/">Dog Classification Project Overview</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/" rel="bookmark"><time class="published dt-published" datetime="2018-11-25T16:33:14-08:00" itemprop="datePublished" title="2018-11-25 16:33">2018-11-25 16:33</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/#orgc208ab0">Project Overview</a></li>
<li><a href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/#org931df8a">The Data</a></li>
<li><a href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/#orgf3d7ba7">Some Rules</a></li>
<li><a href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/#orge43cee8">(Optionally) Accelerating the Training Process</a></li>
<li><a href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/#org7175b0d">Evaluation</a></li>
<li><a href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/#org70082e5">Project Submission</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgc208ab0">
<h2 id="orgc208ab0">Project Overview</h2>
<div class="outline-text-2" id="text-orgc208ab0">
<p>In this project we will build a pipeline that can be used within a web or mobile app to process real-world, user-supplied images. Given an image of a dog, our algorithm will identify an estimate of the canine’s breed. If supplied an image of a human, the code will identify the dog breed that the person most resembles.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org931df8a">
<h2 id="org931df8a">The Data</h2>
<div class="outline-text-2" id="text-org931df8a">
<p>The <a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip">dog dataset</a> is in a zip-file hosted on Amazon Web Services. The folder should contain three folders (<code>test</code>, <code>train</code>, and <code>valid</code>) and each of these folders should have 133 folders, one for each dog-breed. It looks like the <a href="http://vision.stanford.edu/aditya86/ImageNetDogs/">Stanford Dogs Dataset</a>, but the Stanford data set has 120 breeds, so I don't know the actual source. The <a href="http://vis-www.cs.umass.edu/lfw/lfw.tgz">human dataset</a> seems to be the <a href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in the Wild</a> data set which was built to study the problem of facial recognition. It's made up of real photos of people taken from the web. Each photo sits in a sub-folder that was given the name of the person (e.g. <code>Michelle_Yeoh</code>). The folder hasn't been split into train-test-validiation folders the way the dog dataset was.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgf3d7ba7">
<h2 id="orgf3d7ba7">Some Rules</h2>
<div class="outline-text-2" id="text-orgf3d7ba7">
<ul class="org-ul">
<li><b>Unless requested, do not modify code that has already been included.</b></li>
<li>In the notebook, you will need to train CNNs in PyTorch. If your CNN is taking too long to train, feel free to pursue one of the options under the section <i>Accelerating the Training Process</i> below.</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orge43cee8">
<h2 id="orge43cee8">(Optionally) Accelerating the Training Process</h2>
<div class="outline-text-2" id="text-orge43cee8">
<p>If your code is taking too long to run, you will need to either reduce the complexity of your chosen CNN architecture or switch to running your code on a GPU. If you'd like to use a GPU, you can spin up an instance of your own:</p>
</div>
<div class="outline-3" id="outline-container-org333d22c">
<h3 id="org333d22c">Amazon Web Services</h3>
<div class="outline-text-3" id="text-org333d22c">
<p>You can use Amazon Web Services to launch an EC2 GPU instance. (This costs money, but enrolled students should see a coupon code in their student <code>resources</code>.)</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7175b0d">
<h2 id="org7175b0d">Evaluation</h2>
<div class="outline-text-2" id="text-org7175b0d">
<p>Your project will be reviewed by a Udacity reviewer against the CNN project rubric. Review this rubric thoroughly and self-evaluate your project before submission. All criteria found in the rubric must meet specifications for you to pass.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org70082e5">
<h2 id="org70082e5">Project Submission</h2>
<div class="outline-text-2" id="text-org70082e5">
<p>Your submission should consist of the github link to your repository. Your repository should contain:</p>
<ul class="org-ul">
<li>The <code>dog_app.ipynb</code> file with fully functional code, all code cells executed and displaying output, and all questions answered.</li>
<li>An HTML or PDF export of the project notebook with the name <code>report.html</code> or <code>report.pdf</code>.</li>
</ul>
<p>Please do <i>NOT</i> include any of the project data sets provided in the <code>dogImages/</code> or <code>lfw/</code> folders.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/transfer-learning-one-more-time/">Transfer Learning One More Time</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/transfer-learning-one-more-time/" rel="bookmark"><time class="published dt-published" datetime="2018-11-25T14:55:58-08:00" itemprop="datePublished" title="2018-11-25 14:55">2018-11-25 14:55</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/transfer-learning-one-more-time/#org162cdd6">Introduction</a></li>
<li><a href="/posts/nano/pytorch/transfer-learning-one-more-time/#orgcc396d4">Set Up</a></li>
<li><a href="/posts/nano/pytorch/transfer-learning-one-more-time/#org5ea7f5e">The Data</a></li>
<li><a href="/posts/nano/pytorch/transfer-learning-one-more-time/#orga848ca9">The DenseNet Model</a></li>
<li><a href="/posts/nano/pytorch/transfer-learning-one-more-time/#org57e049d">Add Some CUDA</a></li>
<li><a href="/posts/nano/pytorch/transfer-learning-one-more-time/#org481e24c">Train It</a></li>
</ul>
</div>
</div>
<p>I spent so much time debugging the original post that I though I'd re-do it without all the flailing around.</p>
<div class="outline-2" id="outline-container-org162cdd6">
<h2 id="org162cdd6">Introduction</h2>
<div class="outline-text-2" id="text-org162cdd6">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>This uses a model trained on <a href="http://www.image-net.org/">ImageNet</a> (<a href="http://pytorch.org/docs/0.3.0/torchvision/models.html">available from torchvision</a>) to classify the <a href="https://www.kaggle.com/c/dogs-vs-cats">dataset of cat and dog photos</a> that we used earlier. We're going to use a method called <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> where we will use the layers of the pretrained model all the way up until the final classifier which we will define ourselves and train on our new data-set. This way we can take advantage of what the model has already learned for image detection and only train a few layers.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgcc396d4">
<h2 id="orgcc396d4">Set Up</h2>
<div class="outline-text-2" id="text-orgcc396d4"></div>
<div class="outline-3" id="outline-container-org3629617">
<h3 id="org3629617">Imports</h3>
<div class="outline-text-3" id="text-org3629617"></div>
<div class="outline-4" id="outline-container-org7a5127d">
<h4 id="org7a5127d">Python</h4>
<div class="outline-text-4" id="text-org7a5127d">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgeac3af7">
<h4 id="orgeac3af7">PyPi</h4>
<div class="outline-text-4" id="text-orgeac3af7">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf73c784">
<h4 id="orgf73c784">This Project</h4>
<div class="outline-text-4" id="text-orgf73c784">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPathTwo</span>
<span class="kn">from</span> <span class="nn">neurotic.models.fashion</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">train_only</span><span class="p">,</span>
    <span class="n">test_only</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfb62a78">
<h3 id="orgfb62a78">Dotenv</h3>
<div class="outline-text-3" id="text-orgfb62a78">
<p>For some reason dotenv has stopped working unless it's called in the notebook. Maybe this will fix it</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5ea7f5e">
<h2 id="org5ea7f5e">The Data</h2>
<div class="outline-text-2" id="text-org5ea7f5e">
<p>We're going to have to resize the images to be 224x224 to work with the pre-trained models and match the means (<code>[0.485, 0.456, 0.406]</code>) and the standard deviations (<code>[0.229, 0.224, 0.225]</code>) that were used to normalize the original data set.</p>
<div class="highlight">
<pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]</span>
<span class="n">deviations</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]</span>
<span class="n">PIXELS</span> <span class="o">=</span> <span class="mi">224</span>

<span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">PIXELS</span><span class="p">),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">means</span><span class="p">,</span>
                                                            <span class="n">deviations</span><span class="p">)])</span>

<span class="n">test_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="n">PIXELS</span><span class="p">),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">means</span><span class="p">,</span>
                                                           <span class="n">deviations</span><span class="p">)])</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-org10ccd31">
<h3 id="org10ccd31">Load the Data</h3>
<div class="outline-text-3" id="text-org10ccd31">
<p>As I mentioned we're using the same Cat and Dog images as before. So first I make my path-setter (which maybe isn't as useful as it was when I had dotenv working better).</p>
<div class="highlight">
<pre><span></span><span class="n">train_path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"CAT_DOG_TRAIN"</span><span class="p">)</span>
<span class="n">test_path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"CAT_DOG_TEST"</span><span class="p">)</span>
</pre></div>
<p>So now we set up the testing and training data sets.</p>
<div class="highlight">
<pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">train_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span>
                                  <span class="n">transform</span><span class="o">=</span><span class="n">train_transforms</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">test_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span>
                                 <span class="n">transform</span><span class="o">=</span><span class="n">test_transforms</span><span class="p">)</span>
</pre></div>
<p>And create the batch-iterators with a batch-size of 64.</p>
<div class="highlight">
<pre><span></span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">train_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                            <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga848ca9">
<h2 id="orga848ca9">The DenseNet Model</h2>
<div class="outline-text-2" id="text-orga848ca9">
<p>I'm going to load the <a href="http://pytorch.org/docs/0.3.0/torchvision/models.html#id5">DenseNet</a> model.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">densenet121</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>It actually emits a warning that the code is using an incorrect method call somewhere, but I'll ignore that.</p>
<pre class="example">
UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
 nn.init.kaiming_normal(m.weight.data)
</pre></div>
<div class="outline-3" id="outline-container-org4e79474">
<h3 id="org4e79474">Freeze The Model Parameters</h3>
<div class="outline-text-3" id="text-org4e79474">
<p>We need to freeze the parameters before training so we don't end up trying to re-train our pre-trained network.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org72d17f0">
<h3 id="org72d17f0">The Classifier</h3>
<div class="outline-text-3" id="text-org72d17f0">
<p>So this is the part where we add our own classifier at the end so that we can train it on cats and dogs. I'll use the original 500 fully connected nodes instead of the 256 I ended up with in my previous attempt.</p>
<p>To figure out the inputs to the layer we can just look at the original <code>classifier</code> layer in the model.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="p">)</span>
</pre></div>
<pre class="example">
Linear(in_features=1024, out_features=1000, bias=True)

</pre>
<p>So we need to make sure we have 1,024 inputs to our classification layer and change the number of outputs to 2 (since we have only dogs and cats). We're also going to use two layers, the first one will have a ReLU activation and the second (the output) will have a <a href="https://pytorch.org/docs/stable/nn.html?highlight=logsoftmax#torch.nn.LogSoftmax">Log-Softmax</a> activation.</p>
<div class="highlight">
<pre><span></span><span class="n">HIDDEN_NODES</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">INPUT_NODES</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">OUTPUT_NODES</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                          <span class="p">(</span><span class="s1">'fully_connected_layer'</span><span class="p">,</span>
                           <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">INPUT_NODES</span><span class="p">,</span> <span class="n">HIDDEN_NODES</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                          <span class="p">(</span><span class="s2">"dropout"</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s1">'fully_connected_layer_2'</span><span class="p">,</span>
                           <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_NODES</span><span class="p">,</span> <span class="n">OUTPUT_NODES</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s1">'output'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
                          <span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span>
</pre></div>
<p>So we now have a (mostly) pre-trained deep neural network with an untrained classifier.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org57e049d">
<h2 id="org57e049d">Add Some CUDA</h2>
<div class="outline-text-2" id="text-org57e049d">
<p>To speed this up somewhat I'll add (if it's available) a little cuda.</p>
<div class="highlight">
<pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-org753cf77">
<h3 id="org753cf77">Add some more CUDA</h3>
<div class="outline-text-3" id="text-org753cf77">
<p>This next bit doesn't work on any of my machines, but maybe someday.</p>
<div class="highlight">
<pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Using {} GPUs"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Only 1 GPU available"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Only 1 GPU available

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org481e24c">
<h2 id="org481e24c">Train It</h2>
<div class="outline-text-2" id="text-org481e24c">
<p>First we'll set up our criterion - Negative Log Likelihood Loss (<a href="https://pytorch.org/docs/stable/nn.html?highlight=nllloss#torch.nn.NLLLoss">NLLLoss</a>) and optimizer - <a href="https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam">Adam</a> Optimization. Amazingly this only needs one pass through the data set. There's 352 batches in the training data-set so I won't print out each of the outcomes for the epochs.</p>
<div class="highlight">
<pre><span></span><span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.003</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">train_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
                     <span class="n">train_batches</span><span class="p">,</span>
                     <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">emit</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Training Time: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<pre class="example">
Training Time: 0:10:35.847469

</pre>
<div class="highlight">
<pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="n">test_outcome</span> <span class="o">=</span> <span class="n">test_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_batches</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Test Time: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<pre class="example">
Test Time: 0:00:46.695136

</pre>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">test_outcome</span><span class="p">)</span>
</pre></div>
<pre class="example">
0.9788

</pre>
<p>The key bit here was that I was earlier forgetting to add dropout, dropping the accuracy to between .5 and .6.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/tips-tricks-and-other-notes/">Tips, Tricks and Other Notes</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/tips-tricks-and-other-notes/" rel="bookmark"><time class="published dt-published" datetime="2018-11-25T14:13:27-08:00" itemprop="datePublished" title="2018-11-25 14:13">2018-11-25 14:13</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/tips-tricks-and-other-notes/#org1d06731">On Shapes</a></li>
<li><a href="/posts/nano/pytorch/tips-tricks-and-other-notes/#orgedf6310">Troubleshooting Training</a></li>
<li><a href="/posts/nano/pytorch/tips-tricks-and-other-notes/#orgb427f1c">CUDA Problems</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org1d06731">
<h2 id="org1d06731">On Shapes</h2>
<div class="outline-text-2" id="text-org1d06731">
<p>As the tensors go through the model you should check the shapes to make sure they are correct (or at least what you expect).</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgedf6310">
<h2 id="orgedf6310">Troubleshooting Training</h2>
<div class="outline-text-2" id="text-orgedf6310">
<ul class="org-ul">
<li>Make sure you are clearing the gradients in the training loop with <code>optimizer.zero_grad()</code></li>
<li>In the validation loop, set the network to evaluation mode with <code>model.eval()</code> and then back to training mode with <code>model.train</code></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgb427f1c">
<h2 id="orgb427f1c">CUDA Problems</h2>
<div class="outline-text-2" id="text-orgb427f1c">
<p>If you see an error saying pytorch <code>Expected an object of type torch.FloatTensor but found type torch.cuda.FloatTensor</code> then it means something is trying to be run on the CPU but something else wants to use the GPU. Make sure you called <code>.to(device)</code> on the model and all your tensors (including the data).</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/part-8-transfer-learning/">Part 8 - Transfer Learning</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/part-8-transfer-learning/" rel="bookmark"><time class="published dt-published" datetime="2018-11-23T18:01:33-08:00" itemprop="datePublished" title="2018-11-23 18:01">2018-11-23 18:01</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/part-8-transfer-learning/#org63913ea">Introduction</a></li>
<li><a href="/posts/nano/pytorch/part-8-transfer-learning/#org20af05d">Set Up</a></li>
<li><a href="/posts/nano/pytorch/part-8-transfer-learning/#org9cc81f6">The Data</a></li>
<li><a href="/posts/nano/pytorch/part-8-transfer-learning/#org2d0b79b">The DenseNet Model</a></li>
<li><a href="/posts/nano/pytorch/part-8-transfer-learning/#org4c9e57e">Using CUDA</a></li>
<li><a href="/posts/nano/pytorch/part-8-transfer-learning/#orge17859f">Train the Model</a></li>
<li><a href="/posts/nano/pytorch/part-8-transfer-learning/#orgf9cb592">Train Some More</a></li>
<li><a href="/posts/nano/pytorch/part-8-transfer-learning/#org6984cfb">Another Model</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org63913ea">
<h2 id="org63913ea">Introduction</h2>
<div class="outline-text-2" id="text-org63913ea">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>In this notebook, you'll learn how to use pre-trained networks to solved challenging problems in computer vision. Specifically, you'll use networks trained on <a href="http://www.image-net.org/">ImageNet</a> (<a href="http://pytorch.org/docs/0.3.0/torchvision/models.html">available from torchvision</a>).</p>
<p>ImageNet is a massive dataset with over 1 million labeled images in 1000 categories. It's used to train deep neural networks using an architecture called convolutional layers. I'm not going to get into the details of convolutional networks here, but if you want to learn more about them, please <a href="https://www.youtube.com/watch?v=2-Ol7ZB0MmU">watch this</a>.</p>
<p>Once trained, these models work astonishingly well as feature detectors for images they weren't trained on. Using a pre-trained network on images not in the training set is called transfer learning. Here we'll use transfer learning to train a network that can classify our cat and dog photos with near perfect accuracy.</p>
<p>With <a href="https://pytorch.org/docs/stable/torchvision/models.html"><code>torchvision.models</code></a> you can download these pre-trained networks and use them in your applications. We'll include <code>models</code> in our imports now.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org20af05d">
<h2 id="org20af05d">Set Up</h2>
<div class="outline-text-2" id="text-org20af05d"></div>
<div class="outline-3" id="outline-container-orgea40e34">
<h3 id="orgea40e34">Imports</h3>
<div class="outline-text-3" id="text-orgea40e34"></div>
<div class="outline-4" id="outline-container-org547d8ca">
<h4 id="org547d8ca">Python</h4>
<div class="outline-text-4" id="text-org547d8ca">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org9646074">
<h4 id="org9646074">PyPi</h4>
<div class="outline-text-4" id="text-org9646074">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">models</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org7484714">
<h4 id="org7484714">This Project</h4>
<div class="outline-text-4" id="text-org7484714">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPathTwo</span>
<span class="kn">from</span> <span class="nn">neurotic.models.fashion</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">test_only</span><span class="p">,</span>
    <span class="n">train_only</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb7413c2">
<h3 id="orgb7413c2">Dotenv</h3>
<div class="outline-text-3" id="text-orgb7413c2">
<p>For some reason dotenv has stopped working unless it's called in the notebook. Maybe this will fix it</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org9cc81f6">
<h2 id="org9cc81f6">The Data</h2>
<div class="outline-text-2" id="text-org9cc81f6">
<p>Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are <code>[0.485, 0.456, 0.406]</code> and the standard deviations are <code>[0.229, 0.224, 0.225]</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]</span>
<span class="n">deviations</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]</span>

<span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">means</span><span class="p">,</span>
                                                            <span class="n">deviations</span><span class="p">)])</span>

<span class="n">test_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">means</span><span class="p">,</span>
                                                           <span class="n">deviations</span><span class="p">)])</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgd6ac197">
<h3 id="orgd6ac197">Load the Data</h3>
<div class="outline-text-3" id="text-orgd6ac197">
<p>We're going to load the Cat-Dog data set again.</p>
<div class="highlight">
<pre><span></span><span class="n">train_path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"CAT_DOG_TRAIN"</span><span class="p">)</span>
<span class="n">test_path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"CAT_DOG_TEST"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">train_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">train_transforms</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">test_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">test_transforms</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">train_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                            <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org2d0b79b">
<h2 id="org2d0b79b">The DenseNet Model</h2>
<div class="outline-text-2" id="text-org2d0b79b">
<p>We are going to load the <a href="http://pytorch.org/docs/0.3.0/torchvision/models.html#id5">DenseNet</a> model.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">densenet121</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example">
DenseNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace)
    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (denseblock1): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition2): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock3): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition3): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (classifier): Linear(in_features=1024, out_features=1000, bias=True)
)
</pre>
<p>This model is built out of two main parts, the features and the classifier. The features part is a stack of convolutional layers and overall works as a feature detector that can be fed into a classifier. The classifier part is a single fully-connected layer <code>(classifier): Linear(in_features=1024, out_features=1000)</code>. This layer was trained on the ImageNet dataset, so it won't work for our specific problem. That means we need to replace the classifier, but the features will work perfectly on their own. In general, I think about pre-trained networks as amazingly good feature detectors that can be used as the input for simple feed-forward classifiers.</p>
<p>Next we want to freeze the parameters so we don't backprop through them.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">False</span>
</pre></div>
<p>And now we build our classifier model.</p>
<div class="highlight">
<pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                          <span class="p">(</span><span class="s1">'fc1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">500</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                          <span class="p">(</span><span class="s1">'fc2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s1">'output'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
                          <span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org4c9e57e">
<h2 id="org4c9e57e">Using CUDA</h2>
<div class="outline-text-2" id="text-org4c9e57e">
<p>With our model built, we need to train the classifier. However, now we're using a <b>really deep</b> neural network. If you try to train this on a CPU like normal, it will take a long, long time. Instead, we're going to use the GPU to do the calculations. The linear algebra computations are done in parallel on the GPU leading to 100x increased training speeds. It's also possible to train on multiple GPUs, further decreasing training time.</p>
<p>PyTorch, along with pretty much every other deep learning framework, uses <a href="https://developer.nvidia.com/cuda-zone">CUDA</a> to efficiently compute the forward and backwards passes on the GPU. In PyTorch, you move your model parameters and other tensors to the GPU memory using <code>model.to('cuda')</code>. You can move them back from the GPU with <code>model.to('cpu')</code> which you'll commonly do when you need to operate on the network output outside of PyTorch. As a demonstration of the increased speed, I'll compare how long it takes to perform a forward and backward pass with and without a GPU.</p>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">"cpu"</span>
<span class="c1"># Only train the classifier parameters, feature parameters are frozen</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_batches</span><span class="p">):</span>
    <span class="c1"># Move input and label tensors to the GPU</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">index</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
        <span class="k">break</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Device = {}; Time per batch: {} seconds"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>
    <span class="p">))</span>
</pre></div>
<pre class="example">
Device = cpu; Time per batch: 0:00:12.372973 seconds

</pre>
<div class="highlight">
<pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="c1"># Only train the classifier parameters, feature parameters are frozen</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_batches</span><span class="p">):</span>
    <span class="c1"># Move input and label tensors to the GPU</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">index</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
        <span class="k">break</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Device = {}; Time per batch: {} seconds"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">device</span><span class="p">,</span> <span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span>
<span class="p">))</span>
</pre></div>
<pre class="example">
Device = cuda; Time per batch: 0:00:00.008037 seconds

</pre>
<p>So, it takes less than a second compared to 12 seconds. Interestingly, I kept getting a CUDA out of memory error when I had seaborn and matplotlib imported at the top. I don't know what the conflict is, but it's something to watch out for.</p>
<p>You can write device agnostic code which will automatically use CUDA if it's enabled like so at the beginning of your code:</p>
<div class="highlight">
<pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda:0"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
</pre></div>
<p>Then whenever you get a new Tensor or Module it won't copy if they are already on the desired device (it will just return the original object).</p>
<div class="highlight">
<pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
<p>First a short test to make sure this works.</p>
<div class="highlight">
<pre><span></span><span class="n">train_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_batches</span><span class="p">)</span>
<span class="n">train_small</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_iter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
<span class="n">test_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_batches</span><span class="p">)</span>
<span class="n">test_small</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_iter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">train_small</span><span class="p">,</span> <span class="n">test_small</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Epoch: 1/30 Training loss: 0.43 Test Loss: 2.63 Test Accuracy: 0.56

</pre></div>
</div>
<div class="outline-2" id="outline-container-orge17859f">
<h2 id="orge17859f">Train the Model</h2>
<div class="outline-text-2" id="text-orge17859f">
<p>Okay, so now for a long one. Time to get some coffee.</p>
</div>
<div class="outline-3" id="outline-container-orgf338d87">
<h3 id="orgf338d87">Setup CUDA If It's Available</h3>
<div class="outline-text-3" id="text-orgf338d87">
<div class="highlight">
<pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0749395">
<h3 id="org0749395">The Training</h3>
<div class="outline-text-3" id="text-org0749395">
<div class="highlight">
<pre><span></span><span class="o">%</span><span class="n">time</span>

<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">train_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">train_batches</span><span class="p">,</span>
                     <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">"cat_dog_model.pth"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc6ee932">
<h3 id="orgc6ee932">The Accuracy</h3>
<div class="outline-text-3" id="text-orgc6ee932">
<div class="highlight">
<pre><span></span><span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_batches</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">top_p</span><span class="p">,</span> <span class="n">top_class</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">equals</span> <span class="o">=</span> <span class="n">top_class</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">top_class</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">equals</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">mean_accuracy</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_batches</span><span class="p">)</span>
        <span class="n">test_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_batches</span><span class="p">))</span>
        <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_accuracy</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">"Final Loss: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Final Accuracy: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracies</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
<pre class="example">
Final Loss: 1.22
Final Accuracy: 0.64

</pre>
<p>So still not quite good enough.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgf9cb592">
<h2 id="orgf9cb592">Train Some More</h2>
<div class="outline-text-2" id="text-orgf9cb592">
<div class="highlight">
<pre><span></span><span class="n">outcome</span> <span class="o">=</span> <span class="n">train_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">train_batches</span><span class="p">,</span>
                     <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">"cat_dog_model.pth"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_outcome</span> <span class="o">=</span> <span class="n">test_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">test_batches</span><span class="p">,</span> <span class="n">devicej</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">test_outcome</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
Test Loss        1.532174
Test Accuracy    0.630859
Name: 39, dtype: float64

</pre>
<p>So, it hasn't actually gotten better, if anything it got worse. Does this mean it's overfitting?</p>
</div>
</div>
<div class="outline-2" id="outline-container-org6984cfb">
<h2 id="org6984cfb">Another Model</h2>
<div class="outline-text-2" id="text-org6984cfb">
<p>I peeked at the solution notebook and it has fewer nodes in the first linear layer and adds dropout. Interestingly the lecture has more nodes in the first layer, but I'll try fewer first.</p>
</div>
<div class="outline-3" id="outline-container-orga6145db">
<h3 id="orga6145db">The Classifier</h3>
<div class="outline-text-3" id="text-orga6145db">
<div class="highlight">
<pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                          <span class="p">(</span><span class="s2">"fully_connected_layer"</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                          <span class="p">(</span><span class="s2">"dropout"</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s1">'fully_connected_2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
                          <span class="p">(</span><span class="s1">'output'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
                          <span class="p">]))</span>
<span class="n">model</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
<p>Note that I had to do the <code>model.to(device)</code> call again since I added the classifier. I think I could also have done <code>classifier.to(device)</code>, but this seemed to work.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org123a736">
<h3 id="org123a736">More Parallelization</h3>
<div class="outline-text-3" id="text-org123a736">
<p>I noticed on the <a href="https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html">pytorch data parallelization</a> tutorial that they said you need to tell pytorch to use more than one GPU (if you want it to) so I'm going to try and add it here.</p>
<div class="highlight">
<pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Using {} GPUs"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Only 1 GPU available"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Only 1 GPU available

</pre>
<p>Oh, well.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgc19caf1">
<h3 id="orgc19caf1">The Criterion and Optimizer</h3>
<div class="outline-text-3" id="text-orgc19caf1">
<p>The other notebook also used a slightly higher learning rate which I'll copy. It also managed to get 95% with one epoch, which is totally out of whack with what I'm seeing. I'll try it again.</p>
<div class="highlight">
<pre><span></span><span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.003</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
<p>Our loss and optimizer.</p>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
</pre></div>
<p>Now train on one epoch.</p>
<div class="highlight">
<pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">train_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">train_batches</span><span class="p">,</span>
                     <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">"cat_dog_model.pth"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Training Time: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<pre class="example">
Training Time: 0:06:28.712052

</pre>
<div class="highlight">
<pre><span></span><span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="n">test_outcome</span> <span class="o">=</span> <span class="n">test_only</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_batches</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Test Time: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<pre class="example">
Test Time: 0:00:42.637106

</pre>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">test_outcome</span><span class="p">)</span>
</pre></div>
<pre class="example">
0.9776

</pre>
<p>Okay, so I changed the test_only function to use <code>model.eval</code> instead of <code>model.no_grad</code> like we were doing before and it went from 51% to 98%. Hmm…</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/part-7-loading-image-data/">Part 7 - Loading Image Data</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/part-7-loading-image-data/" rel="bookmark"><time class="published dt-published" datetime="2018-11-22T17:08:56-08:00" itemprop="datePublished" title="2018-11-22 17:08">2018-11-22 17:08</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/part-7-loading-image-data/#org284d4e4">Introduction</a></li>
<li><a href="/posts/nano/pytorch/part-7-loading-image-data/#orgf44ef33">Set Up</a></li>
<li><a href="/posts/nano/pytorch/part-7-loading-image-data/#org1d34725">The Data</a></li>
<li><a href="/posts/nano/pytorch/part-7-loading-image-data/#org84740b8">Data Augmentation</a></li>
<li><a href="/posts/nano/pytorch/part-7-loading-image-data/#org808bdf7">A Naive Dropout model</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org284d4e4">
<h2 id="org284d4e4">Introduction</h2>
<div class="outline-text-2" id="text-org284d4e4">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>So far we've been working with fairly artificial datasets that you wouldn't typically be using in real projects (28 x 28 pixels is very low resolution). Instead, you'll likely be dealing with full-sized images like you'd get from cameras. In this notebook, we'll look at how to load images and use them to train neural networks.</p>
<p>We'll be using a <a href="https://www.kaggle.com/c/dogs-vs-cats">dataset of cat and dog photos</a> available from Kaggle that was created to test whether a machine would be able to defeat the <a href="https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual-image-categorization/">Asirra</a> <a href="https://en.wikipedia.org/wiki/CAPTCHA">CAPTCHA</a> system by identifying whether an image had a cat or a dog.</p>
<p>We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgf44ef33">
<h2 id="orgf44ef33">Set Up</h2>
<div class="outline-text-2" id="text-orgf44ef33"></div>
<div class="outline-3" id="outline-container-orgd641c83">
<h3 id="orgd641c83">Imports</h3>
<div class="outline-text-3" id="text-orgd641c83"></div>
<div class="outline-4" id="outline-container-orgeef7a9e">
<h4 id="orgeef7a9e">PyPi</h4>
<div class="outline-text-4" id="text-orgeef7a9e">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6a57e92">
<h4 id="org6a57e92">Udacity Code</h4>
<div class="outline-text-4" id="text-org6a57e92">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">nano.pytorch</span> <span class="kn">import</span> <span class="n">helper</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgdfd9e81">
<h4 id="orgdfd9e81">This Project</h4>
<div class="outline-text-4" id="text-orgdfd9e81">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPathTwo</span>
<span class="kn">from</span> <span class="nn">neurotic.models.fashion</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DropoutModel</span><span class="p">,</span>
    <span class="n">train</span><span class="p">,</span>
    <span class="n">HyperParameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgaa469d4">
<h3 id="orgaa469d4">Plotting</h3>
<div class="outline-text-3" id="text-orgaa469d4">
<div class="highlight">
<pre><span></span><span class="n">get_python</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_python</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>

<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1d34725">
<h2 id="org1d34725">The Data</h2>
<div class="outline-text-2" id="text-org1d34725">
<p>The easiest way to load image data is with <a href="http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder"><code>datasets.ImageFolder</code></a> from <code>torchvision</code>. In general you'll use <code>ImageFolder</code> like so:</p>
<div class="highlight">
<pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="s1">'path/to/data'</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">)</span>
</pre></div>
<p>where <code>path/to/data</code> is the file path to the data directory and <code>transforms</code> is a list of processing steps built with the <a href="http://pytorch.org/docs/master/torchvision/transforms.html"><code>transforms</code></a> module from <code>torchvision</code>. ImageFolder expects the files and directories to be constructed like so:</p>
<pre class="example">
root/dog/xxx.png
root/dog/xxy.png
root/dog/xxz.png

root/cat/123.png
root/cat/nsdf3.png
root/cat/asd932_.png
</pre>
<p>where each class has it's own directory (<code>cat</code> and <code>dog</code>) for the images. The images are then labeled with the class taken from the directory name. So here, the image <code>123.png</code> would be loaded with the class label <code>cat</code>. You can download the dataset already structured like this <a href="https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip">from here</a>. I've also split it into a training set and test set (note that the data-set is almost 600 Megabytes so make sure you have broadband if you want to download it).</p>
</div>
<div class="outline-3" id="outline-container-orga7040fd">
<h3 id="orga7040fd">Transforms</h3>
<div class="outline-text-3" id="text-orga7040fd">
<p>When you load in the data with <code>ImageFolder</code>, you'll need to define some transforms. For example, the images are different sizes but we'll need them to all be the same size for training. You can either resize them with <code>transforms.Resize()</code> or crop with <code>transforms.CenterCrop()</code>, <code>transforms.RandomResizedCrop()</code>, etc. We'll also need to convert the images to PyTorch tensors with <code>transforms.ToTensor()</code>. Typically you'll combine these transforms into a pipeline with <code>transforms.Compose()</code>, which accepts a list of transforms and runs them in sequence. It looks something like this to scale, then crop, then convert to a tensor:</p>
<div class="highlight">
<pre><span></span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
                                 <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                 <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()])</span>
</pre></div>
<p>There are plenty of transforms available, you should read through the <a href="http://pytorch.org/docs/master/torchvision/transforms.html">documentation</a>.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org9f8ae49">
<h3 id="org9f8ae49">Data Loaders</h3>
<div class="outline-text-3" id="text-org9f8ae49">
<p>With the <code>ImageFolder</code> loaded, you have to pass it to a <a href="http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader"><code>DataLoader</code></a>. The <code>DataLoader</code> takes a dataset (such as you would get from <code>ImageFolder</code>) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.</p>
<div class="highlight">
<pre><span></span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>Here <code>dataloader</code> is a <a href="https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained">generator</a>. To get data out of it, you need to loop through it or convert it to an iterator and call <code>next()</code>.</p>
<p>Looping through it, get a batch on each loop:</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="c1"># Get one batch</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org764a3cb">
<h3 id="org764a3cb">Actually Load the Data</h3>
<div class="outline-text-3" id="text-org764a3cb">
<p>Now we're going to actually do what we spoke of earlier.</p>
</div>
<div class="outline-4" id="outline-container-orgc155073">
<h4 id="orgc155073">Set the Path</h4>
<div class="outline-text-4" id="text-orgc155073">
<p>This is where we set the folder path. The actual data-set was a zipped folder on an amazon web server so I downloaded it by hand instead of using the <code>datasets</code> method like we did with the earlier data sets.</p>
<div class="highlight">
<pre><span></span><span class="n">train_path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"CAT_DOG_TRAIN"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga2d7406">
<h4 id="orga2d7406">Transform the Data</h4>
<div class="outline-text-4" id="text-orga2d7406">
<p>We're going to:</p>
<ul class="org-ul">
<li>resize the images (passing in a single number means it will match the smallest side (height or width))</li>
<li>crop the images (CenterCrop means it measures from the center, and a single value makes it a square)</li>
<li>convert the image to a tensor</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">transformations</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc57b0e5">
<h4 id="orgc57b0e5">Load the Training Image Folder</h4>
<div class="outline-text-4" id="text-orgc57b0e5">
<div class="highlight">
<pre><span></span><span class="n">training</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">train_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span>
                                <span class="n">transform</span><span class="o">=</span><span class="n">transformations</span><span class="p">)</span>
</pre></div>
<p>The <code>ImageLoader</code> couldn't handle the <code>~</code> in my path so I changed the <code>DataPathTwo</code> to expand it by default. Now we'll load the data into an iterator that hands out batches of 32 images.</p>
<div class="highlight">
<pre><span></span><span class="n">training_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">training</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>Now we can test the data loader.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">training_batches</span><span class="p">))</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="test_loader.png" src="/posts/nano/pytorch/part-7-loading-image-data/test_loader.png"></p>
</div>
<p>If it worked we should see something that looks like a dog or a cat in a square image.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org84740b8">
<h2 id="org84740b8">Data Augmentation</h2>
<div class="outline-text-2" id="text-org84740b8">
<p>A common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and/or crop your images during training. This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.</p>
<p>To randomly rotate, scale and crop, then flip your images you would define your transforms like this:</p>
<div class="highlight">
<pre><span></span><span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> 
                                                            <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])])</span>
</pre></div>
<p>You'll also typically want to normalize images with <code>transforms.Normalize</code>. You pass in a list of means and list of standard deviations, then the color channels are normalized like so</p>
<div class="highlight">
<pre><span></span><span class="nb">input</span><span class="p">[</span><span class="n">channel</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">channel</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">[</span><span class="n">channel</span><span class="p">])</span> <span class="o">/</span> <span class="n">std</span><span class="p">[</span><span class="n">channel</span><span class="p">]</span>
</pre></div>
<p>Subtracting <code>mean</code> centers the data around zero and dividing by <code>std</code> squishes the values to be between -1 and 1. Normalizing helps keep the network work weights near zero which in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn.</p>
<p>You can find a list of all the available transforms <a href="http://pytorch.org/docs/0.3.0/torchvision/transforms.html">here</a> . When you're testing however, you'll want to use images that aren't altered (except you'll need to normalize the same way). So, for validation/test images, you'll typically just resize and crop.</p>
<p>The Training Transformations:</p>
<ul class="org-ul">
<li>RandomRotation: takes the maximum number of degrees to rotate the image</li>
<li>RandomResizedCrop: scales and crops the image - we're only passing in the expected output size</li>
<li>RandomHorizontalFlip: 50-50 chance that the image will be flipped horizontally.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="n">deviations</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mi">30</span><span class="p">),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="mi">100</span><span class="p">),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                       <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> 
                                                            <span class="n">deviations</span><span class="p">)])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">255</span><span class="p">),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                      <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">means</span><span class="p">,</span>
                                                           <span class="n">std</span><span class="o">=</span><span class="n">deviations</span><span class="p">)])</span>
</pre></div>
<p>Now we create the testing and training data. Although I loaded the training data before, I didn't apply all the extra transforms so I'm going to re-load it</p>
<div class="highlight">
<pre><span></span><span class="n">test_path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"CAT_DOG_TEST"</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">train_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">train_transforms</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">ImageFolder</span><span class="p">(</span><span class="n">test_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">test_transforms</span><span class="p">)</span>

<span class="n">train_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">test_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</pre></div>
<p>Here are the first four images in the training set after they were transformed.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_batches</span><span class="p">)</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="transformed_train_image.png" src="/posts/nano/pytorch/part-7-loading-image-data/transformed_train_image.png"></p>
</div>
<p>At this point you should be able to load data for training and testing. Now, you should try building a network that can classify cats vs dogs. This is quite a bit more complicated than before with the MNIST and Fashion-MNIST datasets. To be honest, you probably won't get it to work with a fully-connected network, no matter how deep. These images have three color channels and at a higher resolution (so far you've seen 28x28 images which are tiny).</p>
</div>
</div>
<div class="outline-2" id="outline-container-org808bdf7">
<h2 id="org808bdf7">A Naive Dropout model</h2>
<div class="outline-text-2" id="text-org808bdf7">
<p>I'm just going to try and apply the Dropout Model from the FASHION-MNIST examples and see what happens. But, it turns out that the input shapes are wrong. Each image is a (3, 100, 100) tensor.</p>
<div class="highlight">
<pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">HyperParameters</span><span class="p">()</span>
<span class="n">parameters</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">parameters</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DropoutModel</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">outcomes</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                 <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                 <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
                 <span class="n">train_batches</span><span class="o">=</span><span class="n">train_batches</span><span class="p">,</span>
                 <span class="n">test_batches</span><span class="o">=</span><span class="n">test_batches</span><span class="p">)</span>
</pre></div>
<p>Okay, this doesn't work, there's a mismatched size problem that I can't figure out. Maybe I'll come back to this.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/part-6-saving-and-loading-models/">Part 6 - Saving and Loading Models</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/part-6-saving-and-loading-models/" rel="bookmark"><time class="published dt-published" datetime="2018-11-21T17:38:28-08:00" itemprop="datePublished" title="2018-11-21 17:38">2018-11-21 17:38</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/part-6-saving-and-loading-models/#org1bd6b51">Introduction</a></li>
<li><a href="/posts/nano/pytorch/part-6-saving-and-loading-models/#org73dddd7">Set Up</a></li>
<li><a href="/posts/nano/pytorch/part-6-saving-and-loading-models/#orgae6a84d">The Data</a></li>
<li><a href="/posts/nano/pytorch/part-6-saving-and-loading-models/#orgac2348a">Training the Network</a></li>
<li><a href="/posts/nano/pytorch/part-6-saving-and-loading-models/#org9995188">Saving and loading networks</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org1bd6b51">
<h2 id="org1bd6b51">Introduction</h2>
<div class="outline-text-2" id="text-org1bd6b51">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>In this notebook we're going to look at how to save and load models with PyTorch.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org73dddd7">
<h2 id="org73dddd7">Set Up</h2>
<div class="outline-text-2" id="text-org73dddd7"></div>
<div class="outline-3" id="outline-container-org25a892d">
<h3 id="org25a892d">Imports</h3>
<div class="outline-text-3" id="text-org25a892d"></div>
<div class="outline-4" id="outline-container-orgc912143">
<h4 id="orgc912143">Python</h4>
<div class="outline-text-4" id="text-orgc912143">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org2174853">
<h4 id="org2174853">PyPi</h4>
<div class="outline-text-4" id="text-org2174853">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgfb5cfdb">
<h4 id="orgfb5cfdb">Nano Program</h4>
<div class="outline-text-4" id="text-orgfb5cfdb">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">nano.pytorch</span> <span class="kn">import</span> <span class="n">helper</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf36207f">
<h4 id="orgf36207f">This Project</h4>
<div class="outline-text-4" id="text-orgf36207f">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPathTwo</span>
<span class="kn">from</span> <span class="nn">fashion</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">label_decoder</span><span class="p">,</span>
    <span class="n">train</span><span class="p">,</span>
    <span class="n">DropoutModel</span><span class="p">,</span>
    <span class="n">HyperParameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org642a714">
<h3 id="org642a714">Plotting</h3>
<div class="outline-text-3" id="text-org642a714">
<div class="highlight">
<pre><span></span><span class="n">get_python</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_python</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgae6a84d">
<h2 id="orgae6a84d">The Data</h2>
<div class="outline-text-2" id="text-orgae6a84d">
<p>Once again we're going to use the <code>fashion-MNIST</code> data.</p>
</div>
<div class="outline-3" id="outline-container-orgdce0d00">
<h3 id="orgdce0d00">The Path</h3>
<div class="outline-text-3" id="text-orgdce0d00">
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"FASHION_MNIST"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">)</span>
</pre></div>
<pre class="example">
~/datasets/F_MNIST

</pre></div>
</div>
<div class="outline-3" id="outline-container-org317f3dd">
<h3 id="org317f3dd">Define a transform to normalize the data</h3>
<div class="outline-text-3" id="text-org317f3dd">
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org49f6559">
<h3 id="org49f6559">Download and Load the Training Data</h3>
<div class="outline-text-3" id="text-org49f6559">
<div class="highlight">
<pre><span></span><span class="n">trainset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">training</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc78e839">
<h3 id="orgc78e839">Download and Load the Test Data</h3>
<div class="outline-text-3" id="text-orgc78e839">
<div class="highlight">
<pre><span></span><span class="n">testset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">testing</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">testset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>Here's one of the images.</p>
<div class="highlight">
<pre><span></span><span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">trainloader</span><span class="p">))</span>
<span class="n">helper</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]);</span>
</pre></div>
<div class="figure">
<p><img alt="image_one.png" src="/posts/nano/pytorch/part-6-saving-and-loading-models/image_one.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">label_decoder</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Sneaker

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgac2348a">
<h2 id="orgac2348a">Training the Network</h2>
<div class="outline-text-2" id="text-orgac2348a">
<p>I'm re-using the <code>DropoutModel</code> from the previous lesson about avoiding over-fitting using dropout. I'm also re-using the (somewhat updated) <code>train</code> function.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DropoutModel</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
      <span class="n">train_batches</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">test_batches</span><span class="o">=</span><span class="n">testing</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<pre class="example">
Epoch: 1/30 Training loss: 2.41 Test Loss: 2.40 Test Accuracy: 0.09
Epoch: 2/30 Training loss: 2.41 Test Loss: 2.40 Test Accuracy: 0.09

</pre></div>
</div>
<div class="outline-2" id="outline-container-org9995188">
<h2 id="org9995188">Saving and loading networks</h2>
<div class="outline-text-2" id="text-org9995188">
<p>Rather than re-training your model every time you want to use it you can instead save it an re-load the pre-trained model when you need it.</p>
<p>The parameters for PyTorch networks are stored in a model's <code>state_dict</code>.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">"Our model: </span><span class="se">\n\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"The state dict keys: </span><span class="se">\n\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
<pre class="example">
Our model: 

 DropoutModel(
  (input_to_hidden): Linear(in_features=784, out_features=256, bias=True)
  (hidden_1_to_hidden_2): Linear(in_features=256, out_features=128, bias=True)
  (hidden_2_to_hidden_3): Linear(in_features=128, out_features=64, bias=True)
  (hidden_3_to_output): Linear(in_features=64, out_features=10, bias=True)
  (dropout): Dropout(p=0.2)
) 

The state dict keys: 

 odict_keys(['input_to_hidden.weight', 'input_to_hidden.bias', 'hidden_1_to_hidden_2.weight', 'hidden_1_to_hidden_2.bias', 'hidden_2_to_hidden_3.weight', 'hidden_2_to_hidden_3.bias', 'hidden_3_to_output.weight', 'hidden_3_to_output.bias'])
</pre>
<p>The simplest thing to do is simply save the state dict with <a href="https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save"><code>torch.save</code></a>, which uses python's <a href="https://docs.python.org/3.6/library/pickle.html">pickle</a> to serialze the settings. PyTorch has <a href="https://pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models">an explanation</a> for why you would prefer saving the settings instead of the entire model.</p>
<p>As an example, we can save our trained model's settings to a file <code>checkpoint.pth</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">file_name</span> <span class="o">=</span> <span class="s2">"checkpoint.pth"</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">file_name</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">check_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"File Size: {} K"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">check_path</span><span class="o">.</span><span class="n">stat</span><span class="p">()</span><span class="o">.</span><span class="n">st_size</span><span class="o">/</span><span class="mi">10</span><span class="o">**</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
<pre class="example">
File Size: 972.392 K

</pre>
<p>So it's almost a megabyte, better remember to clean it up later.</p>
<p>I couldn't find an explanation for the file-extension, but the pytorch documentation mentions that it's a convention to use <code>.pt</code> and <code>.pth</code> as extensions. I'm assuming <i>pt</i> is for PyTorch and the <i>h</i> is for hyper-parameters, but I'm not really sure that it's the case.</p>
<p>To load the model you can use <a href="https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load"><code>torch.load</code></a>.</p>
<div class="highlight">
<pre><span></span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'checkpoint.pth'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">state_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
<pre class="example">
odict_keys(['input_to_hidden.weight', 'input_to_hidden.bias', 'hidden_1_to_hidden_2.weight', 'hidden_1_to_hidden_2.bias', 'hidden_2_to_hidden_3.weight', 'hidden_2_to_hidden_3.bias', 'hidden_3_to_output.weight', 'hidden_3_to_output.bias'])

</pre>
<p>To load the state-dict you take your instantiated but untrained model and call its <a href="https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict"><code>load_state_dict</code></a> method.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
<p>Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. Using a model with a different architecture, this fails.</p>
<div class="highlight">
<pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">HyperParameters</span><span class="p">()</span>
<span class="n">parameters</span><span class="o">.</span><span class="n">hidden_layer_1</span> <span class="o">=</span> <span class="mi">400</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">bad_model</span> <span class="o">=</span> <span class="n">DropoutModel</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
<span class="c1"># This will throw an error because the tensor sizes are wrong!</span>
<span class="n">bad_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">Error</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="ow">in</span> <span class="n">loading</span> <span class="n">state_dict</span> <span class="k">for</span> <span class="n">DropoutModel</span><span class="p">:</span>
        <span class="n">size</span> <span class="n">mismatch</span> <span class="k">for</span> <span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span> <span class="n">copying</span> <span class="n">a</span> <span class="n">param</span> <span class="n">of</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">400</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="kn">from</span> <span class="nn">checkpoint</span><span class="p">,</span> <span class="n">where</span> <span class="n">the</span> <span class="n">shape</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">256</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="ow">in</span> <span class="n">current</span> <span class="n">model</span><span class="o">.</span>
        <span class="n">size</span> <span class="n">mismatch</span> <span class="k">for</span> <span class="n">input_to_hidden</span><span class="o">.</span><span class="n">bias</span><span class="p">:</span> <span class="n">copying</span> <span class="n">a</span> <span class="n">param</span> <span class="n">of</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">400</span><span class="p">])</span> <span class="kn">from</span> <span class="nn">checkpoint</span><span class="p">,</span> <span class="n">where</span> <span class="n">the</span> <span class="n">shape</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">256</span><span class="p">])</span> <span class="ow">in</span> <span class="n">current</span> <span class="n">model</span><span class="o">.</span>
        <span class="n">size</span> <span class="n">mismatch</span> <span class="k">for</span> <span class="n">hidden_1_to_hidden_2</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span> <span class="n">copying</span> <span class="n">a</span> <span class="n">param</span> <span class="n">of</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">400</span><span class="p">])</span> <span class="kn">from</span> <span class="nn">checkpoint</span><span class="p">,</span> <span class="n">where</span> <span class="n">the</span> <span class="n">shape</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">])</span> <span class="ow">in</span> <span class="n">current</span> <span class="n">model</span><span class="o">.</span>
</pre></div>
<p>This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model.</p>
<p>Originally the bad-model was just called 'model' and that seems to have messed up the state-dict so I'm going to re-use the one we made before.</p>
<div class="highlight">
<pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'hyperparameters'</span><span class="p">:</span> <span class="n">HyperParameters</span><span class="p">,</span>
              <span class="s1">'state_dict'</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">}</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
</pre></div>
<p>Remember that this is using pickle under the hood so whatever you save has to be pickleable. It probably would be safer to use parameters instead of a settings object like I did, but I didn't know we were going to be doing this.</p>
<p>Here's a function to load checkpoint-files.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">"""Load the model checkpoint from disk</span>

<span class="sd">    Args:</span>
<span class="sd">     filepath: path to the saved checkpoint</span>
<span class="sd">    """</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DropoutModel</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">"hyperparameters"</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">'state_dict'</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
<p>You can see from the function that the checkpoint is really just pickling a dictionary, and we can add any arbitrary things we want to it. I'm not really sure what it gives that using pickle directly doesn't have.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">load_checkpoint</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example">
DropoutModel(
  (input_to_hidden): Linear(in_features=784, out_features=256, bias=True)
  (hidden_1_to_hidden_2): Linear(in_features=256, out_features=128, bias=True)
  (hidden_2_to_hidden_3): Linear(in_features=128, out_features=64, bias=True)
  (hidden_3_to_output): Linear(in_features=64, out_features=10, bias=True)
  (dropout): Dropout(p=0.2)
)

</pre>
<p>PyTorch has more about saving and loading models in <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">their documentation</a>, including saving your model to continue training later (you need to save more than the model's settings).</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/part-5-inference-and-validation/">Part 5 - Inference and Validation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/part-5-inference-and-validation/" rel="bookmark"><time class="published dt-published" datetime="2018-11-19T22:19:42-08:00" itemprop="datePublished" title="2018-11-19 22:19">2018-11-19 22:19</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/part-5-inference-and-validation/#org5f787bc">Introduction</a></li>
<li><a href="/posts/nano/pytorch/part-5-inference-and-validation/#org29ca5a5">Setup</a></li>
<li><a href="/posts/nano/pytorch/part-5-inference-and-validation/#orgb948117">The Data</a></li>
<li><a href="/posts/nano/pytorch/part-5-inference-and-validation/#orgc3ef7b2">The Model</a></li>
<li><a href="/posts/nano/pytorch/part-5-inference-and-validation/#org6913611">Validation</a></li>
<li><a href="/posts/nano/pytorch/part-5-inference-and-validation/#org01de533">Overfitting</a></li>
<li><a href="/posts/nano/pytorch/part-5-inference-and-validation/#org566aa7b">The Dropout Model</a></li>
<li><a href="/posts/nano/pytorch/part-5-inference-and-validation/#org223a30e">Inference</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org5f787bc">
<h2 id="org5f787bc">Introduction</h2>
<div class="outline-text-2" id="text-org5f787bc">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>Now that you have a trained network, you can use it for making predictions. This is typically called <b>inference</b>, a term borrowed from statistics. However, neural networks have a tendency to perform <b>too well</b> on the training data and aren't able to generalize to data that hasn't been seen before. This is called <b>overfitting</b> and it impairs inference performance. To test for overfitting while training, we measure the performance on data not in the training set called the <b>validation</b> set. We avoid overfitting through regularization such as dropout while monitoring the validation performance during training.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org29ca5a5">
<h2 id="org29ca5a5">Setup</h2>
<div class="outline-text-2" id="text-org29ca5a5"></div>
<div class="outline-3" id="outline-container-org9475b21">
<h3 id="org9475b21">Imports</h3>
<div class="outline-text-3" id="text-org9475b21"></div>
<div class="outline-4" id="outline-container-orge6e0b22">
<h4 id="orge6e0b22">Python</h4>
<div class="outline-text-4" id="text-orge6e0b22">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc12ae25">
<h4 id="orgc12ae25">PyPi</h4>
<div class="outline-text-4" id="text-orgc12ae25">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8b2b113">
<h4 id="org8b2b113">The Nano Degree Repo</h4>
<div class="outline-text-4" id="text-org8b2b113">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">nano.pytorch</span> <span class="kn">import</span> <span class="n">helper</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org41e3448">
<h4 id="org41e3448">This Project</h4>
<div class="outline-text-4" id="text-org41e3448">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">fashion</span> <span class="kn">import</span> <span class="n">label_decoder</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org3ae67b3">
<h3 id="org3ae67b3">Plotting</h3>
<div class="outline-text-3" id="text-org3ae67b3">
<div class="highlight">
<pre><span></span><span class="n">get_python</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_python</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org6c1c95f">
<h3 id="org6c1c95f">The Environment</h3>
<div class="outline-text-3" id="text-org6c1c95f">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">DATA_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"FASHION_MNIST"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">)</span>
</pre></div>
<pre class="example">
~/datasets/F_MNIST/

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb948117">
<h2 id="orgb948117">The Data</h2>
<div class="outline-text-2" id="text-orgb948117">
<p>We're going to load the dataset through torchvision but this time we'll be taking advantage of the test set which you can get by setting <code>train=False</code>.</p>
<p>The test set contains images just like the training set. Typically you'll see 10-20% of the original dataset held out for testing and validation with the rest being used for training.</p>
</div>
<div class="outline-3" id="outline-container-orgaa96a46">
<h3 id="orgaa96a46">Normalize the Data</h3>
<div class="outline-text-3" id="text-orgaa96a46">
<div class="highlight">
<pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="n">spread</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">spread</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb4e202d">
<h3 id="orgb4e202d">Training Data</h3>
<div class="outline-text-3" id="text-orgb4e202d">
<p>Once again we're going to use the <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion MNIST</a> data set.</p>
<div class="highlight">
<pre><span></span><span class="n">training_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span>
                                     <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                     <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                     <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">training_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">training_set</span><span class="p">,</span>
                                               <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                               <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org389dddc">
<h3 id="org389dddc">Test Data</h3>
<div class="outline-text-3" id="text-org389dddc">
<p>By setting <code>train=False</code> in the <code>FashionMNIST</code> constructor you implicitly get the test set.</p>
<div class="highlight">
<pre><span></span><span class="n">test_set</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span>
                                 <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                 <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc3ef7b2">
<h2 id="orgc3ef7b2">The Model</h2>
<div class="outline-text-2" id="text-orgc3ef7b2">
<p>We're going to use the object-oriented approach instead of the pipeline that we used earlier. It's going to have three hidden layers and one output layer.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">HyperParameters</span><span class="p">:</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="mi">28</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">hidden_layer_1</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">hidden_layer_2</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">hidden_layer_3</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.003</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">dropout_probability</span> <span class="o">=</span> <span class="mf">0.2</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_to_hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span>
                                         <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_1_to_hidden_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_1</span><span class="p">,</span>
                                              <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_2_to_hidden_3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_2</span><span class="p">,</span>
                                              <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_3_to_output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_3</span><span class="p">,</span>
                                            <span class="n">HyperParameters</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""One forward-pass through the network"""</span>
        <span class="c1"># make sure input tensor is flattened</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_1_to_hidden_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_2_to_hidden_3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_3_to_output</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                          <span class="n">dim</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org6913611">
<h2 id="org6913611">Validation</h2>
<div class="outline-text-2" id="text-org6913611">
<p>The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. Typically this is just accuracy, the percentage of classes the network predicted correctly. Other options are <a href="https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)">precision and recall</a>, top-5 error rate, etc.. We'll focus on accuracy here. First we'll do a forward pass with one batch from the test set.</p>
<p>Get the next image-batch.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_batches</span><span class="p">))</span>
</pre></div>
<p>Now we'll get the model probabilities for the image-batch.</p>
<div class="highlight">
<pre><span></span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">shape</span>
<span class="k">assert</span> <span class="n">rows</span> <span class="o">==</span> <span class="mi">64</span>
<span class="k">assert</span> <span class="n">columns</span> <span class="o">==</span> <span class="mi">10</span>
</pre></div>
<pre class="example">
torch.Size([64, 10])

</pre>
<p>With the probabilities, we can get the most likely class using the <a href="https://pytorch.org/docs/stable/torch.html#torch.topk"><code>probabilities.topk</code></a> method. This returns the \(k\) highest values in the tensor. Since we just want the most likely class, we can use <code>probabilities.topk(1)</code>. This returns a tuple of the top-\(k\) values and the top-\(k\) indices. If the highest value is the fifth element, we'll get back 4 as the index.</p>
<div class="highlight">
<pre><span></span><span class="n">top_p</span><span class="p">,</span> <span class="n">top_class</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>Look at the most likely classes for the first 10 examples</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">top_class</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:])</span>
</pre></div>
<pre class="example">
tensor([[6],
        [6],
        [6],
        [6],
        [6],
        [6],
        [6],
        [6],
        [5],
        [6]])
</pre>
<p>Now we can check if the predicted classes match the labels. This is simple to do by equating <code>top_class</code> and <code>labels</code>, but we have to be careful of the shapes. Here <code>top_class</code> is a 2D tensor with shape <code>(64, 1)</code> while <code>labels</code> is 1D with shape <code>(64)</code>. To get the equality to work out the way we want, <code>top_class</code> and <code>labels</code> must have the same shape.</p>
<p>If we do this:</p>
<div class="highlight">
<pre><span></span><span class="n">equals</span> <span class="o">=</span> <span class="n">top_class</span> <span class="o">==</span> <span class="n">labels</span>
</pre></div>
<p><code>equals</code> will have shape <code>(64, 64)</code>, try it yourself. What it's doing is comparing the one element in each row of <code>top_class</code> with each element in <code>labels</code> which returns 64 True/False boolean values for each row, so we have to reshape the labels first using the <code>view</code> method.</p>
<div class="highlight">
<pre><span></span><span class="n">equals</span> <span class="o">=</span> <span class="n">top_class</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">top_class</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<p>Now we need to calculate the percentage of correct predictions. <code>equals</code> has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to <code>torch.mean</code>. If only it was that simple. If you try <code>torch.mean(equals)</code>, you'll get an error.</p>
<div class="highlight">
<pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">mean</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">implemented</span> <span class="k">for</span> <span class="nb">type</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span>
</pre></div>
<p>This happens because <code>equals</code> has type <code>torch.ByteTensor</code> but <code>torch.mean</code> isn't implemented for tensors with that type. So we'll need to convert <code>equals</code> to a float tensor. Note that when we take <code>torch.mean</code> it returns a scalar tensor, to get the actual value as a float we'll need to do <code>accuracy.item()</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">equals</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'Accuracy: {accuracy.item()*100}%'</span><span class="p">)</span>
</pre></div>
<pre class="example">
Accuracy: 10.9375%

</pre>
<p>The network is untrained so it's making random guesses and we should see an accuracy around 10%. Now let's train our network and include our validation pass so we can measure how well the network is performing on the test set. Since we're not updating our parameters in the validation pass, we can speed up our code by turning off gradients using <code>torch.no_grad()</code>:</p>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># validation pass here</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
</pre></div>
<p>Implement the validation loop below and print out the total accuracy after the loop. You can largely copy and paste the code from above, but I suggest typing it in because writing it out yourself is essential for building the skill. In general you'll always learn more by typing it rather than copy-pasting. You should be able to get an accuracy above 80%.</p>
<p>The <code>train_losses</code> and <code>test_losses</code> are kept for plotting later on.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">):</span>
    <span class="n">train_losses</span><span class="p">,</span> <span class="n">test_losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">training_batches</span><span class="p">:</span>        
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># images = images.view(images.shape[0], -1)</span>
            <span class="n">log_probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>        
        <span class="k">else</span><span class="p">:</span>
            <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_batches</span><span class="p">:</span>
                    <span class="c1"># images = images.view(images.shape[0], -1)</span>
                    <span class="n">log_probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
                    <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">)</span>
                    <span class="n">top_p</span><span class="p">,</span> <span class="n">top_class</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">equals</span> <span class="o">=</span> <span class="n">top_class</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">top_class</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                    <span class="n">accuracy</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">equals</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">mean_accuracy</span> <span class="o">=</span> <span class="n">accuracy</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_batches</span><span class="p">)</span>
            <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">training_batches</span><span class="p">))</span>
            <span class="n">test_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_batches</span><span class="p">))</span>
            <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_accuracy</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"Epoch: {}/{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">),</span>
                  <span class="s2">"Training loss: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                  <span class="s2">"Test Loss: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
                  <span class="s2">"Test Accuracy: {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mean_accuracy</span><span class="p">)),</span>
    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">test_losses</span><span class="p">,</span> <span class="n">accuracies</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="n">train_losses_0</span><span class="p">,</span> <span class="n">test_losses_0</span><span class="p">,</span> <span class="n">accuracies_0</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</pre></div>
<pre class="example">
Epoch: 1/30 Training loss: 0.51 Test Loss: 0.43 Test Accuracy: 0.84
Epoch: 2/30 Training loss: 0.39 Test Loss: 0.42 Test Accuracy: 0.85
Epoch: 3/30 Training loss: 0.35 Test Loss: 0.38 Test Accuracy: 0.86
Epoch: 4/30 Training loss: 0.33 Test Loss: 0.38 Test Accuracy: 0.86
Epoch: 5/30 Training loss: 0.32 Test Loss: 0.37 Test Accuracy: 0.87
Epoch: 6/30 Training loss: 0.30 Test Loss: 0.37 Test Accuracy: 0.87
Epoch: 7/30 Training loss: 0.29 Test Loss: 0.38 Test Accuracy: 0.87
Epoch: 8/30 Training loss: 0.28 Test Loss: 0.38 Test Accuracy: 0.87
Epoch: 9/30 Training loss: 0.28 Test Loss: 0.39 Test Accuracy: 0.87
Epoch: 10/30 Training loss: 0.27 Test Loss: 0.38 Test Accuracy: 0.87
Epoch: 11/30 Training loss: 0.26 Test Loss: 0.37 Test Accuracy: 0.87
Epoch: 12/30 Training loss: 0.25 Test Loss: 0.38 Test Accuracy: 0.88
Epoch: 13/30 Training loss: 0.25 Test Loss: 0.38 Test Accuracy: 0.88
Epoch: 14/30 Training loss: 0.24 Test Loss: 0.36 Test Accuracy: 0.88
Epoch: 15/30 Training loss: 0.24 Test Loss: 0.40 Test Accuracy: 0.88
Epoch: 16/30 Training loss: 0.23 Test Loss: 0.39 Test Accuracy: 0.88
Epoch: 17/30 Training loss: 0.23 Test Loss: 0.39 Test Accuracy: 0.88
Epoch: 18/30 Training loss: 0.22 Test Loss: 0.42 Test Accuracy: 0.87
Epoch: 19/30 Training loss: 0.22 Test Loss: 0.45 Test Accuracy: 0.87
Epoch: 20/30 Training loss: 0.22 Test Loss: 0.38 Test Accuracy: 0.88
Epoch: 21/30 Training loss: 0.21 Test Loss: 0.38 Test Accuracy: 0.89
Epoch: 22/30 Training loss: 0.20 Test Loss: 0.42 Test Accuracy: 0.88
Epoch: 23/30 Training loss: 0.21 Test Loss: 0.41 Test Accuracy: 0.88
Epoch: 24/30 Training loss: 0.20 Test Loss: 0.42 Test Accuracy: 0.88
Epoch: 25/30 Training loss: 0.20 Test Loss: 0.42 Test Accuracy: 0.88
Epoch: 26/30 Training loss: 0.19 Test Loss: 0.43 Test Accuracy: 0.89
Epoch: 27/30 Training loss: 0.19 Test Loss: 0.44 Test Accuracy: 0.88
Epoch: 28/30 Training loss: 0.19 Test Loss: 0.43 Test Accuracy: 0.88
Epoch: 29/30 Training loss: 0.19 Test Loss: 0.41 Test Accuracy: 0.88
Epoch: 30/30 Training loss: 0.18 Test Loss: 0.41 Test Accuracy: 0.88
</pre>
<div class="highlight">
<pre><span></span><span class="n">train_losses_0</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">train_losses_0</span><span class="p">)</span>
<span class="n">accuracies_0</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">accuracies_0</span><span class="p">)</span>
<span class="n">test_losses_0</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">test_losses_0n</span><span class="p">)</span>
</pre></div>
<p>What do our outcomes look like?</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">print_best</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">decimals</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
               <span class="n">minimum</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""Print a table of the best and last outcomes</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the source of the information</span>
<span class="sd">     label: what to put in the headline</span>
<span class="sd">     decimals: how many decimal places to use</span>
<span class="sd">     minimum: whether we want the lowest score (vs the highest)</span>
<span class="sd">    """</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|{}| Value|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|-+-|"</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="k">if</span> <span class="n">minimum</span> <span class="k">else</span> <span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">best_index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span> <span class="k">if</span> <span class="n">minimum</span> <span class="k">else</span> <span class="n">data</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|Best|{{:.{}f}}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">decimals</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|Best Location|{}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_index</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|Final|{{:.{}f}}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">decimals</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">print_best</span><span class="p">(</span><span class="n">train_losses_0</span><span class="p">,</span> <span class="s2">"Training Loss"</span><span class="p">)</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Training Loss</th>
<th class="org-right" scope="col">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Best</td>
<td class="org-right">0.180</td>
</tr>
<tr>
<td class="org-left">Best Location</td>
<td class="org-right">29</td>
</tr>
<tr>
<td class="org-left">Final</td>
<td class="org-right">0.180</td>
</tr>
</tbody>
</table>
<p>So our best training loss was the final one.</p>
<div class="highlight">
<pre><span></span><span class="n">print_best</span><span class="p">(</span><span class="n">test_losses_0</span><span class="p">,</span> <span class="s2">"Test Loss"</span><span class="p">)</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Test Loss</th>
<th class="org-right" scope="col">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Best</td>
<td class="org-right">0.365</td>
</tr>
<tr>
<td class="org-left">Best Location</td>
<td class="org-right">13</td>
</tr>
<tr>
<td class="org-left">Final</td>
<td class="org-right">0.415</td>
</tr>
</tbody>
</table>
<p>While the test loss was best less than halfway through the epochs.</p>
<div class="highlight">
<pre><span></span><span class="n">print_best</span><span class="p">(</span><span class="n">accuracies_0</span><span class="p">,</span> <span class="s2">"Test Accuracy"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Test Accuracy</th>
<th class="org-right" scope="col">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Best</td>
<td class="org-right">0.854</td>
</tr>
<tr>
<td class="org-left">Best Location</td>
<td class="org-right">17</td>
</tr>
<tr>
<td class="org-left">Final</td>
<td class="org-right">0.851</td>
</tr>
</tbody>
</table>
<p>The accuracy also seems to have peaked almost at the halfway point, although the difference between the best and the final is pretty much just a rounding difference.</p>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="p">(</span><span class="n">axe_0</span><span class="p">,</span> <span class="n">axe_1</span><span class="p">)</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Train and Test Without Dropout"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">y_minimum</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># the top plot</span>
<span class="n">axe_0</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Accuracy"</span><span class="p">)</span>

<span class="c1"># the bottom plot</span>
<span class="n">axe_1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">axe_1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Loss"</span><span class="p">)</span>

<span class="n">test_rolling</span> <span class="o">=</span> <span class="n">test_losses_0</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">axe_1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">),</span> <span class="n">train_losses_0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train"</span><span class="p">)</span>
<span class="n">axe_1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">),</span> <span class="n">test_rolling</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Rolling Test"</span><span class="p">)</span>
<span class="n">axe_1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">),</span> <span class="n">test_losses_0</span><span class="p">,</span> <span class="s2">"."</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Test"</span><span class="p">)</span>
<span class="n">axe_1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="n">y_minimum</span><span class="p">)</span>

<span class="n">axe_0</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="n">y_minimum</span><span class="p">)</span>
<span class="n">axe_0</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">accuracies_0</span><span class="p">)),</span> <span class="n">accuracies_0</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Mean Test Accuracy"</span><span class="p">)</span>
<span class="n">axe_0</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">))</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="losses.png" src="/posts/nano/pytorch/part-5-inference-and-validation/losses.png"></p>
</div>
<p>So, although the accuracy metric on the test set is pretty stable, the training loss keeps going down even as the test loss is creeping upwards. Does this imply that accuracy isn't the right metric? Log-loss differs from accuracy in that it doesn't just penalize you for what you got wrong, but also by how far you were wrong - so if you predict a high probability for the wrong label, you will get penalized more than if you predicted it but with a relatively lower probability, as opposed to accuracy which just use the binary right and wrong. So, even though our accuracy looks stable, the Log-Loss is getting worse because our model is making the same mistakes but it is getting more confident about those bad predictions. So, on to the next section where we look at one way to try and fix this.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org01de533">
<h2 id="org01de533">Overfitting</h2>
<div class="outline-text-2" id="text-org01de533">
<p>If we look at the training and validation losses as we train the network, we can see a phenomenon known as overfitting.</p>
<p>The network learns the training set better and better, resulting in lower training losses. However, it starts having problems generalizing to data outside the training set leading to the validation loss increasing. The ultimate goal of any deep learning model is to make predictions on new data, so we should strive to get the lowest validation loss possible. One option is to use the version of the model with the lowest validation loss, here the one around 8-10 training epochs. This strategy is called <b>early-stopping</b>. In practice, you'd save the model frequently as you're training then later choose the model with the lowest validation loss.</p>
<p>The most common method to reduce overfitting (outside of early-stopping) is <a href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)"><b>dropout</b></a>, where we randomly drop input units. This forces the network to share information between weights, increasing it's ability to generalize to new data. Adding dropout in PyTorch is straightforward using the <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout"><code>nn.Dropout</code></a> module.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

        <span class="c1"># Dropout module with 0.2 drop probability</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># make sure input tensor is flattened</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Now with dropout</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="c1"># output so no dropout here</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>
<p>During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we're using the network to make predictions. To do this, you use <code>model.eval()</code>. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with <code>model.train()</code>. In general, the pattern for the validation loop will look like this, where you turn off gradients, set the model to evaluation mode, calculate the validation loss and metric, then set the model back to train mode.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Turn off gradients</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># set model to evaluation mode</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="c1"># validation pass here</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">testloader</span><span class="p">:</span>
        <span class="o">...</span>

<span class="c1"># set model back to train mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org566aa7b">
<h2 id="org566aa7b">The Dropout Model</h2>
<div class="outline-text-2" id="text-org566aa7b">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">DropoutModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""Model with dropout to prevent overfitting</span>

<span class="sd">    Args:</span>
<span class="sd">     hyperparameters: object with the hyper-parameter settings</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="p">:</span> <span class="nb">object</span><span class="o">=</span><span class="n">HyperParameters</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_to_hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hyperparameters</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span>
                                         <span class="n">hyperparameters</span><span class="o">.</span><span class="n">hidden_layer_1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_1_to_hidden_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hyperparameters</span><span class="o">.</span><span class="n">hidden_layer_1</span><span class="p">,</span>
                                              <span class="n">hyperparameters</span><span class="o">.</span><span class="n">hidden_layer_2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_2_to_hidden_3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hyperparameters</span><span class="o">.</span><span class="n">hidden_layer_2</span><span class="p">,</span>
                                              <span class="n">hyperparameters</span><span class="o">.</span><span class="n">hidden_layer_3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_3_to_output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hyperparameters</span><span class="o">.</span><span class="n">hidden_layer_3</span><span class="p">,</span>
                                            <span class="n">hyperparameters</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>

        <span class="c1"># Dropout module with 0.2 drop probability</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">hyperparameters</span><span class="o">.</span><span class="n">dropout_probability</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""One Forward pass through the network"""</span>
        <span class="c1"># make sure input tensor is flattened</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Now with dropout</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_1_to_hidden_2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_2_to_hidden_3</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

        <span class="c1"># output so no dropout here</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_3_to_output</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
                             <span class="n">dim</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">DropoutModel</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">train_loss_1</span><span class="p">,</span> <span class="n">test_loss_1</span><span class="p">,</span> <span class="n">accuracies_1</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">)</span>
</pre></div>
<pre class="example">
Epoch: 1/30 Training loss: 0.60 Test Loss: 0.53 Test Accuracy: 0.81
Epoch: 2/30 Training loss: 0.49 Test Loss: 0.49 Test Accuracy: 0.83
Epoch: 3/30 Training loss: 0.45 Test Loss: 0.47 Test Accuracy: 0.84
Epoch: 4/30 Training loss: 0.43 Test Loss: 0.48 Test Accuracy: 0.83
Epoch: 5/30 Training loss: 0.43 Test Loss: 0.47 Test Accuracy: 0.84
Epoch: 6/30 Training loss: 0.41 Test Loss: 0.45 Test Accuracy: 0.85
Epoch: 7/30 Training loss: 0.40 Test Loss: 0.45 Test Accuracy: 0.85
Epoch: 8/30 Training loss: 0.40 Test Loss: 0.49 Test Accuracy: 0.84
Epoch: 9/30 Training loss: 0.40 Test Loss: 0.47 Test Accuracy: 0.83
Epoch: 10/30 Training loss: 0.39 Test Loss: 0.44 Test Accuracy: 0.85
Epoch: 11/30 Training loss: 0.38 Test Loss: 0.46 Test Accuracy: 0.85
Epoch: 12/30 Training loss: 0.38 Test Loss: 0.49 Test Accuracy: 0.83
Epoch: 13/30 Training loss: 0.38 Test Loss: 0.44 Test Accuracy: 0.85
Epoch: 14/30 Training loss: 0.37 Test Loss: 0.43 Test Accuracy: 0.85
Epoch: 15/30 Training loss: 0.38 Test Loss: 0.46 Test Accuracy: 0.85
Epoch: 16/30 Training loss: 0.37 Test Loss: 0.47 Test Accuracy: 0.85
Epoch: 17/30 Training loss: 0.37 Test Loss: 0.46 Test Accuracy: 0.85
Epoch: 18/30 Training loss: 0.37 Test Loss: 0.54 Test Accuracy: 0.82
Epoch: 19/30 Training loss: 0.37 Test Loss: 0.44 Test Accuracy: 0.86
Epoch: 20/30 Training loss: 0.37 Test Loss: 0.45 Test Accuracy: 0.85
Epoch: 21/30 Training loss: 0.36 Test Loss: 0.45 Test Accuracy: 0.85
Epoch: 22/30 Training loss: 0.35 Test Loss: 0.47 Test Accuracy: 0.85
Epoch: 23/30 Training loss: 0.36 Test Loss: 0.45 Test Accuracy: 0.86
Epoch: 24/30 Training loss: 0.36 Test Loss: 0.46 Test Accuracy: 0.85
Epoch: 25/30 Training loss: 0.35 Test Loss: 0.46 Test Accuracy: 0.85
Epoch: 26/30 Training loss: 0.35 Test Loss: 0.48 Test Accuracy: 0.85
Epoch: 27/30 Training loss: 0.35 Test Loss: 0.46 Test Accuracy: 0.86
Epoch: 28/30 Training loss: 0.35 Test Loss: 0.45 Test Accuracy: 0.85
Epoch: 29/30 Training loss: 0.35 Test Loss: 0.47 Test Accuracy: 0.86
Epoch: 30/30 Training loss: 0.35 Test Loss: 0.46 Test Accuracy: 0.86
</pre>
<div class="highlight">
<pre><span></span><span class="n">test_loss_1</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">test_loss_1</span><span class="p">)</span>
<span class="n">train_loss_1</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">train_loss_1</span><span class="p">)</span>
<span class="n">accuracies_1</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">accuracies_1</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">print_both</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">data_2</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
               <span class="n">decimals</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">minimum</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""Prints both data sets side by side</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the first data series</span>
<span class="sd">     data_2: the second data series</span>
<span class="sd">     label: something to identify the data sets</span>
<span class="sd">     decimals: the number of decimal places to use</span>
<span class="sd">     minimum: whether minimalization is the optimal</span>
<span class="sd">    """</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|{}|First|Second|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|-+-+-|"</span><span class="p">)</span>
    <span class="n">best</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="k">if</span> <span class="n">minimum</span> <span class="k">else</span> <span class="n">data</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">best_index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span> <span class="k">if</span> <span class="n">minimum</span> <span class="k">else</span> <span class="n">data</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
    <span class="n">best_2</span> <span class="o">=</span> <span class="n">data_2</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="k">if</span> <span class="n">minimum</span> <span class="k">else</span> <span class="n">data_2</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">best_index_2</span> <span class="o">=</span>  <span class="n">data_2</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()</span> <span class="k">if</span> <span class="n">minimum</span> <span class="k">else</span> <span class="n">data_2</span><span class="o">.</span><span class="n">idxmax</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|Best|{{:.{0}f}}|{{:.{0}f}}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">decimals</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">best_2</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|Best Location|{}|{}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">best_index</span><span class="p">,</span> <span class="n">best_index_2</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|Final|{{:.{0}f}}|{{:.{0}f}}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">decimals</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">data_2</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">print_both</span><span class="p">(</span><span class="n">train_losses_0</span><span class="p">,</span> <span class="n">train_loss_1</span><span class="p">,</span> <span class="s2">"Training Loss"</span><span class="p">)</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Training Loss</th>
<th class="org-right" scope="col">First</th>
<th class="org-right" scope="col">Second</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Best</td>
<td class="org-right">0.180</td>
<td class="org-right">0.347</td>
</tr>
<tr>
<td class="org-left">Best Location</td>
<td class="org-right">29</td>
<td class="org-right">29</td>
</tr>
<tr>
<td class="org-left">Final</td>
<td class="org-right">0.180</td>
<td class="org-right">0.347</td>
</tr>
</tbody>
</table>
<p>So the best loss in both the models was the last one, but our new model does considerably worse. Maybe you need more training when the dropout is used.</p>
<div class="highlight">
<pre><span></span><span class="n">print_both</span><span class="p">(</span><span class="n">test_losses_0</span><span class="p">,</span> <span class="n">test_loss_1</span><span class="p">,</span> <span class="s2">"Test Loss"</span><span class="p">)</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Test Loss</th>
<th class="org-right" scope="col">First</th>
<th class="org-right" scope="col">Second</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Best</td>
<td class="org-right">0.365</td>
<td class="org-right">0.434</td>
</tr>
<tr>
<td class="org-left">Best Location</td>
<td class="org-right">13</td>
<td class="org-right">13</td>
</tr>
<tr>
<td class="org-left">Final</td>
<td class="org-right">0.415</td>
<td class="org-right">0.460</td>
</tr>
</tbody>
</table>
<p>Weirdly, they both peak at the same point in the epochs, also weirdly the test loss is still worse for the dropout model.</p>
<div class="highlight">
<pre><span></span><span class="n">print_both</span><span class="p">(</span><span class="n">accuracies_0</span><span class="p">,</span> <span class="n">accuracies_1</span><span class="p">,</span> <span class="s2">"Test Accuracy"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Test Accuracy</th>
<th class="org-right" scope="col">First</th>
<th class="org-right" scope="col">Second</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Best</td>
<td class="org-right">0.886</td>
<td class="org-right">0.859</td>
</tr>
<tr>
<td class="org-left">Best Location</td>
<td class="org-right">25</td>
<td class="org-right">18</td>
</tr>
<tr>
<td class="org-left">Final</td>
<td class="org-right">0.882</td>
<td class="org-right">0.859</td>
</tr>
</tbody>
</table>
<p>Our accuracy seems to peak at a little over half the epochs, but surprisingly, it also does quite a bit worse with dropout…</p>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="p">(</span><span class="n">axe_top</span><span class="p">,</span> <span class="n">axe_bottom</span><span class="p">)</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span>
    <span class="s2">"Training and Test Loss with Dropout (p={})"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">HyperParameters</span><span class="o">.</span><span class="n">dropout_probability</span><span class="p">),</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">axe_bottom</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch"</span><span class="p">)</span>
<span class="n">axe_bottom</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Loss"</span><span class="p">)</span>

<span class="n">rolling_loss</span> <span class="o">=</span> <span class="n">test_loss_1</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">rolling_loss_0</span> <span class="o">=</span> <span class="n">test_losses_0</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">axe_bottom</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">),</span> <span class="n">rolling_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Rolling Mean Test"</span><span class="p">)</span>
<span class="n">axe_bottom</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">),</span> <span class="n">rolling_loss_0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Rolling Mean Test No Dropout"</span><span class="p">)</span>
<span class="n">axe_bottom</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">),</span> <span class="n">train_loss_1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Train"</span><span class="p">)</span>
<span class="n">axe_bottom</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">),</span> <span class="n">test_loss_1</span><span class="p">,</span> <span class="s2">"g.-"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Test"</span><span class="p">)</span>

<span class="n">accuracy_rolling</span> <span class="o">=</span> <span class="n">accuracies_1</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">accuracy_rolling_0</span> <span class="o">=</span> <span class="n">accuracies_0</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_periods</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">axe_top</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Accuracy"</span><span class="p">)</span>
<span class="n">axe_top</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">accuracies_1</span><span class="p">)),</span> <span class="n">accuracy_rolling</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">axe_top</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">accuracies_0</span><span class="p">)),</span> <span class="n">accuracy_rolling_0</span><span class="p">,</span> <span class="s2">"b"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">axe_top</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">accuracies_0</span><span class="p">)),</span> <span class="n">accuracies_0</span><span class="p">,</span> <span class="s2">"b."</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"No Dropout"</span><span class="p">)</span>
<span class="n">axe_top</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">accuracies_1</span><span class="p">)),</span> <span class="n">accuracies_1</span><span class="p">,</span> <span class="s2">"r."</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"With Dropout"</span><span class="p">)</span>
<span class="n">axe_top</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">axe_top</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">axe_bottom</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="dropout_losses.png" src="/posts/nano/pytorch/part-5-inference-and-validation/dropout_losses.png"></p>
</div>
<p>So we seem to have helped the problem of the loss growing at the expense of overall performance. I'm not sure this is really the lesson we're supposed to take away from this. Maybe if we tried more epochs the dropout model would emerge victorious.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org223a30e">
<h2 id="org223a30e">Inference</h2>
<div class="outline-text-2" id="text-org223a30e">
<p>Now that the model is trained, we can use it for inference. We've done this before, but now we need to remember to set the model in inference mode with <code>model.eval()</code>. You'll also want to turn off autograd with the <code>torch.no_grad()</code> context.</p>
</div>
<div class="outline-3" id="outline-container-org3876e10">
<h3 id="org3876e10">Testing the Model</h3>
<div class="outline-text-3" id="text-org3876e10"></div>
<div class="outline-4" id="outline-container-orgb5e07f8">
<h4 id="orgb5e07f8">Get the Test Image</h4>
<div class="outline-text-4" id="text-orgb5e07f8">
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_batches</span><span class="p">)</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org70bef1b">
<h4 id="org70bef1b">Convert the 2D image to a 1D vector</h4>
<div class="outline-text-4" id="text-org70bef1b">
<div class="highlight">
<pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgdd58933">
<h4 id="orgdd58933">Calculate the Class Probabilities (softmax) for the Image</h4>
<div class="outline-text-4" id="text-orgdd58933">
<p>We run the forward pass once with the gradient turned off to get our probabilities.</p>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3a7d358">
<h4 id="org3a7d358">Plot the image and probabilities</h4>
<div class="outline-text-4" id="text-org3a7d358">
<div class="highlight">
<pre><span></span><span class="n">helper</span><span class="o">.</span><span class="n">view_classify</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s1">'Fashion'</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="test_image.png" src="/posts/nano/pytorch/part-5-inference-and-validation/test_image.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">expected</span> <span class="o">=</span> <span class="n">label_decoder</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">label_decoder</span><span class="p">[</span><span class="n">probabilities</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Expected: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Actual: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">actual</span><span class="p">))</span>
<span class="k">assert</span> <span class="n">expected</span> <span class="o">==</span> <span class="n">actual</span>
</pre></div>
<pre class="example">
Expected: Trouser
Actual: Trouser

</pre>
<p>So, it looks like we got it right this time.</p>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/part-4-fashion/">Part 4 - Classifying Fashion-MNIST</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/part-4-fashion/" rel="bookmark"><time class="published dt-published" datetime="2018-11-19T19:15:07-08:00" itemprop="datePublished" title="2018-11-19 19:15">2018-11-19 19:15</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/part-4-fashion/#org0a9f574">Introduction</a></li>
<li><a href="/posts/nano/pytorch/part-4-fashion/#org713f695">Set Up</a></li>
<li><a href="/posts/nano/pytorch/part-4-fashion/#orgbbdbcad">The Data</a></li>
<li><a href="/posts/nano/pytorch/part-4-fashion/#org5607a37">The Network</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org0a9f574">
<h2 id="org0a9f574">Introduction</h2>
<div class="outline-text-2" id="text-org0a9f574">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>This post uses the <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST dataset</a>, a set of article images from <a href="https://www.zalando.com/">Zalando</a>, a fashion retailer. It is meant to be a drop-in replacement for the MNIST dataset. The dataset was created because some people the consider original MNIST too easy, with classical machine learning algorithms achieving better than 97% accuracy. The dataset keeps the 10 classes, but now instead of digits they represent clothing types.</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Label</th>
<th class="org-left" scope="col">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-left">T-shirt/top</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-left">Trouser</td>
</tr>
<tr>
<td class="org-right">2</td>
<td class="org-left">Pullover</td>
</tr>
<tr>
<td class="org-right">3</td>
<td class="org-left">Dress</td>
</tr>
<tr>
<td class="org-right">4</td>
<td class="org-left">Coat</td>
</tr>
<tr>
<td class="org-right">5</td>
<td class="org-left">Sandal</td>
</tr>
<tr>
<td class="org-right">6</td>
<td class="org-left">Shirt</td>
</tr>
<tr>
<td class="org-right">7</td>
<td class="org-left">Sneaker</td>
</tr>
<tr>
<td class="org-right">8</td>
<td class="org-left">Bag</td>
</tr>
<tr>
<td class="org-right">9</td>
<td class="org-left">Ankle boot</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">descriptions</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"T-shirt/top"</span><span class="p">,</span>
                <span class="s2">"Trouser"</span><span class="p">,</span>
                <span class="s2">"Pullover"</span><span class="p">,</span>
                <span class="s2">"Dress"</span><span class="p">,</span>
                <span class="s2">"Coat"</span><span class="p">,</span>
                <span class="s2">"Sandal"</span><span class="p">,</span>
                <span class="s2">"Shirt"</span><span class="p">,</span>
                <span class="s2">"Sneaker"</span><span class="p">,</span>
                <span class="s2">"Bag"</span><span class="p">,</span>
                <span class="s2">"Ankle boot"</span><span class="p">,</span>
                <span class="p">)</span>

<span class="n">label_decoder</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">descriptions</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org713f695">
<h2 id="org713f695">Set Up</h2>
<div class="outline-text-2" id="text-org713f695"></div>
<div class="outline-3" id="outline-container-orgf57e858">
<h3 id="orgf57e858">Imports</h3>
<div class="outline-text-3" id="text-orgf57e858"></div>
<div class="outline-4" id="outline-container-org0336d48">
<h4 id="org0336d48">Python Standard Library</h4>
<div class="outline-text-4" id="text-org0336d48">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgfacc84c">
<h4 id="orgfacc84c">PyPi</h4>
<div class="outline-text-4" id="text-orgfacc84c">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org588f7e3">
<h4 id="org588f7e3">The Udacity Code</h4>
<div class="outline-text-4" id="text-org588f7e3">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">nano.pytorch</span> <span class="kn">import</span> <span class="n">helper</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7f0a0d3">
<h3 id="org7f0a0d3">Plotting</h3>
<div class="outline-text-3" id="text-org7f0a0d3">
<div class="highlight">
<pre><span></span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgbbdbcad">
<h2 id="orgbbdbcad">The Data</h2>
<div class="outline-text-2" id="text-orgbbdbcad"></div>
<div class="outline-3" id="outline-container-org23ecfc0">
<h3 id="org23ecfc0">Normalization</h3>
<div class="outline-text-3" id="text-org23ecfc0">
<p>First, a transform to normalize the data.</p>
<div class="highlight">
<pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">deviations</span> <span class="o">=</span> <span class="n">means</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">deviations</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgcbabd1a">
<h3 id="orgcbabd1a">Load The Data</h3>
<div class="outline-text-3" id="text-orgcbabd1a">
<p>First our training set.</p>
<div class="highlight">
<pre><span></span><span class="n">training</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s1">'~/datasets/F_MNIST/'</span><span class="p">,</span>
                                 <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="n">training_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">training</span><span class="p">,</span>
                                               <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                               <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<pre class="example">
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Processing...
Done!
</pre>
<p>Now our test set.</p>
<div class="highlight">
<pre><span></span><span class="n">testing</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s1">'~/datasets/F_MNIST/'</span><span class="p">,</span>
                                <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

<span class="n">test_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">testing</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>The data is apparently on a european amazon web-service server.</p>
<p>Let's take a look at one of the images.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">show_next_image</span><span class="p">(</span><span class="n">data_set</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""plots the next image</span>

<span class="sd">    Args:</span>
<span class="sd">     data_set: iterator to get the next image from</span>

<span class="sd">    Returns:</span>
<span class="sd">     image, label: the next items in the data set</span>
<span class="sd">    """</span>
    <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">data_set</span><span class="p">)</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">axes_style</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"white"</span><span class="p">,</span> <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)}):</span>
    <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">show_next_image</span><span class="p">(</span><span class="n">training_batches</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="image.png" src="/posts/nano/pytorch/part-4-fashion/image.png"></p>
</div>
<p>Every time I re-run this the image changes. That was originally just a blob.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">label_decoder</span><span class="p">[</span><span class="n">label</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Sneaker

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org5607a37">
<h2 id="org5607a37">The Network</h2>
<div class="outline-text-2" id="text-org5607a37">
<p>Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits or log-softmax from the forward pass. It's up to you how many layers you add and the size of those layers.</p>
</div>
<div class="outline-3" id="outline-container-orgeeaa4b6">
<h3 id="orgeeaa4b6">Hyper Parameters</h3>
<div class="outline-text-3" id="text-orgeeaa4b6">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">HyperParameters</span><span class="p">:</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>
    <span class="n">hidden_layer_1</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">hidden_layer_2</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="mi">200</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org12f6f24">
<h3 id="org12f6f24">The Model</h3>
<div class="outline-text-3" id="text-org12f6f24">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="n">input_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span>
                                  <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_1</span><span class="p">),</span>
        <span class="n">activation_1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_1</span><span class="p">,</span>
                                   <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_2</span><span class="p">),</span>
        <span class="n">activation_2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_output</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_layer_2</span><span class="p">,</span>
                                   <span class="n">HyperParameters</span><span class="o">.</span><span class="n">outputs</span><span class="p">),</span>
        <span class="n">activation_out</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">rows</span><span class="p">),</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga486dfd">
<h3 id="orga486dfd">The Optimizer and Loss</h3>
<div class="outline-text-3" id="text-orga486dfd">
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc9ad54e">
<h3 id="orgc9ad54e">Training</h3>
<div class="outline-text-3" id="text-orgc9ad54e">
<p>The process:</p>
<ul class="org-ul">
<li>Make a forward pass through the network to get the logits</li>
<li>Use the logits to calculate the loss</li>
<li>Perform a backward pass through the network with `loss.backward()` to calculate the gradients</li>
<li>Take a step with the optimizer to update the weights</li>
</ul>
<p>By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">training_batches</span><span class="p">:</span>
        <span class="c1"># some setup</span>
        <span class="c1">## Flatten the images</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1">## Reset the optimizer</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># forward pass</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

        <span class="c1"># back-propagation</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># take the next step</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">10</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Training loss: {running_loss/len(data_batches)}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Training loss: 1.2992842076048414
Training loss: 0.4147487568385057
Training loss: 0.3563503011393903
Training loss: 0.31974349495793963
Training loss: 0.2909906929267495
Training loss: 0.2669587785135836
Training loss: 0.24693025264150298
Training loss: 0.22828677767661334
Training loss: 0.2111341437932525
Training loss: 0.19651830268662368
Training loss: 0.18078892016763498
Training loss: 0.1678272306934984
Training loss: 0.15590339134147427
Training loss: 0.1440456182614509
Training loss: 0.13368237831159188
Training loss: 0.1232291767592115
Training loss: 0.11354898248336462
Training loss: 0.104927517529299
Training loss: 0.09589472461912806
Training loss: 0.08939716171846589
</pre>
<p>Check out a prediction.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_batches</span><span class="p">)</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Convert 2D image to 1D vector</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">axes_style</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">):</span>
    <span class="n">helper</span><span class="o">.</span><span class="n">view_classify</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">probabilities</span><span class="p">,</span>
                         <span class="n">version</span><span class="o">=</span><span class="s1">'Fashion'</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="prediction_image.png" src="/posts/nano/pytorch/part-4-fashion/prediction_image.png"></p>
</div>
<p>That looks pretty good to me.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">label_decoder</span><span class="p">[</span><span class="n">label</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
<span class="k">print</span><span class="p">(</span><span class="n">label_decoder</span><span class="p">[</span><span class="n">probabilities</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Sandal
Sandal

</pre>
<p>So this time we got it right.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">label</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">break</span>
<span class="k">print</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">label_decoder</span><span class="p">[</span><span class="n">probabilities</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
<span class="k">print</span><span class="p">(</span><span class="n">label_decoder</span><span class="p">[</span><span class="n">label</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
</pre></div>
<pre class="example">
10
Dress
Coat

</pre>
<p>Oops, look like we're still having problems.</p>
<div class="highlight">
<pre><span></span><span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_batches</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s1">'Accuracy of the network on the test images: </span><span class="si">%d</span><span class="s1"> </span><span class="si">%%</span><span class="s1">'</span> <span class="o">%</span> <span class="p">(</span>
    <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="p">))</span>
</pre></div>
<pre class="example">
Accuracy of the network on the test images: 88 %

</pre>
<p>Not bad, it could probably be tuned to do better, the loss hasn't stopped reducing, for instance, so maybe more epochs would help.</p>
<div class="highlight">
<pre><span></span><span class="n">class_correct</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mf">0.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">class_total</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mf">0.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"|Item|Accuracy (%)|"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"|-+-|"</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_batches</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)):</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">class_correct</span><span class="p">[</span><span class="n">label</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="o">+=</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">class_total</span><span class="p">[</span><span class="n">label</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="o">+=</span> <span class="mi">1</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'|{}|{:.1f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">label_decoder</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">class_correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Item</th>
<th class="org-right" scope="col">Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">T-shirt/top</td>
<td class="org-right">88.0</td>
</tr>
<tr>
<td class="org-left">Trouser</td>
<td class="org-right">97.5</td>
</tr>
<tr>
<td class="org-left">Pullover</td>
<td class="org-right">87.2</td>
</tr>
<tr>
<td class="org-left">Dress</td>
<td class="org-right">88.0</td>
</tr>
<tr>
<td class="org-left">Coat</td>
<td class="org-right">83.2</td>
</tr>
<tr>
<td class="org-left">Sandal</td>
<td class="org-right">97.4</td>
</tr>
<tr>
<td class="org-left">Shirt</td>
<td class="org-right">57.3</td>
</tr>
<tr>
<td class="org-left">Sneaker</td>
<td class="org-right">95.5</td>
</tr>
<tr>
<td class="org-left">Bag</td>
<td class="org-right">95.7</td>
</tr>
<tr>
<td class="org-left">Ankle boot</td>
<td class="org-right">95.6</td>
</tr>
</tbody>
</table>
<p>Generally it seems to do okay, but the shirt seems to have gotten worse than when I was using fewer epochs. I might be overfitting by putting so many epochs and if I were to improve it I would probably work on other hyper-parameters.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/training-neural-networks/">Training Neural Networks</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/training-neural-networks/" rel="bookmark"><time class="published dt-published" datetime="2018-11-19T16:05:52-08:00" itemprop="datePublished" title="2018-11-19 16:05">2018-11-19 16:05</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orgb1d0f5e">Introduction</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org92bafae">Backpropagation</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orgd777d63">Losses in PyTorch</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orgc0c58ca">Imports</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orgccd0fb4">The Network</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org799dd14">The Network</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org037a46e">Network 2 (with Log Softmax)</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orga2e2c6e">On To Autograd</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org3e7c60e">Loss and Autograd together</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org5db60c8">Training the Network</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org3f2a06a">Training (For Real This Time)</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgb1d0f5e">
<h2 id="orgb1d0f5e">Introduction</h2>
<div class="outline-text-2" id="text-orgb1d0f5e">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>The network we built in the previous part isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.</p>
<p>At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.</p>
<p>To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a <b>loss function</b> (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems</p>
<p>\[ \large \ell = \frac{1}{2n}\sum_i^n{\left(y_i - \hat{y}_i\right)^2} \]</p>
<p>where \(n\) is the number of training examples, \(y_i\) are the true labels, and \(\hat{y}_i\) are the predicted labels.</p>
<p>By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called <b>gradient descent</b>. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org92bafae">
<h2 id="org92bafae">Backpropagation</h2>
<div class="outline-text-2" id="text-org92bafae">
<p>For single layer networks, gradient descent is straightforward to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.</p>
<p>Training multilayer networks is done through <b>backpropagation</b> which is really just an application of the chain rule from calculus. It's easiest to understand if we think of our two layer network as a graph representation.</p>
</div>
<div class="outline-3" id="outline-container-org4d4cf2c">
<h3 id="org4d4cf2c">The Forward Pass</h3>
<div class="outline-text-3" id="text-org4d4cf2c">
<p>In the forward pass through the network, our data and operations go from bottom to top. We pass the input \(x\) through a linear transformation \(L_1\) with weights \(W_1\) and biases \(b_1\). The output then goes through the sigmoid operation \(S\) and another linear transformation \(L_2\). Finally we calculate the loss \(\ell\). We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org474faa4">
<h3 id="org474faa4">Backwards Pass</h3>
<div class="outline-text-3" id="text-org474faa4">
<p>To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.</p>
<p>\[ \large \frac{\partial \ell}{\partial W_1} = \frac{\partial L_1}{\partial W_1} \frac{\partial S}{\partial L_1} \frac{\partial L_2}{\partial S} \frac{\partial \ell}{\partial L_2} \]</p>
<p>We update our weights using this gradient with some learning rate \(\alpha\).</p>
<p>\[ \large W^\prime_1 = W_1 - \alpha \frac{\partial \ell}{\partial W_1} \]</p>
<p>The learning rate \(\alpha\) is set such that the weight update steps are small enough that the iterative method settles in a minimum.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd777d63">
<h2 id="orgd777d63">Losses in PyTorch</h2>
<div class="outline-text-2" id="text-orgd777d63">
<p>Let's start by seeing how we calculate the loss with PyTorch. Through the <code>nn</code> module, PyTorch provides losses such as the cross-entropy loss (<a href="https://pytorch.org/docs/stable/nn.html#crossentropyloss"><code>nn.CrossEntropyLoss</code></a>). You'll usually see the loss assigned to <code>criterion</code>. As noted in the last part, with a classification problem such as MNIST, we're using the softmax function to predict class probabilities. With a softmax output, you want to use cross-entropy as the loss. To actually calculate the loss, you first define the criterion then pass in the output of your network and the correct labels.</p>
<p>There is something really important to note here. Looking at <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss">the documentation for <code>nn.CrossEntropyLoss</code></a>:</p>
<blockquote>
<p>This criterion combines <a href="https://pytorch.org/docs/stable/nn.html?highlight=logsoftmax#torch.nn.LogSoftmax"><code>nn.LogSoftmax()</code></a> and <a href="https://pytorch.org/docs/stable/nn.html#nllloss"><code>nn.NLLLoss()</code></a> in one single class.</p>
<p>The input is expected to contain scores for each class.</p>
</blockquote>
<p>This means we need to pass in the raw output of our network into the loss, not the output of the softmax function. This raw output is usually called the <b>logits</b> or <b>scores</b>. We use the logits because softmax gives you probabilities which will often be very close to zero or one but floating-point numbers can't accurately represent values near zero or one (<a href="https://docs.python.org/3/tutorial/floatingpoint.html">read more here</a>). It's usually best to avoid doing calculations with probabilities, typically we use log-probabilities.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgc0c58ca">
<h2 id="orgc0c58ca">Imports</h2>
<div class="outline-text-2" id="text-orgc0c58ca"></div>
<div class="outline-3" id="outline-container-org77d2e88">
<h3 id="org77d2e88">From Python</h3>
<div class="outline-text-3" id="text-org77d2e88">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9a66462">
<h3 id="org9a66462">From PyPi</h3>
<div class="outline-text-3" id="text-org9a66462">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd3c6390">
<h3 id="orgd3c6390">The Udacity Repository</h3>
<div class="outline-text-3" id="text-orgd3c6390">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">nano.pytorch</span> <span class="kn">import</span> <span class="n">helper</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5ef1f9b">
<h3 id="org5ef1f9b">Plotting</h3>
<div class="outline-text-3" id="text-org5ef1f9b">
<div class="highlight">
<pre><span></span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgccd0fb4">
<h2 id="orgccd0fb4">The Network</h2>
<div class="outline-text-2" id="text-orgccd0fb4"></div>
<div class="outline-3" id="outline-container-org6a41df2">
<h3 id="org6a41df2">Define a Transform</h3>
<div class="outline-text-3" id="text-org6a41df2">
<p>We are going to create a pipeline to normalize the data. The argument for <code>Normalize</code> are a tuple of means and a tuple of standard-deviations. You use tuples because you need to pass in a value for each of the color channels.</p>
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
                                                     <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)),</span>
                              <span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org6ed814b">
<h3 id="org6ed814b">The Data</h3>
<div class="outline-text-3" id="text-org6ed814b">
<p>Once again we're going to use the <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> data-set. It's important to use the same output folder as the last time or you will end up downloading a new copy of the dataset.</p>
<div class="highlight">
<pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">'~/datasets/MNIST/'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">digits</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org799dd14">
<h2 id="org799dd14">The Network</h2>
<div class="outline-text-2" id="text-org799dd14">
<p>We're going to build a feed-forward network using the pipeline-style of network definition and then pass in a batch of image to examine the loss.</p>
</div>
<div class="outline-3" id="outline-container-org133592a">
<h3 id="org133592a">Some Constants</h3>
<div class="outline-text-3" id="text-org133592a">
<p>These are the hyperparameters for our model. The number if inputs is the number of pixels in the images. The number of outputs is the number of digits (so 10).</p>
<div class="highlight">
<pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="mi">28</span><span class="o">**</span><span class="mi">2</span>
<span class="n">hidden_nodes_1</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">hidden_nodes_2</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
<p>Since this gets used way further down I'm going to make a namespace for it so (maybe) it'll be easier to remember where the values are from.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">HyperParameters</span><span class="p">:</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="mi">28</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">hidden_nodes_1</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">hidden_nodes_2</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.003</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5488bbb">
<h3 id="org5488bbb">The Model</h3>
<div class="outline-text-3" id="text-org5488bbb">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="n">input_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_nodes_1</span><span class="p">),</span>
        <span class="n">relu_1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_nodes_1</span><span class="p">,</span> <span class="n">hidden_nodes_2</span><span class="p">),</span>
        <span class="n">relu_2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_output</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_nodes_2</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org482e21c">
<h3 id="org482e21c">The Loss</h3>
<div class="outline-text-3" id="text-org482e21c">
<p>We're going to use <code>CrossEntropyLoss</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org735c8c8">
<h3 id="org735c8c8">The Images</h3>
<div class="outline-text-3" id="text-org735c8c8">
<p>We're going to pull the next (first) batch of images and reshape it.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
torch.Size([64, 1, 28, 28])

</pre>
<p>This will flatten the images.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
torch.Size([64, 784])

</pre>
<p>So, that one isn't so obvious, but when the <code>view</code> method gets passed a <code>-1</code> it interprets it as meaning you want to flatten the tensor. In this case we passed in the number of rows so it just reduces the other dimensions to columns. It kind of seems like you lose a column in there somewhere…</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgecfa703">
<h3 id="orgecfa703">One Pass</h3>
<div class="outline-text-3" id="text-orgecfa703">
<p>We're going to pass our model the images to make a single forward pass and get the <a href="https://en.wikipedia.org/wiki/Logit">logits</a> for them.</p>
<div class="highlight">
<pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</pre></div>
<p>Now we'll calculate our model's loss with the logits and the labels.</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor(2.3135, grad_fn=&lt;NllLossBackward&gt;)

</pre>
<p>According to the original author of this exercise</p>
<blockquote>
<p>…it's more convenient to build the model with a log-softmax output using <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax"><code>nn.LogSoftmax</code></a> or <code>F.log_softmax</code>. Then you can get the actual probabilities by taking the exponential <code>torch.exp(output)</code>. With a log-softmax output, you want to use the negative log likelihood loss, <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss"><code>nn.NLLLoss</code></a>.</p>
</blockquote>
<p>Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss. Note that for <code>nn.LogSoftmax</code> and <code>F.log_softmax</code> you'll need to set the <code>dim</code> keyword argument appropriately. <code>dim=0</code> calculates softmax across the rows, so each column sums to 1, while <code>dim=1</code> calculates across the columns so each row sums to 1. Think about what you want the output to be and choose <code>dim</code> appropriately.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org037a46e">
<h2 id="org037a46e">Network 2 (with Log Softmax)</h2>
<div class="outline-text-2" id="text-org037a46e">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="n">input_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_nodes_1</span><span class="p">),</span>
        <span class="n">relu_1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_nodes_1</span><span class="p">,</span> <span class="n">hidden_nodes_2</span><span class="p">),</span>
        <span class="n">relu_2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_output</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_nodes_2</span><span class="p">,</span> <span class="n">outputs</span><span class="p">),</span>
        <span class="n">log_softmax</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
<p>And now our loss.</p>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span> 
</pre></div>
<p>Now we get the next batch of images.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
</pre></div>
<p>And once again we flatten them.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>A forward pass on the batch.</p>
<div class="highlight">
<pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</pre></div>
<p>Calculate the loss with the logits and the labels</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor(2.3208, grad_fn=&lt;NllLossBackward&gt;)

</pre>
<p>So that's interesting, but what does it mean?</p>
</div>
</div>
<div class="outline-2" id="outline-container-orga2e2c6e">
<h2 id="orga2e2c6e">On To Autograd</h2>
<div class="outline-text-2" id="text-orga2e2c6e">
<p>Now that we know how to calculate a loss, how do we use it to perform backpropagation? Torch provides a module, <code>autograd</code>, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set <code>requires_grad = True</code> on a tensor. You can do this at creation with the <code>requires_grad</code> keyword, or at any time with <code>x.requires_grad_(True)</code>.</p>
<p>You can turn off gradients for a block of code with the <code>torch.no_grad()</code> content:</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="o">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
</pre></div>
<p>Also, you can turn on or off gradients altogether with <code>torch.set_grad_enabled(True|False)</code>.</p>
<p>The gradients are computed with respect to some variable <code>z</code> with <code>z.backward()</code>. This does a backward pass through the operations that created <code>z</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([[-0.7567, -0.2352],
        [-0.9346,  0.3097]], requires_grad=True)

</pre>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([[0.5726, 0.0553],
        [0.8735, 0.0959]], grad_fn=&lt;PowBackward0&gt;)

</pre>
<p>We can see the operation that created <code>y</code>, a power operation <code>PowBackward0</code>.</p>
<p><code>grad_fn</code> shows the function that generated this variable</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</pre></div>
<pre class="example">
&lt;PowBackward0 object at 0x7f591c505c50&gt;

</pre>
<p>The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor <code>y</code> to a scalar value, the mean.</p>
<div class="highlight">
<pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor(0.3993, grad_fn=&lt;MeanBackward1&gt;)

</pre>
<p>You can check the gradients for <code>x</code> and <code>y</code> but they are empty currently.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
<pre class="example">
None

</pre>
<p>To calculate the gradients, you need to run the <code>.backward</code> method on a Variable, <code>z</code> for example. This will calculate the gradient for <code>z</code> with respect to <code>x</code></p>
<p>\[ \frac{\partial z}{\partial x} = \frac{\partial}{\partial x}\left[\frac{1}{n}\sum_i^n x_i^2\right] = \frac{x}{2} \]</p>
<div class="highlight">
<pre><span></span><span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([[-0.3783, -0.1176],
        [-0.4673,  0.1548]])
tensor([[-0.3783, -0.1176],
        [-0.4673,  0.1548]], grad_fn=&lt;DivBackward0&gt;)

</pre>
<p>These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. Once we have the gradients we can make a gradient descent step.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org3e7c60e">
<h2 id="org3e7c60e">Loss and Autograd together</h2>
<div class="outline-text-2" id="text-org3e7c60e">
<p>When we create a network with PyTorch, all of the parameters are initialized with <code>requires_grad = True</code>. This means that when we calculate the loss and call <code>loss.backward()</code>, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. Below you can see an example of calculating the gradients using a backwards pass.</p>
<p>Get the next batch.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>Now get the logits and loss for the batch.</p>
<div class="highlight">
<pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
<p>This is what the weights from the input layer to the first hidden layer look like before and after the backward-pass.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Before backward pass: </span><span class="se">\n</span><span class="s1">{}</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>

<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s1">'After backward pass: </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
<pre class="example">
Before backward pass: 
None

After backward pass: 
 tensor([[ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],
        [ 0.0011,  0.0011,  0.0011,  ...,  0.0011,  0.0011,  0.0011],
        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004],
        ...,
        [ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],
        [ 0.0003,  0.0003,  0.0003,  ...,  0.0003,  0.0003,  0.0003],
        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005]])
</pre></div>
</div>
<div class="outline-2" id="outline-container-org5db60c8">
<h2 id="org5db60c8">Training the Network</h2>
<div class="outline-text-2" id="text-org5db60c8">
<p>There's one last piece we need to start training, an optimizer that we'll use to update the weights with the gradients. We get these from PyTorch's <a href="https://pytorch.org/docs/stable/optim.html"><code>optim</code> package</a>(). For example we can use stochastic gradient descent with <code>optim.SGD</code>. You can see how to define an optimizer below.</p>
<p>Optimizers require the parameters to optimize and a learning rate.</p>
<div class="highlight">
<pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
<p>Now we know how to use all the individual parts so it's time to see how they work together. Let's consider just one learning step before looping through all the data. The general process with PyTorch:</p>
<ol class="org-ol">
<li>Make a forward pass through the network</li>
<li>Use the network output to calculate the loss</li>
<li>Perform a backward pass through the network with <code>loss.backward()</code> to calculate the gradients</li>
<li>Take a step with the optimizer to update the weights</li>
</ol>
<p>Below I'll go through one training step and print out the weights and gradients so you can see how it changes. Note the line of code: <code>optimizer.zero_grad()</code>. When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches.</p>
<p>Here's the weights for the first set of edges in the network before we start:</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Initial weights - '</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
<pre class="example">
Initial weights -  Parameter containing:
tensor([[ 0.0170,  0.0055, -0.0258,  ..., -0.0295, -0.0028,  0.0312],
        [ 0.0246,  0.0314,  0.0259,  ..., -0.0091, -0.0276, -0.0238],
        [ 0.0336, -0.0133,  0.0045,  ..., -0.0284,  0.0278,  0.0029],
        ...,
        [-0.0085, -0.0300,  0.0222,  ...,  0.0066, -0.0162,  0.0062],
        [-0.0303, -0.0324, -0.0237,  ..., -0.0230,  0.0137, -0.0268],
        [-0.0327,  0.0012,  0.0174,  ...,  0.0311,  0.0058,  0.0034]],
       requires_grad=True)

</pre>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
<span class="n">images</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
</pre></div>
<p>Clear the gradients.</p>
<div class="highlight">
<pre><span></span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
<p>Make a forward pass, then a backward pass, then update the weights and check the gradient.</p>
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Gradient -'</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
<pre class="example">
Gradient - tensor([[-0.0076, -0.0076, -0.0076,  ..., -0.0076, -0.0076, -0.0076],
        [-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006],
        [-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],
        ...,
        [-0.0028, -0.0028, -0.0028,  ..., -0.0028, -0.0028, -0.0028],
        [-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],
        [ 0.0027,  0.0027,  0.0027,  ...,  0.0027,  0.0027,  0.0027]])

</pre>
<p>Now take an update step and check out the new weights.</p>
<div class="highlight">
<pre><span></span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Updated weights - '</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
<pre class="example">
Updated weights -  Parameter containing:
tensor([[ 0.0171,  0.0056, -0.0257,  ..., -0.0294, -0.0027,  0.0313],
        [ 0.0246,  0.0314,  0.0259,  ..., -0.0091, -0.0276, -0.0238],
        [ 0.0336, -0.0133,  0.0045,  ..., -0.0284,  0.0278,  0.0029],
        ...,
        [-0.0084, -0.0300,  0.0223,  ...,  0.0066, -0.0161,  0.0062],
        [-0.0303, -0.0324, -0.0237,  ..., -0.0229,  0.0137, -0.0268],
        [-0.0327,  0.0011,  0.0173,  ...,  0.0310,  0.0058,  0.0034]],
       requires_grad=True)

</pre>
<p>If you compare it to the first weights you'll notice that the first cell is the same, but many of the others have very small changes made to them. The first steps in the descent.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org3f2a06a">
<h2 id="org3f2a06a">Training (For Real This Time)</h2>
<div class="outline-text-2" id="text-org3f2a06a">
<p>Now we'll put this algorithm into a loop so we can go through all the images. First some nomenclature - one pass through the entire dataset is called an <b>epoch</b>. So we're going to loop through <code>data_loader</code> to get our training batches. For each batch, we'll do a training pass where we calculate the loss, do a backwards pass, and update the weights. Then we'll start all over again with the batches until we're out of epochs.</p>
</div>
<div class="outline-4" id="outline-container-org905d70a">
<h4 id="org905d70a">Set It Up</h4>
<div class="outline-text-4" id="text-org905d70a">
<p>Since we took a couple of passes with the old model already I'll re-define it (I don't know if there's a reset function).</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="n">input_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span>
                                  <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_nodes_1</span><span class="p">),</span>
        <span class="n">relu_1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_nodes_1</span><span class="p">,</span>
                                   <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_nodes_2</span><span class="p">),</span>
        <span class="n">relu_2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_output</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_nodes_2</span><span class="p">,</span>
                                   <span class="n">HyperParameters</span><span class="o">.</span><span class="n">outputs</span><span class="p">),</span>
        <span class="n">log_softmax</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga904539">
<h4 id="orga904539">Train It</h4>
<div class="outline-text-4" id="text-orga904539">
<div class="highlight">
<pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="c1"># Flatten MNIST images</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Training loss: {running_loss/len(data_loader)}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Training loss: 1.961392556680545
Training loss: 0.9206915147014773
Training loss: 0.5431230474414348
Training loss: 0.4353313447792393
Training loss: 0.38809780185537807
Training loss: 0.3599447336580072
Training loss: 0.3397818624115448
Training loss: 0.323730937088095
Training loss: 0.3114365364696934
Training loss: 0.3002190677198901
</pre>
<p>So there's a little bit of voodoo going on there - we never pass the model to the loss function or the optimizer, but somehow calling them updates the model. It feels a little like matplotlib's state-machine form. It's neat, but I'm not sure I like it as much as I do object-oriented programming.</p>
<p>With the network trained, we can check out it's predictions.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="c1"># Turn off gradients to speed up this part</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="c1"># Output of the network are logits, need to take softmax for probabilities</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">helper</span><span class="o">.</span><span class="n">view_classify</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">probabilities</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="probabilities.png" src="/posts/nano/pytorch/training-neural-networks/probabilities.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">probabilities</span><span class="o">.</span><span class="n">argmax</span><span class="p">())</span>
</pre></div>
<pre class="example">
tensor(6)

</pre>
<p>Amazingly, it did really well. One thing to note is that I originally made the epoch count higher but didn't remember to make a new network, optimizer, and loss, and the network ended up doing poorly. I don't know what messed it up, maybe I reset the network but not the optimizers, or some such, but anyway, here it is.</p>
</div>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="/index-6.html" rel="prev">Newer posts</a></li>
<li class="next"><a href="/index-4.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer">Contents © 2020 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
