<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>In Too Deep (Posts about fastai)</title><link>https://necromuralist.github.io/In-Too-Deep/</link><description></description><atom:link href="https://necromuralist.github.io/In-Too-Deep/categories/fastai.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Mon, 15 Apr 2019 17:36:24 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Dog and Cat Breed Classification (What's Your Pet?)</title><link>https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#orga40848c"&gt;Departure&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#org0749f17"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#orgb521ee3"&gt;Some Setup&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#org8128bb3"&gt;Initiation&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#org8b5b077"&gt;Downloading the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#org481fb7b"&gt;Looking At the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#org4762aeb"&gt;Training: resnet34&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#orgb4b5c54"&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/#org98041bd"&gt;Unfreezing, fine-tuning, and learning rates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga40848c" class="outline-2"&gt;
&lt;h2 id="orga40848c"&gt;Departure&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga40848c"&gt;
&lt;p&gt;
This is lesson one from the &lt;a href="https://www.fast.ai"&gt;fastai&lt;/a&gt; course &lt;a href="https://course.fast.ai/index.html"&gt;Practical Deep Learning for Coders, v3&lt;/a&gt;, which I assume is the third version of the course, and not a reference to a &lt;a href="https://www.wikiwand.com/en/Kamen_Rider_V3"&gt;Japanese television show&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0749f17" class="outline-3"&gt;
&lt;h3 id="org0749f17"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0749f17"&gt;
&lt;p&gt;
We are going to work with the &lt;a href="http://www.fast.ai/2018/10/02/fastai-ai/"&gt;fastai V1 library&lt;/a&gt; which sits on top of &lt;a href="https://hackernoon.com/pytorch-1-0-468332ba5163"&gt;Pytorch 1.0&lt;/a&gt;. The &lt;i&gt;fastai&lt;/i&gt; library provides many useful functions that enable us to quickly and easily build neural networks and train our models.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org98b657d" class="outline-4"&gt;
&lt;h4 id="org98b657d"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org98b657d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
from pathlib import Path
import os
import re
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9b9c8cb" class="outline-4"&gt;
&lt;h4 id="org9b9c8cb"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9b9c8cb"&gt;
&lt;p&gt;
&lt;code&gt;fastai&lt;/code&gt; recommends using &lt;code&gt;*&lt;/code&gt; to import everything, but I'd like to know where everything comes from and not import something that might conflict with my naming conventions so I'm going to (at least try to) import things individually.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from fastai.datasets import untar_data
from fastai.train import ClassificationInterpretation
from fastai.vision.data import get_image_files, imagenet_stats, ImageDataBunch
from fastai.vision.learner import cnn_learner
from fastai.vision.models import resnet34
from fastai.vision.transform import get_transforms
from fastai.metrics import error_rate
from tabulate import tabulate
import holoviews
import matplotlib.pyplot as pyplot
import numpy
import pandas
import seaborn
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org658d15f" class="outline-4"&gt;
&lt;h4 id="org658d15f"&gt;My Stuff&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org658d15f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae.tables import CountPercentage
from graeae.timers import Timer
from graeae.visualization import EmbedHoloview
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb521ee3" class="outline-3"&gt;
&lt;h3 id="orgb521ee3"&gt;Some Setup&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb521ee3"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0a5bac4" class="outline-4"&gt;
&lt;h4 id="org0a5bac4"&gt;The Random Seed&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0a5bac4"&gt;
&lt;p&gt;
To make this reproducible we'll set the random seed in numpy.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;numpy.random.seed(2)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5ef5811" class="outline-4"&gt;
&lt;h4 id="org5ef5811"&gt;Batch Size&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5ef5811"&gt;
&lt;p&gt;
If you're using a computer with an unusually small GPU, you may get an out of memory error when running this notebook. If this happens, reduce the batch size.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;batch_size = 64 # = 16  change this if you run out of memory.
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbf6550e" class="outline-4"&gt;
&lt;h4 id="orgbf6550e"&gt;The Path&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbf6550e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv(".env", override=True)
DATA_PATH = Path(os.environ.get("OXFORD_PET_DATASET")).expanduser()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org90344b8" class="outline-4"&gt;
&lt;h4 id="org90344b8"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org90344b8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0648406" class="outline-5"&gt;
&lt;h5 id="org0648406"&gt;Matplotlib&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-org0648406"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=1)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge97da3b" class="outline-5"&gt;
&lt;h5 id="orge97da3b"&gt;The Bokeh&lt;/h5&gt;
&lt;div class="outline-text-5" id="text-orge97da3b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;holoviews.extension("bokeh")
SLUG = "dog-and-cat-breed-classification"
OUTPUT_FOLDER = Path("../../files/posts/fastai/" + SLUG)
Embed = partial(EmbedHoloview, folder_path=OUTPUT_FOLDER)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Plot:
    width = 1000
    height = 800
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2b34ce9" class="outline-4"&gt;
&lt;h4 id="org2b34ce9"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2b34ce9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0230db4" class="outline-4"&gt;
&lt;h4 id="org0230db4"&gt;Tabulate&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0230db4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ORG_TABLE = partial(tabulate, headers="keys", 
		    showindex=False, 
		    tablefmt="orgtbl")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8128bb3" class="outline-2"&gt;
&lt;h2 id="org8128bb3"&gt;Initiation&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8128bb3"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8b5b077" class="outline-3"&gt;
&lt;h3 id="org8b5b077"&gt;Downloading the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8b5b077"&gt;
&lt;p&gt;
We are going to use the &lt;a href="http://www.robots.ox.ac.uk/~vgg/data/pets/"&gt;Oxford-IIIT Pet Dataset&lt;/a&gt; by &lt;a href="http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf"&gt;O. M. Parkhi et al., 2012&lt;/a&gt; which features 12 cat breeds and 25 dogs breeds. Our model will need to learn to differentiate between these 37 distinct categories. According to their paper, the best accuracy they could get in 2012 was 59.21%, using a complex model that was specific to pet detection, with separate "Image", "Head", and "Body" models for the pet photos. Let's see how accurate we can be using deep learning.
&lt;/p&gt;

&lt;p&gt;
We are going to use the &lt;a href="https://docs.fast.ai/datasets.html#untar_data"&gt;untar_data&lt;/a&gt; function to which we must pass a URL as an argument and which will download and extract the data.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;help(untar_data)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Help on function untar_data in module fastai.datasets:

untar_data(url:str, fname:Union[pathlib.Path, str]=None, dest:Union[pathlib.Path, str]=None, data=True, force_download=False) -&amp;gt; pathlib.Path
    Download `url` to `fname` if it doesn't exist, and un-tgz to folder `dest`.


&lt;/pre&gt;

&lt;p&gt;
This data set is 774 Megabytes and given my over-priced yet still incredibly slow CenturyLink speeds I found downloading it from the &lt;a href="https://course.fast.ai/datasets#image-classification"&gt;fastai datasets page&lt;/a&gt; a little more satisfactory, since the progress widget that runs during the download  when &lt;code&gt;untar_data&lt;/code&gt; downloads the dataset doesn't show up in emacs.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;assert DATA_PATH.is_dir()
print(DATA_PATH)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/athena/data/datasets/images/oxford-iiit-pet

&lt;/pre&gt;

&lt;p&gt;
I didn't know it, but &lt;code&gt;Paths&lt;/code&gt; have an &lt;code&gt;ls&lt;/code&gt; method (so far as I could see this isn't in &lt;a href="https://docs.python.org/3/library/pathlib.html"&gt;python's documentation&lt;/a&gt;).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(DATA_PATH.ls())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[PosixPath('/home/athena/data/datasets/images/oxford-iiit-pet/README.org'), PosixPath('/home/athena/data/datasets/images/oxford-iiit-pet/images'), PosixPath('/home/athena/data/datasets/images/oxford-iiit-pet/annotations')]

&lt;/pre&gt;

&lt;p&gt;
Here's another trick I didn't know about, instead of using the &lt;code&gt;joinpath&lt;/code&gt; method you can just use a forward-slash.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path_to_annotations = DATA_PATH/'annotations'
path_to_images = DATA_PATH/'images'
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org481fb7b" class="outline-3"&gt;
&lt;h3 id="org481fb7b"&gt;Looking At the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org481fb7b"&gt;
&lt;p&gt;
The first thing we do when we approach a problem is to take a look at the data. We &lt;i&gt;always&lt;/i&gt; need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like.
&lt;/p&gt;

&lt;p&gt;
The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the filenames themselves. We will need to extract them to be able to classify the images into the correct categories. Fortunately, the fastai library has a handy function made exactly for this, &lt;a href="https://docs.fast.ai/vision.data.html#ImageDataBunch.from_name_re"&gt;ImageDataBunch.from_name_re&lt;/a&gt; gets the labels from the filenames using a &lt;a href="https://docs.python.org/3.6/library/re.html"&gt;regular expression&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
First we'll get a list of the files in the images folder using &lt;a href="https://docs.fast.ai/vision.data.html#get_image_files"&gt;get_image_files&lt;/a&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;file_names = get_image_files(path_to_images)
for path in file_names[:5]:
    print(path.name)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
havanese_128.jpg
american_bulldog_181.jpg
Ragdoll_196.jpg
havanese_40.jpg
Birman_108.jpg

&lt;/pre&gt;

&lt;p&gt;
Later on we're going to use the labels when we inspect the model so I'm going to make the case standardized.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;UNDERSCORE, SPACE = "_", " "
for path in file_names:
    name, extension = os.path.splitext(path.name)
    name = name.replace(UNDERSCORE, SPACE).title()
    file_name = (name + extension).replace(SPACE, UNDERSCORE)
    target = path.parent.joinpath(file_name)
    path.rename(target)

file_names = get_image_files(path_to_images)
for path in file_names[:2]:
    print(path.name)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Boxer_20.jpg
Saint_Bernard_195.jpg

&lt;/pre&gt;

&lt;p&gt;
This is the pattern to match the file-name.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;end_of_line = "$"
one_or_more = "+"
digit = r"\d"
index = rf"{digit}{one_or_more}"
forward_slash = "/"

anything_but_a_slash = f"[^{forward_slash}]"
label = f'({anything_but_a_slash}{one_or_more})'
file_extension = ".jpg"
expression = rf'{forward_slash}{label}{UNDERSCORE}{index}{file_extension}{end_of_line}'

test = "/home/athena/data/datasets/images/oxford-iiit-pet/images/Havanese_128.jpg"

assert re.search(expression, test).groups()[0] == "Havanese"
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The reason for the forward slash at the beginning of the expression is that we're passing in the entire path to each image, not just the name of the image.
&lt;/p&gt;

&lt;p&gt;
Here's the arguments we need to pass in
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(help(ImageDataBunch.from_name_re))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Help on method from_name_re in module fastai.vision.data:

from_name_re(path:Union[pathlib.Path, str], fnames:Collection[pathlib.Path], pat:str, valid_pct:float=0.2, **kwargs) method of builtins.type instance
    Create from list of `fnames` in `path` with re expression `pat`.

None

&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = ImageDataBunch.from_name_re(path_to_images, 
				   file_names, 
				   expression, 
				   ds_tfms=get_transforms(), 
				   size=224, 
				   bs=batch_size
				  ).normalize(imagenet_stats)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
One of the arguments we passed in isn't particularly obviously named, unless you already know about applying transforms to images, but here's what we passed to it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(help(get_transforms))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Help on function get_transforms in module fastai.vision.transform:

get_transforms(do_flip:bool=True, flip_vert:bool=False, max_rotate:float=10.0, max_zoom:float=1.1, max_lighting:float=0.2, max_warp:float=0.2, p_affine:float=0.75, p_lighting:float=0.75, xtra_tfms:Union[Collection[fastai.vision.image.Transform], NoneType]=None) -&amp;gt; Collection[fastai.vision.image.Transform]
    Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.

None

&lt;/pre&gt;

&lt;p&gt;
&lt;a href="https://docs.fast.ai/vision.transform.html#get_transforms"&gt;get_transforms&lt;/a&gt; adds random changes to the images to help with our training.
&lt;/p&gt;

&lt;p&gt;
We also added a call to &lt;a href="https://docs.fast.ai/vision.data.html#normalize"&gt;normalize&lt;/a&gt; which sets the mean and standard deviation of the images to match those of the images used to train the model that we're going to use (&lt;a href="https://arxiv.org/abs/1512.03385"&gt;ResNet&lt;/a&gt;).
&lt;/p&gt;

&lt;p&gt;
The &lt;a href="https://docs.fast.ai/basic_data.html#DataBunch.show_batch"&gt;show_batch&lt;/a&gt; function is a simple way to show some of the images. It retrieves them randomly so calling the method repeatedly will pull up different images. Unfortunately you can't pass in a figure or axes so it isn't easily configurable.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data.show_batch(rows=3, figsize=(7,6))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/show_batch.png" alt="show_batch.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
ImageDataBunch;

Train: LabelList (5912 items)
x: ImageList
Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
y: CategoryList
Boxer,Saint_Bernard,Saint_Bernard,English_Cocker_Spaniel,Ragdoll
Path: /home/athena/data/datasets/images/oxford-iiit-pet/images;

Valid: LabelList (1478 items)
x: ImageList
Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
y: CategoryList
British_Shorthair,Keeshond,Saint_Bernard,Siamese,British_Shorthair
Path: /home/athena/data/datasets/images/oxford-iiit-pet/images;

Test: None
&lt;/pre&gt;

&lt;p&gt;
So it looks like the &lt;code&gt;ImageDataBunch&lt;/code&gt; created a training and a validation set and each of the images has three channels and is 224 x 224 pixels.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4762aeb" class="outline-3"&gt;
&lt;h3 id="org4762aeb"&gt;Training: resnet34&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4762aeb"&gt;
&lt;p&gt;
Now we will start training our model. We will use a &lt;a href="http://cs231n.github.io/convolutional-networks/"&gt;convolutional neural network&lt;/a&gt; backbone and a fully connected head with a single hidden layer as a classifier. Don't know what these things mean? Not to worry, we will dive deeper in the coming lessons. For the moment you need to know that we are building a model which will take images as input and will output the predicted probability for each of the categories (in this case, it will have 37 outputs).
&lt;/p&gt;

&lt;p&gt;
We will train for 4 epochs (4 cycles through all our data).
&lt;/p&gt;

&lt;p&gt;
First we'll load the model to train into the &lt;a href="https://docs.fast.ai/vision.learner.html#cnn_learner"&gt;cnn_learner&lt;/a&gt;. If you look at the &lt;a href="https://github.com/fastai/fastai/blob/master/fastai/vision/models/__init__.py"&gt;fast ai code&lt;/a&gt; they are importing the &lt;code&gt;resnet34&lt;/code&gt; model from &lt;a href="https://pytorch.org/docs/stable/torchvision/models.html#id3"&gt;pytorch's torchvision&lt;/a&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;learn = cnn_learner(data, resnet34, metrics=error_rate)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
#+RESULTS
&lt;/p&gt;
&lt;pre class="example"&gt;
Downloading: "https://download.pytorch.org/models/resnet34-333f7ec4.pth" to /home/athena/.torch/models/resnet34-333f7ec4.pth
87306240it [00:26, 3321153.99it/s]
&lt;/pre&gt;

&lt;p&gt;
As you can see, it downloaded the stored model parameters from pytorch. This is because I've never downloaded this particular model before if you run it again it shouldn't need to re-download it. Since this is a &lt;a href="https://pytorch.org"&gt;pytorch&lt;/a&gt; model we can look at it's represetantion to see the architecture of the network.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(learn.model)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Sequential(
  (0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (1): Sequential(
    (0): AdaptiveConcatPool2d(
      (ap): AdaptiveAvgPool2d(output_size=1)
      (mp): AdaptiveMaxPool2d(output_size=1)
    )
    (1): Flatten()
    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Dropout(p=0.25)
    (4): Linear(in_features=1024, out_features=512, bias=True)
    (5): ReLU(inplace)
    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): Dropout(p=0.5)
    (8): Linear(in_features=512, out_features=37, bias=True)
  )
)
&lt;/pre&gt;

&lt;p&gt;
That's a pretty big network, but the main thing to notice is the last layer, which has 37 &lt;code&gt;out_features&lt;/code&gt; which corresponds to the number of breeds we have in our data-set.
&lt;/p&gt;

&lt;p&gt;
Now we need to train it using the &lt;a href="https://docs.fast.ai/train.html#fit_one_cycle"&gt;fit_one_cycle&lt;/a&gt; method. At first I thought 'one cycle' meant just one pass through the batches but according to the &lt;a href="https://docs.fast.ai/callbacks.one_cycle.html"&gt;documentation&lt;/a&gt;, this is a reference to a training method called the &lt;a href="https://sgugger.github.io/the-1cycle-policy.html"&gt;1Cycle Policy&lt;/a&gt; proposed by &lt;a href="https://arxiv.org/abs/1803.09820"&gt;Leslie N. Smith&lt;/a&gt; that changes the hyperparameters to make the model train faster.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    learn.fit_one_cycle(4)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Started: 2019-04-14 18:41:37.154885
Ended: 2019-04-14 18:43:34.734434
Elapsed: 0:01:57.579549

&lt;/pre&gt;

&lt;p&gt;
Now we can store the parameters for the trained model.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;learn.save('stage-1')
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-orgb4b5c54" class="outline-3"&gt;
&lt;h3 id="orgb4b5c54"&gt;Results&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb4b5c54"&gt;
&lt;p&gt;
Let's see what results we have got. 
&lt;/p&gt;

&lt;p&gt;
We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly. 
&lt;/p&gt;

&lt;p&gt;
Furthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.
&lt;/p&gt;

&lt;p&gt;
The &lt;a href="https://docs.fast.ai/train.html#ClassificationInterpretation"&gt;ClassificationInterpretation&lt;/a&gt; class contains methods to help look at how the model did.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;interpreter = ClassificationInterpretation.from_learner(learn)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The &lt;a href="https://docs.fast.ai/vision.learner.html#ClassificationInterpretation.top_losses"&gt;top_losses&lt;/a&gt; method returns a tuple of the highest losses along with the indices of the data that gave those losses. By default it actually gives all the losses sorted from largest to smallest, but you could pass in an integer to limit how much it returns.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;losses, indexes = interpreter.top_losses()
print(losses)
print(indexes)
assert len(data.valid_ds)==len(losses)==len(indexes)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
tensor([6.8920e+00, 5.1477e+00, 4.9542e+00,  ..., 5.7220e-06, 3.8147e-06,
        3.8147e-06])
tensor([ 785,  195, 1261,  ..., 1097,  315, 1228])

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;plot = holoviews.Distribution(losses).opts(title="Loss Distribution", 
					   xlabel="Loss", 
					   width=Plot.width, 
					   height=Plot.height)
Embed(plot=plot, file_name="loss_distribution")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/loss_distribution.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
Although it looks like there are negative losses, that's just the way the distribution works out.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(losses.max())
print(losses.min())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
tensor(6.8920)
tensor(3.8147e-06)

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bins = pandas.cut(losses.tolist(), bins=10).value_counts().reset_index()
print(ORG_TABLE(bins, headers="Range Count".split()))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Range&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;(-0.00689, 0.689]&lt;/td&gt;
&lt;td class="org-right"&gt;1355&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(0.689, 1.378]&lt;/td&gt;
&lt;td class="org-right"&gt;52&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(1.378, 2.068]&lt;/td&gt;
&lt;td class="org-right"&gt;39&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(2.068, 2.757]&lt;/td&gt;
&lt;td class="org-right"&gt;16&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(2.757, 3.446]&lt;/td&gt;
&lt;td class="org-right"&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(3.446, 4.135]&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(4.135, 4.824]&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(4.824, 5.514]&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(5.514, 6.203]&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;(6.203, 6.892]&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;p&gt;
Alternatively we can plot the images that had the highest losses.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;interpreter.plot_top_losses(9, figsize=(15,11))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/top_losses.png" alt="top_losses.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
It looks like the ones that had the most loss had some kind of weird flare effect applied to the image. Now that we've used it, maybe we can see how we're supposed to call &lt;code&gt;plot_top_losses&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(help(interpreter.plot_top_losses))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Help on method _cl_int_plot_top_losses in module fastai.vision.learner:

_cl_int_plot_top_losses(k, largest=True, figsize=(12, 12), heatmap:bool=True, heatmap_thresh:int=16, return_fig:bool=None) -&amp;gt; Union[matplotlib.figure.Figure, NoneType] method of fastai.train.ClassificationInterpretation instance
    Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.

None

&lt;/pre&gt;

&lt;p&gt;
&lt;b&gt;Note:&lt;/b&gt; in the original notebook they were using a function called &lt;a href="https://github.com/fastai/fastai/blob/master/fastai/gen_doc/nbdoc.py#L126"&gt;doc&lt;/a&gt;, which tries to open another window and will thus hang when run in emacs. They &lt;i&gt;really&lt;/i&gt; want you to use jupyter.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;interpreter.plot_confusion_matrix(figsize=(12,12), dpi=60)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/confusion_matrix.png" alt="confusion_matrix.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
If you compare the images with the worst losses to the confusion matrix you'll notice that they don't seem to correlate - the worst losses were one-offs, probably due to the flare effect. The most confused was the &lt;i&gt;Ragdoll&lt;/i&gt; being confused for a &lt;i&gt;Birman&lt;/i&gt;, but, as noted in the lecture, &lt;a href="https://pets.thenest.com/birman-vs-ragdoll-cat-11758.html"&gt;distinguishing them is hard for people too&lt;/a&gt;. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(ORG_TABLE(interpreter.most_confused(min_val=2), 
		headers="Actual Predicted Count".split()))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Actual&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Predicted&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Ragdoll&lt;/td&gt;
&lt;td class="org-left"&gt;Birman&lt;/td&gt;
&lt;td class="org-right"&gt;7&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Bengal&lt;/td&gt;
&lt;td class="org-left"&gt;Egyptian_Mau&lt;/td&gt;
&lt;td class="org-right"&gt;5&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;British_Shorthair&lt;/td&gt;
&lt;td class="org-left"&gt;Russian_Blue&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Chihuahua&lt;/td&gt;
&lt;td class="org-left"&gt;Miniature_Pinscher&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Great_Pyrenees&lt;/td&gt;
&lt;td class="org-left"&gt;Samoyed&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;American_Pit_Bull_Terrier&lt;/td&gt;
&lt;td class="org-left"&gt;Staffordshire_Bull_Terrier&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Basset_Hound&lt;/td&gt;
&lt;td class="org-left"&gt;Beagle&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Bengal&lt;/td&gt;
&lt;td class="org-left"&gt;Abyssinian&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Birman&lt;/td&gt;
&lt;td class="org-left"&gt;Ragdoll&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Egyptian_Mau&lt;/td&gt;
&lt;td class="org-left"&gt;Bengal&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Maine_Coon&lt;/td&gt;
&lt;td class="org-left"&gt;Bengal&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Staffordshire_Bull_Terrier&lt;/td&gt;
&lt;td class="org-left"&gt;American_Pit_Bull_Terrier&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Abyssinian&lt;/td&gt;
&lt;td class="org-left"&gt;Sphynx&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;American_Pit_Bull_Terrier&lt;/td&gt;
&lt;td class="org-left"&gt;German_Shorthaired&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Birman&lt;/td&gt;
&lt;td class="org-left"&gt;Persian&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Boxer&lt;/td&gt;
&lt;td class="org-left"&gt;American_Pit_Bull_Terrier&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Boxer&lt;/td&gt;
&lt;td class="org-left"&gt;Staffordshire_Bull_Terrier&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Newfoundland&lt;/td&gt;
&lt;td class="org-left"&gt;German_Shorthaired&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Persian&lt;/td&gt;
&lt;td class="org-left"&gt;Maine_Coon&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Russian_Blue&lt;/td&gt;
&lt;td class="org-left"&gt;Abyssinian&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Russian_Blue&lt;/td&gt;
&lt;td class="org-left"&gt;British_Shorthair&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Saint_Bernard&lt;/td&gt;
&lt;td class="org-left"&gt;American_Bulldog&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Scottish_Terrier&lt;/td&gt;
&lt;td class="org-left"&gt;Havanese&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Siamese&lt;/td&gt;
&lt;td class="org-left"&gt;Birman&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Yorkshire_Terrier&lt;/td&gt;
&lt;td class="org-left"&gt;Havanese&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It doesn't look too bad, actually, other that the first 5 entries, maybe.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org98041bd" class="outline-3"&gt;
&lt;h3 id="org98041bd"&gt;Unfreezing, fine-tuning, and learning rates&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org98041bd"&gt;
&lt;p&gt;
Since our model is working as we expect it to, we will &lt;a href="https://docs.fast.ai/basic_train.html#Learner.unfreeze"&gt;&lt;b&gt;unfreeze&lt;/b&gt;&lt;/a&gt; our model and train some more.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;learn.unfreeze()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Since we are using a pre-trained model we normally freeze all but the last layer to do transfer learning, by unfreezing the mode we'll train all the layers to our dataset.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    learn.fit_one_cycle(1)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Started: 2019-04-14 19:27:34.028566
Ended: 2019-04-14 19:28:15.295003
Elapsed: 0:00:41.266437

&lt;/pre&gt;

&lt;p&gt;
Now we save it again.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;learn.load('stage-1');
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now we're going to use the &lt;a href="https://docs.fast.ai/callbacks.lr_finder.html"&gt;lr_find&lt;/a&gt; method to find the best learning rate.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    learn.lr_find()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Started: 2019-04-14 19:34:59.954658
LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.
Ended: 2019-04-14 19:35:26.561592
Elapsed: 0:00:26.606934

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;learn.recorder.plot()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/learning.png" alt="learning.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
So, it's kind of hard to see the exact number, but you can see that somewhere around a learning rate of 0.0001 we get a good loss and then after that the loss starts to go way up.
&lt;/p&gt;

&lt;p&gt;
So next we're going to re-train it using an interval that hopefully gives us the best loss.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;learn.unfreeze()
with TIMER:
    learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Started: 2019-04-14 20:48:40.194329
Ended: 2019-04-14 20:50:02.024671
Elapsed: 0:01:21.830342

&lt;/pre&gt;


&lt;p&gt;
data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(),
                                   size=299, bs=bs//2).normalize(imagenet_stats)
&lt;/p&gt;


&lt;p&gt;
learn = cnn_learner(data, models.resnet50, metrics=error_rate)
&lt;/p&gt;


&lt;p&gt;
learn.lr_find()
learn.recorder.plot()
&lt;/p&gt;


&lt;p&gt;
learn.fit_one_cycle(8)
&lt;/p&gt;


&lt;p&gt;
learn.save('stage-1-50')
&lt;/p&gt;


&lt;p&gt;
learn.unfreeze()
learn.fit_one_cycle(3, max_lr=slice(1e-6,1e-4))
&lt;/p&gt;


&lt;p&gt;
learn.load('stage-1-50');
&lt;/p&gt;


&lt;p&gt;
interp = ClassificationInterpretation.from_learner(learn)
&lt;/p&gt;


&lt;p&gt;
interp.most_confused(min_val=2)
&lt;/p&gt;


&lt;p&gt;
path = untar_data(URLs.MNIST_SAMPLE); path
&lt;/p&gt;


&lt;p&gt;
tfms = get_transforms(do_flip=False)
data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26)
&lt;/p&gt;


&lt;p&gt;
data.show_batch(rows=3, figsize=(5,5))
&lt;/p&gt;


&lt;p&gt;
learn = cnn_learner(data, models.resnet18, metrics=accuracy)
learn.fit(2)
&lt;/p&gt;


&lt;p&gt;
df = pd.read_csv(path/'labels.csv')
df.head()
&lt;/p&gt;


&lt;p&gt;
data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28)
&lt;/p&gt;


&lt;p&gt;
data.show_batch(rows=3, figsize=(5,5))
data.classes
&lt;/p&gt;


&lt;p&gt;
data = ImageDataBunch.from_df(path, df, ds_tfms=tfms, size=24)
data.classes
&lt;/p&gt;


&lt;p&gt;
fn_paths = [path/name for name in df['name']]; fn_paths[:2]
&lt;/p&gt;


&lt;p&gt;
pat = r"&lt;i&gt;(\d)&lt;/i&gt;\d+\.png$"
data = ImageDataBunch.from_name_re(path, fn_paths, pat=pat, ds_tfms=tfms, size=24)
data.classes
&lt;/p&gt;


&lt;p&gt;
data = ImageDataBunch.from_name_func(path, fn_paths, ds_tfms=tfms, size=24,
        label_func = lambda x: '3' if '&lt;i&gt;3&lt;/i&gt;' in str(x) else '7')
data.classes
&lt;/p&gt;


&lt;p&gt;
labels = [('3' if '&lt;i&gt;3&lt;/i&gt;' in str(x) else '7') for x in fn_paths]
labels[:5]
&lt;/p&gt;


&lt;p&gt;
data = ImageDataBunch.from_lists(path, fn_paths, labels=labels, ds_tfms=tfms, size=24)
data.classes
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>deep learning</category><category>fastai</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/fastai/dog-and-cat-breed-classification/</guid><pubDate>Sat, 13 Apr 2019 23:14:46 GMT</pubDate></item></channel></rss>