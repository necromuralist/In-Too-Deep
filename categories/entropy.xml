<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neurotic Networking (Posts about entropy)</title><link>https://necromuralist.github.io/Neurotic-Networking/</link><description></description><atom:link href="https://necromuralist.github.io/Neurotic-Networking/categories/entropy.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Tue, 18 Aug 2020 13:39:54 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Multi-Class Cross Entropy</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/introduction-to-neural-networks/multi-class-cross-entropy/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/introduction-to-neural-networks/multi-class-cross-entropy/#org16603d0"&gt;Our Probabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/introduction-to-neural-networks/multi-class-cross-entropy/#org6614463"&gt;So, what does this mean?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org16603d0" class="outline-2"&gt;
&lt;h2 id="org16603d0"&gt;Our Probabilities&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org16603d0"&gt;
&lt;p&gt;
Weh have three doors behind which could be one of three animals. These are the probabilities that if you open a door, you will find a particular animal behind it.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Animal&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Door 1&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Door 2&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Door 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Duck&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{11}\)&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{12}\)&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{13}\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Beaver&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{21}\)&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{22}\)&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{23}\)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Walrus&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{31}\)&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{32}\)&lt;/td&gt;
&lt;td class="org-left"&gt;\(P_{33}\)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
\[
\textit{Cross Entropy} = - \sum^n_{i=1} \sum^m_{j=1} y_{ij} \ln (p_{ij})
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6614463" class="outline-2"&gt;
&lt;h2 id="org6614463"&gt;So, what does this mean?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6614463"&gt;
&lt;p&gt;
Cross Entropy is inversely proportional to the the total probability of an outcome - so the higher the cross entropy you calculate, the less likely it is that the outcome you are looking at will happen.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>entropy</category><category>lecture</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/introduction-to-neural-networks/multi-class-cross-entropy/</guid><pubDate>Fri, 26 Oct 2018 04:24:59 GMT</pubDate></item></channel></rss>