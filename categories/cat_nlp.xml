<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>In Too Deep (Posts about NLP)</title><link>https://necromuralist.github.io/In-Too-Deep/</link><description></description><atom:link href="https://necromuralist.github.io/In-Too-Deep/categories/cat_nlp.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Mon, 30 Sep 2019 17:36:53 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>NLP Classification Exercise</title><link>https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orgc80bbaf"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orgcbbc543"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orgfc80db3"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#org3de621d"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#org8a76c4e"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orgb78e97f"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orgf2905f2"&gt;The Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#org4d4abb7"&gt;The Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orga10f168"&gt;Some Constants&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orgef6aa59"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#org20dd0a2"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#org678d36c"&gt;The Tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#org9b560ee"&gt;GloVe&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orge8bbce5"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orgb1b77eb"&gt;Citations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/#orgc31f4fe"&gt;Raw&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc80bbaf" class="outline-2"&gt;
&lt;h2 id="orgc80bbaf"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc80bbaf"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcbbc543" class="outline-3"&gt;
&lt;h3 id="orgcbbc543"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcbbc543"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfc80db3" class="outline-4"&gt;
&lt;h4 id="orgfc80db3"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfc80db3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from argparse import Namespace
from functools import partial
from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3de621d" class="outline-4"&gt;
&lt;h4 id="org3de621d"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3de621d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import hvplot.pandas
import numpy
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8a76c4e" class="outline-4"&gt;
&lt;h4 id="org8a76c4e"&gt;Others&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8a76c4e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import (CountPercentage,
		    EmbedHoloviews,
		    SubPathLoader,
		    ZipDownloader)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb78e97f" class="outline-3"&gt;
&lt;h3 id="orgb78e97f"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb78e97f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf2905f2" class="outline-4"&gt;
&lt;h4 id="orgf2905f2"&gt;The Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf2905f2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;slug = "nlp-classification-exercise"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{slug}")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4d4abb7" class="outline-4"&gt;
&lt;h4 id="org4d4abb7"&gt;The Dataset&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4d4abb7"&gt;
&lt;p&gt;
It isn't mentioned in the notebook where the data originally came from, but it looks like it's the &lt;a href="http://help.sentiment140.com/home"&gt;Sentiment140&lt;/a&gt; dataset, which consists of tweets whose sentiment was inferred by emoticons in each tweet.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;url = "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
path = Path("~/data/datasets/texts/sentiment140/").expanduser()
download = ZipDownloader(url, path)
download()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-29 12:26:31,866 [1mZipDownloader[0m start: ([1mZipDownloader[0m) Started: 2019-09-29 12:26:31.866085
2019-09-29 12:26:31,867 [1mZipDownloader[0m download: Downloading the zip file
2019-09-29 12:26:57,309 [1mZipDownloader[0m end: ([1mZipDownloader[0m) Ended: 2019-09-29 12:26:57.309619
2019-09-29 12:26:57,311 [1mZipDownloader[0m end: ([1mZipDownloader[0m) Elapsed: 0:00:25.443534

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;columns = ["polarity", "tweet_id", "datetime", "query", "user", "text"]
training = pandas.read_csv(path/"training.1600000.processed.noemoticon.csv", 
			   encoding="latin-1", names=columns, header=None)
testing = pandas.read_csv(path/"testdata.manual.2009.06.14.csv", 
			   encoding="latin-1", names=columns, header=None)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga10f168" class="outline-4"&gt;
&lt;h4 id="orga10f168"&gt;Some Constants&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga10f168"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Text = Namespace(
    embedding_dim = 100,
    max_length = 16,
    trunc_type='post',
    padding_type='post',
    oov_tok = "&amp;lt;OOV&amp;gt;",
    training_size=16000,
)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgef6aa59" class="outline-2"&gt;
&lt;h2 id="orgef6aa59"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgef6aa59"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org20dd0a2" class="outline-3"&gt;
&lt;h3 id="org20dd0a2"&gt;The Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org20dd0a2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(training.sample().iloc[0])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
polarity                                                    4
tweet_id                                           2001983272
datetime                         Tue Jun 02 02:44:58 PDT 2009
query                                                NO_QUERY
user                                                    pod13
text        @wkdjellybaby to private email??? I will check...
Name: 1283818, dtype: object

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CountPercentage(training.polarity)()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Count&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Percent (%)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-left"&gt;800,000&lt;/td&gt;
&lt;td class="org-right"&gt;50.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-left"&gt;800,000&lt;/td&gt;
&lt;td class="org-right"&gt;50.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
The &lt;code&gt;polarity&lt;/code&gt; is what might also be called the "sentiment" of the tweet - &lt;i&gt;0&lt;/i&gt; mean a negative tweet and &lt;i&gt;4&lt;/i&gt; means a positive tweet.
&lt;/p&gt;

&lt;p&gt;
But, for our purposes, we would be better off if the positive polarity was &lt;code&gt;1&lt;/code&gt;, not &lt;code&gt;4&lt;/code&gt;, so let's convert it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training.loc[training.polarity==4, "polarity"] = 1
counts = CountPercentage(training.polarity)()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Count&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Percent (%)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-left"&gt;800,000&lt;/td&gt;
&lt;td class="org-right"&gt;50.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-left"&gt;800,000&lt;/td&gt;
&lt;td class="org-right"&gt;50.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org678d36c" class="outline-3"&gt;
&lt;h3 id="org678d36c"&gt;The Tokenizer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org678d36c"&gt;
&lt;p&gt;
As you can see from the sample, the data is still in text form so we need to convert it to a numeric form with a Tokenizer.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tokenizer = Tokenizer()
tokenizer.fit_on_texts(training.text.values)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;word_index = tokenizer.word_index
vocabulary_size = len(tokenizer.word_index)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sequences = tokenizer.texts_to_sequences(training.text.values)
padded = pad_sequences(sequences, maxlen=Text.max_length,
		       truncating=Text.trunc_type)

splits = train_test_split(
    padded, training.polarity, test_size=.2)

training_sequences, test_sequences, training_labels, test_labels = splits
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9b560ee" class="outline-3"&gt;
&lt;h3 id="org9b560ee"&gt;GloVe&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9b560ee"&gt;
&lt;p&gt;
GloVe is short for &lt;i&gt;Global Vectors for Word Representation&lt;/i&gt;. It is an &lt;i&gt;unsupervised&lt;/i&gt; algorithm that creates vector representations for words. They have a &lt;a href="https://nlp.stanford.edu/projects/glove/"&gt;site&lt;/a&gt; where you can download pre-trained models or get the code and train one yourself. We're going to use one of their pre-trained models.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path("~/models/glove/").expanduser()
url = "http://nlp.stanford.edu/data/glove.6B.zip"
ZipDownloader(url, path)()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Files exist, not downloading

&lt;/pre&gt;

&lt;p&gt;
The GloVe data is stored as a series of space separated lines with the first column being the word that's encoded and the rest of the columns being the values for the vector. To make this work we're going to split the word off from the vector and put each into a dictionary.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;embeddings = {}
with open(path/"glove.6B.100d.txt") as lines:
    for line in lines:
	tokens = line.split()
	embeddings[tokens[0]] = numpy.array(tokens[1:])
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(f"{len(embeddings):,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
400,000

&lt;/pre&gt;

&lt;p&gt;
So, our vocabulary consists of 400,000 "words" (tokens is more accurate, since they also include punctuation). The problem we have to deal with next is that our data set wasn't part of the dataset used to train the embeddings, so there will probably be some tokens in our data set that aren't in the embeddings. To handle this we need to add zeroed embeddings for the extra tokens.
&lt;/p&gt;

&lt;p&gt;
Rather than adding to the dict, we'll create a matrix of zeros with rows for each word in our datasets vocabulary, then we'll iterate over the words in our dataset and if there's a match in the GloVE embeddings we'll insert it into the matrix.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;embeddings_matrix = numpy.zeros((vocabulary_size+1, Text.embedding_dim));
for word, index in word_index.items():
    embedding_vector = embeddings.get(word);
    if embedding_vector is not None:
	embeddings_matrix[index] = embedding_vector;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(f"{len(embeddings_matrix):,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
690,961

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge8bbce5" class="outline-2"&gt;
&lt;h2 id="orge8bbce5"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge8bbce5"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb1b77eb" class="outline-3"&gt;
&lt;h3 id="orgb1b77eb"&gt;Citations&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb1b77eb"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc31f4fe" class="outline-2"&gt;
&lt;h2 id="orgc31f4fe"&gt;Raw&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc31f4fe"&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>embeddings</category><category>nlp</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/keras/nlp-classification-exercise/</guid><pubDate>Sun, 29 Sep 2019 18:28:06 GMT</pubDate></item><item><title>Embeddings from Scratch</title><link>https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org4a41309"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org3ff541b"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org093f061"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#orgaf6f43a"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org36b47f9"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#orgce4084e"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org5b4728e"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org7b49415"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org4e302e0"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#orgd45e3e4"&gt;Some Constants&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org1614462"&gt;The Embeddings Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org74c2e38"&gt;The Dataset&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org5010690"&gt;Add Padding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#orgc7b8673"&gt;Checkout a Sample&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org954b9de"&gt;Build a Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#org04d5dbc"&gt;Compile and Train&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/#orgda18e8a"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4a41309" class="outline-2"&gt;
&lt;h2 id="org4a41309"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4a41309"&gt;
&lt;p&gt;
This is a walk-through of the tensorflow &lt;a href="https://www.tensorflow.org/beta/tutorials/text/word_embeddings"&gt;Word Embeddings&lt;/a&gt; tutorial, just to make sure I can do it.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3ff541b" class="outline-3"&gt;
&lt;h3 id="org3ff541b"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3ff541b"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org093f061" class="outline-4"&gt;
&lt;h4 id="org093f061"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org093f061"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from argparse import Namespace
from functools import partial
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgaf6f43a" class="outline-4"&gt;
&lt;h4 id="orgaf6f43a"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgaf6f43a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from tensorflow import keras
from tensorflow.keras import layers
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org36b47f9" class="outline-4"&gt;
&lt;h4 id="org36b47f9"&gt;Others&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org36b47f9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import EmbedHoloviews, Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgce4084e" class="outline-3"&gt;
&lt;h3 id="orgce4084e"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgce4084e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5b4728e" class="outline-4"&gt;
&lt;h4 id="org5b4728e"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5b4728e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;prefix = "../../files/posts/keras/"
slug = "embeddings-from-scratch"

Embed = partial(EmbedHoloviews, folder_path=f"{prefix}{slug}")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7b49415" class="outline-4"&gt;
&lt;h4 id="org7b49415"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7b49415"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4e302e0" class="outline-2"&gt;
&lt;h2 id="org4e302e0"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4e302e0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd45e3e4" class="outline-3"&gt;
&lt;h3 id="orgd45e3e4"&gt;Some Constants&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd45e3e4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Text = Namespace(
    vocabulary_size=1000,
    embeddings_size=16,
    max_length=500,
    padding="post",
)

Tokens = Namespace(
    padding = "&amp;lt;PAD&amp;gt;",
    start = "&amp;lt;START&amp;gt;",
    unknown = "&amp;lt;UNKNOWN&amp;gt;",
    unused = "&amp;lt;UNUSED&amp;gt;",
)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1614462" class="outline-3"&gt;
&lt;h3 id="org1614462"&gt;The Embeddings Layer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1614462"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(layers.Embedding.__doc__)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Turns positive integers (indexes) into dense vectors of fixed size.

  e.g. `[[4], [20]] -&amp;gt; [[0.25, 0.1], [0.6, -0.2]]`

  This layer can only be used as the first layer in a model.

  Example:

  ```python
  model = Sequential()
  model.add(Embedding(1000, 64, input_length=10))
  # the model will take as input an integer matrix of size (batch,
  # input_length).
  # the largest integer (i.e. word index) in the input should be no larger
  # than 999 (vocabulary size).
  # now model.output_shape == (None, 10, 64), where None is the batch
  # dimension.

  input_array = np.random.randint(1000, size=(32, 10))

  model.compile('rmsprop', 'mse')
  output_array = model.predict(input_array)
  assert output_array.shape == (32, 10, 64)
  ```

  Arguments:
    input_dim: int &amp;gt; 0. Size of the vocabulary,
      i.e. maximum integer index + 1.
    output_dim: int &amp;gt;= 0. Dimension of the dense embedding.
    embeddings_initializer: Initializer for the `embeddings` matrix.
    embeddings_regularizer: Regularizer function applied to
      the `embeddings` matrix.
    embeddings_constraint: Constraint function applied to
      the `embeddings` matrix.
    mask_zero: Whether or not the input value 0 is a special "padding"
      value that should be masked out.
      This is useful when using recurrent layers
      which may take variable length input.
      If this is `True` then all subsequent layers
      in the model need to support masking or an exception will be raised.
      If mask_zero is set to True, as a consequence, index 0 cannot be
      used in the vocabulary (input_dim should equal size of
      vocabulary + 1).
    input_length: Length of input sequences, when it is constant.
      This argument is required if you are going to connect
      `Flatten` then `Dense` layers upstream
      (without it, the shape of the dense outputs cannot be computed).

  Input shape:
    2D tensor with shape: `(batch_size, input_length)`.

  Output shape:
    3D tensor with shape: `(batch_size, input_length, output_dim)`.
  
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;embedding_layer = layers.Embedding(Text.vocabulary_size, Text.embeddings_size)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The first argument is the number of possible words in the vocabulary and the second is the number of dimensions. The Emebdding is a sort of lookup table that maps an integer that represents a word to a vector. In this case we're going to build a vocabulary of 1,000 words represented by vectors with a length of 32. The weights in the vectors are learned when we train the model and will encode the distance between words.
&lt;/p&gt;

&lt;p&gt;
The input to the embeddings layer is a 2D tensor of integers with the shape (&lt;code&gt;number of samples&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;). The sequences are integer-encoded sentences of the same length - so you have to pad the shorter sentences to match the longest one (the &lt;code&gt;sequence_length&lt;/code&gt;).
&lt;/p&gt;

&lt;p&gt;
The ouput of the embeddings layer is a 3D tensor with the shape (&lt;code&gt;number of samples&lt;/code&gt;, &lt;code&gt;sequence_length&lt;/code&gt;, &lt;code&gt;embedding_dimensionality&lt;/code&gt;).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org74c2e38" class="outline-3"&gt;
&lt;h3 id="org74c2e38"&gt;The Dataset&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org74c2e38"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(train_data, test_data), info = tensorflow_datasets.load(
    "imdb_reviews/subwords8k",
    split=(tensorflow_datasets.Split.TRAIN,
	   tensorflow_datasets.Split.TEST),
    with_info=True, as_supervised=True)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;encoder = info.features["text"].encoder
print(encoder.subwords[:10])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br']

&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5010690" class="outline-4"&gt;
&lt;h4 id="org5010690"&gt;Add Padding&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5010690"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;padded_shapes = ([None], ())
train_batches = train_data.shuffle(Text.vocabulary_size).padded_batch(
    10, padded_shapes=padded_shapes)
test_batches = test_data.shuffle(Text.vocabulary_size).padded_batch(
    10, padded_shapes=padded_shapes
)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc7b8673" class="outline-4"&gt;
&lt;h4 id="orgc7b8673"&gt;Checkout a Sample&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc7b8673"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;batch, labels = next(iter(train_batches))
print(batch.numpy())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[[  62    9    4 ...    0    0    0]
 [  19 2428    6 ...    0    0    0]
 [ 691    2  594 ... 7961 1457 7975]
 ...
 [6072 5644 8043 ...    0    0    0]
 [ 977   15   57 ...    0    0    0]
 [5646    2    1 ...    0    0    0]]

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org954b9de" class="outline-3"&gt;
&lt;h3 id="org954b9de"&gt;Build a Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org954b9de"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = keras.Sequential([
    layers.Embedding(encoder.vocab_size, Text.embeddings_size),
    layers.GlobalAveragePooling1D(),
    layers.Dense(1, activation="sigmoid")
])
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(model.summary())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 16)          130960    
_________________________________________________________________
global_average_pooling1d (Gl (None, 16)                0         
_________________________________________________________________
dense (Dense)                (None, 1)                 17        
=================================================================
Total params: 130,977
Trainable params: 130,977
Non-trainable params: 0
_________________________________________________________________
None
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org04d5dbc" class="outline-3"&gt;
&lt;h3 id="org04d5dbc"&gt;Compile and Train&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org04d5dbc"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
ONCE_PER_EPOCH = 2
with TIMER:
    history = model.fit(train_batches, epochs=10,
			validation_data=test_batches,
			verbose=ONCE_PER_EPOCH,
			validation_steps=20)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-28 17:14:52,764 graeae.timers.timer start: Started: 2019-09-28 17:14:52.764725
I0928 17:14:52.764965 140515023214400 timer.py:70] Started: 2019-09-28 17:14:52.764725
W0928 17:14:52.806057 140515023214400 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Epoch 1/10
 val_loss: 0.3015 - val_accuracy: 0.8900
2019-09-28 17:17:36,036 graeae.timers.timer end: Ended: 2019-09-28 17:17:36.036090
I0928 17:17:36.036139 140515023214400 timer.py:77] Ended: 2019-09-28 17:17:36.036090
2019-09-28 17:17:36,037 graeae.timers.timer end: Elapsed: 0:02:43.271365
I0928 17:17:36.037808 140515023214400 timer.py:78] Elapsed: 0:02:43.271365
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgda18e8a" class="outline-2"&gt;
&lt;h2 id="orgda18e8a"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgda18e8a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="Training/Validation Performance",
			  width=1000,
			  height=800)
Embed(plot=plot, file_name="training")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/training.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
Amazingly, even with such a simple model, it managed a 92 % validation accuracy.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>embeddings</category><category>keras</category><category>nlp</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/keras/embeddings-from-scratch/</guid><pubDate>Wed, 25 Sep 2019 20:30:12 GMT</pubDate></item><item><title>IMDB GRU With Tokenization</title><link>https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org7522313"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org1489e63"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org0293677"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#orgd91cbee"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org1f51413"&gt;Other&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#orgedf8918"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#orgb78a946"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org2ba920a"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org2bf2feb"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org2edbe0b"&gt;Set Up the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org5526922"&gt;Building Up the Tokenizer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org0e285dc"&gt;Split Up the Sentences and Their Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org2783779"&gt;Some Constants&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#orgee2f0ee"&gt;Build the Tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org8f912ea"&gt;Decoder Ring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#orgb404e0f"&gt;Build the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org60dde02"&gt;Train it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#org0ff73a7"&gt;Plot It&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/#orga02d6d3"&gt;Raw&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7522313" class="outline-2"&gt;
&lt;h2 id="org7522313"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7522313"&gt;
&lt;p&gt;
This is another version of the RNN model to classify the IMDB reviews, but this time we're going to tokenize it ourselves and use a GRU, instead of using the tensorflow-datasets version.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1489e63" class="outline-3"&gt;
&lt;h3 id="org1489e63"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1489e63"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0293677" class="outline-4"&gt;
&lt;h4 id="org0293677"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0293677"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from argparse import Namespace
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd91cbee" class="outline-4"&gt;
&lt;h4 id="orgd91cbee"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd91cbee"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import hvplot.pandas
import numpy
import pandas
import tensorflow
import tensorflow_datasets
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1f51413" class="outline-4"&gt;
&lt;h4 id="org1f51413"&gt;Other&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1f51413"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import Timer, EmbedHoloviews
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgedf8918" class="outline-3"&gt;
&lt;h3 id="orgedf8918"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgedf8918"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb78a946" class="outline-4"&gt;
&lt;h4 id="orgb78a946"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb78a946"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2ba920a" class="outline-4"&gt;
&lt;h4 id="org2ba920a"&gt;Plotting&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2bf2feb" class="outline-2"&gt;
&lt;h2 id="org2bf2feb"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2bf2feb"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2edbe0b" class="outline-3"&gt;
&lt;h3 id="org2edbe0b"&gt;Set Up the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2edbe0b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;imdb, info = tensorflow_datasets.load("imdb_reviews",
				      with_info=True,
				      as_supervised=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
WARNING: Logging before flag parsing goes to stderr.
W0924 21:52:10.158111 139862640383808 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.

&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training, testing = imdb["train"], imdb["test"]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5526922" class="outline-3"&gt;
&lt;h3 id="org5526922"&gt;Building Up the Tokenizer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5526922"&gt;
&lt;p&gt;
Since we didn't pass in a specifier for the configuration we wanted (e.g. &lt;code&gt;imdb/subwords8k&lt;/code&gt;) it defaulted to giving us the plain text reviews (and their labels) so we have to build the tokenizer ourselves.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0e285dc" class="outline-4"&gt;
&lt;h4 id="org0e285dc"&gt;Split Up the Sentences and Their Labels&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0e285dc"&gt;
&lt;p&gt;
As you might recall, the data set consists of 50,000 IMDB movie reviews categorized as positive or negative. To build the tokenize we first have to split the sentences from their labels
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_sentences = []
training_labels = []
testing_sentences = []
testing_labels = []
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    for sentence, label in training:
	training_sentences.append(str(sentence.numpy()))
	training_labels.append(str(label.numpy()))


    for sentence, label in testing:
	testing_sentences.append(str(sentence.numpy))
	testing_labels.append(str(label.numpy()))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-24 21:52:11,396 graeae.timers.timer start: Started: 2019-09-24 21:52:11.395126
I0924 21:52:11.396310 139862640383808 timer.py:70] Started: 2019-09-24 21:52:11.395126
2019-09-24 21:52:18,667 graeae.timers.timer end: Ended: 2019-09-24 21:52:18.667789
I0924 21:52:18.667830 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:18.667789
2019-09-24 21:52:18,670 graeae.timers.timer end: Elapsed: 0:00:07.272663
I0924 21:52:18.670069 139862640383808 timer.py:78] Elapsed: 0:00:07.272663

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_labels_final = numpy.array(training_labels)
testing_labels_final = numpy.array(testing_labels)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2783779" class="outline-4"&gt;
&lt;h4 id="org2783779"&gt;Some Constants&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2783779"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Text = Namespace(
    vocab_size = 10000,
    embedding_dim = 16,
    max_length = 120,
    trunc_type='post',
    oov_token = "&amp;lt;OOV&amp;gt;",
)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgee2f0ee" class="outline-3"&gt;
&lt;h3 id="orgee2f0ee"&gt;Build the Tokenizer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgee2f0ee"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tokenizer = Tokenizer(num_words=Text.vocab_size, oov_token=Text.oov_token)
with TIMER:
    tokenizer.fit_on_texts(training_sentences)

    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(training_sentences)
    padded = pad_sequences(sequences, maxlen=Text.max_length, truncating=Text.trunc_type)

    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
    testing_padded = pad_sequences(testing_sequences, maxlen=Text.max_length)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-24 21:52:21,705 graeae.timers.timer start: Started: 2019-09-24 21:52:21.705287
I0924 21:52:21.705317 139862640383808 timer.py:70] Started: 2019-09-24 21:52:21.705287
2019-09-24 21:52:32,152 graeae.timers.timer end: Ended: 2019-09-24 21:52:32.152267
I0924 21:52:32.152314 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:32.152267
2019-09-24 21:52:32,154 graeae.timers.timer end: Elapsed: 0:00:10.446980
I0924 21:52:32.154620 139862640383808 timer.py:78] Elapsed: 0:00:10.446980

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8f912ea" class="outline-3"&gt;
&lt;h3 id="org8f912ea"&gt;Decoder Ring&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8f912ea"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;index_to_word = {value: key for key, value in word_index.items()}

def decode_review(text: numpy.array) -&amp;gt; str:
    return " ".join([index_to_word.get(item, "&amp;lt;?&amp;gt;") for item in text])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb404e0f" class="outline-3"&gt;
&lt;h3 id="orgb404e0f"&gt;Build the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb404e0f"&gt;
&lt;p&gt;
This time we're going to build a four-layer model with one Bidirectional layer that uses a &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU"&gt;GRU&lt;/a&gt; (&lt;a href="https://www.wikiwand.com/en/Gated_recurrent_unit"&gt;Gated Recurrent Unit&lt;/a&gt;) instead of a LSTM.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(Text.vocab_size, Text.embedding_dim, input_length=Text.max_length),
    tensorflow.keras.layers.Bidirectional(tensorflow.compat.v2.keras.layers.GRU(32)),
    tensorflow.keras.layers.Dense(6, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(model.summary())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
bidirectional (Bidirectional (None, 64)                9600      
_________________________________________________________________
dense (Dense)                (None, 6)                 390       
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 169,997
Trainable params: 169,997
Non-trainable params: 0
_________________________________________________________________
None
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org60dde02" class="outline-3"&gt;
&lt;h3 id="org60dde02"&gt;Train it&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org60dde02"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;EPOCHS = 50
ONCE_PER_EPOCH = 2
batch_size = 8
history = model.fit(padded, training_labels_final,
		    epochs=EPOCHS,
		    batch_size=batch_size,
		    validation_data=(testing_padded, testing_labels_final),
		    verbose=ONCE_PER_EPOCH)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0ff73a7" class="outline-3"&gt;
&lt;h3 id="org0ff73a7"&gt;Plot It&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0ff73a7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="GRU Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="gru_training")()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga02d6d3" class="outline-2"&gt;
&lt;h2 id="orga02d6d3"&gt;Raw&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga02d6d3"&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>gru</category><category>nlp</category><category>tokenization</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-lstm-with-tokenization/</guid><pubDate>Mon, 23 Sep 2019 21:14:04 GMT</pubDate></item><item><title>Multi-Layer LSTM</title><link>https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#orgd994ec2"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#orgf3ec838"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org3b50025"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org0d45145"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#orgd79ef7f"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org37e70b6"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#orgade859f"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#orgb3f6621"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org792f5d6"&gt;The Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org7a2bc37"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org0ec5523"&gt;Set Up the Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org0fe8824"&gt;The Model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org144495a"&gt;Embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#orgf9646ef"&gt;Bidirectional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#orgbf69ab5"&gt;LSTM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org2987b9b"&gt;Compile It&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#org552337b"&gt;Train the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/#orgc6932fb"&gt;Looking at the Performance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd994ec2" class="outline-2"&gt;
&lt;h2 id="orgd994ec2"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd994ec2"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf3ec838" class="outline-3"&gt;
&lt;h3 id="orgf3ec838"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf3ec838"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3b50025" class="outline-4"&gt;
&lt;h4 id="org3b50025"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3b50025"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
from pathlib import Path
import pickle
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0d45145" class="outline-4"&gt;
&lt;h4 id="org0d45145"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0d45145"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import holoviews
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd79ef7f" class="outline-4"&gt;
&lt;h4 id="orgd79ef7f"&gt;Others&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd79ef7f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import Timer, EmbedHoloviews
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org37e70b6" class="outline-3"&gt;
&lt;h3 id="org37e70b6"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org37e70b6"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgade859f" class="outline-4"&gt;
&lt;h4 id="orgade859f"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgade859f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb3f6621" class="outline-4"&gt;
&lt;h4 id="orgb3f6621"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb3f6621"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Embed = partial(EmbedHoloviews,
		folder_path="../../files/posts/keras/multi-layer-lstm/")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org792f5d6" class="outline-4"&gt;
&lt;h4 id="org792f5d6"&gt;The Dataset&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org792f5d6"&gt;
&lt;p&gt;
This once again uses the &lt;a href="https://www.tensorflow.org/datasets/catalog/imdb_reviews"&gt;IMDB dataset&lt;/a&gt; with 50,000 reviews. It has already been converted from strings to integers - each word is encoded as its own integer. Adding &lt;code&gt;with_info=True&lt;/code&gt; returns an object that contains the dictionary with the word to integer mapping. Passing in &lt;code&gt;imdb_reviews/subwords8k&lt;/code&gt; limits the vocabulary to 8,000 words.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Note:&lt;/b&gt;&lt;/b&gt; The first time you run this it will download a fairly large dataset so it might appear to hang, but after the first time it is fairly quick.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dataset, info = tensorflow_datasets.load("imdb_reviews/subwords8k",
					 with_info=True,
					 as_supervised=True)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7a2bc37" class="outline-2"&gt;
&lt;h2 id="org7a2bc37"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7a2bc37"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0ec5523" class="outline-3"&gt;
&lt;h3 id="org0ec5523"&gt;Set Up the Datasets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0ec5523"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_dataset, test_dataset = dataset["train"], dataset["test"]
tokenizer = info.features['text'].encoder
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now we're going to shuffle and padd the data. The &lt;code&gt;BUFFER_SIZE&lt;/code&gt; argument sets the size of the data to sample from. In this case 10,000 entries in the training set will be selected to be put in the buffer and then the "shuffle" is created by randomly selecting items from the buffer, replacing each item as it's selected until all the data has been through the buffer. The &lt;code&gt;padded_batch&lt;/code&gt; method creates batches of consecutive data and pads them so that they are all the same shape.
&lt;/p&gt;

&lt;p&gt;
The BATCH_SIZE needs to be tuned a little. If it's too big the amount of memory needed might keep the GPU from being able to use it (and it might not generalize), and if it's too small, you will take a long time to train, so you have to do a little tuning. If you train it and the GPU process percentage stays at 0, try reducing the Batch Size.
&lt;/p&gt;

&lt;p&gt;
Also note that if you change the batch-size you have to go back to the previous step and re-define &lt;code&gt;train_dataset&lt;/code&gt; and &lt;code&gt;test_dataset&lt;/code&gt; because we alter them in the next step and re-altering them makes the shape wrong somehow.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;BUFFER_SIZE = 10000
# if the batch size is too big it will run out of memory on the GPU 
# so you might have to experiment with this
BATCH_SIZE = 32

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0fe8824" class="outline-3"&gt;
&lt;h3 id="org0fe8824"&gt;The Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0fe8824"&gt;
&lt;p&gt;
The previous model had one Bidirectional layer, this will add a second one.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org144495a" class="outline-4"&gt;
&lt;h4 id="org144495a"&gt;Embedding&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org144495a"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/guide/embedding"&gt;Embedding layer&lt;/a&gt; converts our inputs of integers and converts them to vectors of real-numbers, which is a better input for a neural network.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf9646ef" class="outline-4"&gt;
&lt;h4 id="orgf9646ef"&gt;Bidirectional&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf9646ef"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional"&gt;Bidirectional layer&lt;/a&gt; is a wrapper for Recurrent Neural Networks.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbf69ab5" class="outline-4"&gt;
&lt;h4 id="orgbf69ab5"&gt;LSTM&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbf69ab5"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM"&gt;LSTM layer&lt;/a&gt; implements Long-Short-Term Memory. The first argument is the size of the outputs. This is similar to the model that we ran previously on the same data, but it has an extra layer (so it uses more memory).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tensorflow.keras.layers.Bidirectional(
	tensorflow.keras.layers.LSTM(64, return_sequences=True)),
    tensorflow.keras.layers.Bidirectional(
	tensorflow.keras.layers.LSTM(32)),
    tensorflow.keras.layers.Dense(64, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(model.summary())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         66048     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 64)                41216     
_________________________________________________________________
dense (Dense)                (None, 64)                4160      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 635,329
Trainable params: 635,329
Non-trainable params: 0
_________________________________________________________________
None
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2987b9b" class="outline-4"&gt;
&lt;h4 id="org2987b9b"&gt;Compile It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2987b9b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.compile(loss='binary_crossentropy',
	      optimizer="adam",
	      metrics=['accuracy'])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org552337b" class="outline-3"&gt;
&lt;h3 id="org552337b"&gt;Train the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org552337b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ONCE_PER_EPOCH = 2
NUM_EPOCHS = 10
with TIMER:
    history = model.fit(train_dataset,
			epochs=NUM_EPOCHS,
			validation_data=test_dataset,
			verbose=ONCE_PER_EPOCH)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-21 17:26:50,395 graeae.timers.timer start: Started: 2019-09-21 17:26:50.394797
I0921 17:26:50.395130 140275698915136 timer.py:70] Started: 2019-09-21 17:26:50.394797
Epoch 1/10
W0921 17:26:51.400280 140275698915136 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
782/782 - 224s - loss: 0.6486 - accuracy: 0.6039 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/10
782/782 - 214s - loss: 0.4941 - accuracy: 0.7661 - val_loss: 0.6706 - val_accuracy: 0.6744
Epoch 3/10
782/782 - 216s - loss: 0.4087 - accuracy: 0.8266 - val_loss: 0.4024 - val_accuracy: 0.8222
Epoch 4/10
782/782 - 217s - loss: 0.2855 - accuracy: 0.8865 - val_loss: 0.3343 - val_accuracy: 0.8645
Epoch 5/10
782/782 - 216s - loss: 0.2097 - accuracy: 0.9217 - val_loss: 0.2936 - val_accuracy: 0.8837
Epoch 6/10
782/782 - 217s - loss: 0.1526 - accuracy: 0.9467 - val_loss: 0.3188 - val_accuracy: 0.8771
Epoch 7/10
782/782 - 215s - loss: 0.1048 - accuracy: 0.9657 - val_loss: 0.3750 - val_accuracy: 0.8710
Epoch 8/10
782/782 - 216s - loss: 0.0764 - accuracy: 0.9757 - val_loss: 0.3821 - val_accuracy: 0.8762
Epoch 9/10
782/782 - 216s - loss: 0.0585 - accuracy: 0.9832 - val_loss: 0.4747 - val_accuracy: 0.8683
Epoch 10/10
782/782 - 216s - loss: 0.0438 - accuracy: 0.9883 - val_loss: 0.4441 - val_accuracy: 0.8704
2019-09-21 18:02:56,353 graeae.timers.timer end: Ended: 2019-09-21 18:02:56.353722
I0921 18:02:56.353781 140275698915136 timer.py:77] Ended: 2019-09-21 18:02:56.353722
2019-09-21 18:02:56,356 graeae.timers.timer end: Elapsed: 0:36:05.958925
I0921 18:02:56.356238 140275698915136 timer.py:78] Elapsed: 0:36:05.958925
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc6932fb" class="outline-3"&gt;
&lt;h3 id="orgc6932fb"&gt;Looking at the Performance&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc6932fb"&gt;
&lt;p&gt;
To get the history I had to pickle it and then copy it over to the machine with this org-notebook, so you can't just run this notebook and make it work unless everything is run on the same machine (which it wasn't).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path("~/history.pkl").expanduser()
with path.open("wb") as writer:
    pickle.dump(history.history, writer)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path("~/history.pkl").expanduser()
with path.open("rb") as reader:
    history = pickle.load(reader)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame(history)
best = data.val_loss.idxmin()
best_line = holoviews.VLine(best)
plot = (data.hvplot() * best_line).opts(
    title="Two-Layer LSTM Model",
    width=1000,
    height=800)
Embed(plot=plot, file_name="lstm_training")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/lstm_training.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
It looks like the best epoch was the fifth one, with a validation loss of 0.29 and a validation accuracy of 0.88, after that it looks like it overfits. It seems that text might be a harder problem than images.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>lstm</category><category>nlp</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/</guid><pubDate>Thu, 19 Sep 2019 23:07:27 GMT</pubDate></item><item><title>IMDB Reviews Tensorflow Dataset</title><link>https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orge23fb75"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orgad65138"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org5b58e19"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orgdcddfd8"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org9805f61"&gt;Graeae&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orgb570205"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org2693d03"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orgde0647f"&gt;Timer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org5520663"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org36ab843"&gt;Get the Dataset&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org226b164"&gt;Load It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orgbc03c79"&gt;Split It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org55e3295"&gt;The Tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org1c32dba"&gt;Set Up Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org1e2b834"&gt;The Model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orga907404"&gt;Compile It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orgd3feb5f"&gt;Train It&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org1efc999"&gt;Plot the Performance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org79f019b"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orgd827336"&gt;Citation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge23fb75" class="outline-2"&gt;
&lt;h2 id="orge23fb75"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge23fb75"&gt;
&lt;p&gt;
We're going to use the &lt;a href="https://www.tensorflow.org/datasets/catalog/imdb_reviews"&gt;IMDB Reviews Dataset&lt;/a&gt; (used in &lt;a href="https://www.tensorflow.org/tutorials/keras/basic_text_classification"&gt;this tutorial&lt;/a&gt;) - a set of 50,000 movie reviews taken from the &lt;a href="https://www.imdb.com/"&gt;Internet Movie Database&lt;/a&gt; that have been classified as either positive or negative. It looks like the original source is from a page on Stanford University's web sight title &lt;a href="http://ai.stanford.edu/~amaas/data/sentiment/"&gt;Large Movie Review Dataset&lt;/a&gt;. The dataset seems to be widely available (the Stanford page and &lt;a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"&gt;Kaggle&lt;/a&gt; for instance) but this will serve as practice for using tensorflow datasets as well.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgad65138" class="outline-3"&gt;
&lt;h3 id="orgad65138"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgad65138"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5b58e19" class="outline-4"&gt;
&lt;h4 id="org5b58e19"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5b58e19"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdcddfd8" class="outline-4"&gt;
&lt;h4 id="orgdcddfd8"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdcddfd8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hvplot.pandas&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow_datasets&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9805f61" class="outline-4"&gt;
&lt;h4 id="org9805f61"&gt;Graeae&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9805f61"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EmbedHoloviews&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb570205" class="outline-3"&gt;
&lt;h3 id="orgb570205"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb570205"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2693d03" class="outline-4"&gt;
&lt;h4 id="org2693d03"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2693d03"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;SLUG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"imdb-reviews-tensorflow-dataset"&lt;/span&gt;
&lt;span class="n"&gt;Embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EmbedHoloviews&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;folder_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"../../files/posts/keras/{SLUG}"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgde0647f" class="outline-4"&gt;
&lt;h4 id="orgde0647f"&gt;Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgde0647f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;TIMER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5520663" class="outline-2"&gt;
&lt;h2 id="org5520663"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5520663"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org36ab843" class="outline-3"&gt;
&lt;h3 id="org36ab843"&gt;Get the Dataset&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org36ab843"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org226b164" class="outline-4"&gt;
&lt;h4 id="org226b164"&gt;Load It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org226b164"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/load"&gt;load&lt;/a&gt; function takes quite a few parameters, in this case we're just passing in three - the name of the dataset, &lt;code&gt;with_info&lt;/code&gt; which tells it to return both a &lt;a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"&gt;Dataset&lt;/a&gt; and a &lt;a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetInfo"&gt;DatasetInfo&lt;/a&gt; object, and &lt;code&gt;as_supervised&lt;/code&gt;, which tells the builder to return the &lt;code&gt;Dataset&lt;/code&gt; as a series of &lt;code&gt;(input, label)&lt;/code&gt; tuples.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tensorflow_datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'imdb_reviews/subwords8k'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					 &lt;span class="n"&gt;with_info&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					 &lt;span class="n"&gt;as_supervised&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbc03c79" class="outline-4"&gt;
&lt;h4 id="orgbc03c79"&gt;Split It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbc03c79"&gt;
&lt;p&gt;
The &lt;code&gt;dataset&lt;/code&gt; is a dict with three keys:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
dict_keys(['test', 'train', 'unsupervised'])

&lt;/pre&gt;

&lt;p&gt;
As you might guess, we don't use the &lt;code&gt;unsupervised&lt;/code&gt; key.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'train'&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'test'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org55e3295" class="outline-4"&gt;
&lt;h4 id="org55e3295"&gt;The Tokenizer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org55e3295"&gt;
&lt;p&gt;
One of the advantages of using the tensorflow dataset version of this is that it comes with a pre-built tokenizer inside the DatasetInfo object.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=&amp;lt;SubwordTextEncoder vocab_size=8185&amp;gt;),
})

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'text'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encoder&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
&amp;lt;SubwordTextEncoder vocab_size=8185&amp;gt;

&lt;/pre&gt;

&lt;p&gt;
The &lt;code&gt;tokenizer&lt;/code&gt; is a &lt;a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder"&gt;SubwordTextEncoder&lt;/a&gt; with a vocabulary size of 8,185.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1c32dba" class="outline-4"&gt;
&lt;h4 id="org1c32dba"&gt;Set Up Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1c32dba"&gt;
&lt;p&gt;
We're going to shuffle the training data and then add padding to both sets so theyre all the same size.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;BUFFER_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20000&lt;/span&gt;
&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;
&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BUFFER_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;padded_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_shapes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;padded_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_shapes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1e2b834" class="outline-3"&gt;
&lt;h3 id="org1e2b834"&gt;The Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1e2b834"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vocab_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Bidirectional&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LSTM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
    &lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'sigmoid'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, 128)               66048     
_________________________________________________________________
dense (Dense)                (None, 64)                8256      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 598,209
Trainable params: 598,209
Non-trainable params: 0
_________________________________________________________________
&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga907404" class="outline-4"&gt;
&lt;h4 id="orga907404"&gt;Compile It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga907404"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	      &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'adam'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	      &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'accuracy'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd3feb5f" class="outline-4"&gt;
&lt;h4 id="orgd3feb5f"&gt;Train It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd3feb5f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;EPOCHS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;SILENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;ONCE_PER_EPOCH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;TIMER&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;EPOCHS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			&lt;span class="n"&gt;validation_data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ONCE_PER_EPOCH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-21 15:52:50,469 graeae.timers.timer start: Started: 2019-09-21 15:52:50.469787
I0921 15:52:50.469841 140086305412928 timer.py:70] Started: 2019-09-21 15:52:50.469787
Epoch 1/10
391/391 - 80s - loss: 0.3991 - accuracy: 0.8377 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/10
391/391 - 80s - loss: 0.3689 - accuracy: 0.8571 - val_loss: 0.4595 - val_accuracy: 0.8021
Epoch 3/10
391/391 - 80s - loss: 0.3664 - accuracy: 0.8444 - val_loss: 0.5262 - val_accuracy: 0.7228
Epoch 4/10
391/391 - 80s - loss: 0.5611 - accuracy: 0.7133 - val_loss: 0.6832 - val_accuracy: 0.6762
Epoch 5/10
391/391 - 80s - loss: 0.6151 - accuracy: 0.6597 - val_loss: 0.5164 - val_accuracy: 0.7844
Epoch 6/10
391/391 - 80s - loss: 0.3842 - accuracy: 0.8340 - val_loss: 0.4970 - val_accuracy: 0.7996
Epoch 7/10
391/391 - 80s - loss: 0.2449 - accuracy: 0.9058 - val_loss: 0.3639 - val_accuracy: 0.8463
Epoch 8/10
391/391 - 80s - loss: 0.1896 - accuracy: 0.9306 - val_loss: 0.3698 - val_accuracy: 0.8614
Epoch 9/10
391/391 - 80s - loss: 0.1555 - accuracy: 0.9456 - val_loss: 0.3896 - val_accuracy: 0.8535
Epoch 10/10
391/391 - 80s - loss: 0.1195 - accuracy: 0.9606 - val_loss: 0.4878 - val_accuracy: 0.8428
2019-09-21 16:06:09,935 graeae.timers.timer end: Ended: 2019-09-21 16:06:09.935707
I0921 16:06:09.935745 140086305412928 timer.py:77] Ended: 2019-09-21 16:06:09.935707
2019-09-21 16:06:09,938 graeae.timers.timer end: Elapsed: 0:13:19.465920
I0921 16:06:09.938812 140086305412928 timer.py:78] Elapsed: 0:13:19.465920
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1efc999" class="outline-4"&gt;
&lt;h4 id="org1efc999"&gt;Plot the Performance&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1efc999"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;Note&lt;/b&gt;&lt;/b&gt;: This only works if your kernel is on the local machine, running it remotely gives an error, as it tries to save it on the remote machine.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"loss"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Training Loss"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="s2"&gt;"accuracy"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Training Accuracy"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="s2"&gt;"val_loss"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Validation Loss"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="s2"&gt;"val_accuracy"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"Validation Accuracy"&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hvplot&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"LSTM IMDB Performance"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Embed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"model_performance"&lt;/span&gt;&lt;span class="p"&gt;)()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/model_performance.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
It looks like I over-trained it, as the loss is getting high. (Also note that I used this notebook to troubleshoot so there was actually one extra epoch that isn't shown).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org79f019b" class="outline-2"&gt;
&lt;h2 id="org79f019b"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org79f019b"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd827336" class="outline-3"&gt;
&lt;h3 id="orgd827336"&gt;Citation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd827336"&gt;
&lt;p&gt;
This is the paper where the dataset was originally used.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><category>sentiment</category><category>tensorflow</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/</guid><pubDate>Mon, 09 Sep 2019 23:24:46 GMT</pubDate></item><item><title>BBC News Classification</title><link>https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#orgd856751"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#orgdd0ffba"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org574e07d"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org1b09336"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#orge53059d"&gt;Graeae&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org660701c"&gt;Setup&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#orge49f651"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org31f8dbb"&gt;The Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org005e72d"&gt;Spacy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#orge8de0c2"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#orga3816f5"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org7551a07"&gt;Load the Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#orga7da45d"&gt;The Tokenizers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#orgd7f8f0d"&gt;Making the Sequences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org7ca8ff3"&gt;Make training and testing sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org3bccb4f"&gt;The Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org55194b8"&gt;Plotting the Performance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/#org4364f82"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd856751" class="outline-2"&gt;
&lt;h2 id="orgd856751"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd856751"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdd0ffba" class="outline-3"&gt;
&lt;h3 id="orgdd0ffba"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdd0ffba"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org574e07d" class="outline-4"&gt;
&lt;h4 id="org574e07d"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org574e07d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
from pathlib import Path
import csv
import random
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1b09336" class="outline-4"&gt;
&lt;h4 id="org1b09336"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1b09336"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from sklearn.model_selection import train_test_split
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import hvplot.pandas
import numpy
import pandas
import spacy
import tensorflow
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge53059d" class="outline-4"&gt;
&lt;h4 id="orge53059d"&gt;Graeae&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge53059d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import EmbedHoloviews, SubPathLoader, Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org660701c" class="outline-3"&gt;
&lt;h3 id="org660701c"&gt;Setup&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org660701c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge49f651" class="outline-4"&gt;
&lt;h4 id="orge49f651"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge49f651"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org31f8dbb" class="outline-4"&gt;
&lt;h4 id="org31f8dbb"&gt;The Environment&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org31f8dbb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ENVIRONMENT = SubPathLoader('DATASETS')
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org005e72d" class="outline-4"&gt;
&lt;h4 id="org005e72d"&gt;Spacy&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org005e72d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;spacy.prefer_gpu()
nlp = spacy.load("en_core_web_lg")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge8de0c2" class="outline-4"&gt;
&lt;h4 id="orge8de0c2"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge8de0c2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;SLUG = "bbc-news-classification"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{SLUG}")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga3816f5" class="outline-2"&gt;
&lt;h2 id="orga3816f5"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga3816f5"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7551a07" class="outline-3"&gt;
&lt;h3 id="org7551a07"&gt;Load the Datasets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7551a07"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()

texts = []
labels = []
with TIMER:
    with path.open() as csvfile:
	lines = csv.DictReader(csvfile)
	for line in lines:
	    labels.append(line["category"])
	    texts.append(nlp(line["text"]))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
WARNING: Logging before flag parsing goes to stderr.
I0908 13:32:14.804769 139839933974336 environment.py:35] Environment Path: /home/athena/.env
I0908 13:32:14.806000 139839933974336 environment.py:90] Environment Path: /home/athena/.config/datasets/env
2019-09-08 13:32:14,806 graeae.timers.timer start: Started: 2019-09-08 13:32:14.806861
I0908 13:32:14.806965 139839933974336 timer.py:70] Started: 2019-09-08 13:32:14.806861
2019-09-08 13:33:37,430 graeae.timers.timer end: Ended: 2019-09-08 13:33:37.430228
I0908 13:33:37.430259 139839933974336 timer.py:77] Ended: 2019-09-08 13:33:37.430228
2019-09-08 13:33:37,431 graeae.timers.timer end: Elapsed: 0:01:22.623367
I0908 13:33:37.431128 139839933974336 timer.py:78] Elapsed: 0:01:22.623367

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(texts[random.randrange(len(texts))])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
candidate resigns over bnp link a prospective candidate for the uk independence party (ukip) has resigned after admitting a  brief attachment  to the british national party(bnp).  nicholas betts-green  who had been selected to fight the suffolk coastal seat  quit after reports in a newspaper that he attended a bnp meeting. the former teacher confirmed he had attended the meeting but said that was the only contact he had with the group. mr betts-green resigned after being questioned by the party s leadership. a ukip spokesman said mr betts-green s resignation followed disclosures in the east anglian daily times last month about his attendance at a bnp meeting.  he did once attend a bnp meeting. he did not like what he saw and heard and will take no further part of it   the spokesman added. a meeting of suffolk coastal ukip members is due to be held next week to discuss a replacement. mr betts-green  of woodbridge  suffolk  has also resigned as ukip s branch chairman.

&lt;/pre&gt;

&lt;p&gt;
So, it looks like the text has been lower-cased but there's still punctuation and extra white-space.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(f"Rows: {len(labels):,}")
print(f"Unique Labels: {len(set(labels)):,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Rows: 2,225
Unique Labels: 5

&lt;/pre&gt;

&lt;p&gt;
Since there's only five maybe we should plot it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;labels_frame = pandas.DataFrame({"label": labels})
counts = labels_frame.label.value_counts().reset_index().rename(
    columns={"index": "Category", "label": "Articles"})
plot = counts.hvplot.bar("Category", "Articles").opts(
    title="Count of BBC News Articles by Category",
    height=800, width=1000)
Embed(plot=plot, file_name="bbc_category_counts")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/bbc_category_counts.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
It looks like the categories are somewhat unevenly distributed. Now to normalize the tokens.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    cleaned = [[token.lemma_ for token in text if not any((token.is_stop, token.is_space, token.is_punct))]
	       for text in texts]
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-08 13:33:40,257 graeae.timers.timer start: Started: 2019-09-08 13:33:40.257908
I0908 13:33:40.257930 139839933974336 timer.py:70] Started: 2019-09-08 13:33:40.257908
2019-09-08 13:33:40,810 graeae.timers.timer end: Ended: 2019-09-08 13:33:40.810135
I0908 13:33:40.810176 139839933974336 timer.py:77] Ended: 2019-09-08 13:33:40.810135
2019-09-08 13:33:40,811 graeae.timers.timer end: Elapsed: 0:00:00.552227
I0908 13:33:40.811067 139839933974336 timer.py:78] Elapsed: 0:00:00.552227

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga7da45d" class="outline-3"&gt;
&lt;h3 id="orga7da45d"&gt;The Tokenizers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga7da45d"&gt;
&lt;p&gt;
Even though I've already tokenized the texts, we need to eventually one-hot-encode them so I'll use the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"&gt;tensorflow keras Tokenizer&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Note:&lt;/b&gt; The labels tokenizer doesn't get the out-of-vocabulary token, only the text-tokenizer does.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tokenizer = Tokenizer(num_words=1000, oov_token="&amp;lt;OOV&amp;gt;")
labels_tokenizer = Tokenizer()
labels_tokenizer.fit_on_texts(labels)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The &lt;code&gt;num_words&lt;/code&gt; is the total amount of words that will be kept in the word index - I don't know why a thousand, I just found that in the "answer" notebook. The &lt;code&gt;oov_token&lt;/code&gt; is what's used when a word is encountered outside of the words we're building into our word-index (&lt;i&gt;Out Of Vocabulary&lt;/i&gt;). The next step is to create the word-index by fitting the tokenizer to the text.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    tokenizer.fit_on_texts(cleaned)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-08 14:59:30,671 graeae.timers.timer start: Started: 2019-09-08 14:59:30.671536
I0908 14:59:30.671563 139839933974336 timer.py:70] Started: 2019-09-08 14:59:30.671536
2019-09-08 14:59:30,862 graeae.timers.timer end: Ended: 2019-09-08 14:59:30.862483
I0908 14:59:30.862523 139839933974336 timer.py:77] Ended: 2019-09-08 14:59:30.862483
2019-09-08 14:59:30,863 graeae.timers.timer end: Elapsed: 0:00:00.190947
I0908 14:59:30.863504 139839933974336 timer.py:78] Elapsed: 0:00:00.190947

&lt;/pre&gt;

&lt;p&gt;
The tokenizer now has a dictionary named &lt;code&gt;word_index&lt;/code&gt; that holds the words:index pairs for all the tokens found (it only uses the &lt;code&gt;num_words&lt;/code&gt; when you call tokenizer's methods according to &lt;a href="https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work"&gt;Stack Overflow&lt;/a&gt;).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(f"{len(tokenizer.word_index):,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
24,339

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd7f8f0d" class="outline-3"&gt;
&lt;h3 id="orgd7f8f0d"&gt;Making the Sequences&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd7f8f0d"&gt;
&lt;p&gt;
I've trained the Tokenizer so that it has a word-index, but now we have to one hot encode our texts and pad them so they're all the same length.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;MAX_LENGTH = 120
sequences = tokenizer.texts_to_sequences(cleaned)
padded = pad_sequences(sequences, padding="post", maxlen=MAX_LENGTH)
labels_sequenced = labels_tokenizer.texts_to_sequences(labels)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7ca8ff3" class="outline-3"&gt;
&lt;h3 id="org7ca8ff3"&gt;Make training and testing sets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7ca8ff3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TESTING = 0.2
x_train, x_test, y_train, y_test = train_test_split(
    padded, labels_sequenced,
    test_size=TESTING)
x_train, x_validation, y_train, y_validation = train_test_split(
    x_train, y_train, test_size=TESTING)

y_train = numpy.array(y_train)
y_test = numpy.array(y_test)
y_validation = numpy.array(y_validation)

print(f"Training: {x_train.shape}")
print(f"Validation: {x_validation.shape}")
print(f"Testing: {x_test.shape}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Training: (1424, 120)
Validation: (356, 120)
Testing: (445, 120)

&lt;/pre&gt;

&lt;p&gt;
&lt;b&gt;Note:&lt;/b&gt; I originally forgot to pass the &lt;code&gt;TESTING&lt;/code&gt; variable with the keyword &lt;code&gt;test_size&lt;/code&gt; and got an error that I couldn't use a Singleton array - don't forget the keywords when you pass in anything other than the data to &lt;code&gt;train_test_split&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3bccb4f" class="outline-3"&gt;
&lt;h3 id="org3bccb4f"&gt;The Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3bccb4f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vocabulary_size = 1000
embedding_dimension = 16
max_length=120

model = tensorflow.keras.Sequential([
    layers.Embedding(vocabulary_size, embedding_dimension,
		     input_length=max_length),
    layers.GlobalAveragePooling1D(),
    layers.Dense(24, activation="relu"),
    layers.Dense(6, activation="softmax"),
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 120, 16)           16000     
_________________________________________________________________
global_average_pooling1d_1 ( (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 24)                408       
_________________________________________________________________
dense_3 (Dense)              (None, 6)                 150       
=================================================================
Total params: 16,558
Trainable params: 16,558
Non-trainable params: 0
_________________________________________________________________
None
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.fit(x_train, y_train, epochs=30,
	  validation_data=(x_validation, y_validation), verbose=2)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Train on 1424 samples, validate on 356 samples
Epoch 1/30
1424/1424 - 0s - loss: 1.7623 - accuracy: 0.2879 - val_loss: 1.7257 - val_accuracy: 0.5000
Epoch 2/30
1424/1424 - 0s - loss: 1.6871 - accuracy: 0.5190 - val_loss: 1.6332 - val_accuracy: 0.5281
Epoch 3/30
1424/1424 - 0s - loss: 1.5814 - accuracy: 0.4782 - val_loss: 1.5118 - val_accuracy: 0.4944
Epoch 4/30
1424/1424 - 0s - loss: 1.4417 - accuracy: 0.4677 - val_loss: 1.3543 - val_accuracy: 0.5365
Epoch 5/30
1424/1424 - 0s - loss: 1.2706 - accuracy: 0.5934 - val_loss: 1.1850 - val_accuracy: 0.7022
Epoch 6/30
1424/1424 - 0s - loss: 1.1075 - accuracy: 0.6749 - val_loss: 1.0387 - val_accuracy: 0.8006
Epoch 7/30
1424/1424 - 0s - loss: 0.9606 - accuracy: 0.8483 - val_loss: 0.9081 - val_accuracy: 0.8567
Epoch 8/30
1424/1424 - 0s - loss: 0.8244 - accuracy: 0.8869 - val_loss: 0.7893 - val_accuracy: 0.8848
Epoch 9/30
1424/1424 - 0s - loss: 0.6963 - accuracy: 0.9164 - val_loss: 0.6747 - val_accuracy: 0.8961
Epoch 10/30
1424/1424 - 0s - loss: 0.5815 - accuracy: 0.9228 - val_loss: 0.5767 - val_accuracy: 0.9185
Epoch 11/30
1424/1424 - 0s - loss: 0.4831 - accuracy: 0.9375 - val_loss: 0.4890 - val_accuracy: 0.9270
Epoch 12/30
1424/1424 - 0s - loss: 0.3991 - accuracy: 0.9473 - val_loss: 0.4195 - val_accuracy: 0.9326
Epoch 13/30
1424/1424 - 0s - loss: 0.3321 - accuracy: 0.9508 - val_loss: 0.3669 - val_accuracy: 0.9438
Epoch 14/30
1424/1424 - 0s - loss: 0.2800 - accuracy: 0.9572 - val_loss: 0.3268 - val_accuracy: 0.9494
Epoch 15/30
1424/1424 - 0s - loss: 0.2385 - accuracy: 0.9656 - val_loss: 0.2936 - val_accuracy: 0.9438
Epoch 16/30
1424/1424 - 0s - loss: 0.2053 - accuracy: 0.9740 - val_loss: 0.2693 - val_accuracy: 0.9466
Epoch 17/30
1424/1424 - 0s - loss: 0.1775 - accuracy: 0.9761 - val_loss: 0.2501 - val_accuracy: 0.9466
Epoch 18/30
1424/1424 - 0s - loss: 0.1557 - accuracy: 0.9789 - val_loss: 0.2332 - val_accuracy: 0.9494
Epoch 19/30
1424/1424 - 0s - loss: 0.1362 - accuracy: 0.9831 - val_loss: 0.2189 - val_accuracy: 0.9522
Epoch 20/30
1424/1424 - 0s - loss: 0.1209 - accuracy: 0.9853 - val_loss: 0.2082 - val_accuracy: 0.9551
Epoch 21/30
1424/1424 - 0s - loss: 0.1070 - accuracy: 0.9860 - val_loss: 0.1979 - val_accuracy: 0.9579
Epoch 22/30
1424/1424 - 0s - loss: 0.0952 - accuracy: 0.9888 - val_loss: 0.1897 - val_accuracy: 0.9551
Epoch 23/30
1424/1424 - 0s - loss: 0.0854 - accuracy: 0.9902 - val_loss: 0.1815 - val_accuracy: 0.9579
Epoch 24/30
1424/1424 - 0s - loss: 0.0765 - accuracy: 0.9916 - val_loss: 0.1761 - val_accuracy: 0.9522
Epoch 25/30
1424/1424 - 0s - loss: 0.0689 - accuracy: 0.9930 - val_loss: 0.1729 - val_accuracy: 0.9579
Epoch 26/30
1424/1424 - 0s - loss: 0.0618 - accuracy: 0.9951 - val_loss: 0.1680 - val_accuracy: 0.9551
Epoch 27/30
1424/1424 - 0s - loss: 0.0559 - accuracy: 0.9958 - val_loss: 0.1633 - val_accuracy: 0.9551
Epoch 28/30
1424/1424 - 0s - loss: 0.0505 - accuracy: 0.9958 - val_loss: 0.1594 - val_accuracy: 0.9579
Epoch 29/30
1424/1424 - 0s - loss: 0.0457 - accuracy: 0.9965 - val_loss: 0.1559 - val_accuracy: 0.9522
Epoch 30/30
1424/1424 - 0s - loss: 0.0416 - accuracy: 0.9972 - val_loss: 0.1544 - val_accuracy: 0.9551
&lt;/pre&gt;

&lt;p&gt;
It seems to get good suprisingly fast - it might be overfitting toward the end.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;loss, accuracy =model.evaluate(x_test, y_test, verbose=0)
print(f"Loss: {loss: .2f} Accuracy: {accuracy:.2f}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Loss:  0.16 Accuracy: 0.95

&lt;/pre&gt;

&lt;p&gt;
It does pretty well, even on the test set.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org55194b8" class="outline-3"&gt;
&lt;h3 id="org55194b8"&gt;Plotting the Performance&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org55194b8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame(model.history.history)
plot = data.hvplot().opts(title="Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="model_performance")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/model_performance.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
Unlike with the image classifications, the validation performance never quite matches the training performance (although it's quite good), probably because we aren't doing any kind of augmentation the way you tend to do with images.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4364f82" class="outline-2"&gt;
&lt;h2 id="org4364f82"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4364f82"&gt;
&lt;p&gt;
Okay, so we seem to have a decent model, but is that really the end-game? No, we want to be able to predict what classification a new input should get.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;index_to_label = {value:key for (key, value) in labels_tokenizer.word_index.items()}

def category(text: str) -&amp;gt; None:
    """Categorizes the text

    Args:
     text: text to categorize
    """
    text = tokenizer.texts_to_sequences([text])
    predictions = model.predict(pad_sequences(text, maxlen=MAX_LENGTH))
    print(f"Predicted Category: {index_to_label[predictions.argmax()]}")
    return
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;text = "crickets are nutritious and delicious but make for such a silly game"
category(text)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Predicted Category: sport

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;text = "i like butts that are big and round, something something like a xxx throw down, and so does the house of parliament"
category(text)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Predicted Category: sport

&lt;/pre&gt;

&lt;p&gt;
It kind of looks like it's biased toward sports.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;text = "tv future hand viewer home theatre"
category(text)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Predicted Category: sport

&lt;/pre&gt;

&lt;p&gt;
Something isn't right here.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/keras/bbc-news-classification/</guid><pubDate>Mon, 26 Aug 2019 22:28:56 GMT</pubDate></item><item><title>Cleaning the BBC News Archive</title><link>https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#org6a45e48"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#org84adb27"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#orga73eb97"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#orgafcfd84"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#orgd8393d9"&gt;Graeae&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#orgeba8be9"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#org50ac458"&gt;The Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#org448ec37"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#org00150a6"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#orge52bdf5"&gt;The DataSet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#orga02ae95"&gt;The Tokenizer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#orgbd051c7"&gt;Convert the Texts To Sequences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#org451dd05"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#org736c393"&gt;Sources&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/#org4e2251d"&gt;The Original Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6a45e48" class="outline-2"&gt;
&lt;h2 id="org6a45e48"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6a45e48"&gt;
&lt;p&gt;
This is an initial look at cleaning up &lt;a href="http://mlg.ucd.ie/datasets/bbc.html"&gt;a text dataset&lt;/a&gt; from the BBC News archives. Although the exercise sites this as the source the dataset provided doesn't look like the actual raw dataset which is broken up into folders that classify the contents and each news item is in a separate file. Instead we're starting with a &lt;a href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv"&gt;partially pre-processed&lt;/a&gt; CSV that has been lower-cased and the classification is given as the first column in the dataset.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org84adb27" class="outline-3"&gt;
&lt;h3 id="org84adb27"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org84adb27"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga73eb97" class="outline-4"&gt;
&lt;h4 id="orga73eb97"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga73eb97"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgafcfd84" class="outline-4"&gt;
&lt;h4 id="orgafcfd84"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgafcfd84"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd8393d9" class="outline-4"&gt;
&lt;h4 id="orgd8393d9"&gt;Graeae&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd8393d9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import SubPathLoader, Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgeba8be9" class="outline-3"&gt;
&lt;h3 id="orgeba8be9"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgeba8be9"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org50ac458" class="outline-4"&gt;
&lt;h4 id="org50ac458"&gt;The Environment&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org50ac458"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ENVIRONMENT = SubPathLoader("DATASETS")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org448ec37" class="outline-4"&gt;
&lt;h4 id="org448ec37"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org448ec37"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org00150a6" class="outline-2"&gt;
&lt;h2 id="org00150a6"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org00150a6"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge52bdf5" class="outline-3"&gt;
&lt;h3 id="orge52bdf5"&gt;The DataSet&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge52bdf5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bbc_path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()
with TIMER:
    data = pandas.read_csv(bbc_path/"bbc-text.csv")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-08-25 18:51:38,411 graeae.timers.timer start: Started: 2019-08-25 18:51:38.411196
2019-08-25 18:51:38,658 graeae.timers.timer end: Ended: 2019-08-25 18:51:38.658181
2019-08-25 18:51:38,658 graeae.timers.timer end: Elapsed: 0:00:00.246985

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
(2225, 2)

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data.sample().iloc[0])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
category                                                sport
text        bell set for england debut bath prop duncan be...
Name: 2134, dtype: object

&lt;/pre&gt;

&lt;p&gt;
So we have two columns - &lt;code&gt;category&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;, text being the one we have to clean up.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data.text.dtype)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
object

&lt;/pre&gt;

&lt;p&gt;
That's not such an informative answer, but I checked and each row of text is a single string.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga02ae95" class="outline-3"&gt;
&lt;h3 id="orga02ae95"&gt;The Tokenizer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga02ae95"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"&gt;Keras Tokenizer&lt;/a&gt; tokenizes the text for us as well as removing the punctuation, lower-casing the text, and some other things. We're also going to use a Out-of-Vocabulary token of "&amp;lt;OOV&amp;gt;" to identify words that are outside of the vocabulary when converting new texts to sequences.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tokenizer = Tokenizer(oov_token="&amp;lt;OOV&amp;gt;", num_words=100)
tokenizer.fit_on_texts(data.text)
word_index = tokenizer.word_index
print(len(word_index))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
29727

&lt;/pre&gt;

&lt;p&gt;
The word-index is a dict that maps words found in the documents to counts.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbd051c7" class="outline-4"&gt;
&lt;h4 id="orgbd051c7"&gt;Convert the Texts To Sequences&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbd051c7"&gt;
&lt;p&gt;
We're going to convert each of our texts to a sequence of numbers representing the words in them (one-hot-encoding). The &lt;code&gt;pad_sequences&lt;/code&gt; function adds zeros to the end of sequences that are shorter than the longest one so that they are all the same size.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sequences = tokenizer.texts_to_sequences(data.text)
padded = pad_sequences(sequences, padding="post")
print(padded[0])
print(padded.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[1 1 7 ... 0 0 0]
(2225, 4491)

&lt;/pre&gt;

&lt;p&gt;
Strangely there doesn't appear to be a good way to use stopwords. Maybe sklearn is more appropriate here.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vectorizer = CountVectorizer(stop_words=stopwords.words("english"),
			     lowercase=True, min_df=3,
			     max_df=0.9, max_features=5000)
vectors = vectorizer.fit_transform(data.text)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org451dd05" class="outline-2"&gt;
&lt;h2 id="org451dd05"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org451dd05"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org736c393" class="outline-3"&gt;
&lt;h3 id="org736c393"&gt;Sources&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org736c393"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4e2251d" class="outline-4"&gt;
&lt;h4 id="org4e2251d"&gt;The Original Dataset&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4e2251d"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;D. Greene and P. Cunningham. "Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering", Proc. ICML 2006. [PDF] [BibTeX].&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cleaning</category><category>nlp</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/keras/cleaning-the-bbc-news-archive/</guid><pubDate>Mon, 26 Aug 2019 00:14:54 GMT</pubDate></item></channel></rss>