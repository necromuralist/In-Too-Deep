<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neurotic Networking (Posts about exercise)</title><link>https://necromuralist.github.io/Neurotic-Networking/</link><description></description><atom:link href="https://necromuralist.github.io/Neurotic-Networking/categories/exercise.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img id="license-image" alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /&gt;&lt;/a&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.</copyright><lastBuildDate>Fri, 23 Oct 2020 01:36:44 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Horses And Humans</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org067acbf"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org8c709bc"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org5e69dab"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org5a41435"&gt;The Data Set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org48b62d8"&gt;A Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org5f64229"&gt;Compile the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#orga2a757d"&gt;Transform the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org8e183e0"&gt;Training the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#orgda93db8"&gt;Looking At Some Predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#orgc115930"&gt;Visualizing The Layer Outputs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org5afba98"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/#org7df6871"&gt;Source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org067acbf" class="outline-2"&gt;
&lt;h2 id="org067acbf"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org067acbf"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8c709bc" class="outline-3"&gt;
&lt;h3 id="org8c709bc"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8c709bc"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgae1dc10" class="outline-4"&gt;
&lt;h4 id="orgae1dc10"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgae1dc10"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
from pathlib import Path
import random
import zipfile
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org73cdff4" class="outline-4"&gt;
&lt;h4 id="org73cdff4"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org73cdff4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from expects import (
    be_true,
    expect,
)
from holoviews.operation.datashader import datashade
from keras import backend
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import (ImageDataGenerator,
						  img_to_array, load_img)
import cv2
import holoviews
import matplotlib.pyplot as pyplot
import numpy
import requests
import tensorflow
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd3126c3" class="outline-4"&gt;
&lt;h4 id="orgd3126c3"&gt;My Stuff&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd3126c3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import EmbedHoloviews
Embed = partial(EmbedHoloviews, 
		folder_path="../../files/posts/keras/horses-and-humans/")
holoviews.extension("bokeh")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5e69dab" class="outline-2"&gt;
&lt;h2 id="org5e69dab"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5e69dab"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5a41435" class="outline-3"&gt;
&lt;h3 id="org5a41435"&gt;The Data Set&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5a41435"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;OUTPUT = "~/data/datasets/images/horse-or-human/training/"
output_path = Path(OUTPUT).expanduser()
if not output_path.is_dir():
    print("Downloading the images")
    URL = ("https://storage.googleapis.com/"
	   "laurencemoroney-blog.appspot.com/"
	   "horse-or-human.zip")
    response = requests.get(URL)
    ZIP = "/tmp/horse-or-human.zip"
    with open(ZIP, "wb") as writer:
	writer.write(response.content)
    print(f"Downloaded zip to {ZIP}")
    with zipfile.ZipFile(ZIP, "r") as unzipper:
	unzipper.extractall(output_path)
else:
    print("Files exist, not downloading")
expect(output_path.is_dir()).to(be_true)
for thing in output_path.iterdir():
    print(thing)
data_path = output_path
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Files exist, not downloading
/home/athena/data/datasets/images/horse-or-human/training/horses
/home/athena/data/datasets/images/horse-or-human/training/humans
&lt;/pre&gt;


&lt;p&gt;
The convention for training models for computer vision appears to be that you use the folder names to label the contents of the images within them. In this case we have &lt;code&gt;horses&lt;/code&gt; and &lt;code&gt;humans&lt;/code&gt;.
&lt;/p&gt;


&lt;p&gt;
Here's what some of the files themselves are named.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;horses_path = output_path/"horses"
humans_path = output_path/"humans"

for path in (horses_path, humans_path):
    print(path.name)
    for index, image in enumerate(path.iterdir()):
	print(f"File: {image.name}")
	if index == 9:
	    break
    print()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
horses
File: horse48-5.png
File: horse45-8.png
File: horse13-5.png
File: horse34-4.png
File: horse46-5.png
File: horse02-3.png
File: horse06-3.png
File: horse32-1.png
File: horse25-3.png
File: horse04-3.png

humans
File: human01-07.png
File: human02-11.png
File: human13-07.png
File: human10-10.png
File: human15-06.png
File: human05-15.png
File: human06-18.png
File: human16-28.png
File: human02-24.png
File: human10-05.png

&lt;/pre&gt;

&lt;p&gt;
So, in this case you can tell what they are from the file-names as well. How many images are there?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;horse_files = list(horses_path.iterdir())
human_files = list(humans_path.iterdir())
print(f"Horse Images: {len(horse_files)}")
print(f"Human Images: {len(human_files)})")
print(f"Image Shape: {pyplot.imread(str(horse_files[0])).shape}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Horse Images: 500
Human Images: 527)
Image Shape: (300, 300, 4)
&lt;/pre&gt;


&lt;p&gt;
This is sort of a small data-set, and it's odd that there are more humans than horses. Let's see what some of them look like. I'm assuming all the files have the same shape. In this case it looks like they are 300 x 300 with four channels (RGB and alpha?).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;height = width = 300
count = 4
columns = 2
horse_plots = [datashade(holoviews.RGB.load_image(str(horse)).opts(
    height=height,
    width=width,
))
	       for horse in horse_files[:count]]
human_plots = [datashade(holoviews.RGB.load_image(str(human))).opts(
    height=height,
    width=width,
)
	       for human in human_files[:count]]

plot = holoviews.Layout(horse_plots + human_plots).cols(2).opts(
    title="Horses and Humans")
Embed(plot=plot, file_name="horses_and_humans", 
      height_in_pixels=900)()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/horses_and_humans.html" style="width:100%" height="900"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
As you can see, the people in the images aren't really humans (and it may not be so obvious, but they aren't horses either), these are computer-generated images.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org48b62d8" class="outline-3"&gt;
&lt;h3 id="org48b62d8"&gt;A Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org48b62d8"&gt;
&lt;p&gt;
As before, the model will be a sequential model with convolutional layers. In this case we'll have five convolutional layers before passing the convolved images to the fully-connected layer. Although my inspection showed that the images have 4 channels, the model in the example only uses 3.
&lt;/p&gt;

&lt;p&gt;
Also, in this case we are doing a binary classification (it's either a human or a horse, so instead of the softmax activation function on the output layer we're using a &lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function?oldformat=true"&gt;Sigmoid function&lt;/a&gt; (&lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid"&gt;documentation link&lt;/a&gt;).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = tensorflow.keras.models.Sequential()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org061d6ff" class="outline-4"&gt;
&lt;h4 id="org061d6ff"&gt;The Input Layer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org061d6ff"&gt;
&lt;p&gt;
The input layer is a Convolutional layer with 16 layers and a 3 x 3 filter (all the convolutions use the same filter shape). All the convolutional layers are also followed by a max-pooling layer that halves their size.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.add(tensorflow.keras.layers.Conv2D(16, (3,3), 
					 activation='relu', 
					 input_shape=(300, 300, 3)))
model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge65c7cc" class="outline-4"&gt;
&lt;h4 id="orge65c7cc"&gt;The Rest Of The Convolutional Layers&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge65c7cc"&gt;
&lt;p&gt;
The remaining convolutional layers increase the depth by doubling until they reach 64.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# The second convolution
model.add(tensorflow.keras.layers.Conv2D(32, (3,3), 
					 activation='relu'))
model.add(tensorflow.keras.layers.MaxPooling2D(2,2))

# The third convolution
model.add(tensorflow.keras.layers.Conv2D(64, (3,3), 
					 activation='relu'))
model.add(tensorflow.keras.layers.MaxPooling2D(2,2))

# The fourth convolution
model.add(tensorflow.keras.layers.Conv2D(64, (3,3), 
					 activation='relu'))
model.add(tensorflow.keras.layers.MaxPooling2D(2,2))

# The fifth convolution
model.add(tensorflow.keras.layers.Conv2D(64, (3,3), 
					 activation='relu'))
model.add(tensorflow.keras.layers.MaxPooling2D(2,2))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0713135" class="outline-4"&gt;
&lt;h4 id="org0713135"&gt;The Fully Connected Layer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0713135"&gt;
&lt;p&gt;
Once we have the convolved version of our image, we feed it into the fully-connected layer to get a classification.
&lt;/p&gt;

&lt;p&gt;
First we flatten the input into a vector.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.add(tensorflow.keras.layers.Flatten())
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Then we feed the input into a 512 neuron fully-connected layer.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.add(tensorflow.keras.layers.Dense(512, activation='relu'))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
And now we get to our output layer which makes the prediction of whether the image is a human or a horse.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.add(tensorflow.keras.layers.Dense(1, activation='sigmoid'))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
One thing that's not so obvious is what the output means - is it predicting that it's a human or that it's a horse? There isn't really anything to indicate which is which. Presumably, like the case with the MNIST and Fashion MNIST, the alphabetical ordering of the folders is what determines what we're predicting.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf2902d2" class="outline-4"&gt;
&lt;h4 id="orgf2902d2"&gt;A Summary of the Model.&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf2902d2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(model.summary())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_10 (Conv2D)           (None, 298, 298, 16)      448       
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 149, 149, 16)      0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 147, 147, 32)      4640      
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 73, 73, 32)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 71, 71, 64)        18496     
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 35, 35, 64)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 33, 33, 64)        36928     
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 14, 14, 64)        36928     
_________________________________________________________________
max_pooling2d_14 (MaxPooling (None, 7, 7, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 3136)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 512)               1606144   
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 513       
=================================================================
Total params: 1,704,097
Trainable params: 1,704,097
Non-trainable params: 0
_________________________________________________________________
None
&lt;/pre&gt;

&lt;p&gt;
That's a lot of parametersâ¦ It's interesting to note that by the time the data gets fed into the &lt;code&gt;Flatten&lt;/code&gt; layer it has been reduced to a 7 x 7 x 64 matrix.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(f"300 x 300 x 3 = {300 * 300 * 3:,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
300 x 300 x 3 = 270,000
&lt;/pre&gt;


&lt;p&gt;
So the original input has been reduced form 270,000 pixels to 3,136 when it gets to the fully-connected layer.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5f64229" class="outline-3"&gt;
&lt;h3 id="org5f64229"&gt;Compile the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5f64229"&gt;
&lt;p&gt;
The optimizer we're going to use is the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop"&gt;RMSprop&lt;/a&gt; optimizer, which, unlike  SGD, tunes the learning rate as it progresses. Also, since we only have two categories, the loss function will be &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/binary_crossentropy"&gt;binary crossentropy&lt;/a&gt;. Our metric will once again be &lt;i&gt;accuracy&lt;/i&gt;.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.compile(loss='binary_crossentropy',
	      optimizer=RMSprop(lr=0.001),
	      metrics=['acc'])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga2a757d" class="outline-3"&gt;
&lt;h3 id="orga2a757d"&gt;Transform the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga2a757d"&gt;
&lt;p&gt;
We're going to use the &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"&gt;ImageDataGenerator&lt;/a&gt; to preprocess the images to get them to normalized and batched. This class also supports transforming the images to create more variety in the training set.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_data_generator = ImageDataGenerator(rescale=1/255)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory"&gt;flow_from_directory&lt;/a&gt; method takes a path to the directory of images and generates batches of augmented data.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_batches = training_data_generator.flow_from_directory(
    data_path, 
    target_size=(300, 300),
    batch_size=128,
    class_mode='binary')
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Found 1027 images belonging to 2 classes.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8e183e0" class="outline-3"&gt;
&lt;h3 id="org8e183e0"&gt;Training the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8e183e0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;history = model.fit_generator(
    training_batches,
    steps_per_epoch=8,  
    epochs=15,
    verbose=2)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch 1/15
8/8 - 5s - loss: 0.7879 - acc: 0.5732
Epoch 2/15
8/8 - 4s - loss: 0.7427 - acc: 0.6615
Epoch 3/15
8/8 - 4s - loss: 0.8984 - acc: 0.6897
Epoch 4/15
8/8 - 4s - loss: 0.3973 - acc: 0.8165
Epoch 5/15
8/8 - 4s - loss: 0.2011 - acc: 0.9188
Epoch 6/15
8/8 - 5s - loss: 1.2254 - acc: 0.7373
Epoch 7/15
8/8 - 4s - loss: 0.2228 - acc: 0.8902
Epoch 8/15
8/8 - 4s - loss: 0.1798 - acc: 0.9333
Epoch 9/15
8/8 - 5s - loss: 0.2079 - acc: 0.9287
Epoch 10/15
8/8 - 4s - loss: 0.3128 - acc: 0.8999
Epoch 11/15
8/8 - 4s - loss: 0.0782 - acc: 0.9722
Epoch 12/15
8/8 - 4s - loss: 0.0683 - acc: 0.9711
Epoch 13/15
8/8 - 4s - loss: 0.1263 - acc: 0.9789
Epoch 14/15
8/8 - 5s - loss: 0.6828 - acc: 0.8574
Epoch 15/15
8/8 - 4s - loss: 0.0453 - acc: 0.9855
&lt;/pre&gt;

&lt;p&gt;
The training loss is very low and we seem to have reached 100% accuracy. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgda93db8" class="outline-3"&gt;
&lt;h3 id="orgda93db8"&gt;Looking At Some Predictions&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgda93db8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test_path = Path("~/test_images/").expanduser()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;height = width = 400
plots = [datashade(holoviews.RGB.load_image(str(path))).opts(
    title=f"{path.name}",
    height=height,
    width=width
) for path in test_path.iterdir()]
plot = holoviews.Layout(plots).cols(2).opts(title="Test Images")
Embed(plot=plot, file_name="test_images", height_in_pixels=900)()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/test_images.html" style="width:100%" height="900"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3ffb15b" class="outline-4"&gt;
&lt;h4 id="org3ffb15b"&gt;Horse&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3ffb15b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;target_size = (300, 300)

images = (("horse.jpg", "Horse"), 
	  ("centaur.jpg", "Centaur"), 
	  ("tomb_figure.jpg", "Statue of a Man Riding a Horse"),
	  ("rembrandt.jpg", "Woman"))
for filename, label in images:
    loaded = cv2.imread(str(test_path/filename))
    x = cv2.resize(loaded, target_size)
    x = numpy.reshape(x, (1, 300, 300, 3))
    prediction = model.predict(x)
    predicted = "human" if prediction[0] &amp;gt; 0.5 else "horse"
    print(f"The {label} is a {predicted}.")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
The Horse is a horse.
The Centaur is a horse.
The Statue of a Man Riding a Horse is a human.
The Woman is a horse.
&lt;/pre&gt;


&lt;p&gt;
Strangely, the model predicted the woman was a horse.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc115930" class="outline-3"&gt;
&lt;h3 id="orgc115930"&gt;Visualizing The Layer Outputs&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc115930"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;outputs = [layer.output for layer in model.layers[1:]]
new_model = Model(inputs=model.input, outputs=outputs)
image_path = random.choice(horse_files + human_files)
image = load_img(image_path, target_size=target_size)
x = img_to_array(image)
x = x.reshape((1,) + x.shape)

x /= 255.

predictions = new_model.predict(x)
layer_names = [layer.name for layer in model.layers]
for layer_name, feature_map in zip(layer_names, predictions):
  if len(feature_map.shape) == 4:
    # Just do this for the conv / maxpool layers, not the fully-connected layers
    n_features = feature_map.shape[-1]  # number of features in feature map
    # The feature map has shape (1, size, size, n_features)
    size = feature_map.shape[1]
    # We will tile our images in this matrix
    display_grid = numpy.zeros((size, size * n_features))
    for i in range(n_features):
      # Postprocess the feature to make it visually palatable
      x = feature_map[0, :, :, i]
      x -= x.mean()
      x /= x.std()
      x *= 64
      x += 128
      x = numpy.clip(x, 0, 255).astype('uint8')
      # We'll tile each filter into this big horizontal grid
      display_grid[:, i * size : (i + 1) * size] = x
    # Display the grid
    scale = 20. / n_features
    pyplot.figure(figsize=(scale * n_features, scale))
    pyplot.title(layer_name)
    pyplot.grid(False)
    pyplot.imshow(display_grid, aspect='auto', cmap='viridis')
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/layer_visualization.png" alt="layer_visualization.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Some of the images seem blank (or nearly so). It's hard to really interpret what's going on here.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5afba98" class="outline-2"&gt;
&lt;h2 id="org5afba98"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5afba98"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7df6871" class="outline-3"&gt;
&lt;h3 id="org7df6871"&gt;Source&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7df6871"&gt;
&lt;p&gt;
This is a walk-through of the &lt;a href="https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%208%20-%20Lesson%202%20-%20Notebook.ipynb"&gt;Course 1 - Part 8 - Lesson 2 - Notebook.ipynb&lt;/a&gt; on github.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>exercise</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/</guid><pubDate>Thu, 04 Jul 2019 23:36:16 GMT</pubDate></item><item><title>Handwriting Recognition Exercise</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#orgb1039e0"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#org5a70fcf"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#org4752cc8"&gt;The Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#orge42f48f"&gt;The Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#orgf46b570"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#org82fe831"&gt;The Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#org3632ab0"&gt;The Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#orgb37d9bc"&gt;The Callback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#orga36a1b9"&gt;Trying Some Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#orgbc429a7"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/#org344c2f6"&gt;Source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb1039e0" class="outline-2"&gt;
&lt;h2 id="orgb1039e0"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb1039e0"&gt;
&lt;p&gt;
The goal of this exercise is to train a classifier on the &lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST&lt;/a&gt; dataset that reaches 99% during training without using a fixed number of training epochs.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5a70fcf" class="outline-3"&gt;
&lt;h3 id="org5a70fcf"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5a70fcf"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org07559e0" class="outline-4"&gt;
&lt;h4 id="org07559e0"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org07559e0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Namespace&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdbaefcc" class="outline-4"&gt;
&lt;h4 id="orgdbaefcc"&gt;From PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdbaefcc"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;holoviews&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfee9eae" class="outline-4"&gt;
&lt;h4 id="orgfee9eae"&gt;My Stuff&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfee9eae"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graeae.visualization.embed&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EmbedHoloview&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4752cc8" class="outline-3"&gt;
&lt;h3 id="org4752cc8"&gt;The Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4752cc8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;embed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;EmbedHoloview&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    &lt;span class="n"&gt;folder_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"../../files/posts/keras/handwriting-recognition-exercise/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Namespace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;600&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extension&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"bokeh"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge42f48f" class="outline-3"&gt;
&lt;h3 id="orge42f48f"&gt;The Dataset&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge42f48f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testing_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;testing_labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mnist&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_data&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf46b570" class="outline-2"&gt;
&lt;h2 id="orgf46b570"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf46b570"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org82fe831" class="outline-3"&gt;
&lt;h3 id="org82fe831"&gt;The Dataset&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org82fe831"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd313092" class="outline-4"&gt;
&lt;h4 id="orgd313092"&gt;What do we have here?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd313092"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Training Images: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;,&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; (&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; x &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Training Images: 60,000 (28 x 28)
&lt;/pre&gt;


&lt;p&gt;
The Fashion MNIST dataset that I looked at previously was meant to be a drop-in replacement for this data set so it has the same number of images and the images are the same size.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;plot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;holoviews&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;opts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"hover"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"MNIST Handwritten &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;embed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"sample_image"&lt;/span&gt;&lt;span class="p"&gt;)()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/sample_image.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
The dataset is a set of hand-written digits (one each image) that we want to be able to classify.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
0
255
&lt;/pre&gt;


&lt;p&gt;
The images are 28 x 28 matrices of values from 0 (representing black) to 255 (representing white).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8a69703" class="outline-4"&gt;
&lt;h4 id="org8a69703"&gt;Normalizing the Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8a69703"&gt;
&lt;p&gt;
We want the values to be from 0 to 1 so I'm going to normalize them.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;training_images_normalized&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;training_images&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;
&lt;span class="n"&gt;testing_images_normalized&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;testing_images&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_images_normalized&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_images_normalized&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
1.0
0.0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3632ab0" class="outline-3"&gt;
&lt;h3 id="org3632ab0"&gt;The Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3632ab0"&gt;
&lt;p&gt;
This is going to be a model with one hidden layer.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Build a sequential model with one hidden layer&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     units: number of units in the hidden layer&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="c1"&gt;# flatten the image&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="c1"&gt;# the hidden layer&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
					    &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					    &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb37d9bc" class="outline-3"&gt;
&lt;h3 id="orgb37d9bc"&gt;The Callback&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb37d9bc"&gt;
&lt;p&gt;
To make the training end at 99% accuracy I'll add a callback.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Stop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tensorflow&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;callbacks&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Callback&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;on_epoch_end&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;logs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{}):&lt;/span&gt;
	&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"acc"&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;logs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt;  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"acc"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.99&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Stopping point reached at epoch &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Model Accuracy: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;logs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'accuracy'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stop_training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Build and trains the model&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     units: number of neurons in the hidden layer&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;callbacks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Stop&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;build_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;units&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"adam"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"sparse_categorical_crossentropy"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"accuracy"&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_images_normalized&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	      &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;callbacks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;callbacks&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;outcome_key&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""tests the model"""&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;evaluate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testing_images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;testing_labels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;outcomes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;outcome_key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"Testing: Loss=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, Accuracy: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;Testing A Prediction"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;classifications&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testing_images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classifications&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;selected&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifications&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;selected&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"expected label: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;testing_labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;"actual label: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;selected&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga36a1b9" class="outline-3"&gt;
&lt;h3 id="orga36a1b9"&gt;Trying Some Models&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga36a1b9"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org57381e6" class="outline-4"&gt;
&lt;h4 id="org57381e6"&gt;128 Nodes&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org57381e6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"128 Nodes"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch 1/100
{'loss': 0.2586968289529284, 'acc': 0.92588335}
60000/60000 - 2s - loss: 0.2587 - acc: 0.9259
Epoch 2/100
{'loss': 0.11452680859503647, 'acc': 0.9655833}
60000/60000 - 2s - loss: 0.1145 - acc: 0.9656
Epoch 3/100
{'loss': 0.0795439642144988, 'acc': 0.97606665}
60000/60000 - 2s - loss: 0.0795 - acc: 0.9761
Epoch 4/100
{'loss': 0.05808031236998116, 'acc': 0.9816667}
60000/60000 - 2s - loss: 0.0581 - acc: 0.9817
Epoch 5/100
{'loss': 0.04466566459426346, 'acc': 0.98588336}
60000/60000 - 2s - loss: 0.0447 - acc: 0.9859
Epoch 6/100
{'loss': 0.03590909656824855, 'acc': 0.9885333}
60000/60000 - 2s - loss: 0.0359 - acc: 0.9885
Epoch 7/100
{'loss': 0.02741284582785641, 'acc': 0.9912}
Stopping point reached at epoch 6
Model Accuracy: None
60000/60000 - 2s - loss: 0.0274 - acc: 0.9912
Testing: Loss=15.376291691160201, Accuracy: 0.9764000177383423

Testing A Prediction
[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
expected label: 0
actual label: 0
&lt;/pre&gt;

&lt;p&gt;
Well, here we can see why the Fashion MNIST data set was created, even with this simple network we were able to reach our goal in 7 epochs. Even the testing accuracy and loss was pretty good.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbc429a7" class="outline-2"&gt;
&lt;h2 id="orgbc429a7"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgbc429a7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org344c2f6" class="outline-3"&gt;
&lt;h3 id="org344c2f6"&gt;Source&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org344c2f6"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Taken from the &lt;a href="https://github.com/lmoroney/dlaicourse/tree/master/Exercises/Exercise%202%20-%20Handwriting%20Recognition"&gt;Exercise 2 - Handwriting Recognition&lt;/a&gt; notebook on github&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>computer vision</category><category>exercise</category><category>keras</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/keras/handwriting-recognition-exercise/</guid><pubDate>Sun, 30 Jun 2019 20:57:26 GMT</pubDate></item><item><title>Style Transfer</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org9dc0c20"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#orge7d10ff"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org2dfc020"&gt;The VGG 19 Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org4b86059"&gt;Load in Content and Style Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org23144c8"&gt;VGG19 Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org901ad6d"&gt;Content and Style Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org1fb10d0"&gt;Gram Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org56ae996"&gt;Putting it all Together&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org3c8696a"&gt;Loss and Weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#orgcf0a709"&gt;Updating the Target &amp;amp; Calculating Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org0d95a49"&gt;Content Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#orgb7e9957"&gt;Style Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org7e8b5a6"&gt;Total Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#orgb5729de"&gt;Display the Target Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/#org42f8cd8"&gt;A Holhwein Transfer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9dc0c20" class="outline-2"&gt;
&lt;h2 id="org9dc0c20"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9dc0c20"&gt;
&lt;p&gt;
In this notebook, weâll &lt;b&gt;recreate&lt;/b&gt; a style transfer method that is outlined in the paper, &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf"&gt;Image Style Transfer Using Convolutional Neural Networks, by Gatys&lt;/a&gt; in PyTorch.
&lt;/p&gt;

&lt;p&gt;
In this paper, style transfer uses the features found in the 19-layer VGG Network, which is comprised of a series of convolutional and pooling layers, and a few fully-connected layers. In the image below, the convolutional layers are named by stack and their order in the stack. Conv_1_1 is the first convolutional layer that an image is passed through, in the first stack. Conv_2_1 is the first convolutional layer in the &lt;b&gt;second&lt;/b&gt; stack. The deepest convolutional layer in the network is conv_5_4.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgad05a45" class="outline-3"&gt;
&lt;h3 id="orgad05a45"&gt;Separating Style and Content&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgad05a45"&gt;
&lt;p&gt;
Style transfer relies on separating the content and style of an image. Given one content image and one style image, we aim to create a new, &lt;i&gt;target&lt;/i&gt; image which should contain our desired content and style components:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;objects and their arrangement are similar to that of the &lt;b&gt;&lt;b&gt;content image&lt;/b&gt;&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;style, colors, and textures are similar to that of the &lt;b&gt;&lt;b&gt;style image&lt;/b&gt;&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
In this notebook, we'll use a pre-trained VGG19 Net to extract content or style features from a passed in image. We'll then formalize the idea of content and style &lt;i&gt;losses&lt;/i&gt; and use those to iteratively update our target image until we get a result that we want. You are encouraged to use a style and content image of your own and share your work on Twitter with @udacity; we'd love to see what you come up with!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge7d10ff" class="outline-2"&gt;
&lt;h2 id="orge7d10ff"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge7d10ff"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9e24f32" class="outline-3"&gt;
&lt;h3 id="org9e24f32"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9e24f32"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org91e5377" class="outline-4"&gt;
&lt;h4 id="org91e5377"&gt;Python Standard Library&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org91e5377"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from datetime import datetime
import pathlib
from typing import Union
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6f4e587" class="outline-4"&gt;
&lt;h4 id="org6f4e587"&gt;From PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6f4e587"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
from dotenv import load_dotenv
from PIL import Image
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.optim as optim
import torch.nn.functional as F
from torchvision import transforms, models
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Elapsed: 0:00:03.711236
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgaaf8201" class="outline-4"&gt;
&lt;h4 id="orgaaf8201"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgaaf8201"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from neurotic.tangles.data_paths import DataPathTwo
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org569c7e4" class="outline-3"&gt;
&lt;h3 id="org569c7e4"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org569c7e4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
		"font.size": 12,
		"xtick.labelsize": 10,
		"ytick.labelsize": 10,
		"axes.titlesize": 12,
		"figure.figsize": (8, 6),
	    },
	    font_scale=3)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc00563e" class="outline-3"&gt;
&lt;h3 id="orgc00563e"&gt;Typing&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc00563e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;PathType = Union[pathlib.Path, str]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2dfc020" class="outline-2"&gt;
&lt;h2 id="org2dfc020"&gt;The VGG 19 Network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2dfc020"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org876c936" class="outline-3"&gt;
&lt;h3 id="org876c936"&gt;Load in VGG19 (features)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org876c936"&gt;
&lt;p&gt;
VGG19 is split into two portions:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;vgg19.features&lt;/code&gt;, which are all the convolutional and pooling layers&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vgg19.classifier&lt;/code&gt;, which are the three linear, classifier layers at the end&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
We only need the &lt;code&gt;features&lt;/code&gt; portion, which we're going to load in and "freeze" the weights of, below.
&lt;/p&gt;

&lt;p&gt;
Get the "features" portion of VGG19 (we will not need the "classifier" portion).
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
vgg = models.vgg19(pretrained=True).features
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Elapsed: 0:00:03.197737
&lt;/pre&gt;


&lt;p&gt;
Freeze all VGG parameters since we're only optimizing the target image.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for param in vgg.parameters():
    param.requires_grad_(False)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
move the model to GPU, if available
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vgg.to(device)
print("Using: {}".format(device))
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Using: cuda
Elapsed: 0:00:04.951571
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4b86059" class="outline-2"&gt;
&lt;h2 id="org4b86059"&gt;Load in Content and Style Images&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4b86059"&gt;
&lt;p&gt;
You can load in any images you want! Below, we've provided a helper function for loading in any type and size of image. The &lt;code&gt;load_image&lt;/code&gt; function also converts images to normalized Tensors.
&lt;/p&gt;

&lt;p&gt;
Additionally, it will be easier to have smaller images and to squish the content and style images so that they are of the same size.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def load_image(img_path: PathType, max_size: int=400, shape=None):
    ''' Load in and transform an image, making sure the image
       is &amp;lt;= max_size pixels in the x-y dims.'''

    image = Image.open(img_path).convert('RGB')

    # large images will slow down processing
    if max(image.size) &amp;gt; max_size:
	size = max_size
    else:
	size = max(image.size)

    if shape is not None:
	size = shape

    in_transform = transforms.Compose([
			transforms.Resize(size),
			transforms.ToTensor(),
			transforms.Normalize((0.485, 0.456, 0.406), 
					     (0.229, 0.224, 0.225))])

    # discard the transparent, alpha channel (that's the :3) and add the batch dimension
    image = in_transform(image)[:3,:,:].unsqueeze(0)

    return image
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Next, I'm loading in images by file name and forcing the style image to be the same size as the content image.
&lt;/p&gt;

&lt;p&gt;
Load in content and style image.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
max_size = 400 if torch.cuda.is_available() else 128
path = DataPathTwo(folder_key="IMAGES", filename_key="RAVEN")
content = load_image(path.from_folder, max_size=max_size).to(device)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Resize style to match content, makes code easier
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;style_path = DataPathTwo(filename_key="VERMEER", folder_key="IMAGES")
style = load_image(style_path.from_folder, shape=content.shape[-2:]).to(device)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
A helper function for un-normalizing an image and converting it from a Tensor image to a NumPy image for display.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def im_convert(tensor: torch.Tensor) -&amp;gt; numpy.ndarray:
    """ Display a tensor as an image.

    Args:
     tensor: tensor with image

    Returns:
     numpy image from tensor
    """

    image = tensor.to("cpu").clone().detach()
    image = image.numpy().squeeze()
    image = image.transpose(1,2,0)
    image = image * numpy.array((0.229, 0.224, 0.225)) + numpy.array((0.485, 0.456, 0.406))
    image = image.clip(0, 1)
    return image
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Display the images.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, (ax1, ax2) = pyplot.subplots(1, 2)
figure.suptitle("Content and Style Images Side-By-Side", weight="bold", y=0.75)
ax1.set_title("Raven (content)")
ax2.set_title("Girl With a Pearl Earring (style)")
ax1.imshow(im_convert(content))
image = ax2.imshow(im_convert(style))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/images.png" alt="images.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org23144c8" class="outline-2"&gt;
&lt;h2 id="org23144c8"&gt;VGG19 Layers&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org23144c8"&gt;
&lt;p&gt;
To get the content and style representations of an image, we have to pass an image forward through the VGG19 network until we get to the desired layer(s) and then get the output from that layer.
&lt;/p&gt;

&lt;p&gt;
Print out VGG19 structure so you can see the names of various layers.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(vgg)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace)
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU(inplace)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): ReLU(inplace)
  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (8): ReLU(inplace)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (11): ReLU(inplace)
  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (13): ReLU(inplace)
  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (15): ReLU(inplace)
  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (17): ReLU(inplace)
  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (20): ReLU(inplace)
  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (22): ReLU(inplace)
  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (24): ReLU(inplace)
  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (26): ReLU(inplace)
  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (29): ReLU(inplace)
  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (31): ReLU(inplace)
  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (33): ReLU(inplace)
  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (35): ReLU(inplace)
  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org901ad6d" class="outline-2"&gt;
&lt;h2 id="org901ad6d"&gt;Content and Style Features&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org901ad6d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def get_features(image, model, layers=None):
    """ Run an image forward through a model and get the features for 
	a set of layers. Default layers are for VGGNet matching Gatys et al (2016)
    """
    if layers is None:
	layers = {'0': 'conv1_1',
		  '5': 'conv2_1',
		  '10': 'conv3_1', 
		  '19': 'conv4_1',
		  '21': 'conv4_2',  ## content representation
		  '28': 'conv5_1'}


    ## -- do not need to change the code below this line -- ##
    features = {}
    x = image
    # model._modules is a dictionary holding each module in the model
    for name, layer in model._modules.items():
	x = layer(x)
	if name in layers:
	    features[layers[name]] = x            
    return features
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1fb10d0" class="outline-2"&gt;
&lt;h2 id="org1fb10d0"&gt;Gram Matrix&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1fb10d0"&gt;
&lt;p&gt;
The output of every convolutional layer is a Tensor with dimensions associated with the &lt;code&gt;batch_size&lt;/code&gt;, a depth, &lt;code&gt;d&lt;/code&gt; and some height and width (&lt;code&gt;h&lt;/code&gt;, &lt;code&gt;w&lt;/code&gt;). The Gram matrix of a convolutional layer can be calculated as follows:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Get the depth, height, and width of a tensor using &lt;code&gt;batch_size, d, h, w = tensor.size&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Reshape that tensor so that the spatial dimensions are flattened&lt;/li&gt;
&lt;li&gt;Calculate the gram matrix by multiplying the reshaped tensor by it's transpose&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;b&gt;Note: You can multiply two matrices using &lt;code&gt;torch.mm(matrix1, matrix2)&lt;/code&gt;.&lt;/b&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def gram_matrix(tensor: torch.Tensor) -&amp;gt; torch.Tensor:
    """ Calculate the Gram Matrix of a given tensor 
	Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix
    """
    batch_size, depth, height, width = tensor.size()
    tensor = tensor.view(batch_size * depth, height * width)
    gram = torch.mm(tensor, tensor.t())
    return gram 
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org56ae996" class="outline-2"&gt;
&lt;h2 id="org56ae996"&gt;Putting it all Together&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org56ae996"&gt;
&lt;p&gt;
Now that we've written functions for extracting features and computing the gram matrix of a given convolutional layer; let's put all these pieces together! We'll extract our features from our images and calculate the gram matrices for each layer in our style representation.
&lt;/p&gt;

&lt;p&gt;
Get content and style features only once before forming the target image.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;content_features = get_features(content, vgg)
style_features = get_features(style, vgg)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
calculate the gram matrices for each layer of our style representation
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Create a third "target" image and prep it for change. It is a good idea to start off with the target as a copy of our &lt;b&gt;content&lt;/b&gt; image then iteratively change its style.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;target = content.clone().requires_grad_(True).to(device)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3c8696a" class="outline-2"&gt;
&lt;h2 id="org3c8696a"&gt;Loss and Weights&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3c8696a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org29c104b" class="outline-3"&gt;
&lt;h3 id="org29c104b"&gt;Individual Layer Style Weights&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org29c104b"&gt;
&lt;p&gt;
Below, you are given the option to weight the style representation at each relevant layer. It's suggested that you use a range between 0-1 to weight these layers. By weighting earlier layers (&lt;code&gt;conv1_1&lt;/code&gt; and &lt;code&gt;conv2_1&lt;/code&gt;) more, you can expect to get &lt;i&gt;larger&lt;/i&gt; style artifacts in your resulting, target image. Should you choose to weight later layers, you'll get more emphasis on smaller features. This is because each layer is a different size and together they create a multi-scale style representation!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbf4df9f" class="outline-3"&gt;
&lt;h3 id="orgbf4df9f"&gt;Content and Style Weight&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbf4df9f"&gt;
&lt;p&gt;
Just like in the paper, we define an alpha (&lt;code&gt;content_weight&lt;/code&gt;) and a beta (&lt;code&gt;style_weight&lt;/code&gt;). This ratio will affect how &lt;i&gt;stylized&lt;/i&gt; your final image is. It's recommended that you leave the content_weight = 1 and set the style_weight to achieve the ratio you want.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-org2b8991f" class="outline-3"&gt;
&lt;h3 id="org2b8991f"&gt;Weights For Each Style Layer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2b8991f"&gt;
&lt;p&gt;
Weighting earlier layers more will result in &lt;b&gt;larger&lt;/b&gt; style artifacts. Notice we are excluding &lt;code&gt;conv4_2&lt;/code&gt; our content representation.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;style_weights = {'conv1_1': 1.,
		 'conv2_1': 0.8,
		 'conv3_1': 0.6,
		 'conv4_1': 0.4,
		 'conv5_1': 0.2}
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;content_weight = 1  # alpha
style_weight = 1e6  # beta
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcf0a709" class="outline-2"&gt;
&lt;h2 id="orgcf0a709"&gt;Updating the Target &amp;amp; Calculating Losses&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgcf0a709"&gt;
&lt;p&gt;
You'll decide on a number of steps for which to update your image, this is similar to the training loop that you've seen before, only we are changing our &lt;span class="underline"&gt;target&lt;/span&gt; image and nothing else about VGG19 or any other image. Therefore, the number of steps is really up to you to set! &lt;b&gt;&lt;b&gt;I recommend using at least 2000 steps for good results.&lt;/b&gt;&lt;/b&gt; But, you may want to start out with fewer steps if you are just testing out different weight values or experimenting with different images.
&lt;/p&gt;

&lt;p&gt;
Inside the iteration loop, you'll calculate the content and style losses and update your target image, accordingly.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0d95a49" class="outline-2"&gt;
&lt;h2 id="org0d95a49"&gt;Content Loss&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0d95a49"&gt;
&lt;p&gt;
The content loss will be the mean squared difference between the target and content features at layer &lt;code&gt;conv4_2&lt;/code&gt;. This can be calculated as follows: 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;content_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;target_features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'conv4_2'&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;content_features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;'conv4_2'&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb7e9957" class="outline-2"&gt;
&lt;h2 id="orgb7e9957"&gt;Style Loss&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb7e9957"&gt;
&lt;p&gt;
The style loss is calculated in a similar way, only you have to iterate through a number of layers, specified by name in our dictionary &lt;code&gt;style_weights&lt;/code&gt;. 
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;You'll calculate the gram matrix for the target image, &lt;code&gt;target_gram&lt;/code&gt; and style image &lt;code&gt;style_gram&lt;/code&gt; at each of these layers and compare those gram matrices, calculating the &lt;code&gt;layer_style_loss&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Later, you'll see that this value is normalized by the size of the layer.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7e8b5a6" class="outline-2"&gt;
&lt;h2 id="org7e8b5a6"&gt;Total Loss&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7e8b5a6"&gt;
&lt;p&gt;
Finally, you'll create the total loss by adding up the style and content losses and weighting them with your specified alpha and beta!
&lt;/p&gt;

&lt;p&gt;
Intermittently, we'll print out this loss; don't be alarmed if the loss is very large. It takes some time for an image's style to change and you should focus on the appearance of your target image rather than any loss value. Still, you should see that this loss decreases over some number of iterations.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;show_every = 400

# iteration hyperparameters
optimizer = optim.Adam([target], lr=0.003)
steps = 2000  # decide how many iterations to update your image (5000)
CONTENT_LAYER = "conv4_2"
start = datetime.now()
for repetition in range(1, steps+1):
    target_features = get_features(target, vgg)
    content_loss = F.mse_loss(target_features[CONTENT_LAYER],
			      content_features[CONTENT_LAYER])

    # the style loss
    # initialize the style loss to 0
    style_loss = 0
    # iterate through each style layer and add to the style loss
    for layer in style_weights:
	# get the "target" style representation for the layer
	target_feature = target_features[layer]
	_, d, h, w = target_feature.shape

	target_gram = gram_matrix(target_feature)

	style_gram = style_grams[layer]

	layer_style_loss = style_weights[layer] * F.mse_loss(target_gram,
							     style_gram)
	# add to the style loss
	style_loss += layer_style_loss / (d * h * w)

    total_loss = content_weight * content_loss + style_weight * style_loss

    ## -- do not need to change code, below -- ##
    # update your target image
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    # display intermediate images and print the loss
    if  repetition % show_every == 0:
	print('({}) Total loss: {}'.format(repetition, total_loss.item()))
	#plt.imshow(im_convert(target))
	#plt.show()
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
(400) Total loss: 26489776.0
(800) Total loss: 12765434.0
(1200) Total loss: 8439541.0
(1600) Total loss: 6268045.0
(2000) Total loss: 4820489.5
Elapsed: 0:08:03.885520
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb5729de" class="outline-2"&gt;
&lt;h2 id="orgb5729de"&gt;Display the Target Image&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb5729de"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, (ax1, ax2) = pyplot.subplots(1, 2)
figure.suptitle("Vermeer Raven", weight="bold", y=0.75)
ax1.imshow(im_convert(content))
image = ax2.imshow(im_convert(target))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/raven_vermeer.png" alt="raven_vermeer.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org42f8cd8" class="outline-2"&gt;
&lt;h2 id="org42f8cd8"&gt;A Holhwein Transfer&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org42f8cd8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;max_size = 400 if torch.cuda.is_available() else 128
path = DataPathTwo(folder_key="IMAGES", filename_key="RAVEN")
content = load_image(path.from_folder, max_size=max_size).to(device)

style_path = DataPathTwo(filename_key="HOHLWEIN", folder_key="IMAGES")
style = load_image(style_path.from_folder, shape=content.shape[-2:]).to(device)

content_features = get_features(content, vgg)
target = content.clone().requires_grad_(True).to(device)
content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)
style_features = get_features(style, vgg)
style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;show_every = 400
vgg = models.vgg19(pretrained=True).features
for param in vgg.parameters():
    param.requires_grad_(False)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vgg.to(device)
# iteration hyperparameters
optimizer = optim.Adam([target], lr=0.003)
steps = 2000  # decide how many iterations to update your image (5000)
CONTENT_LAYER = "conv4_2"
start = datetime.now()
for repetition in range(1, steps+1):
    target_features = get_features(target, vgg)
    content_loss = F.mse_loss(target_features[CONTENT_LAYER],
			      content_features[CONTENT_LAYER])

    # the style loss
    # initialize the style loss to 0
    style_loss = 0
    # iterate through each style layer and add to the style loss
    for layer in style_weights:
	# get the "target" style representation for the layer
	target_feature = target_features[layer]
	_, d, h, w = target_feature.shape

	target_gram = gram_matrix(target_feature)

	style_gram = style_grams[layer]

	layer_style_loss = style_weights[layer] * F.mse_loss(target_gram,
							     style_gram)
	# add to the style loss
	style_loss += layer_style_loss / (d * h * w)

    total_loss = content_weight * content_loss + style_weight * style_loss

    ## -- do not need to change code, below -- ##
    # update your target image
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    # display intermediate images and print the loss
    if  repetition % show_every == 0:
	print('({}) Total loss: {}'.format(repetition, total_loss.item()))
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
(400) Total loss: 38191616.0
(800) Total loss: 19276114.0
(1200) Total loss: 12646590.0
(1600) Total loss: 9095670.0
(2000) Total loss: 6934397.0
Elapsed: 0:08:09.517655
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, (ax1, ax2) = pyplot.subplots(1, 2)
figure.suptitle("Hohlwein Raven", weight="bold", y=.8)
ax1.imshow(im_convert(content))
image = ax2.imshow(im_convert(target))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/hohlwein_raven.png" alt="hohlwein_raven.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>exercise</category><category>style transfer</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/style-transfer/style-transfer/</guid><pubDate>Sat, 22 Dec 2018 22:44:24 GMT</pubDate></item><item><title>Denoising Autoencoder</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/#orgf76e14f"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/#orgf0cd451"&gt;Visualize the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/#org3de8dea"&gt;Denoising&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/#orge43cff7"&gt;Define the NN Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/#org51808e7"&gt;Initialize The NN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/#orgd8ff382"&gt;Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/#org442cf88"&gt;Checking out the results&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
Sticking with the MNIST dataset, let's add noise to our data and see if we can define and train an autoencoder to &lt;i&gt;de&lt;/i&gt;-noise the images.
&lt;/p&gt;
&lt;div id="outline-container-orgf76e14f" class="outline-2"&gt;
&lt;h2 id="orgf76e14f"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf76e14f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2a05b39" class="outline-3"&gt;
&lt;h3 id="org2a05b39"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2a05b39"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6eb39f9" class="outline-4"&gt;
&lt;h4 id="org6eb39f9"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6eb39f9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from collections import namedtuple
from datetime import datetime
from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgab62830" class="outline-4"&gt;
&lt;h4 id="orgab62830"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgab62830"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from torchvision import datasets
from graphviz import Graph
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga94bf4f" class="outline-3"&gt;
&lt;h3 id="orga94bf4f"&gt;The Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga94bf4f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=3)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfcb48cf" class="outline-3"&gt;
&lt;h3 id="orgfcb48cf"&gt;The Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfcb48cf"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge850886" class="outline-4"&gt;
&lt;h4 id="orge850886"&gt;The Transform&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge850886"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;transform = transforms.ToTensor()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org740257f" class="outline-4"&gt;
&lt;h4 id="org740257f"&gt;Load the Training and Test Datasets&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org740257f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path("~/datasets/MNIST/").expanduser()
print(path.is_dir())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
True
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_data = datasets.MNIST(root=path, train=True,
			    download=True, transform=transform)
test_data = datasets.MNIST(root=path, train=False,
			   download=True, transform=transform)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdb19a3a" class="outline-4"&gt;
&lt;h4 id="orgdb19a3a"&gt;Create training and test dataloaders&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgdb19a3a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;NUM_WORKERS = 0
BATCH_SIZE = 20
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
					   num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,
					  num_workers=NUM_WORKERS)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5acbb12" class="outline-3"&gt;
&lt;h3 id="org5acbb12"&gt;Test for &lt;a href="http://pytorch.org/docs/stable/cuda.html"&gt;CUDA&lt;/a&gt;&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5acbb12"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using: {}".format(device))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Using: cuda:0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf0cd451" class="outline-2"&gt;
&lt;h2 id="orgf0cd451"&gt;Visualize the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf0cd451"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org83577e0" class="outline-3"&gt;
&lt;h3 id="org83577e0"&gt;Obtain One Batch of Training Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org83577e0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4fa6fcb" class="outline-3"&gt;
&lt;h3 id="org4fa6fcb"&gt;Get One Image From the Batch&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4fa6fcb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;img = numpy.squeeze(images[0])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbd9d04a" class="outline-3"&gt;
&lt;h3 id="orgbd9d04a"&gt;Plot&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbd9d04a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
figure.suptitle("Sample Image", weight="bold")
image = axe.imshow(img, cmap='gray')
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/first_image.png" alt="first_image.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3de8dea" class="outline-2"&gt;
&lt;h2 id="org3de8dea"&gt;Denoising&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3de8dea"&gt;
&lt;p&gt;
As I've mentioned before, autoencoders like the ones you've built so far aren't too useful in practive. However, they can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;We'll use noisy images as input and the original, clean images as targets.&lt;/b&gt;&lt;/b&gt; 
&lt;/p&gt;

&lt;p&gt;
Since this is a harder problem for the network, we'll want to use &lt;i&gt;deeper&lt;/i&gt; convolutional layers here; layers with more feature maps. You might also consider adding additional layers. I suggest starting with a depth of 32 for the convolutional layers in the encoder, and the same depths going backward through the decoder.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge43cff7" class="outline-2"&gt;
&lt;h2 id="orge43cff7"&gt;Define the NN Architecture&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge43cff7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;graph = Graph(format="png")

# Input layer
graph.node("a", "28x28x1 Input")

# the Encoder
graph.node("b", "28x28x32 Convolution")
graph.node("c", "14x14x32 MaxPool")
graph.node("d", "14x14x16 Convolution")
graph.node("e", "7x7x16 MaxPool")
graph.node("f", "7x7x8 Convolution")
graph.node("g", "3x3x8 MaxPool")

# The Decoder
graph.node("h", "7x7x8 Transpose Convolution")
graph.node("i", "14x14x16 Transpose Convolution")
graph.node("j", "28x28x32 Transpose Convolution")
graph.node("k", "28x28x1 Convolution")

# The Output
graph.node("l", "28x28x1 Output")

edges = "abcdefghijkl"
graph.edges([edges[edge] + edges[edge+1] for edge in range(len(edges) - 1)])

graph.render("graphs/network.dot")
graph
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/network.dot.png" alt="network.dot.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Layer = namedtuple("Layer", "kernel stride in_depth out_depth padding".split())
Layer.__new__.__defaults__= (0,)
def output_size(input_size: int, layer: Layer, expected: int) -&amp;gt; int:
    """Calculates the output size of the layer

    Args:
     input_size: the size of the input to the layer
     layer: named tuple with values for the layer
     expected: the value you are expecting

    Returns:
     the size of the output

    Raises:
     AssertionError: the calculated value wasn't the expected one
    """
    size = 1 + int(
	(input_size - layer.kernel + 2 * layer.padding)/layer.stride)
    print(layer)
    print("Layer Output: {0} x {0} x {1}".format(size, layer.out_depth))
    assert size == expected, size
    return size
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2a9fafa" class="outline-3"&gt;
&lt;h3 id="org2a9fafa"&gt;The Encoder Layers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2a9fafa"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org758f758" class="outline-4"&gt;
&lt;h4 id="org758f758"&gt;Layer One&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org758f758"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; INPUT_DEPTH = 1
 convolution_one = Layer(kernel = 3,
			 padding = 1,
			 stride = 1,
			 in_depth=INPUT_DEPTH,
			 out_depth = 32)
 INPUT_ONE = 28
 OUTPUT_ONE = output_size(INPUT_ONE, convolution_one, INPUT_ONE)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=3, stride=1, in_depth=1, out_depth=32, padding=1)
Layer Output: 28 x 28 x 32
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org34db513" class="outline-4"&gt;
&lt;h4 id="org34db513"&gt;Layer Two&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org34db513"&gt;
&lt;p&gt;
The second layer is a MaxPool layer that will keep the depth of thirty-two but will halve the size to fourteen. According to the &lt;a href="https://cs231n.github.io/convolutional-networks/"&gt;CS 231 n&lt;/a&gt; page on Convolutional Networks, there are only two values for the kernel size that are usually used - 2 and 3, and the stride is usually just 2, with a kernel size of 2 being more common, and as it turns out, a kernel size of 2 and a stride of 2 will reduce our input dimensions by a half, which is what we want.
&lt;/p&gt;

\begin{align}
W &amp;amp;= \frac{28 - 2}{2} + 1\\
  &amp;amp;= 14\\
\end{align}

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; max_pool_one = Layer(kernel=2, stride=2,
		      in_depth=convolution_one.out_depth,
		      out_depth=convolution_one.out_depth)
 OUTPUT_TWO = output_size(OUTPUT_ONE, max_pool_one, 14)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=2, stride=2, in_depth=32, out_depth=32, padding=0)
Layer Output: 14 x 14 x 32
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf1ed424" class="outline-4"&gt;
&lt;h4 id="orgf1ed424"&gt;Layer Three&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf1ed424"&gt;
&lt;p&gt;
Our third layer is another convolutional layer that preserves the input width and height but this time the output will have a depth of 16.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;convolution_two = Layer(kernel=3, stride=1, in_depth=max_pool_one.out_depth,
			out_depth=16, padding=1)
OUTPUT_THREE = output_size(OUTPUT_TWO, convolution_two, OUTPUT_TWO)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=3, stride=1, in_depth=32, out_depth=16, padding=1)
Layer Output: 14 x 14 x 16
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org12a79b1" class="outline-4"&gt;
&lt;h4 id="org12a79b1"&gt;Layer Four&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org12a79b1"&gt;
&lt;p&gt;
The fourth layer is another max-pool layer that will halve the dimensions.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;max_pool_two = Layer(kernel=2, stride=2, in_depth=convolution_two.out_depth,
			out_depth=convolution_two.out_depth)
OUTPUT_FOUR = output_size(OUTPUT_THREE, max_pool_two, 7)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=2, stride=2, in_depth=16, out_depth=16, padding=0)
Layer Output: 7 x 7 x 16
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org30a35f0" class="outline-4"&gt;
&lt;h4 id="org30a35f0"&gt;Layer Five&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org30a35f0"&gt;
&lt;p&gt;
The fifth layer is another convolutional layer that will reduce the depth to eight.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;convolution_three = Layer(kernel=3, stride=1,
			  in_depth=max_pool_two.out_depth, out_depth=8,
			  padding=1)
OUTPUT_FIVE = output_size(OUTPUT_FOUR, convolution_three, 7)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=3, stride=1, in_depth=16, out_depth=8, padding=1)
Layer Output: 7 x 7 x 8
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb847529" class="outline-4"&gt;
&lt;h4 id="orgb847529"&gt;Layer Six&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb847529"&gt;
&lt;p&gt;
The last layer in the encoder is a max pool layer that reduces the previous layer by half (to dimensions of 3) while preserving the depth.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;max_pool_three = Layer(kernel=2, stride=2,
		       in_depth=convolution_three.out_depth,
		       out_depth=convolution_three.out_depth)
OUTPUT_SIX = output_size(OUTPUT_FIVE, max_pool_three, 3)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=2, stride=2, in_depth=8, out_depth=8, padding=0)
Layer Output: 3 x 3 x 8
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6abef21" class="outline-3"&gt;
&lt;h3 id="org6abef21"&gt;Decoders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6abef21"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb32f7cc" class="outline-4"&gt;
&lt;h4 id="orgb32f7cc"&gt;Layer Six&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb32f7cc"&gt;
&lt;p&gt;
This is a transpose convolution layer to (more than) double the size of the image. The image put out by the encoder is 3x3, but we want a 7x7 output, not a 6x6, so the kernel has to be upped to 3.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;transpose_one = Layer(kernel=3, stride=2, out_depth=8,
		      in_depth=max_pool_three.out_depth)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge8038ae" class="outline-4"&gt;
&lt;h4 id="orge8038ae"&gt;Layer Seven&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge8038ae"&gt;
&lt;p&gt;
This will double the size again (to 14x14) and increase the depth to 16.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;transpose_two = Layer(kernel=2, stride=2, out_depth=16,
		      in_depth=transpose_one.out_depth)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org660b20e" class="outline-4"&gt;
&lt;h4 id="org660b20e"&gt;Layer Eight&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org660b20e"&gt;
&lt;p&gt;
This will double the size to 28x28 and up the depth back again to 32, the size of our original encoding convolution.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;transpose_three = Layer(kernel=2, stride=2, out_depth=32,
			in_depth=transpose_two.out_depth)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org74f9b33" class="outline-4"&gt;
&lt;h4 id="org74f9b33"&gt;Layer Nine&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org74f9b33"&gt;
&lt;p&gt;
This is a convolution layer to bring the depth back to one.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;convolution_out = Layer(kernel=3, stride=1, in_depth=transpose_three.out_depth,
			out_depth=1, padding=1)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org10085b9" class="outline-3"&gt;
&lt;h3 id="org10085b9"&gt;The Implementation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org10085b9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class ConvDenoiser(nn.Module):
    def __init__(self):
	super().__init__()
	## encoder layers ##
	self.convolution_1 =  nn.Conv2d(in_channels=convolution_one.in_depth,
				       out_channels=convolution_one.out_depth,
				       kernel_size=convolution_one.kernel,
				       padding=convolution_one.padding)

	self.convolution_2 = nn.Conv2d(in_channels=convolution_two.in_depth,
				       out_channels=convolution_two.out_depth,
				       kernel_size=convolution_two.kernel,
				       padding=convolution_two.padding)

	self.convolution_3 = nn.Conv2d(in_channels=convolution_three.in_depth,
				       out_channels=convolution_three.out_depth,
				       kernel_size=convolution_three.kernel,
				       padding=convolution_three.padding)

	self.max_pool = nn.MaxPool2d(kernel_size=max_pool_one.kernel,
				     stride=max_pool_one.stride)

	## decoder layers ##
	## a kernel of 2 and a stride of 2 will increase the spatial dims by 2
	self.transpose_convolution_1 = nn.ConvTranspose2d(
	    in_channels=transpose_one.in_depth,
	    out_channels=transpose_one.out_depth,
	    kernel_size=transpose_one.kernel,
	    stride=transpose_one.stride)

	self.transpose_convolution_2 = nn.ConvTranspose2d(
	    in_channels=transpose_two.in_depth, 
	    out_channels=transpose_two.out_depth,
	    kernel_size=transpose_two.kernel,
	    stride=transpose_two.stride)

	self.transpose_convolution_3 = nn.ConvTranspose2d(
	    in_channels=transpose_three.in_depth,
	    out_channels=transpose_three.out_depth,
	    kernel_size=transpose_three.kernel,
	    stride=transpose_three.stride)

	self.convolution_out = nn.Conv2d(in_channels=convolution_out.in_depth,
					 out_channels=convolution_out.out_depth,
					 kernel_size=convolution_out.kernel,
					 padding=convolution_out.padding)

	self.relu = nn.ReLU()
	self.sigmoid = nn.Sigmoid()
	return


    def forward(self, x):
	## encode ##
	x = self.max_pool(self.relu(self.convolution_1(x)))
	x = self.max_pool(self.relu(self.convolution_2(x)))
	x = self.max_pool(self.relu(self.convolution_3(x)))

	## decode ##
	x = self.relu(self.transpose_convolution_1(x))
	x = self.relu(self.transpose_convolution_2(x))
	x = self.relu(self.transpose_convolution_3(x))
	return self.sigmoid(self.convolution_out(x))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org51808e7" class="outline-2"&gt;
&lt;h2 id="org51808e7"&gt;Initialize The NN&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org51808e7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = ConvDenoiser()
print(model)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
ConvDenoiser(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (t_conv1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2))
  (t_conv2): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))
  (t_conv3): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))
  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test = ConvDenoiser()
dataiter = iter(train_loader)
images, labels = dataiter.next()
x = test.convolution_1(images)
assert x.shape == torch.Size([BATCH_SIZE, 32, 28, 28])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 32, 14, 14])
print(x.shape)

x = test.convolution_2(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 14, 14])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 7, 7])
print(x.shape)

x = test.convolution_3(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 7, 7])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 3, 3]), x.shape

x = test.transpose_convolution_1(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 7, 7]), x.shape
print(x.shape)

x = test.transpose_convolution_2(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 14, 14])
print(x.shape)

x = test.transpose_convolution_3(x)
assert x.shape == torch.Size([BATCH_SIZE, 32, 28, 28])
print(x.shape)

x = test.convolution_out(x)
assert x.shape == torch.Size([BATCH_SIZE, 1, 28, 28])
print(x.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
torch.Size([20, 32, 28, 28])
torch.Size([20, 32, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 7, 7])
torch.Size([20, 8, 7, 7])
torch.Size([20, 8, 7, 7])
torch.Size([20, 16, 14, 14])
torch.Size([20, 32, 28, 28])
torch.Size([20, 1, 28, 28])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd8ff382" class="outline-2"&gt;
&lt;h2 id="orgd8ff382"&gt;Training&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd8ff382"&gt;
&lt;p&gt;
We are only concerned with the training images, which we can get from the &lt;code&gt;train_loader&lt;/code&gt;.
&lt;/p&gt;

&lt;p&gt;
In this case, we are actually &lt;b&gt;&lt;b&gt;adding some noise&lt;/b&gt;&lt;/b&gt; to these images and we'll feed these &lt;code&gt;noisy_imgs&lt;/code&gt; to our model. The model will produce reconstructed images based on the noisy input. But, we want it to produce &lt;span class="underline"&gt;normal&lt;/span&gt; un-noisy images, and so, when we calculate the loss, we will still compare the reconstructed outputs to the original images!
&lt;/p&gt;

&lt;p&gt;
Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use &lt;code&gt;MSELoss&lt;/code&gt;. And compare output images and input images as follows:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Warning:&lt;/b&gt;&lt;/b&gt; I spent an unreasonable amount of time trying to de-bug this thing because I was passing in the model's parameters to the optimizer before passing it to the GPU. I don't know why it didn't throw an error, but it didn't, it just never learned and gave me really high losses. I think it's because the style of these notebooks is to create the parts all over the place so there might have been another 'model' variable in the namespace. In any case, move away from this style and start putting everything into functions and classes - especially the stuff that comes from udacity.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Trainer:
    """Trains our model

    Args:
     data: data-iterator for training
     epochs: number of times to train on the data
     noise: factor for the amount of noise to add
     learning_rate: rate for the optimizer
    """
    def __init__(self, data: torch.utils.data.DataLoader, epochs: int=30,
		 noise:float=0.5,
		 learning_rate:float=0.001) -&amp;gt; None:
	self.data = data
	self.epochs = epochs
	self.learning_rate = learning_rate
	self.noise = noise
	self._criterion = None
	self._model = None
	self._device = None
	self._optimizer = None
	return

    @property
    def device(self) -&amp;gt; torch.device:
	"""CUDA or CPU"""
	if self._device is None:
	    self._device = torch.device(
		"cuda:0" if torch.cuda.is_available() else "cpu")
	return self._device

    @property
    def criterion(self) -&amp;gt; nn.MSELoss:
	"""Loss-calculator"""
	if self._criterion is None:
	    self._criterion = nn.MSELoss()
	return self._criterion

    @property
    def model(self) -&amp;gt; ConvDenoiser:
	"""Our model"""
	if self._model is None:
	    self._model = ConvDenoiser()
	    self.model.to(self.device)
	return self._model

    @property
    def optimizer(self) -&amp;gt; torch.optim.Adam:
	"""The gradient descent optimizer"""
	if self._optimizer is None:
	    self._optimizer = torch.optim.Adam(self.model.parameters(),
					       lr=self.learning_rate)
	return self._optimizer

    def __call__(self) -&amp;gt; None:
	"""Trains the model on the data"""
	self.model.train()
	started = datetime.now()
	for epoch in range(1, self.epochs + 1):
	    train_loss = 0.0
	    for batch in self.data:
		images, _ = batch
		images = images.to(self.device)
		## add random noise to the input images
		noisy_imgs = (images
			      + self.noise
			      * torch.randn(*images.shape).to(self.device))
		# Clip the images to be between 0 and 1
		noisy_imgs = numpy.clip(noisy_imgs, 0., 1.).to(self.device)

		# clear the gradients of all optimized variables
		self.optimizer.zero_grad()
		## forward pass: compute predicted outputs by passing *noisy* images to the model
		outputs = self.model(noisy_imgs)
		# calculate the loss
		# the "target" is still the original, not-noisy images
		loss = self.criterion(outputs, images)
		# backward pass: compute gradient of the loss with respect to model parameters
		loss.backward()
		# perform a single optimization step (parameter update)
		self.optimizer.step()
		# update running training loss
		train_loss += loss.item() * images.size(0)

	    # print avg training statistics 
	    train_loss = train_loss/len(train_loader)
	    print('Epoch: {} \tTraining Loss: {:.6f}'.format(
		epoch, 
		train_loss
		))
	ended = datetime.now()
	print("Ended: {}".format(ended))
	print("Elapsed: {}".format(ended - started))
	return
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_the_model = Trainer(train_loader)
train_the_model()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 1 	Training Loss: 0.952294
Epoch: 2 	Training Loss: 0.686571
Epoch: 3 	Training Loss: 0.647284
Epoch: 4 	Training Loss: 0.628790
Epoch: 5 	Training Loss: 0.615522
Epoch: 6 	Training Loss: 0.604566
Epoch: 7 	Training Loss: 0.595838
Epoch: 8 	Training Loss: 0.585816
Epoch: 9 	Training Loss: 0.578257
Epoch: 10 	Training Loss: 0.572502
Epoch: 11 	Training Loss: 0.566983
Epoch: 12 	Training Loss: 0.562720
Epoch: 13 	Training Loss: 0.558449
Epoch: 14 	Training Loss: 0.554410
Epoch: 15 	Training Loss: 0.550995
Epoch: 16 	Training Loss: 0.546916
Epoch: 17 	Training Loss: 0.543798
Epoch: 18 	Training Loss: 0.541859
Epoch: 19 	Training Loss: 0.539242
Epoch: 20 	Training Loss: 0.536748
Epoch: 21 	Training Loss: 0.534675
Epoch: 22 	Training Loss: 0.532690
Epoch: 23 	Training Loss: 0.531692
Epoch: 24 	Training Loss: 0.529910
Epoch: 25 	Training Loss: 0.528826
Epoch: 26 	Training Loss: 0.526354
Epoch: 27 	Training Loss: 0.526260
Epoch: 28 	Training Loss: 0.525294
Epoch: 29 	Training Loss: 0.524029
Epoch: 30 	Training Loss: 0.523341
Epoch: 31 	Training Loss: 0.522387
Epoch: 32 	Training Loss: 0.521689
Ended: 2018-12-22 14:10:08.869789
Elapsed: 0:14:14.036518
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org442cf88" class="outline-2"&gt;
&lt;h2 id="org442cf88"&gt;Checking out the results&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org442cf88"&gt;
&lt;p&gt;
Here I'm adding noise to the test images and passing them through the autoencoder. It does a suprising great job of removing the noise, even though it's sometimes difficult to tell what the original number is.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# obtain one batch of test images
dataiter = iter(test_loader)
images, labels = dataiter.next()

# add noise to the test images
noisy_imgs = images + noise_factor * torch.randn(*images.shape)
noisy_imgs = numpy.clip(noisy_imgs, 0., 1.)

# get sample outputs
noisy_imgs = noisy_imgs.to(train_the_model.device)
output = train_the_model.model(noisy_imgs)
# prep images for display
noisy_imgs = noisy_imgs.cpu().numpy()

# output is resized into a batch of iages
output = output.view(BATCH_SIZE, 1, 28, 28)
# use detach when it's an output that requires_grad
output = output.detach().cpu().numpy()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# plot the first ten input images and then reconstructed images
fig, axes = pyplot.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))

# input images on top row, reconstructions on bottom
for noisy_imgs, row in zip([noisy_imgs, output], axes):
    for img, ax in zip(noisy_imgs, row):
	ax.imshow(numpy.squeeze(img), cmap='gray')
	ax.get_xaxis().set_visible(False)
	ax.get_yaxis().set_visible(False)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/de-noised.png" alt="de-noised.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
That did surprisingly well.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>autoencoder</category><category>cnn</category><category>exercise</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/denoising-autoencoder/</guid><pubDate>Sat, 22 Dec 2018 02:07:29 GMT</pubDate></item><item><title>Convolutional Autoencoder</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#orgfaef222"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#org15b8fc1"&gt;Compressed Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#org68eac86"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#org3c4e078"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#org905ec0d"&gt;Visualize the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#org953b1d2"&gt;Convolutional  Autoencoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#orgb2fb44c"&gt;Transpose Convolutions, Decoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#org64522aa"&gt;Initialize The NN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#org7e5532c"&gt;Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/#org49e2bc4"&gt;Checking out the results&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfaef222" class="outline-2"&gt;
&lt;h2 id="orgfaef222"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfaef222"&gt;
&lt;p&gt;
Sticking with the &lt;a href="https://en.wikipedia.org/wiki/MNIST_database"&gt;MNIST&lt;/a&gt; dataset, let's improve our autoencoder's performance using convolutional layers. We'll build a convolutional autoencoder to compress the MNIST dataset.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;The encoder portion will be made of convolutional and pooling layers and the decoder will be made of &lt;b&gt;&lt;b&gt;transpose convolutional layers&lt;/b&gt;&lt;/b&gt; that learn to "upsample" a compressed representation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org15b8fc1" class="outline-2"&gt;
&lt;h2 id="org15b8fc1"&gt;Compressed Representation&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org15b8fc1"&gt;
&lt;p&gt;
A compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or other kinds of reconstruction and transformation!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org68eac86" class="outline-2"&gt;
&lt;h2 id="org68eac86"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org68eac86"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgea44606" class="outline-3"&gt;
&lt;h3 id="orgea44606"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgea44606"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org005a091" class="outline-4"&gt;
&lt;h4 id="org005a091"&gt;Python Standard Library&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org005a091"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd7850f6" class="outline-4"&gt;
&lt;h4 id="orgd7850f6"&gt;From PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd7850f6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dotenv&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;graphviz&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Graph&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torchvision&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pyplot&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torchvision.transforms&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;transforms&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcb649af" class="outline-3"&gt;
&lt;h3 id="orgcb649af"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcb649af"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;get_ipython&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_line_magic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'matplotlib'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'inline'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;get_ipython&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_line_magic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'config'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"InlineBackend.figure_format = 'retina'"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"whitegrid"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"axes.grid"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		&lt;span class="s2"&gt;"font.family"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"sans-serif"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
		&lt;span class="s2"&gt;"font.sans-serif"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Open Sans"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Latin Modern Sans"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Lato"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
		&lt;span class="s2"&gt;"figure.figsize"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)},&lt;/span&gt;
	    &lt;span class="n"&gt;font_scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org12e5d61" class="outline-3"&gt;
&lt;h3 id="org12e5d61"&gt;Test for &lt;a href="http://pytorch.org/docs/stable/cuda.html"&gt;CUDA&lt;/a&gt;&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org12e5d61"&gt;
&lt;p&gt;
The test-code uses the check later on so I'll save it to the &lt;code&gt;train_on_gpu&lt;/code&gt; variable.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_on_gpu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"cuda:0"&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;train_on_gpu&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s2"&gt;"cpu"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Using: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Using: cuda:0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3c4e078" class="outline-2"&gt;
&lt;h2 id="org3c4e078"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3c4e078"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdf51715" class="outline-3"&gt;
&lt;h3 id="orgdf51715"&gt;Setup the Data Transform&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdf51715"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1888696" class="outline-3"&gt;
&lt;h3 id="org1888696"&gt;Load the Training and Test Datasets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1888696"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"~/datasets/MNIST/"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expanduser&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_dir&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/MNIST
True
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MNIST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MNIST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			   &lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7f4c6b0" class="outline-3"&gt;
&lt;h3 id="org7f4c6b0"&gt;Create training and test dataloaders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7f4c6b0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;NUM_WORKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="c1"&gt;# how many samples per batch to load&lt;/span&gt;
&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2d80d8c" class="outline-3"&gt;
&lt;h3 id="org2d80d8c"&gt;Prepare Data Loaders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2d80d8c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_loader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
					   &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					   &lt;span class="n"&gt;num_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NUM_WORKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_loader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					  &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					  &lt;span class="n"&gt;num_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;NUM_WORKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org905ec0d" class="outline-2"&gt;
&lt;h2 id="org905ec0d"&gt;Visualize the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org905ec0d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgddd3583" class="outline-3"&gt;
&lt;h3 id="orgddd3583"&gt;Obtain One Batch of Training Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgddd3583"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataiter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc17eae0" class="outline-3"&gt;
&lt;h3 id="orgc17eae0"&gt;Get One Image From the Batch&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc17eae0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org88efff6" class="outline-3"&gt;
&lt;h3 id="org88efff6"&gt;Plot&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org88efff6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"First Image"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/first_image.png" alt="first_image.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org953b1d2" class="outline-2"&gt;
&lt;h2 id="org953b1d2"&gt;Convolutional  Autoencoder&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org953b1d2"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc6540f0" class="outline-3"&gt;
&lt;h3 id="orgc6540f0"&gt;Encoder&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc6540f0"&gt;
&lt;p&gt;
The encoder part of the network will be a typical convolutional pyramid. Each convolutional layer will be followed by a max-pooling layer to reduce the dimensions of the layers. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3c03eba" class="outline-3"&gt;
&lt;h3 id="org3c03eba"&gt;Decoder&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3c03eba"&gt;
&lt;p&gt;
The decoder, though, might be something new to you. The decoder needs to convert from a narrow representation to a wide, reconstructed image. For example, the representation could be a 7x7x4 max-pool layer. This is the output of the encoder, but also the input to the decoder. We want to get a 28x28x1 image out from the decoder so we need to work our way back up from the compressed representation. A schematic of the network is shown below.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Graph&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"png"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Input layer&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"a"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"28x28x1 Input"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# the Encoder&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"b"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"28x28x16 Convolution"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"c"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"14x14x16 MaxPool"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"d"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"14x14x4 Convolution"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"e"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"7x7x4 MaxPool"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# The Decoder&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"f"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"14x14x16 Transpose Convolution"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"g"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"28x28x1 Transpose Convolution"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# The Output&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"h"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"28x28x1 Output"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;edges&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"abcdefgh"&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;edge&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;edge&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;edge&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;edges&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

&lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"graphs/network_graph.dot"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;graph&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/files/posts/nano/autoencoders/convolutional-autoencoder/network_graph.dot.png" alt="network_graph.dot.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/network_graph.dot.png" alt="network_graph.dot.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
Here our final encoder layer has size 7x7x4 = 196. The original images have size 28x28 = 784, so the encoded vector is 25% the size of the original image. These are just suggested sizes for each of the layers. Feel free to change the depths and sizes, in fact, you're encouraged to add additional layers to make this representation even smaller! Remember our goal here is to find a small representation of the input data.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb2fb44c" class="outline-2"&gt;
&lt;h2 id="orgb2fb44c"&gt;Transpose Convolutions, Decoder&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb2fb44c"&gt;
&lt;p&gt;
This decoder uses &lt;b&gt;&lt;b&gt;transposed convolutional&lt;/b&gt;&lt;/b&gt; layers to increase the width and height of the input layers. They work almost exactly the same as convolutional layers, but in reverse. A stride in the input layer results in a larger stride in the transposed convolution layer. For example, if you have a 3x3 kernel, a 3x3 patch in the input layer will be reduced to one unit in a convolutional layer. Comparatively, one unit in the input layer will be expanded to a 3x3 path in a transposed convolution layer. PyTorch provides us with an easy way to create the layers, &lt;a href="https://pytorch.org/docs/stable/nn.html#convtranspose2d"&gt;&lt;code&gt;nn.ConvTranspose2d&lt;/code&gt;&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
It is important to note that transpose convolution layers can lead to artifacts in the final images, such as checkerboard patterns. This is due to overlap in the kernels which can be avoided by setting the stride and kernel size equal. In &lt;a href="http://distill.pub/2016/deconv-checkerboard/"&gt;this Distill article&lt;/a&gt; from Augustus Odena, &lt;b&gt;et al&lt;/b&gt;, the authors show that these checkerboard artifacts can be avoided by resizing the layers using nearest neighbor or bilinear interpolation (upsampling) followed by a convolutional layer. 
&lt;/p&gt;

&lt;p&gt;
We'll show this approach in another notebook, so you can experiment with it and see the difference.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Build the encoder out of a series of convolutional and pooling layers.&lt;/li&gt;
&lt;li&gt;When building the decoder, recall that transpose convolutional layers can upsample an input by a factor of 2 using a stride and kernel_size of 2.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
See:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d"&gt;Conv2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html?highlight=maxpool#torch.nn.MaxPool2d"&gt;MaxPool2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#relu"&gt;ReLU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#sigmoid"&gt;Sigmoid&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
To get the output size of our Convolutional Layers you use the formula:
&lt;/p&gt;

&lt;p&gt;
\[
o = \frac{W - F + 2P}{S} + 1
\]
&lt;/p&gt;

&lt;p&gt;
Where &lt;i&gt;W&lt;/i&gt; is the input size (28 here), &lt;i&gt;F&lt;/i&gt; is the filter size, &lt;i&gt;P&lt;/i&gt; is the zero-padding, and &lt;i&gt;S&lt;/i&gt; is the stride. For our first layer we want to keep the output the same size as the input.
&lt;/p&gt;

&lt;p&gt;
The output for a maxpool layer uses a similar set of equations.
&lt;/p&gt;

\begin{align}
W_2 &amp;amp;= \frac{W_1 - F}{S} + 1\\
H_2 &amp;amp;= \frac{H_Y - F}{S} + 1\\
D_2 = D_1\\
\end{align}

&lt;p&gt;
Where &lt;i&gt;W&lt;/i&gt; is the width, &lt;i&gt;H&lt;/i&gt; is the height, and &lt;i&gt;D&lt;/i&gt; is the depth.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;namedtuple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Layer"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"kernel stride depth padding"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;Layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__new__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__defaults__&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;output_size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""Calculates the output size of the layer&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     input_size: the size of the input to the layer&lt;/span&gt;
&lt;span class="sd"&gt;     layer: named tuple with values for the layer&lt;/span&gt;
&lt;span class="sd"&gt;     expected: the value you are expecting&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     the size of the output&lt;/span&gt;

&lt;span class="sd"&gt;    Raises:&lt;/span&gt;
&lt;span class="sd"&gt;     AssertionError: the calculated value wasn't the expected one&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;input_size&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Layer Output Size: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;expected&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org34370f0" class="outline-3"&gt;
&lt;h3 id="org34370f0"&gt;The Encoder Layers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org34370f0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org48dec25" class="outline-4"&gt;
&lt;h4 id="org48dec25"&gt;Layer One&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org48dec25"&gt;
&lt;p&gt;
The first layer is a Convolutional Layer that we want to have the same size output as the input but with a depth of sixteen. The &lt;a href="https://cs231n.github.io/convolutional-networks/"&gt;CS 231&lt;/a&gt; page notes that to keep the size of the output the same as the input you should set the stride to one and once you have decided on your kernle size (&lt;i&gt;F&lt;/i&gt;) then you can find your padding using this equation:
&lt;/p&gt;

&lt;p&gt;
\[
P = \frac{F - 1}{2}
\]
&lt;/p&gt;


&lt;p&gt;
In this case I'm going to use a filter size of three so our padding will be:
&lt;/p&gt;

\begin{align}
P &amp;amp;= \frac{3 - 1}{2}\\
  &amp;amp;= 1\\
\end{align}

&lt;p&gt;
We can double-check this by plugging the values back intoo the equation for output size.
&lt;/p&gt;

\begin{align}
W' &amp;amp;= \frac{W - F + 2P}{S} + 1\\
   &amp;amp;= \frac{28 - 3 + 2(1)}{1} + 1\\
   &amp;amp;= 28\\
\end{align}

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Variable&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;&lt;i&gt;W&lt;/i&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;One dimension of the input&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;&lt;i&gt;F&lt;/i&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;One dimension of the Kernel (filter)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;&lt;i&gt;S&lt;/i&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;Stride&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="n"&gt;layer_one&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		   &lt;span class="n"&gt;padding&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		   &lt;span class="n"&gt;stride&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		   &lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

 &lt;span class="n"&gt;INPUT_ONE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;
 &lt;span class="n"&gt;OUTPUT_ONE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INPUT_ONE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layer_one&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;INPUT_ONE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
 &lt;span class="n"&gt;INPUT_DEPTH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=3, stride=1, depth=16, padding=1)
Layer Output Size: 28.0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7dc8d3c" class="outline-4"&gt;
&lt;h4 id="org7dc8d3c"&gt;Layer Two&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7dc8d3c"&gt;
&lt;p&gt;
The second layer is a MaxPool layer that will keep the depth of six but will halve the size to fourteen. According to the &lt;a href="https://cs231n.github.io/convolutional-networks/"&gt;CS 231 n&lt;/a&gt; page on Convolutional Networks, there are only two values for the kernel size that are usually used - 2 and 3, and the stride is usually just 2, with a kernel size of 2 being more common, and as it turns out, a kernel size of 2 and a stride of 2 will reduce our input dimensions by a half, which is what we want.
&lt;/p&gt;

\begin{align}
W &amp;amp;= \frac{28 - 2}{2} + 1\\
  &amp;amp;= 14\\
\end{align}

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="n"&gt;layer_two&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_one&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
 &lt;span class="n"&gt;OUTPUT_TWO&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_ONE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layer_two&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=2, stride=2, depth=16, padding=0)
Layer Output Size: 14.0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org88760cd" class="outline-4"&gt;
&lt;h4 id="org88760cd"&gt;Layer Three&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org88760cd"&gt;
&lt;p&gt;
Our third layer is another convolutional layer that preserves the input width and height but this time the output will have a depth of 4.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;layer_three&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;OUTPUT_THREE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_TWO&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layer_three&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;OUTPUT_TWO&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=3, stride=1, depth=4, padding=1)
Layer Output Size: 14.0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3539692" class="outline-4"&gt;
&lt;h4 id="org3539692"&gt;Layer Four&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3539692"&gt;
&lt;p&gt;
The last layer in the encoder is a max pool layer that reduces the previous layer by half (to dimensions of 7) while preserving the depth.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;layer_four&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_three&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;OUTPUT_FOUR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OUTPUT_THREE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layer_four&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Layer(kernel=2, stride=2, depth=4, padding=0)
Layer Output Size: 7.0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org518fb33" class="outline-3"&gt;
&lt;h3 id="org518fb33"&gt;Decoders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org518fb33"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org16c85e1" class="outline-4"&gt;
&lt;h4 id="org16c85e1"&gt;Layer Five&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org16c85e1"&gt;
&lt;p&gt;
We want an output of 14 x 14 x 16 from an input of 7 x 7 x 4. The comments given with this exercise say that using a kernel of 2 and stride of 2 will double the dimensions, much as those same values halve the dimensions with Max-Pooling.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;layer_five&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb5fa887" class="outline-4"&gt;
&lt;h4 id="orgb5fa887"&gt;Layer Six&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb5fa887"&gt;
&lt;p&gt;
This layer will expand the image back to its original size of 28 x 28 x 1
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;layer_six&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8c1c313" class="outline-3"&gt;
&lt;h3 id="org8c1c313"&gt;Define the NN Architecture&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8c1c313"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ConvAutoencoder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""A CNN AutoEncoder-Decoder"""&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="c1"&gt;## encoder layers ##&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convolution_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;INPUT_DEPTH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				       &lt;span class="n"&gt;out_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_one&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				       &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_one&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
				       &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_one&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				       &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_one&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_two&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				       &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_two&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convolution_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;in_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_two&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				       &lt;span class="n"&gt;out_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_three&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				       &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_three&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				       &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_three&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				       &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_three&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	&lt;span class="c1"&gt;## decoder layers ##&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose_convolution_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ConvTranspose2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="n"&gt;in_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_four&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
	    &lt;span class="n"&gt;out_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_five&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_five&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_five&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose_convolution_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ConvTranspose2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="n"&gt;in_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_five&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
	    &lt;span class="n"&gt;out_channels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_six&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="n"&gt;kernel_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_six&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="n"&gt;stride&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;layer_six&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kernel&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sigmoid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="c1"&gt;## encode ##&lt;/span&gt;
	&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convolution_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
	&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convolution_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
	&lt;span class="c1"&gt;## decode ##&lt;/span&gt;
	&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose_convolution_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose_convolution_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConvAutoencoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;dataiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataiter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convolution_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;convolution_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max_pool_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose_convolution_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose_convolution_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
torch.Size([20, 16, 28, 28])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 4, 14, 14])
torch.Size([20, 4, 7, 7])
torch.Size([20, 4, 7, 7])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 1, 28, 28])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org64522aa" class="outline-2"&gt;
&lt;h2 id="org64522aa"&gt;Initialize The NN&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org64522aa"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConvAutoencoder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
ConvAutoencoder(
  (convolution_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (convolution_2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (transpose_convolution_1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
  (transpose_convolution_2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))
  (relu): ReLU()
  (sigmoid): Sigmoid()
)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7e5532c" class="outline-2"&gt;
&lt;h2 id="org7e5532c"&gt;Training&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7e5532c"&gt;
&lt;p&gt;
Here I'll write a bit of code to train the network. I'm not too interested in validation here, so I'll just monitor the training loss and the test loss afterwards. 
&lt;/p&gt;

&lt;p&gt;
We are not concerned with labels in this case, just images, which we can get from the &lt;code&gt;train_loader&lt;/code&gt;. Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use &lt;a href="https://pytorch.org/docs/stable/nn.html?highlight=mseloss#torch.nn.MSELoss"&gt;MSELoss&lt;/a&gt;. And compare output images and input images as follows:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Otherwise, this is pretty straightfoward training with PyTorch. Since this is a convolutional autoencoder, our images &lt;i&gt;do not&lt;/i&gt; need to be flattened before being passed in an input to our model.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org73f48e7" class="outline-3"&gt;
&lt;h3 id="org73f48e7"&gt;Train the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org73f48e7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MSELoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;n_epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;
&lt;span class="n"&gt;started&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# monitor training loss&lt;/span&gt;
    &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;

    &lt;span class="c1"&gt;###################&lt;/span&gt;
    &lt;span class="c1"&gt;# train the model #&lt;/span&gt;
    &lt;span class="c1"&gt;###################&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="c1"&gt;# _ stands in for labels, here&lt;/span&gt;
	&lt;span class="c1"&gt;# no need to flatten images&lt;/span&gt;
	&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;
	&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="c1"&gt;# clear the gradients of all optimized variables&lt;/span&gt;
	&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="c1"&gt;# forward pass: compute predicted outputs by passing inputs to the model&lt;/span&gt;
	&lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="c1"&gt;# calculate the loss&lt;/span&gt;
	&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="c1"&gt;# backward pass: compute gradient of the loss with respect to model parameters&lt;/span&gt;
	&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="c1"&gt;# perform a single optimization step (parameter update)&lt;/span&gt;
	&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="c1"&gt;# update running training loss&lt;/span&gt;
	&lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# print avg training statistics &lt;/span&gt;
    &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Epoch: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt; &lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s1"&gt;Training Loss: &lt;/span&gt;&lt;span class="si"&gt;{:.6f}&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
	&lt;span class="n"&gt;train_loss&lt;/span&gt;
	&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;ended&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Ended: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ended&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Elapsed: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ended&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;started&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 1 	Training Loss: 0.259976
Epoch: 2 	Training Loss: 0.244956
Epoch: 3 	Training Loss: 0.235354
Epoch: 4 	Training Loss: 0.226544
Epoch: 5 	Training Loss: 0.216255
Epoch: 6 	Training Loss: 0.207204
Epoch: 7 	Training Loss: 0.200490
Epoch: 8 	Training Loss: 0.195582
Epoch: 9 	Training Loss: 0.191870
Epoch: 10 	Training Loss: 0.189247
Epoch: 11 	Training Loss: 0.187027
Epoch: 12 	Training Loss: 0.185084
Epoch: 13 	Training Loss: 0.183055
Epoch: 14 	Training Loss: 0.181224
Epoch: 15 	Training Loss: 0.179749
Epoch: 16 	Training Loss: 0.178564
Epoch: 17 	Training Loss: 0.177572
Epoch: 18 	Training Loss: 0.176735
Epoch: 19 	Training Loss: 0.176076
Epoch: 20 	Training Loss: 0.175518
Epoch: 21 	Training Loss: 0.175040
Epoch: 22 	Training Loss: 0.174629
Epoch: 23 	Training Loss: 0.174230
Epoch: 24 	Training Loss: 0.173856
Epoch: 25 	Training Loss: 0.173497
Epoch: 26 	Training Loss: 0.173166
Epoch: 27 	Training Loss: 0.172838
Epoch: 28 	Training Loss: 0.172520
Epoch: 29 	Training Loss: 0.172212
Epoch: 30 	Training Loss: 0.171920
Ended: 2018-12-21 17:41:26.461977
Elapsed: 0:07:50.942721
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org49e2bc4" class="outline-2"&gt;
&lt;h2 id="org49e2bc4"&gt;Checking out the results&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org49e2bc4"&gt;
&lt;p&gt;
Below I've plotted some of the test images along with their reconstructions. These look a little rough around the edges, likely due to the checkerboard effect we mentioned above that tends to happen with transpose layers.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org91102e7" class="outline-3"&gt;
&lt;h3 id="org91102e7"&gt;Obtain One Batch Of Test Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org91102e7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataiter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org33e3a11" class="outline-3"&gt;
&lt;h3 id="org33e3a11"&gt;Get Sample Outputs&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org33e3a11"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3bc0c3a" class="outline-3"&gt;
&lt;h3 id="org3bc0c3a"&gt;Prep Images for Display&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3bc0c3a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org149898a" class="outline-3"&gt;
&lt;h3 id="org149898a"&gt;Output Is Resized Into a Batch Of Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org149898a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7e46dea" class="outline-3"&gt;
&lt;h3 id="org7e46dea"&gt;Use Detach When It's An Output That Requires Grad&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7e46dea"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;detach&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7632b9e" class="outline-3"&gt;
&lt;h3 id="org7632b9e"&gt;plot the first ten input images and then reconstructed images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7632b9e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nrows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ncols&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sharex&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sharey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Auto-Encoded/Decoded Images"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# input images on top row, reconstructions on bottom&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;axes&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xaxis&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_visible&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_yaxis&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_visible&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/reconstructed.png" alt="reconstructed.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
That is better than I would have thought it would be.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>autoencoder</category><category>cnn</category><category>exercise</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/autoencoders/convolutional-autoencoder/</guid><pubDate>Wed, 19 Dec 2018 20:15:02 GMT</pubDate></item><item><title>Weight Initialization</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#orgd4edcef"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#org5ce6e0c"&gt;Initial Weights and Observing Training Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#org1ef16d9"&gt;Dataset and Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#orgc4b563b"&gt;Import Libraries and Load the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#org446dab2"&gt;Visualize Some Training Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#org03900c4"&gt;Define the Model Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#orgc61376d"&gt;Initialize Weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#orgdc6f931"&gt;Compare Model Behavior&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#org9f5678b"&gt;General rule for setting weights&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#orgf62d812"&gt;Normal Distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#org03d0150"&gt;Automatic Initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/#org390dc0c"&gt;evaluate the behavior using helpers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd4edcef" class="outline-2"&gt;
&lt;h2 id="orgd4edcef"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd4edcef"&gt;
&lt;p&gt;
In this lesson, you'll learn how to find good initial weights for a neural network. Weight initialization happens once, when a model is created and before it trains. Having good initial weights can place the neural network close to the optimal solution. This allows the neural network to come to the best solution quicker. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5ce6e0c" class="outline-2"&gt;
&lt;h2 id="org5ce6e0c"&gt;Initial Weights and Observing Training Loss&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5ce6e0c"&gt;
&lt;p&gt;
To see how different weights perform, we'll test on the same dataset and neural network. That way, we know that any changes in model behavior are due to the weights and not any changing data or model structure. 
 We'll instantiate at least two of the same models, with &lt;i&gt;different&lt;/i&gt; initial weights and see how the training loss decreases over time.
&lt;/p&gt;


&lt;p&gt;
Sometimes the differences in training loss, over time, will be large and other times, certain weights offer only small improvements.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1ef16d9" class="outline-2"&gt;
&lt;h2 id="org1ef16d9"&gt;Dataset and Model&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1ef16d9"&gt;
&lt;p&gt;
We'll train an MLP to classify images from the &lt;a href="https://github.com/zalandoresearch/fashion-mnist"&gt;Fashion-MNIST database&lt;/a&gt; to demonstrate the effect of different initial weights. As a reminder, the FashionMNIST dataset contains images of clothing types; &lt;code&gt;classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']&lt;/code&gt;. The images are normalized so that their pixel values are in a range [0.0 - 1.0).  Run the cell below to download and load the dataset.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc4b563b" class="outline-2"&gt;
&lt;h2 id="orgc4b563b"&gt;Import Libraries and Load the &lt;a href="http://pytorch.org/docs/stable/torchvision/datasets.html"&gt;Data&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc4b563b"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org18d3d20" class="outline-3"&gt;
&lt;h3 id="org18d3d20"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org18d3d20"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# python
from functools import partial
from typing import Collection, Tuple
# from pypi
from dotenv import load_dotenv
from sklearn.model_selection import train_test_split
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision import datasets
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms

# udacity
import nano.helpers as helpers

# this project
from neurotic.tangles.data_paths import DataPathTwo
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc9aad30" class="outline-3"&gt;
&lt;h3 id="orgc9aad30"&gt;Load the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc9aad30"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# number of subprocesses to use for data loading
subprocesses = 0
# how many samples per batch to load
batch_size = 100
# percentage of training set to use as validation
VALIDATION_FRACTION = 0.2
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Convert the data to a  torch.FloatTensor.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;transform = transforms.ToTensor()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
path = DataPathTwo(folder_key="FASHION")
print(path.folder)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/brunhilde/datasets/FASHION
&lt;/pre&gt;


&lt;p&gt;
Choose the training and test datasets.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_data = datasets.FashionMNIST(root=path.folder, train=True,
				   download=True, transform=transform)
test_data = datasets.FashionMNIST(root=path.folder, train=False,
				  download=True, transform=transform)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Processing...
Done!
&lt;/pre&gt;

&lt;p&gt;
Obtain training indices that will be used for validation.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;indices = list(range(len(train_data)))
train_idx, valid_idx = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Define samplers for obtaining training and validation batches.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Prepare data loaders (combine dataset and sampler).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,
					   sampler=train_sampler, num_workers=subprocesses)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, 
					   sampler=valid_sampler, num_workers=subprocesses)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, 
					  num_workers=subprocesses)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 
    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org446dab2" class="outline-2"&gt;
&lt;h2 id="org446dab2"&gt;Visualize Some Training Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org446dab2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
		"figure.figsize": (10, 8)},
	    font_scale=1)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Obtain one batch of training images.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Plot the images in the batch, along with the corresponding labels.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fig = pyplot.figure(figsize=(12, 10))
fig.suptitle("Sample FASHION Images", weight="bold")
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    ax.imshow(np.squeeze(images[idx]), cmap='gray')
    ax.set_title(classes[labels[idx]])
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/image_one.png" alt="image_one.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org03900c4" class="outline-2"&gt;
&lt;h2 id="org03900c4"&gt;Define the Model Architecture&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org03900c4"&gt;
&lt;p&gt;
We've defined the MLP that we'll use for classifying the dataset.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd591d32" class="outline-3"&gt;
&lt;h3 id="orgd591d32"&gt;Neural Network&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd591d32"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;A 3 layer MLP with hidden dimensions of 256 and 128.&lt;/li&gt;
&lt;li&gt;This MLP accepts a flattened image (784-value long vector) as input and produces 10 class scores as output.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
We'll test the effect of different initial weights on this 3 layer neural network with ReLU activations and an Adam optimizer. The lessons you learn apply to other neural networks, including different activations and optimizers.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc61376d" class="outline-2"&gt;
&lt;h2 id="orgc61376d"&gt;Initialize Weights&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc61376d"&gt;
&lt;p&gt;
Let's start looking at some initial weights.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org778cf27" class="outline-3"&gt;
&lt;h3 id="org778cf27"&gt;All Zeros or Ones&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org778cf27"&gt;
&lt;p&gt;
If you follow the principle of &lt;a href="https://en.wikipedia.org/wiki/Occam's_razor"&gt;Occam's razor&lt;/a&gt;, you might think setting all the weights to 0 or 1 would be the best solution.  This is not the case.
&lt;/p&gt;

&lt;p&gt;
With every weight the same, all the neurons at each layer are producing the same output.  This makes it hard to decide which weights to adjust.
&lt;/p&gt;

&lt;p&gt;
Let's compare the loss with all ones and all zero weights by defining two models with those constant weights.
&lt;/p&gt;

&lt;p&gt;
Below, we are using PyTorch's &lt;a href="https://pytorch.org/docs/stable/nn.html#torch-nn-init"&gt;nn.init&lt;/a&gt; to initialize each Linear layer with a constant weight. The init library provides a number of weight initialization functions that give you the ability to initialize the weights of each layer according to layer type.
&lt;/p&gt;

&lt;p&gt;
In the case below, we look at every layer/module in our model. If it is a Linear layer (as all three layers are for this MLP), then we initialize those layer weights to be a &lt;code&gt;constant_weight&lt;/code&gt; with &lt;code&gt;bias=0&lt;/code&gt; using the following code:
&lt;/p&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;constant_weight&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;constant_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The &lt;code&gt;constant_weight&lt;/code&gt; is a value that you can pass in when you instantiate the model.
&lt;/p&gt;
&lt;/div&gt;


&lt;div id="outline-container-orgfd74e97" class="outline-4"&gt;
&lt;h4 id="orgfd74e97"&gt;Define the NN architecture&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgfd74e97"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Net(nn.Module):
    def __init__(self, hidden_1=256, hidden_2=128, constant_weight=None):
	super(Net, self).__init__()
	# linear layer (784 -&amp;gt; hidden_1)
	self.fc1 = nn.Linear(28 * 28, hidden_1)
	# linear layer (hidden_1 -&amp;gt; hidden_2)
	self.fc2 = nn.Linear(hidden_1, hidden_2)
	# linear layer (hidden_2 -&amp;gt; 10)
	self.fc3 = nn.Linear(hidden_2, 10)
	# dropout layer (p=0.2)
	self.dropout = nn.Dropout(0.2)

	# initialize the weights to a specified, constant value
	if(constant_weight is not None):
	    for m in self.modules():
		if isinstance(m, nn.Linear):
		    nn.init.constant_(m.weight, constant_weight)
		    nn.init.constant_(m.bias, 0)


    def forward(self, x):
	# flatten image input
	x = x.view(-1, 28 * 28)
	# add hidden layer, with relu activation function
	x = F.relu(self.fc1(x))
	# add dropout layer
	x = self.dropout(x)
	# add hidden layer, with relu activation function
	x = F.relu(self.fc2(x))
	# add dropout layer
	x = self.dropout(x)
	# add output layer
	x = self.fc3(x)
	return x
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-orgdc6f931" class="outline-2"&gt;
&lt;h2 id="orgdc6f931"&gt;Compare Model Behavior&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdc6f931"&gt;
&lt;p&gt;
Below, we are using &lt;code&gt;helpers.compare_init_weights&lt;/code&gt; to compare the training and validation loss for the two models we defined above, &lt;code&gt;model_0&lt;/code&gt; and &lt;code&gt;model_1&lt;/code&gt;.  This function takes in a list of models (each with different initial weights), the name of the plot to produce, and the training and validation dataset loaders. For each given model, it will plot the training loss for the first 100 batches and print out the validation accuracy after 2 training epochs. &lt;b&gt;Note: if you've used a small batch_size, you may want to increase the number of epochs here to better compare how models behave after seeing a few hundred images.&lt;/b&gt; 
&lt;/p&gt;

&lt;p&gt;
We plot the loss over the first 100 batches to better judge which model weights performed better at the start of training. &lt;b&gt;&lt;b&gt;I recommend that you take a look at the code in &lt;code&gt;helpers.py&lt;/code&gt; to look at the details behind how the models are trained, validated, and compared.&lt;/b&gt;&lt;/b&gt;
&lt;/p&gt;

&lt;p&gt;
Run the cell below to see the difference between weights of all zeros against all ones.
&lt;/p&gt;

&lt;p&gt;
Initialize two NN's with 0 and 1 constant weights.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_0 = Net(constant_weight=0)
model_1 = Net(constant_weight=1)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Put them in list form to compare.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_list = [(model_0, 'All Zeros'),
	      (model_1, 'All Ones')]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ModelLabel = Tuple[nn.Module, str]
ModelLabels = Collection[ModelLabel]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def plot_models(title:str, models_labels:ModelLabels):
    """Plots the models

    Args:
     title: the title for the plots
     models_labels: collections of model, plot-label tuples
    """
    figure, axe = pyplot.subplots()
    figure.suptitle(title, weight="bold")    
    axe.set_xlabel("Batches")
    axe.set_ylabel("Loss")

    for model, label in models_labels:
	loss, validation_accuracy = helpers._get_loss_acc(model, train_loader, valid_loader)
	axe.plot(loss[:100], label=label)
    legend = axe.legend()
    return
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Plot the loss over the first 100 batches.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;plot_models("All Zeros vs All Ones",
	    ((model_0, "All Zeros"),
	     (model_1, "All ones")))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/zeros_ones.png" alt="zeros_ones.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
After 2 Epochs:
Validation Accuracy
    9.475% -- All Zeros
   10.175% -- All Ones
Training Loss
    2.304  -- All Zeros
  1914.703  -- All Ones
&lt;/pre&gt;

&lt;p&gt;
As you can see the accuracy is close to guessing for both zeros and ones, around 10%.
&lt;/p&gt;

&lt;p&gt;
The neural network is having a hard time determining which weights need to be changed, since the neurons have the same output for each layer.  To avoid neurons with the same output, let's use unique weights.  We can also randomly select these weights to avoid being stuck in a local minimum for each run.
&lt;/p&gt;

&lt;p&gt;
A good solution for getting these random weights is to sample from a uniform distribution.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgeb665ba" class="outline-3"&gt;
&lt;h3 id="orgeb665ba"&gt;Uniform Distribution&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgeb665ba"&gt;
&lt;p&gt;
A &lt;a href="https://en.wikipedia.org/wiki/Uniform_distribution"&gt;uniform distribution&lt;/a&gt; has the equal probability of picking any number from a set of numbers. We'll be picking from a continuous distribution, so the chance of picking the same number is low. We'll use NumPy's &lt;code&gt;np.random.uniform&lt;/code&gt; function to pick random numbers from a uniform distribution.
&lt;/p&gt;

&lt;p&gt;
&lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.uniform.html"&gt;&lt;code&gt;np.random_uniform(low=0.0, high=1.0, size=None)&lt;/code&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
Outputs random values from a uniform distribution.
&lt;/p&gt;

&lt;p&gt;
The generated values follow a uniform distribution in the range [low, high). The lower bound minval is included in the range, while the upper bound maxval is excluded.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;low:&lt;/b&gt;&lt;/b&gt; The lower bound on the range of random values to generate. Defaults to 0.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;high:&lt;/b&gt;&lt;/b&gt; The upper bound on the range of random values to generate. Defaults to 1.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;size:&lt;/b&gt;&lt;/b&gt; An int or tuple of ints that specify the shape of the output array.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
We can visualize the uniform distribution by using a histogram. Let's map the values from &lt;code&gt;np.random_uniform(-3, 3, [1000])&lt;/code&gt; to a histogram using the &lt;code&gt;helper.hist_dist&lt;/code&gt; function. This will be &lt;code&gt;1000&lt;/code&gt; random float values from &lt;code&gt;-3&lt;/code&gt; to &lt;code&gt;3&lt;/code&gt;, excluding the value &lt;code&gt;3&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
figure.suptitle("Random Uniform", weight="bold")
data = numpy.random.uniform(-3, 3, [1000])
grid = seaborn.distplot(data)
#helpers.hist_dist('Random Uniform (low=-3, high=3)', )
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/uniform_distribution.png" alt="uniform_distribution.png"&gt;
&lt;/p&gt;
&lt;/div&gt;




&lt;p&gt;
Now that you understand the uniform function, let's use PyTorch's &lt;code&gt;nn.init&lt;/code&gt; to apply it to a model's initial weights.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgca7e7f8" class="outline-3"&gt;
&lt;h3 id="orgca7e7f8"&gt;Uniform Initialization, Baseline&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgca7e7f8"&gt;
&lt;p&gt;
Let's see how well the neural network trains using a uniform weight initialization, where &lt;code&gt;low=0.0&lt;/code&gt; and &lt;code&gt;high=1.0&lt;/code&gt;. Below, I'll show you another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, you can:
&lt;/p&gt;

&lt;ol class="org-ol"&gt;
&lt;li&gt;Define a function that assigns weights by the type of network layer, &lt;b&gt;then&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Apply those weights to an initialized model using &lt;code&gt;model.apply(fn)&lt;/code&gt;, which applies a function to each model layer.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
This time, we'll use &lt;code&gt;weight.data.uniform_&lt;/code&gt; to initialize the weights of our model, directly.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def weights_init_uniform(m: nn.Module, start=0.0, stop=1.0) -&amp;gt; None:
    """takes in a module and applies the specified weight initialization

    Args:
     m: A model instance
    """
    classname = m.__class__.__name__
    # for every Linear layer in a model..
    if classname.startswith('Linear'):
	# apply a uniform distribution to the weights and a bias=0
	m.weight.data.uniform_(start, stop)
	m.bias.data.fill_(0)
    return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org45375d2" class="outline-4"&gt;
&lt;h4 id="org45375d2"&gt;Create A New Model With These Weights&lt;/h4&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd87c393" class="outline-4"&gt;
&lt;h4 id="orgd87c393"&gt;Evaluate Behavior&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd87c393"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_uniform = Net()
model_uniform.apply(weights_init_uniform)
plot_models("Uniform Baseline", ((model_uniform, "UNIFORM WEIGHTS"),))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/uniform_weights.png" alt="uniform_weights.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;
The loss graph is showing the neural network is learning, which it didn't with all zeros or all ones. We're headed in the right direction!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9f5678b" class="outline-2"&gt;
&lt;h2 id="org9f5678b"&gt;General rule for setting weights&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9f5678b"&gt;
&lt;p&gt;
The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. A good practice is to start your weights in the range of \([-y, y]\) where \(y=1/\sqrt{n}\) (\(n\) is the number of inputs to a given neuron).
&lt;/p&gt;

&lt;p&gt;
Let's see if this holds true; let's create a baseline to compare with and center our uniform range over zero by shifting it over by 0.5.  This will give us the range [-0.5, 0.5).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;weights_init_uniform_center = partial(weights_init_uniform, -0.5, 0.5)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgec38dc2" class="outline-3"&gt;
&lt;h3 id="orgec38dc2"&gt;create a new model with these weights&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgec38dc2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_centered = Net()
model_centered.apply(weights_init_uniform_center)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;
Now let's create a distribution and model that uses the &lt;b&gt;&lt;b&gt;general rule&lt;/b&gt;&lt;/b&gt; for weight initialization; using the range \([-y, y]\), where \(y=1/\sqrt{n}\) .
&lt;/p&gt;

&lt;p&gt;
And finally, we'll compare the two models.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def weights_init_uniform_rule(m: nn.Module) -&amp;gt; None:
    """takes in a module and applies the specified weight initialization

    Args:
     m: Model instance
    """
    classname = m.__class__.__name__
    # for every Linear layer in a model..
    if classname.find('Linear') != -1:
	# get the number of the inputs
	n = m.in_features
	y = 1.0/numpy.sqrt(n)
	m.weight.data.uniform_(-y, y)
	m.bias.data.fill_(0)
    return
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_rule = Net()
model_rule.apply(weights_init_uniform_rule)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;plot_models("Uniform Centered vs General Rule", (
    (model_centered, 'Centered Weights [-0.5, 0.5)'), 
    (model_rule, 'General Rule [-y, y)'),
))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/general_rule.png" alt="general_rule.png"&gt;
This behavior is really promising! Not only is the loss decreasing, but it seems to do so very quickly for our uniform weights that follow the general rule; after only two epochs we get a fairly high validation accuracy and this should give you some intuition for why starting out with the right initial weights can really help your training process!
&lt;/p&gt;

&lt;p&gt;
Since the uniform distribution has the same chance to pick &lt;b&gt;any value&lt;/b&gt; in a range, what if we used a distribution that had a higher chance of picking numbers closer to 0?  Let's look at the normal distribution.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf62d812" class="outline-2"&gt;
&lt;h2 id="orgf62d812"&gt;Normal Distribution&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf62d812"&gt;
&lt;p&gt;
Unlike the uniform distribution, the &lt;a href="https://en.wikipedia.org/wiki/Normal_distribution"&gt;normal distribution&lt;/a&gt; has a higher likelihood of picking number close to it's mean. To visualize it, let's plot values from NumPy's &lt;code&gt;np.random.normal&lt;/code&gt; function to a histogram.
&lt;/p&gt;

&lt;p&gt;
&lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html"&gt;np.random.normal(loc=0.0, scale=1.0, size=None)&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
Outputs random values from a normal distribution.
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;loc:&lt;/b&gt;&lt;/b&gt; The mean of the normal distribution.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;scale:&lt;/b&gt;&lt;/b&gt; The standard deviation of the normal distribution.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;shape:&lt;/b&gt;&lt;/b&gt; The shape of the output array.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
figure.suptitle("Standard Normal Distribution", weight="bold")
grid = seaborn.distplot(numpy.random.normal(size=[1000]))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/normal_distribution.png" alt="normal_distribution.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Let's compare the normal distribution against the previous, rule-based, uniform distribution.
&lt;/p&gt;

&lt;p&gt;
The normal distribution should have a mean of 0 and a standard deviation of \(y=1/\sqrt{n}\)
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def weights_init_normal(m: nn.Module) -&amp;gt; None:
    '''Takes in a module and initializes all linear layers with weight
       values taken from a normal distribution.'''

    classname = m.__class__.__name__
    if classname.startswith("Linear"):    
	m.weight.data.normal_(mean=0, std=1/numpy.sqrt(m.in_features))
	m.bias.data.fill_(0)
    return
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;
create a new model with the rule-based, uniform weights
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_uniform_rule = Net()
model_uniform_rule.apply(weights_init_uniform_rule)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
create a new model with the rule-based, NORMAL weights
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_normal_rule = Net()
model_normal_rule.apply(weights_init_normal)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
compare the two models
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;plot_models('Uniform vs Normal',
	    ((model_uniform_rule, 'Uniform Rule [-y, y)'), 
	     (model_normal_rule, 'Normal Distribution')))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/normal_vs_uniform.png" alt="normal_vs_uniform.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The normal distribution gives us pretty similar behavior compared to the uniform distribution, in this case. This is likely because our network is so small; a larger neural network will pick more weight values from each of these distributions, magnifying the effect of both initialization styles. In general, a normal distribution will result in better performance for a model.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org03d0150" class="outline-2"&gt;
&lt;h2 id="org03d0150"&gt;Automatic Initialization&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org03d0150"&gt;
&lt;p&gt;
Let's quickly take a look at what happens &lt;b&gt;without any explicit weight initialization&lt;/b&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9b362a6" class="outline-3"&gt;
&lt;h3 id="org9b362a6"&gt;Instantiate a model with &lt;span class="underline"&gt;no&lt;/span&gt; explicit weight initialization&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org390dc0c" class="outline-2"&gt;
&lt;h2 id="org390dc0c"&gt;evaluate the behavior using helpers&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org390dc0c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_normal_rule = Net()
model_normal_rule.apply(weights_init_normal)
model_default = Net()
model_rule = Net()
model_rule.apply(weights_init_uniform_rule)

plot_models("Default vs Normal vs General Rule", (
    (model_default, "Default"),
    (model_normal_rule, "Normal"),
    (model_rule, "General Rule")))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/default.png" alt="default.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
They all sort of look the same at this point.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>exercise</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/weight-initialization/</guid><pubDate>Mon, 17 Dec 2018 21:03:41 GMT</pubDate></item><item><title>Transfer Learning Exercise</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#org44dfd83"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#orgdf3e458"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#org5d19d37"&gt;Flower power&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#org6d6ea06"&gt;Download the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#orgffb5b78"&gt;Transforming the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#orgc4ca6a6"&gt;DataLoaders and Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#orgdb00252"&gt;Visualize some sample data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#org56462dc"&gt;Plot The Images In The Batch, Along With The Corresponding Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#orgc403503"&gt;Define the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#org785129a"&gt;Final Classifier Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#org36b4203"&gt;Specify Loss Function and Optimizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/#orge6fea59"&gt;Training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org44dfd83" class="outline-2"&gt;
&lt;h2 id="org44dfd83"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org44dfd83"&gt;
&lt;p&gt;
Most of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. Instead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune.
&lt;/p&gt;

&lt;p&gt;
In this notebook, you'll be using &lt;a href="https://arxiv.org/pdf/1409.1556.pdf"&gt;VGGNet&lt;/a&gt; trained on the &lt;a href="http://www.image-net.org/"&gt;ImageNet dataset&lt;/a&gt; as a feature extractor. 
&lt;/p&gt;

&lt;p&gt;
VGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but &lt;b&gt;&lt;b&gt;replace the final fully-connected layer&lt;/b&gt;&lt;/b&gt; with our own classifier. This way we can use VGGNet as a &lt;b&gt;fixed feature extractor&lt;/b&gt; for our images then easily train a simple classifier on top of that. 
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Use all but the last fully-connected layer as a fixed feature extractor.&lt;/li&gt;
&lt;li&gt;Define a new, final classification layer and apply it to a task of our choice!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
You can read more about transfer learning from &lt;a href="http://cs231n.github.io/transfer-learning/"&gt;the CS231n Stanford course notes&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdf3e458" class="outline-2"&gt;
&lt;h2 id="orgdf3e458"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdf3e458"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# python
from collections import OrderedDict
from datetime import datetime
import os

# pypi
from dotenv import load_dotenv
from torch import nn
from sklearn.model_selection import train_test_split
from torch.utils.data.sampler import SubsetRandomSampler

import matplotlib
import numpy
import seaborn
import torch
import torch.optim as optimize
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as pyplot

# this project
from neurotic.tangles.data_paths import DataPathTwo
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org69a125d" class="outline-3"&gt;
&lt;h3 id="org69a125d"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org69a125d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.size": 8,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=3)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5d19d37" class="outline-2"&gt;
&lt;h2 id="org5d19d37"&gt;Flower power&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5d19d37"&gt;
&lt;p&gt;
Here we'll be using VGGNet to classify images of flowers. We'll start, as usual, by importing our usual resources. And checking if we can train our model on the GPU.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6d6ea06" class="outline-2"&gt;
&lt;h2 id="org6d6ea06"&gt;Download the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6d6ea06"&gt;
&lt;p&gt;
Download the flower data from &lt;a href="https://s3.amazonaws.com/video.udacity-data.com/topher/2018/September/5baa60a0_flower-photos/flower-photos.zip"&gt;this link&lt;/a&gt;, save it in the home directory of this notebook and extract the zip file to get the directory &lt;code&gt;flower_photos/&lt;/code&gt;. &lt;b&gt;&lt;b&gt;Make sure the directory has this exact name for accessing data: flower_photos&lt;/b&gt;&lt;/b&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
path = DataPathTwo(folder_key="FLOWERS")
print(path.folder)
for target in path.folder.iterdir():
    print(target)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/flower_photos
/home/hades/datasets/flower_photos/.DS_Store
/home/hades/datasets/flower_photos/train
/home/hades/datasets/flower_photos/test
/home/hades/datasets/flower_photos/LICENSE.txt
&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge4c9738" class="outline-3"&gt;
&lt;h3 id="orge4c9738"&gt;Check If CUDA Is Available&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge4c9738"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(device)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
cuda:0
&lt;/pre&gt;


&lt;p&gt;
CUDA is running out of memory and crashing so don't use CUDA.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;device = "cpu"
print(device)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
cpu
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org72a8c3b" class="outline-3"&gt;
&lt;h3 id="org72a8c3b"&gt;Load and Transform our Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org72a8c3b"&gt;
&lt;p&gt;
We'll be using PyTorch's &lt;a href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder"&gt;ImageFolder&lt;/a&gt; class which makes is very easy to load data from a directory. For example, the training images are all stored in a directory path that looks like this:
&lt;/p&gt;

&lt;pre class="example"&gt;
root/class_1/xxx.png
root/class_1/xxy.png
root/class_1/xxz.png

root/class_2/123.png
root/class_2/nsdf3.png
root/class_2/asd932_.png
&lt;/pre&gt;

&lt;p&gt;
Where, in this case, the root folder for training is &lt;code&gt;flower_photos/train/&lt;/code&gt; and the classes are the names of flower types.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcae3541" class="outline-3"&gt;
&lt;h3 id="orgcae3541"&gt;Define Training and Test Data Directories&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcae3541"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_dir = path.folder.joinpath('train/')
test_dir = path.folder.joinpath('test/')
print(train_dir)
print(test_dir)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/flower_photos/train
/home/hades/datasets/flower_photos/test
&lt;/pre&gt;


&lt;p&gt;
&lt;i&gt;Classes&lt;/i&gt; are folders in each directory with these names:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']
CLASS_COUNT = len(classes)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgffb5b78" class="outline-2"&gt;
&lt;h2 id="orgffb5b78"&gt;Transforming the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgffb5b78"&gt;
&lt;p&gt;
When we perform transfer learning, we have to shape our input data into the shape that the pre-trained model expects. VGG16 expects `224`-dim square images as input and so, we resize each flower image to fit this mold.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org703b471" class="outline-3"&gt;
&lt;h3 id="org703b471"&gt;Load And Transform Data Using ImageFolder&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org703b471"&gt;
&lt;p&gt;
VGG-16 Takes 224x224 images as input, so we resize all of them.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data_transform = transforms.Compose([transforms.RandomResizedCrop(224), 
				      transforms.ToTensor()])

train_data = datasets.ImageFolder(train_dir, transform=data_transform)
test_data = datasets.ImageFolder(test_dir, transform=data_transform)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf11f126" class="outline-3"&gt;
&lt;h3 id="orgf11f126"&gt;Print Out Some Data Stats&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf11f126"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print('Num training images: ', len(train_data))
print('Num test images: ', len(test_data))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Num training images:  3130
Num test images:  540
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;VALIDATION_FRACTION = 0.2
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;indices = list(range(len(train_data)))
training_indices, validation_indices = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc4ca6a6" class="outline-2"&gt;
&lt;h2 id="orgc4ca6a6"&gt;DataLoaders and Data Visualization&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc4ca6a6"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0209eac" class="outline-3"&gt;
&lt;h3 id="org0209eac"&gt;Define Dataloader Parameters&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0209eac"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;BATCH_SIZE = 20
NUM_WORKERS=4
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_sampler = SubsetRandomSampler(training_indices)
valid_sampler = SubsetRandomSampler(validation_indices)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org88bd07d" class="outline-3"&gt;
&lt;h3 id="org88bd07d"&gt;Prepare Data Loaders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org88bd07d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, 
					   sampler=train_sampler,
					   num_workers=NUM_WORKERS)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, 
					   sampler=valid_sampler, num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, 
					  num_workers=num_workers, shuffle=True)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdb00252" class="outline-2"&gt;
&lt;h2 id="orgdb00252"&gt;Visualize some sample data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdb00252"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd1ef521" class="outline-3"&gt;
&lt;h3 id="orgd1ef521"&gt;obtain one batch of training images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd1ef521"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy() # convert images to numpy for display
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org56462dc" class="outline-2"&gt;
&lt;h2 id="org56462dc"&gt;Plot The Images In The Batch, Along With The Corresponding Labels&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org56462dc"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fig = pyplot.figure(figsize=(12, 10))
pyplot.rc("axes", titlesize=10)
for idx in numpy.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    pyplot.imshow(numpy.transpose(images[idx], (1, 2, 0)))
    ax.set_title(classes[labels[idx]])
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/sample_batches.png" alt="sample_batches.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc403503" class="outline-2"&gt;
&lt;h2 id="orgc403503"&gt;Define the Model&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc403503"&gt;
&lt;p&gt;
To define a model for training we'll follow these steps:
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Load in a pre-trained VGG16 model&lt;/li&gt;
&lt;li&gt;"Freeze" all the parameters, so the net acts as a fixed feature extractor&lt;/li&gt;
&lt;li&gt;Remove the last layer&lt;/li&gt;
&lt;li&gt;Replace the last layer with a linear classifier of our own&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
/Freezing simply means that the parameters in the pre-trained model will &lt;b&gt;not&lt;/b&gt; change during training.**
&lt;/p&gt;

&lt;p&gt;
Load the pretrained model from pytorch
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vgg16 = models.vgg16(pretrained=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Print Out The Model Structure
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(vgg16)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace)
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace)
    (5): Dropout(p=0.5)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
&lt;/pre&gt;

&lt;p&gt;
Since we're only going to change the last (classification) layer, it might be helpful to see how many inputs and outpts it has.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(vgg16.classifier[6].in_features) 
print(vgg16.classifier[6].out_features) 
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
4096
1000
&lt;/pre&gt;


&lt;p&gt;
So, the original model output 1,000 classes - we're going to need to change that to our five classes (eventually).
&lt;/p&gt;

&lt;p&gt;
Freeze training for all "features" layers
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for param in vgg16.features.parameters():
    param.requires_grad = False
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org785129a" class="outline-2"&gt;
&lt;h2 id="org785129a"&gt;Final Classifier Layer&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org785129a"&gt;
&lt;p&gt;
Once you have the pre-trained feature extractor, you just need to modify and/or add to the final, fully-connected classifier layers. In this case, we suggest that you replace the last layer in the vgg classifier group of layers. 
&lt;/p&gt;

&lt;p&gt;
This layer should see as input the number of features produced by the portion of the network that you are not changing, and produce an appropriate number of outputs for the flower classification task.
&lt;/p&gt;

&lt;p&gt;
You can access any layer in a pretrained network by name and (sometimes) number, i.e. &lt;code&gt;vgg16.classifier[6]&lt;/code&gt; is the sixth layer in a group of layers named "classifier".
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;classifier = nn.Sequential(OrderedDict([
    ("Fullly Connected Classifier", nn.Linear(in_features=4096, out_features=CLASS_COUNT, bias=True)),
]))
vgg16.classifier[6] = classifier
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
after completing your model, if GPU is available, move the model to GPU
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vgg16.to(device)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org36b4203" class="outline-2"&gt;
&lt;h2 id="org36b4203"&gt;Specify &lt;a href="http://pytorch.org/docs/stable/nn.html#loss-functions"&gt;Loss Function&lt;/a&gt; and &lt;a href="http://pytorch.org/docs/stable/optim.html"&gt;Optimizer&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org36b4203"&gt;
&lt;p&gt;
Below we'll use cross-entropy loss and stochastic gradient descent with a small learning rate. Note that the optimizer accepts as input &lt;i&gt;only&lt;/i&gt; the trainable parameters &lt;code&gt;vgg.classifier.parameters()&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga97938c" class="outline-3"&gt;
&lt;h3 id="orga97938c"&gt;Specify Loss Function (Categorical Cross-Entropy)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga97938c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;criterion = nn.CrossEntropyLoss()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
specify optimizer (stochastic gradient descent) and learning rate = 0.001
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;optimizer = optimize.SGD(vgg16.classifier.parameters(), lr=0.001)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge6fea59" class="outline-2"&gt;
&lt;h2 id="orge6fea59"&gt;Training&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge6fea59"&gt;
&lt;p&gt;
Here, we'll train the network.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Exercise:&lt;/b&gt;&lt;/b&gt; So far we've been providing the training code for you. Here, I'm going to give you a bit more of a challenge and have you write the code to train the network. Of course, you'll be able to see my solution if you need help.
&lt;/p&gt;

&lt;p&gt;
number of epochs to train the model
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;n_epochs = EPOCHS = 2
def train(model: nn.Module, epochs: int=EPOCHS, model_number: int=0,
	  epoch_offset: int=1, print_every: int=10) -&amp;gt; tuple:
    """Train, validate, and save the model
    This trains the model and validates it, saving the best 
    (based on validation loss) as =model_&amp;lt;number&amp;gt;_cifar.pth=

    Args:
     model: the network to train
     epochs: number of times to repeat training
     model_number: an identifier for the saved hyperparameters file
     epoch_offset: amount of epochs that have occurred previously
     print_every: how often to print output
    Returns:
     filename, training-loss, validation-loss, improvements: the outcomes for the training
    """
    optimizer = optimize.SGD(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    output_file = "model_{}_vgg.pth".format(model_number)
    training_losses = []
    validation_losses = []
    improvements = []
    valid_loss_min = numpy.Inf # track change in validation loss
    epoch_start = epoch_offset
    last_epoch = epoch_start + epochs + 1
    for epoch in range(epoch_start, last_epoch):

	# keep track of training and validation loss
	train_loss = 0.0
	valid_loss = 0.0

	model.train()
	for data, target in train_loader:
	    # move tensors to GPU if CUDA is available            
	    data, target = data.to(device), target.to(device)
	    # clear the gradients of all optimized variables
	    optimizer.zero_grad()
	    # forward pass: compute predicted outputs by passing inputs to the model
	    output = model(data)
	    # calculate the batch loss
	    loss = criterion(output, target)
	    # backward pass: compute gradient of the loss with respect to model parameters
	    loss.backward()
	    # perform a single optimization step (parameter update)
	    optimizer.step()
	    # update training loss
	    train_loss += loss.item() * data.size(0)

	model.eval()
	for data, target in valid_loader:
	    # move tensors to GPU if CUDA is available
	    data, target = data.to(device), target.to(device)
	    # forward pass: compute predicted outputs by passing inputs to the model
	    output = model(data)
	    # calculate the batch loss
	    loss = criterion(output, target)
	    # update total validation loss 
	    valid_loss += loss.item() * data.size(0)

	# calculate average losses
	train_loss = train_loss/len(train_loader.dataset)
	valid_loss = valid_loss/len(valid_loader.dataset)

	# print training/validation statistics 
	if not (epoch % print_every):
	    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
		epoch, train_loss, valid_loss))
	training_losses.append(train_loss)
	validation_losses.append(valid_loss)
	# save model if validation loss has decreased
	if valid_loss &amp;lt;= valid_loss_min:
	    print('Validation loss decreased ({:.6f} --&amp;gt; {:.6f}).  Saving model ...'.format(
	    valid_loss_min,
	    valid_loss))
	    torch.save(model.state_dict(), output_file)
	    valid_loss_min = valid_loss
	    improvements.append(epoch - 1)
    return output_file, training_losses, validation_losses, improvements
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def test(best_model):
    criterion = nn.CrossEntropyLoss()
    # track test loss
    test_loss = 0.0
    class_correct = list(0. for i in range(10))
    class_total = list(0. for i in range(10))

    best_model.to(device)
    best_model.eval()
    # iterate over test data
    for data, target in test_loader:
	# move tensors to GPU if CUDA is available
	data, target = data.to(device), target.to(device)
	# forward pass: compute predicted outputs by passing inputs to the model
	output = best_model(data)
	# calculate the batch loss
	loss = criterion(output, target)
	# update test loss 
	test_loss += loss.item() * data.size(0)
	# convert output probabilities to predicted class
	_, pred = torch.max(output, 1)    
	# compare predictions to true label
	correct_tensor = pred.eq(target.data.view_as(pred))
	correct = (
	    numpy.squeeze(correct_tensor.numpy())
	    if not train_on_gpu
	    else numpy.squeeze(correct_tensor.cpu().numpy()))
	# calculate test accuracy for each object class
	for i in range(BATCH_SIZE):
	    label = target.data[i]
	    class_correct[label] += correct[i].item()
	    class_total[label] += 1

    # average test loss
    test_loss = test_loss/len(test_loader.dataset)
    print('Test Loss: {:.6f}\n'.format(test_loss))

    for i in range(10):
	if class_total[i] &amp;gt; 0:
	    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
		classes[i], 100 * class_correct[i] / class_total[i],
		numpy.sum(class_correct[i]), numpy.sum(class_total[i])))
	else:
	    print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

    print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
	100. * numpy.sum(class_correct) / numpy.sum(class_total),
	numpy.sum(class_correct), numpy.sum(class_total)))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;output_file, training_losses, validation_losses, improvements = train(vgg16, print_every=1)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_losses = []
validation_losses = []
improvements = []
valid_loss_min = numpy.Inf # track change in validation loss
for epoch in range(1, 3):

    # keep track of training and validation loss
    train_loss = 0.0
    valid_loss = 0.0

    vgg16.train()
    for data, target in train_loader:
	# move tensors to GPU if CUDA is available            
	data, target = data.to(device), target.to(device)
	# clear the gradients of all optimized variables
	optimizer.zero_grad()
	# forward pass: compute predicted outputs by passing inputs to the model
	output = vgg16(data)
	# calculate the batch loss
	loss = criterion(output, target)
	# backward pass: compute gradient of the loss with respect to model parameters
	loss.backward()
	# perform a single optimization step (parameter update)
	optimizer.step()
	# update training loss
	train_loss += loss.item() * data.size(0)

    vgg16.eval()
    for data, target in valid_loader:
	# move tensors to GPU if CUDA is available
	data, target = data.to(device), target.to(device)
	# forward pass: compute predicted outputs by passing inputs to the model
	output = vgg16(data)
	# calculate the batch loss
	loss = criterion(output, target)
	# update total validation loss 
	valid_loss += loss.item() * data.size(0)

    # calculate average losses
    train_loss = train_loss/len(train_loader.dataset)
    valid_loss = valid_loss/len(valid_loader.dataset)

    # print training/validation statistics 
    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
	epoch, train_loss, valid_loss))
    training_losses.append(train_loss)
    validation_losses.append(valid_loss)
    # save model if validation loss has decreased
    if valid_loss &amp;lt;= valid_loss_min:
	print('Validation loss decreased ({:.6f} --&amp;gt; {:.6f}).  Saving model ...'.format(
	valid_loss_min,
	valid_loss))
	torch.save(vgg16.state_dict(), output_file)
	valid_loss_min = valid_loss
	improvements.append(epoch - 1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;
test_loss = 0.0
class_correct = list(0. for i in range(5))
class_total = list(0. for i in range(5))
&lt;/p&gt;

&lt;p&gt;
vgg16.eval() # eval mode
&lt;/p&gt;

&lt;p&gt;
for data, target in test_loader:
&lt;/p&gt;

&lt;p&gt;
if train_on_gpu:
    data, target = data.cuda(), target.cuda()
&lt;/p&gt;

&lt;p&gt;
output = vgg16(data)
&lt;/p&gt;

&lt;p&gt;
loss = criterion(output, target)
&lt;/p&gt;

&lt;p&gt;
test_loss += loss.item()*data.size(0)
&lt;/p&gt;

&lt;p&gt;
_, pred = torch.max(output, 1)    
&lt;/p&gt;

&lt;p&gt;
correct_tensor = pred.eq(target.data.view_as(pred))
correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())
&lt;/p&gt;

&lt;p&gt;
for i in range(batch_size):
    label = target.data[i]
    class_correct[label] += correct[i].item()
    class_total[label] += 1
&lt;/p&gt;

&lt;p&gt;
test_loss = test_loss/len(test_loader.dataset)
print('Test Loss: {:.6f}\n'.format(test_loss))
&lt;/p&gt;

&lt;p&gt;
for i in range(5):
    if class_total[i] &amp;gt; 0:
        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
            classes[i], 100 * class_correct[i] / class_total[i],
            np.sum(class_correct[i]), np.sum(class_total[i])))
    else:
        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))
&lt;/p&gt;

&lt;p&gt;
print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;* np.sum(class_correct) / np.sum(class_total),&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
np.sum(class_correct), np.sum(class_total)))
&lt;/p&gt;


&lt;p&gt;
dataiter = iter(test_loader)
images, labels = dataiter.next()
images.numpy()
&lt;/p&gt;

&lt;p&gt;
if train_on_gpu:
    images = images.cuda()
&lt;/p&gt;

&lt;p&gt;
output = vgg16(images)
&lt;/p&gt;

&lt;p&gt;
_, preds_tensor = torch.max(output, 1)
preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())
&lt;/p&gt;

&lt;p&gt;
fig = plt.figure(figsize=(25, 4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    plt.imshow(np.transpose(images[idx], (1, 2, 0)))
    ax.set_title("{} ({})".format(classes[preds[idx]], classes[labels[idx]]),
                 color=("green" if preds[idx]==labels[idx].item() else "red"))
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>exercise</category><category>transfer learning</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/transfer-learning-exercise/</guid><pubDate>Sat, 15 Dec 2018 22:50:47 GMT</pubDate></item><item><title>CIFAR-10</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#orgd93b7e1"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org7dd55c8"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#orgc611681"&gt;Visualize a Batch of Training Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#orgfdada31"&gt;Define the Network Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org52e3c70"&gt;Define a model with multiple convolutional layers, and define the feedforward network behavior.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org4eab06d"&gt;Output volume for a convolutional layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org0b89487"&gt;Specify Loss Function and Optimizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org913b50a"&gt;Train the Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org018e3eb"&gt;Load the Model with the Lowest Validation Loss&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org0753a1e"&gt;Test the Trained Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#orgbf3c4cc"&gt;Make it Easier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org2829f7d"&gt;Take two&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/#org8efe260"&gt;Change the Training and Validation Sets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd93b7e1" class="outline-2"&gt;
&lt;h2 id="orgd93b7e1"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd93b7e1"&gt;
&lt;p&gt;
This is from &lt;a href="https://github.com/udacity/deep-learning-v2-pytorch.git"&gt;Udacity's Deep Learning Repository&lt;/a&gt; which supports their Deep Learning Nanodegree. This will use a &lt;b&gt;Convolutional Neural Network (CNN)&lt;/b&gt; to classify images from the &lt;a href="https://en.wikipedia.org/wiki/CIFAR-10"&gt;CIFAR-10&lt;/a&gt; data set.
&lt;/p&gt;

&lt;p&gt;
The images in this data set are small color images that fall into one of ten classes:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;airplane&lt;/li&gt;
&lt;li&gt;automobile&lt;/li&gt;
&lt;li&gt;bird&lt;/li&gt;
&lt;li&gt;cat&lt;/li&gt;
&lt;li&gt;deer&lt;/li&gt;
&lt;li&gt;dog&lt;/li&gt;
&lt;li&gt;frog&lt;/li&gt;
&lt;li&gt;horse&lt;/li&gt;
&lt;li&gt;ship&lt;/li&gt;
&lt;li&gt;truck&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
There is another description of it on the &lt;a href="https://www.cs.toronto.edu/~kriz/cifar.html"&gt;University of Toronto's&lt;/a&gt; page for it.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7dd55c8" class="outline-2"&gt;
&lt;h2 id="org7dd55c8"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7dd55c8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb8fddb9" class="outline-3"&gt;
&lt;h3 id="orgb8fddb9"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb8fddb9"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6611010" class="outline-4"&gt;
&lt;h4 id="org6611010"&gt;From Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6611010"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from datetime import datetime
from pathlib import Path
from typing import Tuple
import os
import pickle
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcb6a15f" class="outline-4"&gt;
&lt;h4 id="orgcb6a15f"&gt;From PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgcb6a15f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from sklearn.model_selection import train_test_split
from torchvision import datasets
from torch.utils.data.sampler import SubsetRandomSampler
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optimize
import torchvision.transforms as transforms
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge4d6f69" class="outline-4"&gt;
&lt;h4 id="orge4d6f69"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge4d6f69"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from neurotic.tangles.data_paths import DataPathTwo
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org733720d" class="outline-3"&gt;
&lt;h3 id="org733720d"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org733720d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=3)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org29deeb8" class="outline-3"&gt;
&lt;h3 id="org29deeb8"&gt;Test for &lt;a href="http://pytorch.org/docs/stable/cuda.html"&gt;CUDA&lt;/a&gt;&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org29deeb8"&gt;
&lt;p&gt;
The test-code uses the check later on so I'll save it to the &lt;code&gt;train_on_gpu&lt;/code&gt; variable.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if os.environ.get("USER") == "brunhilde":
    train_on_gpu = False
    device = torch.device("cpu")
else:
    train_on_gpu = torch.cuda.is_available()
    device = torch.device("cuda:0" if train_on_gpu else "cpu")
print("Using: {}".format(device))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Using: cuda:0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc7c5c8b" class="outline-3"&gt;
&lt;h3 id="orgc7c5c8b"&gt;Load the &lt;a href="http://pytorch.org/docs/stable/torchvision/datasets.html"&gt;Data&lt;/a&gt;&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc7c5c8b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# subprocesses to use
NUM_WORKERS = 0
# how many samples per batch to load
BATCH_SIZE = 20
# percentage of training set to use as validation
VALIDATION_FRACTION = 0.2

IMAGE_SIZE = 32
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Convert the data to a normalized &lt;code&gt;torch.FloatTensor&lt;/code&gt; using a pipeline. I'm also going to introduce some randomness to help the model generalize.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;means = deviations = (0.5, 0.5, 0.5)
train_transform = transforms.Compose([
    transforms.RandomRotation(30),
    transforms.RandomResizedCrop(IMAGE_SIZE),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(means, deviations)
    ])
test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(means,
			 deviations)])
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Choose the training and test datasets.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
path = DataPathTwo(folder_key="CIFAR")
print(path.folder)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/CIFAR
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_data = datasets.CIFAR10(path.folder, train=True,
			      download=True, transform=train_transform)
test_data = datasets.CIFAR10(path.folder, train=False,
			     download=True, transform=test_transforms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Files already downloaded and verified
Files already downloaded and verified
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for item in path.folder.iterdir():
    print(item)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/CIFAR/cifar-10-batches-py
/home/hades/datasets/CIFAR/cifar-10-python.tar.gz
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge4060a3" class="outline-3"&gt;
&lt;h3 id="orge4060a3"&gt;Obtain Training Indices For Validation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge4060a3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;indices = list(range(len(training_data)))
training_indices, validation_indices = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf1c6439" class="outline-3"&gt;
&lt;h3 id="orgf1c6439"&gt;Define Samplers For Training And Validation Batches&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf1c6439"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_sampler = SubsetRandomSampler(training_indices)
valid_sampler = SubsetRandomSampler(validation_indices)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org56f65a0" class="outline-3"&gt;
&lt;h3 id="org56f65a0"&gt;Prepare Data Loaders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org56f65a0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE,
    sampler=train_sampler, num_workers=NUM_WORKERS)
valid_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE, 
    sampler=valid_sampler, num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, 
    num_workers=NUM_WORKERS)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6a027ce" class="outline-3"&gt;
&lt;h3 id="org6a027ce"&gt;The Image Classes&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6a027ce"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',
	   'dog', 'frog', 'horse', 'ship', 'truck']
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc611681" class="outline-2"&gt;
&lt;h2 id="orgc611681"&gt;Visualize a Batch of Training Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc611681"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf2e415c" class="outline-3"&gt;
&lt;h3 id="orgf2e415c"&gt;helper function to un-normalize and display an image&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf2e415c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def imshow(img):
    img = img / 2 + 0.5  # unnormalize
    pyplot.imshow(numpy.transpose(img, (1, 2, 0)))  # convert from Tensor image
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org768ee5d" class="outline-3"&gt;
&lt;h3 id="org768ee5d"&gt;obtain one batch of training images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org768ee5d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy() # convert images to numpy for display
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org45642b0" class="outline-3"&gt;
&lt;h3 id="org45642b0"&gt;plot the images in the batch, along with the corresponding labels&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org45642b0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure = pyplot.figure(figsize=(25, 4))
# display 20 images
figure.suptitle("Batch Sample", weight="bold")
for idx in numpy.arange(20):
    ax = figure.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    imshow(images[idx])
    ax.set_title(classes[labels[idx]])
#pyplot.subplots_adjust(top=0.7)
pyplot.tight_layout(rect=[0, 0.03, 1, 0.95])
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/batch.png" alt="batch.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-org250e93a" class="outline-3"&gt;
&lt;h3 id="org250e93a"&gt;View an Image in More Detail&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org250e93a"&gt;
&lt;p&gt;
Here, we look at the normalized red, green, and blue (RGB) color channels as three separate, grayscale intensity images.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rgb_img = numpy.squeeze(images[3])
channels = ['red channel', 'green channel', 'blue channel']

fig = pyplot.figure(figsize = (36, 36)) 
for idx in numpy.arange(rgb_img.shape[0]):
    ax = fig.add_subplot(1, 3, idx + 1)
    img = rgb_img[idx]
    ax.imshow(img, cmap='gray')
    ax.set_title(channels[idx])
    width, height = img.shape
    thresh = img.max()/2.5
    for x in range(width):
	for y in range(height):
	    val = round(img[x][y],2) if img[x][y] !=0 else 0
	    ax.annotate(str(val), xy=(y,x),
		    horizontalalignment='center',
		    verticalalignment='center', size=8,
		    color='white' if img[x][y]&amp;lt;thresh else 'black')
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/rgb.png" alt="rgb.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfdada31" class="outline-2"&gt;
&lt;h2 id="orgfdada31"&gt;Define the Network &lt;a href="http://pytorch.org/docs/stable/nn.html"&gt;Architecture&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfdada31"&gt;
&lt;p&gt;
This time, you'll define a CNN architecture. Instead of an MLP, which used linear, fully-connected layers, you'll use the following:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#conv2d"&gt;Convolutional layers&lt;/a&gt;, which can be thought of as stack of filtered images.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#maxpool2d"&gt;Maxpooling layers&lt;/a&gt;, which reduce the x-y size of an input, keeping only the most &lt;i&gt;active&lt;/i&gt; pixels from the previous layer.&lt;/li&gt;
&lt;li&gt;The usual Linear + Dropout layers to avoid overfitting and produce a 10-dim output.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org52e3c70" class="outline-2"&gt;
&lt;h2 id="org52e3c70"&gt;Define a model with multiple convolutional layers, and define the feedforward network behavior.&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org52e3c70"&gt;
&lt;p&gt;
The more convolutional layers you include, the more complex patterns in color and shape a model can detect. It's suggested that your final model include 2 or 3 convolutional layers as well as linear layers + dropout in between to avoid overfitting. 
&lt;/p&gt;

&lt;p&gt;
It's good practice to look at existing research and implementations of related models as a starting point for defining your own models. You may find it useful to look at &lt;a href="https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py"&gt;this PyTorch classification example&lt;/a&gt; or &lt;a href="https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py"&gt;this, more complex Keras example&lt;/a&gt; to help decide on a final structure.
&lt;/p&gt;

&lt;p&gt;
This is taken from the &lt;a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-an-image-classifier"&gt;pytorch tutorial&lt;/a&gt;, with padding and dropout added. I also changed the kernel size to 3.
&lt;/p&gt;

&lt;p&gt;
See:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d"&gt;Conv2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html?highlight=nn%20maxpool#torch.nn.MaxPool2d"&gt;MaxPool2d&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#linear-layers"&gt;Linear&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/nn.html#dropout-layers"&gt;Dropout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view"&gt;view&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;KERNEL_SIZE = 3
CHANNELS_IN = 3
CHANNELS_OUT_1 = 6
CHANNELS_OUT_2 = 16
CLASSES = 10
PADDING = 1
STRIDE = 1
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;convolutional_1 = nn.Conv2d(CHANNELS_IN, CHANNELS_OUT_1,
			    KERNEL_SIZE, 
			    stride=STRIDE, padding=PADDING)
pool = nn.MaxPool2d(2, 2)
convolutional_2 = nn.Conv2d(CHANNELS_OUT_1, CHANNELS_OUT_2,
			    KERNEL_SIZE,
			    stride=STRIDE, padding=PADDING)

c_no_padding_1 = nn.Conv2d(CHANNELS_IN, CHANNELS_OUT_1, KERNEL_SIZE)
c_no_padding_2 = nn.Conv2d(CHANNELS_OUT_1, CHANNELS_OUT_2, KERNEL_SIZE)
fully_connected_1 = nn.Linear(CHANNELS_OUT_2 * (KERNEL_SIZE + PADDING)**3, 120)
fully_connected_1A = nn.Linear(CHANNELS_OUT_2 * (KERNEL_SIZE)**2, 120)
fully_connected_2 = nn.Linear(120, 84)
fully_connected_3 = nn.Linear(84, CLASSES)
cnn_dropout = nn.Dropout(0.25)
connected_dropout = nn.Dropout(0.5)

dataiter = iter(train_loader)
images, labels = dataiter.next()
input_image = torch.Tensor(images)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("Input Shape: {}".format(input_image.shape))
x = cnn_dropout(pool(F.relu(convolutional_1(input_image))))
print("Output 1: {}".format(x.shape))
x = cnn_dropout(pool(F.relu(convolutional_2(x))))
print("Output 2: {}".format(x.shape))
x = x.view(x.size()[0], -1)
print("reshaped: {}".format(x.shape))
x = connected_dropout(F.relu(fully_connected_1(x)))
print("Connected Shape: {}".format(x.shape))
x = F.relu(fully_connected_2(x))
print("Connected Shape 2: {}".format(x.shape))
x = fully_connected_3(x)
print("Connected Shape 3: {}".format(x.shape))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Input Shape: torch.Size([20, 3, 32, 32])
Output 1: torch.Size([20, 6, 16, 16])
Output 2: torch.Size([20, 16, 8, 8])
reshaped: torch.Size([20, 1024])
Connected Shape: torch.Size([20, 120])
Connected Shape 2: torch.Size([20, 84])
Connected Shape 3: torch.Size([20, 10])
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("Input Shape: {}".format(input_image.shape))
x = cnn_dropout(pool(F.relu(c_no_padding_1(input_image))))
print("Output 1: {}".format(x.shape))
x = cnn_dropout(pool(F.relu(c_no_padding_2(x))))
print("Output 2: {}".format(x.shape))
x = x.view(-1, CHANNELS_OUT_2 * (KERNEL_SIZE)**2)
print("reshaped: {}".format(x.shape))
x = connected_dropout(F.relu(fully_connected_1A(x)))
print("Connected Shape: {}".format(x.shape))
x = F.relu(fully_connected_2(x))
print("Connected Shape 2: {}".format(x.shape))
x = fully_connected_3(x)
print("Connected Shape 3: {}".format(x.shape))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Input Shape: torch.Size([20, 3, 32, 32])
Output 1: torch.Size([20, 6, 15, 15])
Output 2: torch.Size([20, 16, 6, 6])
reshaped: torch.Size([80, 144])
Connected Shape: torch.Size([80, 120])
Connected Shape 2: torch.Size([80, 84])
Connected Shape 3: torch.Size([80, 10])
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class CNN(nn.Module):
    """A convolutional neural network for CIFAR-10 images"""
    def __init__(self, filter_size=5) -&amp;gt; None:
	super().__init__()
	self.convolutional_1 = nn.Conv2d(CHANNELS_IN, CHANNELS_OUT_1,
					 KERNEL_SIZE, 
					 stride=STRIDE, padding=PADDING)
	self.pool = nn.MaxPool2d(2, 2)
	self.convolutional_2 = nn.Conv2d(CHANNELS_OUT_1, CHANNELS_OUT_2,
					 KERNEL_SIZE,
					 stride=STRIDE, padding=PADDING)
	self.fully_connected_1 = nn.Linear(CHANNELS_OUT_2 * (KERNEL_SIZE + PADDING)**3, 120)
	self.fully_connected_2 = nn.Linear(120, 84)
	self.fully_connected_3 = nn.Linear(84, CLASSES)
	self.cnn_dropout = nn.Dropout(0.25)
	self.connected_dropout = nn.Dropout(0.5)
	return

    def forward(self, x: torch.Tensor) -&amp;gt; torch.Tensor:
	"""Passes the image through the layers of the network

	Args:
	 image: CIFAR image to process
	"""
	x = self.cnn_dropout(self.pool(F.relu(self.convolutional_1(x))))
	x = self.cnn_dropout(self.pool(F.relu(self.convolutional_2(x))))
	# flatten to a vector
	x = x.view(x.size()[0], -1)
	x = self.connected_dropout(F.relu(self.fully_connected_1(x)))
	x = F.relu(self.fully_connected_2(x))
	return self.fully_connected_3(x)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = CNN()
dataiter = iter(train_loader)
images, labels = dataiter.next()
print(images.shape)
print(labels.shape)
output = model(images)
print(output.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
torch.Size([20, 3, 32, 32])
torch.Size([20])
torch.Size([20, 10])
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4eab06d" class="outline-2"&gt;
&lt;h2 id="org4eab06d"&gt;Output volume for a convolutional layer&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4eab06d"&gt;
&lt;p&gt;
To compute the output size of a given convolutional layer we can perform the following calculation (taken from &lt;a href="http://cs231n.github.io/convolutional-networks/#layers"&gt;Stanford's cs231n course&lt;/a&gt;):
&lt;/p&gt;

&lt;p&gt;
We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by &lt;code&gt;(WâF+2P)/S+1&lt;/code&gt;. 
&lt;/p&gt;

&lt;p&gt;
For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0b89487" class="outline-2"&gt;
&lt;h2 id="org0b89487"&gt;Specify &lt;a href="http://pytorch.org/docs/stable/nn.html#loss-functions"&gt;Loss Function&lt;/a&gt; and &lt;a href="http://pytorch.org/docs/stable/optim.html"&gt;Optimizer&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0b89487"&gt;
&lt;p&gt;
Decide on a loss and optimization function that is best suited for this classification task. The linked code examples from above, may be a good starting point; &lt;a href="https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py"&gt;this PyTorch classification example&lt;/a&gt; or &lt;a href="https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py"&gt;this, more complex Keras example&lt;/a&gt;. Pay close attention to the value for &lt;b&gt;&lt;b&gt;learning rate&lt;/b&gt;&lt;/b&gt; as this value determines how your model converges to a small error.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;criterion = nn.CrossEntropyLoss()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org913b50a" class="outline-2"&gt;
&lt;h2 id="org913b50a"&gt;Train the Network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org913b50a"&gt;
&lt;p&gt;
Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def train(model: nn.Module, epochs: int=10, model_number: int=0, 
	  epoch_offset: int=1, print_every: int=10) -&amp;gt; tuple:
    """Train, validate, and save the model
    This trains the model and validates it, saving the best 
    (based on validation loss) as =model_&amp;lt;number&amp;gt;_cifar.pth=

    Args:
     model: the network to train
     epochs: number of times to repeat training
     model_number: an identifier for the saved hyperparameters file
     epoch_offset: amount of epochs that have occurred previously
     print_every: how often to print output
    Returns:
     filename, training-loss, validation-loss, improvements: the outcomes for the training
    """
    optimizer = optimize.SGD(model.parameters(), lr=0.001, momentum=0.9)
    criterion = nn.CrossEntropyLoss()
    output_file = "model_{}_cifar.pth".format(model_number)
    training_losses = []
    validation_losses = []
    improvements = []
    valid_loss_min = numpy.Inf # track change in validation loss
    epoch_start = epoch_offset
    last_epoch = epoch_start + epochs + 1
    for epoch in range(epoch_start, last_epoch):

	# keep track of training and validation loss
	train_loss = 0.0
	valid_loss = 0.0

	model.train()
	for data, target in train_loader:
	    # move tensors to GPU if CUDA is available            
	    data, target = data.to(device), target.to(device)
	    # clear the gradients of all optimized variables
	    optimizer.zero_grad()
	    # forward pass: compute predicted outputs by passing inputs to the model
	    output = model(data)
	    # calculate the batch loss
	    loss = criterion(output, target)
	    # backward pass: compute gradient of the loss with respect to model parameters
	    loss.backward()
	    # perform a single optimization step (parameter update)
	    optimizer.step()
	    # update training loss
	    train_loss += loss.item() * data.size(0)

	model.eval()
	for data, target in valid_loader:
	    # move tensors to GPU if CUDA is available
	    data, target = data.to(device), target.to(device)
	    # forward pass: compute predicted outputs by passing inputs to the model
	    output = model(data)
	    # calculate the batch loss
	    loss = criterion(output, target)
	    # update total validation loss 
	    valid_loss += loss.item() * data.size(0)

	# calculate average losses
	train_loss = train_loss/len(train_loader.dataset)
	valid_loss = valid_loss/len(valid_loader.dataset)

	# print training/validation statistics 
	if not (epoch % print_every):
	    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
		epoch, train_loss, valid_loss))
	training_losses.append(train_loss)
	validation_losses.append(valid_loss)
	# save model if validation loss has decreased
	if valid_loss &amp;lt;= valid_loss_min:
	    print('Validation loss decreased ({:.6f} --&amp;gt; {:.6f}).  Saving model ...'.format(
	    valid_loss_min,
	    valid_loss))
	    torch.save(model.state_dict(), output_file)
	    valid_loss_min = valid_loss
	    improvements.append(epoch - 1)
    return output_file, training_losses, validation_losses, improvements
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org54b9ec8" class="outline-3"&gt;
&lt;h3 id="org54b9ec8"&gt;Pytorch Tutorial Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org54b9ec8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;EPOCHS = 250
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This is only to avoid re-running the initial training and use the saved model. &lt;b&gt;Note:&lt;/b&gt; If you use DataParallel you need to save the model using &lt;code&gt;model.module.state_dict()&lt;/code&gt; in order to load it later without it. This won't matter if you always use it or never use it, but here I have a model that was trained on a GPU and I'm trying to extend the training with a computer whos GPU is too old for pytorch to use it, so it crashes unless I disable the DataParallel (because I didn't originally save it with &lt;code&gt;model.module.state_dict&lt;/code&gt;). 
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Note 2&lt;/b&gt;: But if you don't have it in DataParallel then don't use &lt;code&gt;model.module.state_dict&lt;/code&gt; because it won't have the &lt;code&gt;module&lt;/code&gt; attribute. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def train_and_pickle(model:nn.Module, epochs:int=EPOCHS,
		     model_number:int=2, print_every: int=10) -&amp;gt; dict:
    """Trains and pickles the outcomes of training"""
    path = Path("model_{}_outcomes.pkl".format(model_number))
    existed = False
    epoch_offset = 0
    if path.is_file():
	existed = True
	with path.open("rb") as reader:
	    outcomes = pickle.load(reader)
	    epoch_offset = len(outcomes["training_loss"])
	    model.load_state_dict(torch.load(
		outcome["hyperparameters_file"],
		map_location=device))
    filename, training_loss, validation_loss, improvements  = train(
	model,
	epochs=epochs,
	model_number=model_number,
	epoch_offset=epoch_offset,
	print_every=print_every,
	)

    if existed:
	outcomes["training_loss"] += outcomes["training_loss"]
	outcomes["validation_loss"] += outcomes["validation_loss"]
	outcomes["improvements"] += outcomes["improvements"]
    else:
	outcomes = dict(
	    hyperparameters_file=filename,
	    outcomes_pickle=path.name,
	    training_loss=training_loss,
	    validation_loss=validation_loss,
	    improvements=improvements,
	)
    with path.open("wb") as writer:
	pickle.dump(outcomes, writer)
    return outcomes
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def update_outcome(outcome: dict, new_outcome: dict) -&amp;gt; dict:
    """Updates the lists in the outcome

    Args:
     outcome: original output of train_and_pickle
     new_outcome: new output of train_and_pickle

    Returns:
     outcome: updated outcome
    """
    for key in ("training_loss", "validation_loss", "improvements"):
	outcome[key] += new_outcome[key]
    return outcome
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8c15697" class="outline-3"&gt;
&lt;h3 id="org8c15697"&gt;First Model Training&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8c15697"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_2 = CNN()
model_2.to(device)
start = datetime.now()
outcome = train_and_pickle(
    model_2,
    epochs=100,
    model_number=2)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 0 	Training Loss: 1.834230 	Validation Loss: 0.446434
Validation loss decreased (inf --&amp;gt; 0.446434).  Saving model ...
Epoch: 1 	Training Loss: 1.685185 	Validation Loss: 0.403314
Validation loss decreased (0.446434 --&amp;gt; 0.403314).  Saving model ...
Epoch: 2 	Training Loss: 1.602409 	Validation Loss: 0.389758
Validation loss decreased (0.403314 --&amp;gt; 0.389758).  Saving model ...
Epoch: 3 	Training Loss: 1.551087 	Validation Loss: 0.376669
Validation loss decreased (0.389758 --&amp;gt; 0.376669).  Saving model ...
Epoch: 4 	Training Loss: 1.524230 	Validation Loss: 0.371581
Validation loss decreased (0.376669 --&amp;gt; 0.371581).  Saving model ...
Epoch: 5 	Training Loss: 1.496748 	Validation Loss: 0.367056
Validation loss decreased (0.371581 --&amp;gt; 0.367056).  Saving model ...
Epoch: 6 	Training Loss: 1.479645 	Validation Loss: 0.359889
Validation loss decreased (0.367056 --&amp;gt; 0.359889).  Saving model ...
Epoch: 7 	Training Loss: 1.462357 	Validation Loss: 0.358887
Validation loss decreased (0.359889 --&amp;gt; 0.358887).  Saving model ...
Epoch: 8 	Training Loss: 1.454448 	Validation Loss: 0.353885
Validation loss decreased (0.358887 --&amp;gt; 0.353885).  Saving model ...
Epoch: 9 	Training Loss: 1.442392 	Validation Loss: 0.349046
Validation loss decreased (0.353885 --&amp;gt; 0.349046).  Saving model ...
Epoch: 10 	Training Loss: 1.435758 	Validation Loss: 0.345204
Validation loss decreased (0.349046 --&amp;gt; 0.345204).  Saving model ...
Epoch: 11 	Training Loss: 1.428880 	Validation Loss: 0.344610
Validation loss decreased (0.345204 --&amp;gt; 0.344610).  Saving model ...
Epoch: 12 	Training Loss: 1.420400 	Validation Loss: 0.343866
Validation loss decreased (0.344610 --&amp;gt; 0.343866).  Saving model ...
Epoch: 13 	Training Loss: 1.409974 	Validation Loss: 0.341221
Validation loss decreased (0.343866 --&amp;gt; 0.341221).  Saving model ...
Epoch: 14 	Training Loss: 1.400003 	Validation Loss: 0.340469
Validation loss decreased (0.341221 --&amp;gt; 0.340469).  Saving model ...
Epoch: 15 	Training Loss: 1.396430 	Validation Loss: 0.338332
Validation loss decreased (0.340469 --&amp;gt; 0.338332).  Saving model ...
Epoch: 16 	Training Loss: 1.396793 	Validation Loss: 0.338963
Epoch: 17 	Training Loss: 1.391945 	Validation Loss: 0.337340
Validation loss decreased (0.338332 --&amp;gt; 0.337340).  Saving model ...
Epoch: 18 	Training Loss: 1.383872 	Validation Loss: 0.335848
Validation loss decreased (0.337340 --&amp;gt; 0.335848).  Saving model ...
Epoch: 19 	Training Loss: 1.371348 	Validation Loss: 0.335116
Validation loss decreased (0.335848 --&amp;gt; 0.335116).  Saving model ...
Epoch: 20 	Training Loss: 1.374097 	Validation Loss: 0.330697
Validation loss decreased (0.335116 --&amp;gt; 0.330697).  Saving model ...
Epoch: 21 	Training Loss: 1.373342 	Validation Loss: 0.334281
Epoch: 22 	Training Loss: 1.366379 	Validation Loss: 0.331197
Epoch: 23 	Training Loss: 1.366043 	Validation Loss: 0.332052
Epoch: 24 	Training Loss: 1.359814 	Validation Loss: 0.328743
Validation loss decreased (0.330697 --&amp;gt; 0.328743).  Saving model ...
Epoch: 25 	Training Loss: 1.359745 	Validation Loss: 0.328860
Epoch: 26 	Training Loss: 1.353130 	Validation Loss: 0.329480
Epoch: 27 	Training Loss: 1.352457 	Validation Loss: 0.329386
Epoch: 28 	Training Loss: 1.348608 	Validation Loss: 0.331024
Epoch: 29 	Training Loss: 1.346584 	Validation Loss: 0.325815
Validation loss decreased (0.328743 --&amp;gt; 0.325815).  Saving model ...
Epoch: 30 	Training Loss: 1.341498 	Validation Loss: 0.332342
Epoch: 31 	Training Loss: 1.339088 	Validation Loss: 0.325358
Validation loss decreased (0.325815 --&amp;gt; 0.325358).  Saving model ...
Epoch: 32 	Training Loss: 1.347376 	Validation Loss: 0.326178
Epoch: 33 	Training Loss: 1.342424 	Validation Loss: 0.331979
Epoch: 34 	Training Loss: 1.339343 	Validation Loss: 0.324638
Validation loss decreased (0.325358 --&amp;gt; 0.324638).  Saving model ...
Epoch: 35 	Training Loss: 1.332784 	Validation Loss: 0.322740
Validation loss decreased (0.324638 --&amp;gt; 0.322740).  Saving model ...
Epoch: 36 	Training Loss: 1.335403 	Validation Loss: 0.324083
Epoch: 37 	Training Loss: 1.332313 	Validation Loss: 0.334746
Epoch: 38 	Training Loss: 1.329136 	Validation Loss: 0.324193
Epoch: 39 	Training Loss: 1.327429 	Validation Loss: 0.327056
Epoch: 40 	Training Loss: 1.328106 	Validation Loss: 0.327257
Epoch: 41 	Training Loss: 1.330462 	Validation Loss: 0.321711
Validation loss decreased (0.322740 --&amp;gt; 0.321711).  Saving model ...
Epoch: 42 	Training Loss: 1.326317 	Validation Loss: 0.324698
Epoch: 43 	Training Loss: 1.325379 	Validation Loss: 0.324895
Epoch: 44 	Training Loss: 1.322629 	Validation Loss: 0.322434
Epoch: 45 	Training Loss: 1.320261 	Validation Loss: 0.326130
Epoch: 46 	Training Loss: 1.316204 	Validation Loss: 0.325013
Epoch: 47 	Training Loss: 1.315747 	Validation Loss: 0.324042
Epoch: 48 	Training Loss: 1.313305 	Validation Loss: 0.324592
Epoch: 49 	Training Loss: 1.313723 	Validation Loss: 0.318290
Validation loss decreased (0.321711 --&amp;gt; 0.318290).  Saving model ...
Epoch: 50 	Training Loss: 1.313054 	Validation Loss: 0.320845
Epoch: 51 	Training Loss: 1.316062 	Validation Loss: 0.321215
Epoch: 52 	Training Loss: 1.316187 	Validation Loss: 0.319871
Epoch: 53 	Training Loss: 1.312232 	Validation Loss: 0.324769
Epoch: 54 	Training Loss: 1.315246 	Validation Loss: 0.321788
Epoch: 55 	Training Loss: 1.307923 	Validation Loss: 0.318943
Epoch: 56 	Training Loss: 1.316049 	Validation Loss: 0.324919
Epoch: 57 	Training Loss: 1.310584 	Validation Loss: 0.319344
Epoch: 58 	Training Loss: 1.305451 	Validation Loss: 0.320848
Epoch: 59 	Training Loss: 1.309900 	Validation Loss: 0.322148
Epoch: 60 	Training Loss: 1.306200 	Validation Loss: 0.323148
Epoch: 61 	Training Loss: 1.303626 	Validation Loss: 0.322406
Epoch: 62 	Training Loss: 1.304654 	Validation Loss: 0.322471
Epoch: 63 	Training Loss: 1.302740 	Validation Loss: 0.322596
Epoch: 64 	Training Loss: 1.306964 	Validation Loss: 0.323696
Epoch: 65 	Training Loss: 1.301964 	Validation Loss: 0.319375
Epoch: 66 	Training Loss: 1.302925 	Validation Loss: 0.320327
Epoch: 67 	Training Loss: 1.302062 	Validation Loss: 0.319882
Epoch: 68 	Training Loss: 1.299821 	Validation Loss: 0.318813
Epoch: 69 	Training Loss: 1.298885 	Validation Loss: 0.325837
Epoch: 70 	Training Loss: 1.303130 	Validation Loss: 0.320493
Epoch: 71 	Training Loss: 1.301353 	Validation Loss: 0.321375
Epoch: 72 	Training Loss: 1.294933 	Validation Loss: 0.315513
Validation loss decreased (0.318290 --&amp;gt; 0.315513).  Saving model ...
Epoch: 73 	Training Loss: 1.303322 	Validation Loss: 0.322531
Epoch: 74 	Training Loss: 1.298327 	Validation Loss: 0.323503
Epoch: 75 	Training Loss: 1.298817 	Validation Loss: 0.318616
Epoch: 76 	Training Loss: 1.296895 	Validation Loss: 0.323739
Epoch: 77 	Training Loss: 1.301932 	Validation Loss: 0.325410
Epoch: 78 	Training Loss: 1.291901 	Validation Loss: 0.327083
Epoch: 79 	Training Loss: 1.295766 	Validation Loss: 0.317765
Epoch: 80 	Training Loss: 1.295147 	Validation Loss: 0.316187
Epoch: 81 	Training Loss: 1.294392 	Validation Loss: 0.318913
Epoch: 82 	Training Loss: 1.290720 	Validation Loss: 0.320984
Epoch: 83 	Training Loss: 1.296386 	Validation Loss: 0.322005
Epoch: 84 	Training Loss: 1.294445 	Validation Loss: 0.319135
Epoch: 85 	Training Loss: 1.288677 	Validation Loss: 0.317673
Epoch: 86 	Training Loss: 1.292154 	Validation Loss: 0.318644
Epoch: 87 	Training Loss: 1.292221 	Validation Loss: 0.317595
Epoch: 88 	Training Loss: 1.295039 	Validation Loss: 0.319856
Epoch: 89 	Training Loss: 1.289999 	Validation Loss: 0.320703
Epoch: 90 	Training Loss: 1.290199 	Validation Loss: 0.317269
Epoch: 91 	Training Loss: 1.289213 	Validation Loss: 0.318887
Epoch: 92 	Training Loss: 1.284553 	Validation Loss: 0.320420
Epoch: 93 	Training Loss: 1.292121 	Validation Loss: 0.319414
Epoch: 94 	Training Loss: 1.281610 	Validation Loss: 0.314129
Validation loss decreased (0.315513 --&amp;gt; 0.314129).  Saving model ...
Epoch: 95 	Training Loss: 1.292147 	Validation Loss: 0.317541
Epoch: 96 	Training Loss: 1.288869 	Validation Loss: 0.316178
Epoch: 97 	Training Loss: 1.284419 	Validation Loss: 0.326122
Epoch: 98 	Training Loss: 1.292448 	Validation Loss: 0.314851
Epoch: 99 	Training Loss: 1.287391 	Validation Loss: 0.315212
Epoch: 100 	Training Loss: 1.285748 	Validation Loss: 0.320298
Elapsed: 1:26:31.644031
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pickle_path = Path("model_2_outcomes.pkl")
with pickle_path.open("rb") as reader:
    outcome = pickle.load(reader)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_2 = CNN()
model_2.to(device)
start = datetime.now()
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
				   map_location=device))
outcome_2 = train_and_pickle(model_2, epochs=200, model_number=2)
outcome = update_outcome(outcome, outcome_2)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 101 	Training Loss: 1.293572 	Validation Loss: 0.323292
Validation loss decreased (inf --&amp;gt; 0.323292).  Saving model ...
Epoch: 102 	Training Loss: 1.286175 	Validation Loss: 0.316041
Validation loss decreased (0.323292 --&amp;gt; 0.316041).  Saving model ...
Epoch: 103 	Training Loss: 1.292286 	Validation Loss: 0.318805
Epoch: 104 	Training Loss: 1.287122 	Validation Loss: 0.318283
Epoch: 105 	Training Loss: 1.285004 	Validation Loss: 0.316454
Epoch: 106 	Training Loss: 1.288655 	Validation Loss: 0.328694
Epoch: 107 	Training Loss: 1.286483 	Validation Loss: 0.311118
Validation loss decreased (0.316041 --&amp;gt; 0.311118).  Saving model ...
Epoch: 108 	Training Loss: 1.286722 	Validation Loss: 0.322617
Epoch: 109 	Training Loss: 1.281688 	Validation Loss: 0.317284
Epoch: 110 	Training Loss: 1.286374 	Validation Loss: 0.316699
Epoch: 111 	Training Loss: 1.285399 	Validation Loss: 0.315800
Epoch: 112 	Training Loss: 1.283735 	Validation Loss: 0.321917
Epoch: 113 	Training Loss: 1.283596 	Validation Loss: 0.311436
Epoch: 114 	Training Loss: 1.285218 	Validation Loss: 0.314240
Epoch: 115 	Training Loss: 1.282439 	Validation Loss: 0.315108
Epoch: 116 	Training Loss: 1.282893 	Validation Loss: 0.317056
Epoch: 117 	Training Loss: 1.282942 	Validation Loss: 0.313947
Epoch: 118 	Training Loss: 1.287284 	Validation Loss: 0.316639
Epoch: 119 	Training Loss: 1.285622 	Validation Loss: 0.321113
Epoch: 120 	Training Loss: 1.284308 	Validation Loss: 0.319277
Epoch: 121 	Training Loss: 1.282111 	Validation Loss: 0.314455
Epoch: 122 	Training Loss: 1.283129 	Validation Loss: 0.313159
Epoch: 123 	Training Loss: 1.284335 	Validation Loss: 0.322168
Epoch: 124 	Training Loss: 1.278320 	Validation Loss: 0.318971
Epoch: 125 	Training Loss: 1.281218 	Validation Loss: 0.313987
Epoch: 126 	Training Loss: 1.279132 	Validation Loss: 0.328925
Epoch: 127 	Training Loss: 1.279555 	Validation Loss: 0.316594
Epoch: 128 	Training Loss: 1.273169 	Validation Loss: 0.315559
Epoch: 129 	Training Loss: 1.277613 	Validation Loss: 0.319802
Epoch: 130 	Training Loss: 1.280081 	Validation Loss: 0.322822
Epoch: 131 	Training Loss: 1.281299 	Validation Loss: 0.317239
Epoch: 132 	Training Loss: 1.280862 	Validation Loss: 0.317907
Epoch: 133 	Training Loss: 1.280196 	Validation Loss: 0.323627
Epoch: 134 	Training Loss: 1.278056 	Validation Loss: 0.315584
Epoch: 135 	Training Loss: 1.271644 	Validation Loss: 0.317295
Epoch: 136 	Training Loss: 1.276935 	Validation Loss: 0.325810
Epoch: 137 	Training Loss: 1.279832 	Validation Loss: 0.320269
Epoch: 138 	Training Loss: 1.276127 	Validation Loss: 0.320572
Epoch: 139 	Training Loss: 1.276283 	Validation Loss: 0.319130
Epoch: 140 	Training Loss: 1.274293 	Validation Loss: 0.324264
Epoch: 141 	Training Loss: 1.276226 	Validation Loss: 0.318521
Epoch: 142 	Training Loss: 1.273648 	Validation Loss: 0.317698
Epoch: 143 	Training Loss: 1.280384 	Validation Loss: 0.318762
Epoch: 144 	Training Loss: 1.271613 	Validation Loss: 0.321056
Epoch: 145 	Training Loss: 1.279159 	Validation Loss: 0.319677
Epoch: 146 	Training Loss: 1.277133 	Validation Loss: 0.313412
Epoch: 147 	Training Loss: 1.273115 	Validation Loss: 0.316693
Epoch: 148 	Training Loss: 1.276824 	Validation Loss: 0.324270
Epoch: 149 	Training Loss: 1.271500 	Validation Loss: 0.317610
Epoch: 150 	Training Loss: 1.274339 	Validation Loss: 0.319794
Epoch: 151 	Training Loss: 1.276326 	Validation Loss: 0.316618
Epoch: 152 	Training Loss: 1.274265 	Validation Loss: 0.317560
Epoch: 153 	Training Loss: 1.273693 	Validation Loss: 0.315664
Epoch: 154 	Training Loss: 1.271308 	Validation Loss: 0.314383
Epoch: 155 	Training Loss: 1.275785 	Validation Loss: 0.311731
Epoch: 156 	Training Loss: 1.269926 	Validation Loss: 0.317802
Epoch: 157 	Training Loss: 1.272163 	Validation Loss: 0.326034
Epoch: 158 	Training Loss: 1.272792 	Validation Loss: 0.323937
Epoch: 159 	Training Loss: 1.270623 	Validation Loss: 0.314596
Epoch: 160 	Training Loss: 1.274752 	Validation Loss: 0.318708
Epoch: 161 	Training Loss: 1.269636 	Validation Loss: 0.315447
Epoch: 162 	Training Loss: 1.268630 	Validation Loss: 0.318611
Epoch: 163 	Training Loss: 1.269201 	Validation Loss: 0.321739
Epoch: 164 	Training Loss: 1.268440 	Validation Loss: 0.318679
Epoch: 165 	Training Loss: 1.267896 	Validation Loss: 0.317043
Epoch: 166 	Training Loss: 1.268580 	Validation Loss: 0.319146
Epoch: 167 	Training Loss: 1.275538 	Validation Loss: 0.317928
Epoch: 168 	Training Loss: 1.268560 	Validation Loss: 0.323980
Epoch: 169 	Training Loss: 1.268632 	Validation Loss: 0.313479
Epoch: 170 	Training Loss: 1.264794 	Validation Loss: 0.318113
Epoch: 171 	Training Loss: 1.270822 	Validation Loss: 0.313195
Epoch: 172 	Training Loss: 1.267813 	Validation Loss: 0.317769
Epoch: 173 	Training Loss: 1.270347 	Validation Loss: 0.315005
Epoch: 174 	Training Loss: 1.266662 	Validation Loss: 0.314660
Epoch: 175 	Training Loss: 1.268849 	Validation Loss: 0.319801
Epoch: 176 	Training Loss: 1.271820 	Validation Loss: 0.320086
Epoch: 177 	Training Loss: 1.273374 	Validation Loss: 0.318641
Epoch: 178 	Training Loss: 1.265961 	Validation Loss: 0.314708
Epoch: 179 	Training Loss: 1.271811 	Validation Loss: 0.322507
Epoch: 180 	Training Loss: 1.263662 	Validation Loss: 0.323136
Epoch: 181 	Training Loss: 1.269750 	Validation Loss: 0.314223
Epoch: 182 	Training Loss: 1.269853 	Validation Loss: 0.321011
Epoch: 183 	Training Loss: 1.267138 	Validation Loss: 0.313789
Epoch: 184 	Training Loss: 1.271545 	Validation Loss: 0.321742
Epoch: 185 	Training Loss: 1.268025 	Validation Loss: 0.316022
Epoch: 186 	Training Loss: 1.272954 	Validation Loss: 0.324468
Epoch: 187 	Training Loss: 1.267895 	Validation Loss: 0.314698
Epoch: 188 	Training Loss: 1.266716 	Validation Loss: 0.318999
Epoch: 189 	Training Loss: 1.263130 	Validation Loss: 0.319963
Epoch: 190 	Training Loss: 1.270730 	Validation Loss: 0.319453
Epoch: 191 	Training Loss: 1.265955 	Validation Loss: 0.314691
Epoch: 192 	Training Loss: 1.267399 	Validation Loss: 0.321611
Epoch: 193 	Training Loss: 1.264792 	Validation Loss: 0.320243
Epoch: 194 	Training Loss: 1.262446 	Validation Loss: 0.314628
Epoch: 195 	Training Loss: 1.262605 	Validation Loss: 0.312932
Epoch: 196 	Training Loss: 1.265456 	Validation Loss: 0.313259
Epoch: 197 	Training Loss: 1.269357 	Validation Loss: 0.311136
Epoch: 198 	Training Loss: 1.262179 	Validation Loss: 0.312693
Epoch: 199 	Training Loss: 1.266902 	Validation Loss: 0.313880
Epoch: 200 	Training Loss: 1.265160 	Validation Loss: 0.312400
Epoch: 201 	Training Loss: 1.266844 	Validation Loss: 0.316210
Epoch: 202 	Training Loss: 1.264941 	Validation Loss: 0.317070
Epoch: 203 	Training Loss: 1.267308 	Validation Loss: 0.321297
Epoch: 204 	Training Loss: 1.265302 	Validation Loss: 0.318993
Epoch: 205 	Training Loss: 1.265829 	Validation Loss: 0.313469
Epoch: 206 	Training Loss: 1.261570 	Validation Loss: 0.321749
Epoch: 207 	Training Loss: 1.266412 	Validation Loss: 0.310708
Validation loss decreased (0.311118 --&amp;gt; 0.310708).  Saving model ...
Epoch: 208 	Training Loss: 1.266944 	Validation Loss: 0.318451
Epoch: 209 	Training Loss: 1.265850 	Validation Loss: 0.315396
Epoch: 210 	Training Loss: 1.264065 	Validation Loss: 0.315393
Epoch: 211 	Training Loss: 1.258434 	Validation Loss: 0.315945
Epoch: 212 	Training Loss: 1.262104 	Validation Loss: 0.317880
Epoch: 213 	Training Loss: 1.266053 	Validation Loss: 0.326606
Epoch: 214 	Training Loss: 1.264815 	Validation Loss: 0.317249
Epoch: 215 	Training Loss: 1.265139 	Validation Loss: 0.319844
Epoch: 216 	Training Loss: 1.266425 	Validation Loss: 0.320103
Epoch: 217 	Training Loss: 1.265218 	Validation Loss: 0.313683
Epoch: 218 	Training Loss: 1.261013 	Validation Loss: 0.316373
Epoch: 219 	Training Loss: 1.262247 	Validation Loss: 0.313101
Epoch: 220 	Training Loss: 1.264393 	Validation Loss: 0.314501
Epoch: 221 	Training Loss: 1.264149 	Validation Loss: 0.315623
Epoch: 222 	Training Loss: 1.259319 	Validation Loss: 0.318756
Epoch: 223 	Training Loss: 1.258570 	Validation Loss: 0.319732
Epoch: 224 	Training Loss: 1.259029 	Validation Loss: 0.311516
Epoch: 225 	Training Loss: 1.266348 	Validation Loss: 0.314770
Epoch: 226 	Training Loss: 1.259851 	Validation Loss: 0.321516
Epoch: 227 	Training Loss: 1.262397 	Validation Loss: 0.314634
Epoch: 228 	Training Loss: 1.258319 	Validation Loss: 0.314885
Epoch: 229 	Training Loss: 1.257705 	Validation Loss: 0.313776
Epoch: 230 	Training Loss: 1.265772 	Validation Loss: 0.317983
Epoch: 231 	Training Loss: 1.256625 	Validation Loss: 0.315058
Epoch: 232 	Training Loss: 1.259640 	Validation Loss: 0.315233
Epoch: 233 	Training Loss: 1.257951 	Validation Loss: 0.312612
Epoch: 234 	Training Loss: 1.259246 	Validation Loss: 0.318067
Epoch: 235 	Training Loss: 1.254118 	Validation Loss: 0.319640
Epoch: 236 	Training Loss: 1.261764 	Validation Loss: 0.323842
Epoch: 237 	Training Loss: 1.257337 	Validation Loss: 0.312940
Epoch: 238 	Training Loss: 1.261468 	Validation Loss: 0.312802
Epoch: 239 	Training Loss: 1.256006 	Validation Loss: 0.317805
Epoch: 240 	Training Loss: 1.259415 	Validation Loss: 0.313486
Epoch: 241 	Training Loss: 1.256178 	Validation Loss: 0.314875
Epoch: 242 	Training Loss: 1.256519 	Validation Loss: 0.313054
Epoch: 243 	Training Loss: 1.255753 	Validation Loss: 0.310222
Validation loss decreased (0.310708 --&amp;gt; 0.310222).  Saving model ...
Epoch: 244 	Training Loss: 1.258942 	Validation Loss: 0.329567
Epoch: 245 	Training Loss: 1.258942 	Validation Loss: 0.311769
Epoch: 246 	Training Loss: 1.262446 	Validation Loss: 0.313582
Epoch: 247 	Training Loss: 1.261230 	Validation Loss: 0.318076
Epoch: 248 	Training Loss: 1.261161 	Validation Loss: 0.314736
Epoch: 249 	Training Loss: 1.259770 	Validation Loss: 0.313956
Epoch: 250 	Training Loss: 1.256420 	Validation Loss: 0.312800
Epoch: 251 	Training Loss: 1.262006 	Validation Loss: 0.316093
Epoch: 252 	Training Loss: 1.259628 	Validation Loss: 0.314459
Epoch: 253 	Training Loss: 1.255323 	Validation Loss: 0.320948
Epoch: 254 	Training Loss: 1.251152 	Validation Loss: 0.312966
Epoch: 255 	Training Loss: 1.263651 	Validation Loss: 0.324031
Epoch: 256 	Training Loss: 1.258022 	Validation Loss: 0.317772
Epoch: 257 	Training Loss: 1.260936 	Validation Loss: 0.316249
Epoch: 258 	Training Loss: 1.257661 	Validation Loss: 0.318002
Epoch: 259 	Training Loss: 1.253739 	Validation Loss: 0.317531
Epoch: 260 	Training Loss: 1.259165 	Validation Loss: 0.318186
Epoch: 261 	Training Loss: 1.255523 	Validation Loss: 0.315747
Epoch: 262 	Training Loss: 1.260258 	Validation Loss: 0.323450
Epoch: 263 	Training Loss: 1.256247 	Validation Loss: 0.315790
Epoch: 264 	Training Loss: 1.256523 	Validation Loss: 0.322588
Epoch: 265 	Training Loss: 1.256251 	Validation Loss: 0.316159
Epoch: 266 	Training Loss: 1.254540 	Validation Loss: 0.317133
Epoch: 267 	Training Loss: 1.256788 	Validation Loss: 0.320573
Epoch: 268 	Training Loss: 1.261198 	Validation Loss: 0.326142
Epoch: 269 	Training Loss: 1.255286 	Validation Loss: 0.311760
Epoch: 270 	Training Loss: 1.256038 	Validation Loss: 0.320824
Epoch: 271 	Training Loss: 1.252561 	Validation Loss: 0.313171
Epoch: 272 	Training Loss: 1.257770 	Validation Loss: 0.318307
Epoch: 273 	Training Loss: 1.254161 	Validation Loss: 0.309804
Validation loss decreased (0.310222 --&amp;gt; 0.309804).  Saving model ...
Epoch: 274 	Training Loss: 1.256829 	Validation Loss: 0.318989
Epoch: 275 	Training Loss: 1.264886 	Validation Loss: 0.317026
Epoch: 276 	Training Loss: 1.250972 	Validation Loss: 0.315094
Epoch: 277 	Training Loss: 1.255500 	Validation Loss: 0.324168
Epoch: 278 	Training Loss: 1.253158 	Validation Loss: 0.321396
Epoch: 279 	Training Loss: 1.258170 	Validation Loss: 0.320225
Epoch: 280 	Training Loss: 1.258867 	Validation Loss: 0.318569
Epoch: 281 	Training Loss: 1.254345 	Validation Loss: 0.316465
Epoch: 282 	Training Loss: 1.255778 	Validation Loss: 0.314160
Epoch: 283 	Training Loss: 1.254325 	Validation Loss: 0.313069
Epoch: 284 	Training Loss: 1.253357 	Validation Loss: 0.328138
Epoch: 285 	Training Loss: 1.251017 	Validation Loss: 0.316133
Epoch: 286 	Training Loss: 1.252227 	Validation Loss: 0.316984
Epoch: 287 	Training Loss: 1.253182 	Validation Loss: 0.313943
Epoch: 288 	Training Loss: 1.250671 	Validation Loss: 0.318114
Epoch: 289 	Training Loss: 1.255845 	Validation Loss: 0.316618
Epoch: 290 	Training Loss: 1.255237 	Validation Loss: 0.312792
Epoch: 291 	Training Loss: 1.262059 	Validation Loss: 0.314828
Epoch: 292 	Training Loss: 1.255877 	Validation Loss: 0.318905
Epoch: 293 	Training Loss: 1.254416 	Validation Loss: 0.314216
Epoch: 294 	Training Loss: 1.253497 	Validation Loss: 0.314790
Epoch: 295 	Training Loss: 1.255368 	Validation Loss: 0.321991
Epoch: 296 	Training Loss: 1.257793 	Validation Loss: 0.317706
Epoch: 297 	Training Loss: 1.251250 	Validation Loss: 0.316808
Epoch: 298 	Training Loss: 1.252172 	Validation Loss: 0.315334
Epoch: 299 	Training Loss: 1.251001 	Validation Loss: 0.314154
Epoch: 300 	Training Loss: 1.252786 	Validation Loss: 0.320209
Epoch: 301 	Training Loss: 1.257268 	Validation Loss: 0.319915
Elapsed: 1:15:46.335776
&lt;/pre&gt;

&lt;p&gt;
It seems to be improving, but really slowly.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test(model_2)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Loss: 1.307058

Test Accuracy of airplane: 57% (572/1000)
Test Accuracy of automobile: 73% (735/1000)
Test Accuracy of  bird: 26% (266/1000)
Test Accuracy of   cat: 35% (357/1000)
Test Accuracy of  deer: 52% (525/1000)
Test Accuracy of   dog: 19% (193/1000)
Test Accuracy of  frog: 79% (798/1000)
Test Accuracy of horse: 59% (598/1000)
Test Accuracy of  ship: 81% (810/1000)
Test Accuracy of truck: 49% (494/1000)

Test Accuracy (Overall): 53% (5348/10000)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_2 = CNN()
model_2.to(device)
start = datetime.now()
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
				   map_location=device))
outcome_2 = train_and_pickle(model_2, epochs=200, model_number=2)
outcome = update_outcome(outcome, outcome_2)
print("Elapsed: {}".format(datetime.now() - start))
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
				   map_location=device))

test(model_2)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 202 	Training Loss: 1.256388 	Validation Loss: 0.313784
Validation loss decreased (inf --&amp;gt; 0.313784).  Saving model ...
Epoch: 203 	Training Loss: 1.258825 	Validation Loss: 0.317360
Epoch: 204 	Training Loss: 1.256599 	Validation Loss: 0.316243
Epoch: 205 	Training Loss: 1.253339 	Validation Loss: 0.322061
Epoch: 206 	Training Loss: 1.260164 	Validation Loss: 0.319589
Epoch: 207 	Training Loss: 1.252303 	Validation Loss: 0.318219
Epoch: 208 	Training Loss: 1.257676 	Validation Loss: 0.326530
Epoch: 209 	Training Loss: 1.258256 	Validation Loss: 0.322288
Epoch: 210 	Training Loss: 1.257436 	Validation Loss: 0.316848
Epoch: 211 	Training Loss: 1.256364 	Validation Loss: 0.313047
Validation loss decreased (0.313784 --&amp;gt; 0.313047).  Saving model ...
Epoch: 212 	Training Loss: 1.259785 	Validation Loss: 0.321005
Epoch: 213 	Training Loss: 1.254453 	Validation Loss: 0.307325
Validation loss decreased (0.313047 --&amp;gt; 0.307325).  Saving model ...
Epoch: 214 	Training Loss: 1.254806 	Validation Loss: 0.320826
Epoch: 215 	Training Loss: 1.252779 	Validation Loss: 0.320929
Epoch: 216 	Training Loss: 1.252038 	Validation Loss: 0.320515
Epoch: 217 	Training Loss: 1.252444 	Validation Loss: 0.317522
Epoch: 218 	Training Loss: 1.254665 	Validation Loss: 0.313467
Epoch: 219 	Training Loss: 1.255900 	Validation Loss: 0.315710
Epoch: 220 	Training Loss: 1.252430 	Validation Loss: 0.321523
Epoch: 221 	Training Loss: 1.256561 	Validation Loss: 0.310884
Epoch: 222 	Training Loss: 1.255160 	Validation Loss: 0.309861
Epoch: 223 	Training Loss: 1.254754 	Validation Loss: 0.319757
Epoch: 224 	Training Loss: 1.255497 	Validation Loss: 0.318309
Epoch: 225 	Training Loss: 1.260697 	Validation Loss: 0.314599
Epoch: 226 	Training Loss: 1.253136 	Validation Loss: 0.318721
Epoch: 227 	Training Loss: 1.257839 	Validation Loss: 0.312620
Epoch: 228 	Training Loss: 1.248965 	Validation Loss: 0.320385
Epoch: 229 	Training Loss: 1.251453 	Validation Loss: 0.318191
Epoch: 230 	Training Loss: 1.252814 	Validation Loss: 0.324980
Epoch: 231 	Training Loss: 1.256732 	Validation Loss: 0.318312
Epoch: 232 	Training Loss: 1.251452 	Validation Loss: 0.319930
Epoch: 233 	Training Loss: 1.251726 	Validation Loss: 0.311095
Epoch: 234 	Training Loss: 1.250112 	Validation Loss: 0.318118
Epoch: 235 	Training Loss: 1.255064 	Validation Loss: 0.311329
Epoch: 236 	Training Loss: 1.250156 	Validation Loss: 0.322847
Epoch: 237 	Training Loss: 1.249897 	Validation Loss: 0.310835
Epoch: 238 	Training Loss: 1.251495 	Validation Loss: 0.322079
Epoch: 239 	Training Loss: 1.247715 	Validation Loss: 0.321563
Epoch: 240 	Training Loss: 1.248373 	Validation Loss: 0.328171
Epoch: 241 	Training Loss: 1.250492 	Validation Loss: 0.321683
Epoch: 242 	Training Loss: 1.255231 	Validation Loss: 0.313710
Epoch: 243 	Training Loss: 1.247742 	Validation Loss: 0.318332
Epoch: 244 	Training Loss: 1.251414 	Validation Loss: 0.315995
Epoch: 245 	Training Loss: 1.258454 	Validation Loss: 0.317433
Epoch: 246 	Training Loss: 1.253335 	Validation Loss: 0.317605
Epoch: 247 	Training Loss: 1.253148 	Validation Loss: 0.316049
Epoch: 248 	Training Loss: 1.251510 	Validation Loss: 0.312951
Epoch: 249 	Training Loss: 1.251977 	Validation Loss: 0.321403
Epoch: 250 	Training Loss: 1.256146 	Validation Loss: 0.320409
Epoch: 251 	Training Loss: 1.248189 	Validation Loss: 0.317272
Epoch: 252 	Training Loss: 1.254679 	Validation Loss: 0.317682
Epoch: 253 	Training Loss: 1.253137 	Validation Loss: 0.317845
Epoch: 254 	Training Loss: 1.258417 	Validation Loss: 0.317278
Epoch: 255 	Training Loss: 1.253359 	Validation Loss: 0.319818
Epoch: 256 	Training Loss: 1.247390 	Validation Loss: 0.320857
Epoch: 257 	Training Loss: 1.255359 	Validation Loss: 0.317702
Epoch: 258 	Training Loss: 1.247608 	Validation Loss: 0.316204
Epoch: 259 	Training Loss: 1.249561 	Validation Loss: 0.312899
Epoch: 260 	Training Loss: 1.248591 	Validation Loss: 0.322027
Epoch: 261 	Training Loss: 1.248232 	Validation Loss: 0.316189
Epoch: 262 	Training Loss: 1.252761 	Validation Loss: 0.317912
Epoch: 263 	Training Loss: 1.246621 	Validation Loss: 0.317565
Epoch: 264 	Training Loss: 1.249730 	Validation Loss: 0.321344
Epoch: 265 	Training Loss: 1.253313 	Validation Loss: 0.317789
Epoch: 266 	Training Loss: 1.250943 	Validation Loss: 0.319828
Epoch: 267 	Training Loss: 1.248345 	Validation Loss: 0.319927
Epoch: 268 	Training Loss: 1.248811 	Validation Loss: 0.316677
Epoch: 269 	Training Loss: 1.250617 	Validation Loss: 0.311661
Epoch: 270 	Training Loss: 1.250927 	Validation Loss: 0.324976
Epoch: 271 	Training Loss: 1.246129 	Validation Loss: 0.321428
Epoch: 272 	Training Loss: 1.247270 	Validation Loss: 0.313739
Epoch: 273 	Training Loss: 1.252439 	Validation Loss: 0.314271
Epoch: 274 	Training Loss: 1.249031 	Validation Loss: 0.315256
Epoch: 275 	Training Loss: 1.248926 	Validation Loss: 0.318519
Epoch: 276 	Training Loss: 1.253851 	Validation Loss: 0.317292
Epoch: 277 	Training Loss: 1.248241 	Validation Loss: 0.312578
Epoch: 278 	Training Loss: 1.246958 	Validation Loss: 0.317017
Epoch: 279 	Training Loss: 1.247038 	Validation Loss: 0.317870
Epoch: 280 	Training Loss: 1.247711 	Validation Loss: 0.320040
Epoch: 281 	Training Loss: 1.250939 	Validation Loss: 0.319092
Epoch: 282 	Training Loss: 1.250168 	Validation Loss: 0.318878
Epoch: 283 	Training Loss: 1.249140 	Validation Loss: 0.323233
Epoch: 284 	Training Loss: 1.247192 	Validation Loss: 0.320423
Epoch: 285 	Training Loss: 1.248637 	Validation Loss: 0.321254
Epoch: 286 	Training Loss: 1.246468 	Validation Loss: 0.322253
Epoch: 287 	Training Loss: 1.247990 	Validation Loss: 0.316660
Epoch: 288 	Training Loss: 1.245704 	Validation Loss: 0.327530
Epoch: 289 	Training Loss: 1.244317 	Validation Loss: 0.316667
Epoch: 290 	Training Loss: 1.247457 	Validation Loss: 0.316587
Epoch: 291 	Training Loss: 1.244423 	Validation Loss: 0.323431
Epoch: 292 	Training Loss: 1.245140 	Validation Loss: 0.319670
Epoch: 293 	Training Loss: 1.247903 	Validation Loss: 0.315965
Epoch: 294 	Training Loss: 1.248071 	Validation Loss: 0.314560
Epoch: 295 	Training Loss: 1.244779 	Validation Loss: 0.321430
Epoch: 296 	Training Loss: 1.250301 	Validation Loss: 0.314018
Epoch: 297 	Training Loss: 1.251302 	Validation Loss: 0.316015
Epoch: 298 	Training Loss: 1.253560 	Validation Loss: 0.315506
Epoch: 299 	Training Loss: 1.246812 	Validation Loss: 0.323061
Epoch: 300 	Training Loss: 1.248937 	Validation Loss: 0.315299
Epoch: 301 	Training Loss: 1.248918 	Validation Loss: 0.318701
Epoch: 302 	Training Loss: 1.247325 	Validation Loss: 0.315778
Epoch: 303 	Training Loss: 1.241974 	Validation Loss: 0.315274
Epoch: 304 	Training Loss: 1.250347 	Validation Loss: 0.315380
Epoch: 305 	Training Loss: 1.244912 	Validation Loss: 0.316511
Epoch: 306 	Training Loss: 1.247815 	Validation Loss: 0.317746
Epoch: 307 	Training Loss: 1.250566 	Validation Loss: 0.314758
Epoch: 308 	Training Loss: 1.249454 	Validation Loss: 0.317377
Epoch: 309 	Training Loss: 1.249325 	Validation Loss: 0.316275
Epoch: 310 	Training Loss: 1.248658 	Validation Loss: 0.319433
Epoch: 311 	Training Loss: 1.244979 	Validation Loss: 0.312409
Epoch: 312 	Training Loss: 1.250389 	Validation Loss: 0.319627
Epoch: 313 	Training Loss: 1.245450 	Validation Loss: 0.318461
Epoch: 314 	Training Loss: 1.247308 	Validation Loss: 0.318554
Epoch: 315 	Training Loss: 1.247195 	Validation Loss: 0.316582
Epoch: 316 	Training Loss: 1.244136 	Validation Loss: 0.318103
Epoch: 317 	Training Loss: 1.249054 	Validation Loss: 0.319848
Epoch: 318 	Training Loss: 1.248777 	Validation Loss: 0.323786
Epoch: 319 	Training Loss: 1.247198 	Validation Loss: 0.315047
Epoch: 320 	Training Loss: 1.251294 	Validation Loss: 0.318657
Epoch: 321 	Training Loss: 1.249177 	Validation Loss: 0.337516
Epoch: 322 	Training Loss: 1.247499 	Validation Loss: 0.326684
Epoch: 323 	Training Loss: 1.246539 	Validation Loss: 0.319658
Epoch: 324 	Training Loss: 1.248925 	Validation Loss: 0.313511
Epoch: 325 	Training Loss: 1.243196 	Validation Loss: 0.315549
Epoch: 326 	Training Loss: 1.244999 	Validation Loss: 0.321060
Epoch: 327 	Training Loss: 1.248777 	Validation Loss: 0.317293
Epoch: 328 	Training Loss: 1.248694 	Validation Loss: 0.317218
Epoch: 329 	Training Loss: 1.251560 	Validation Loss: 0.317921
Epoch: 330 	Training Loss: 1.252284 	Validation Loss: 0.317201
Epoch: 331 	Training Loss: 1.246083 	Validation Loss: 0.321029
Epoch: 332 	Training Loss: 1.244893 	Validation Loss: 0.316990
Epoch: 333 	Training Loss: 1.240543 	Validation Loss: 0.317590
Epoch: 334 	Training Loss: 1.246393 	Validation Loss: 0.325721
Epoch: 335 	Training Loss: 1.248191 	Validation Loss: 0.320632
Epoch: 336 	Training Loss: 1.241560 	Validation Loss: 0.324130
Epoch: 337 	Training Loss: 1.243119 	Validation Loss: 0.318852
Epoch: 338 	Training Loss: 1.242660 	Validation Loss: 0.319926
Epoch: 339 	Training Loss: 1.249028 	Validation Loss: 0.315266
Epoch: 340 	Training Loss: 1.244741 	Validation Loss: 0.324272
Epoch: 341 	Training Loss: 1.244523 	Validation Loss: 0.318710
Epoch: 342 	Training Loss: 1.241070 	Validation Loss: 0.319939
Epoch: 343 	Training Loss: 1.244101 	Validation Loss: 0.321822
Epoch: 344 	Training Loss: 1.239239 	Validation Loss: 0.315630
Epoch: 345 	Training Loss: 1.245509 	Validation Loss: 0.318808
Epoch: 346 	Training Loss: 1.245012 	Validation Loss: 0.320597
Epoch: 347 	Training Loss: 1.251397 	Validation Loss: 0.318575
Epoch: 348 	Training Loss: 1.240546 	Validation Loss: 0.313607
Epoch: 349 	Training Loss: 1.245582 	Validation Loss: 0.317309
Epoch: 350 	Training Loss: 1.240588 	Validation Loss: 0.319662
Epoch: 351 	Training Loss: 1.241194 	Validation Loss: 0.316204
Epoch: 352 	Training Loss: 1.243081 	Validation Loss: 0.321423
Epoch: 353 	Training Loss: 1.244287 	Validation Loss: 0.316278
Epoch: 354 	Training Loss: 1.248997 	Validation Loss: 0.322080
Epoch: 355 	Training Loss: 1.243133 	Validation Loss: 0.314357
Epoch: 356 	Training Loss: 1.240463 	Validation Loss: 0.317619
Epoch: 357 	Training Loss: 1.249085 	Validation Loss: 0.317623
Epoch: 358 	Training Loss: 1.244508 	Validation Loss: 0.316843
Epoch: 359 	Training Loss: 1.252762 	Validation Loss: 0.317262
Epoch: 360 	Training Loss: 1.246585 	Validation Loss: 0.321501
Epoch: 361 	Training Loss: 1.240622 	Validation Loss: 0.318065
Epoch: 362 	Training Loss: 1.246144 	Validation Loss: 0.317386
Epoch: 363 	Training Loss: 1.246127 	Validation Loss: 0.314560
Epoch: 364 	Training Loss: 1.244285 	Validation Loss: 0.318059
Epoch: 365 	Training Loss: 1.244826 	Validation Loss: 0.317295
Epoch: 366 	Training Loss: 1.244527 	Validation Loss: 0.313897
Epoch: 367 	Training Loss: 1.244683 	Validation Loss: 0.325274
Epoch: 368 	Training Loss: 1.245969 	Validation Loss: 0.325050
Epoch: 369 	Training Loss: 1.245889 	Validation Loss: 0.317678
Epoch: 370 	Training Loss: 1.240173 	Validation Loss: 0.321540
Epoch: 371 	Training Loss: 1.244970 	Validation Loss: 0.318374
Epoch: 372 	Training Loss: 1.242400 	Validation Loss: 0.322875
Epoch: 373 	Training Loss: 1.245613 	Validation Loss: 0.319608
Epoch: 374 	Training Loss: 1.243773 	Validation Loss: 0.322040
Epoch: 375 	Training Loss: 1.243070 	Validation Loss: 0.320554
Epoch: 376 	Training Loss: 1.245695 	Validation Loss: 0.321315
Epoch: 377 	Training Loss: 1.245310 	Validation Loss: 0.321394
Epoch: 378 	Training Loss: 1.240203 	Validation Loss: 0.316470
Epoch: 379 	Training Loss: 1.245251 	Validation Loss: 0.317234
Epoch: 380 	Training Loss: 1.250027 	Validation Loss: 0.330051
Epoch: 381 	Training Loss: 1.243686 	Validation Loss: 0.322005
Epoch: 382 	Training Loss: 1.243251 	Validation Loss: 0.315280
Epoch: 383 	Training Loss: 1.243953 	Validation Loss: 0.326072
Epoch: 384 	Training Loss: 1.245808 	Validation Loss: 0.316741
Epoch: 385 	Training Loss: 1.242827 	Validation Loss: 0.315943
Epoch: 386 	Training Loss: 1.244012 	Validation Loss: 0.310488
Epoch: 387 	Training Loss: 1.245015 	Validation Loss: 0.314874
Epoch: 388 	Training Loss: 1.244292 	Validation Loss: 0.317309
Epoch: 389 	Training Loss: 1.250823 	Validation Loss: 0.313929
Epoch: 390 	Training Loss: 1.248937 	Validation Loss: 0.314966
Epoch: 391 	Training Loss: 1.249134 	Validation Loss: 0.321290
Epoch: 392 	Training Loss: 1.246164 	Validation Loss: 0.316047
Epoch: 393 	Training Loss: 1.249995 	Validation Loss: 0.318678
Epoch: 394 	Training Loss: 1.240377 	Validation Loss: 0.327256
Epoch: 395 	Training Loss: 1.247659 	Validation Loss: 0.317254
Epoch: 396 	Training Loss: 1.238285 	Validation Loss: 0.314723
Epoch: 397 	Training Loss: 1.245013 	Validation Loss: 0.324809
Epoch: 398 	Training Loss: 1.247650 	Validation Loss: 0.330501
Epoch: 399 	Training Loss: 1.250368 	Validation Loss: 0.318667
Epoch: 400 	Training Loss: 1.246211 	Validation Loss: 0.323798
Epoch: 401 	Training Loss: 1.239634 	Validation Loss: 0.322877
Epoch: 402 	Training Loss: 1.248236 	Validation Loss: 0.321464
Elapsed: 1:17:57.592411
Test Loss: 1.336450

Test Accuracy of airplane: 55% (553/1000)
Test Accuracy of automobile: 58% (583/1000)
Test Accuracy of  bird: 23% (234/1000)
Test Accuracy of   cat: 30% (307/1000)
Test Accuracy of  deer: 36% (365/1000)
Test Accuracy of   dog: 25% (257/1000)
Test Accuracy of  frog: 88% (880/1000)
Test Accuracy of horse: 69% (694/1000)
Test Accuracy of  ship: 76% (766/1000)
Test Accuracy of truck: 61% (611/1000)

Test Accuracy (Overall): 52% (5250/10000)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_2 = CNN()
model_2.to(device)
start = datetime.now()
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
				   map_location=device))
outcome_2 = train_and_pickle(model_2, epochs=200, model_number=2)
outcome = update_outcome(outcome, outcome_2)
print("Elapsed: {}".format(datetime.now() - start))
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
				   map_location=device))
test(model_2)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 400 	Training Loss: 1.085763 	Validation Loss: 0.282825
Validation loss decreased (inf --&amp;gt; 0.282825).  Saving model ...
Epoch: 401 	Training Loss: 1.094224 	Validation Loss: 0.282336
Validation loss decreased (0.282825 --&amp;gt; 0.282336).  Saving model ...
Epoch: 402 	Training Loss: 1.090027 	Validation Loss: 0.283988
Epoch: 403 	Training Loss: 1.088251 	Validation Loss: 0.282374
Epoch: 404 	Training Loss: 1.088617 	Validation Loss: 0.280398
Validation loss decreased (0.282336 --&amp;gt; 0.280398).  Saving model ...
Epoch: 405 	Training Loss: 1.092081 	Validation Loss: 0.280428
Epoch: 406 	Training Loss: 1.091815 	Validation Loss: 0.278766
Validation loss decreased (0.280398 --&amp;gt; 0.278766).  Saving model ...
Epoch: 407 	Training Loss: 1.088024 	Validation Loss: 0.281447
Epoch: 408 	Training Loss: 1.094500 	Validation Loss: 0.283386
Epoch: 409 	Training Loss: 1.089597 	Validation Loss: 0.281148
Epoch: 410 	Training Loss: 1.091652 	Validation Loss: 0.283893
Epoch: 411 	Training Loss: 1.087357 	Validation Loss: 0.281366
Epoch: 412 	Training Loss: 1.091122 	Validation Loss: 0.286320
Epoch: 413 	Training Loss: 1.089693 	Validation Loss: 0.282684
Epoch: 414 	Training Loss: 1.088109 	Validation Loss: 0.284077
Epoch: 415 	Training Loss: 1.087701 	Validation Loss: 0.280002
Epoch: 416 	Training Loss: 1.085328 	Validation Loss: 0.282377
Epoch: 417 	Training Loss: 1.089087 	Validation Loss: 0.282623
Epoch: 418 	Training Loss: 1.086825 	Validation Loss: 0.278291
Validation loss decreased (0.278766 --&amp;gt; 0.278291).  Saving model ...
Epoch: 419 	Training Loss: 1.086601 	Validation Loss: 0.282585
Epoch: 420 	Training Loss: 1.082824 	Validation Loss: 0.282660
Epoch: 421 	Training Loss: 1.089363 	Validation Loss: 0.281838
Epoch: 422 	Training Loss: 1.087070 	Validation Loss: 0.279197
Epoch: 423 	Training Loss: 1.084032 	Validation Loss: 0.281605
Epoch: 424 	Training Loss: 1.087307 	Validation Loss: 0.281069
Epoch: 425 	Training Loss: 1.090275 	Validation Loss: 0.286235
Epoch: 426 	Training Loss: 1.084863 	Validation Loss: 0.286024
Epoch: 427 	Training Loss: 1.086919 	Validation Loss: 0.283765
Epoch: 428 	Training Loss: 1.087431 	Validation Loss: 0.287237
Epoch: 429 	Training Loss: 1.084115 	Validation Loss: 0.279592
Epoch: 430 	Training Loss: 1.093677 	Validation Loss: 0.283081
Epoch: 431 	Training Loss: 1.090348 	Validation Loss: 0.281837
Epoch: 432 	Training Loss: 1.088213 	Validation Loss: 0.277247
Validation loss decreased (0.278291 --&amp;gt; 0.277247).  Saving model ...
Epoch: 433 	Training Loss: 1.089605 	Validation Loss: 0.278821
Epoch: 434 	Training Loss: 1.085192 	Validation Loss: 0.276951
Validation loss decreased (0.277247 --&amp;gt; 0.276951).  Saving model ...
Epoch: 435 	Training Loss: 1.085776 	Validation Loss: 0.281023
Epoch: 436 	Training Loss: 1.086465 	Validation Loss: 0.283929
Epoch: 437 	Training Loss: 1.087985 	Validation Loss: 0.282887
Epoch: 438 	Training Loss: 1.086791 	Validation Loss: 0.278656
Epoch: 439 	Training Loss: 1.087146 	Validation Loss: 0.284559
Epoch: 440 	Training Loss: 1.086268 	Validation Loss: 0.284008
Epoch: 441 	Training Loss: 1.074737 	Validation Loss: 0.282008
Epoch: 442 	Training Loss: 1.090836 	Validation Loss: 0.280691
Epoch: 443 	Training Loss: 1.086444 	Validation Loss: 0.283169
Epoch: 444 	Training Loss: 1.083751 	Validation Loss: 0.277424
Epoch: 445 	Training Loss: 1.084478 	Validation Loss: 0.282735
Epoch: 446 	Training Loss: 1.087853 	Validation Loss: 0.279917
Epoch: 447 	Training Loss: 1.087905 	Validation Loss: 0.278547
Epoch: 448 	Training Loss: 1.083655 	Validation Loss: 0.284014
Epoch: 449 	Training Loss: 1.085713 	Validation Loss: 0.284066
Epoch: 450 	Training Loss: 1.082967 	Validation Loss: 0.283472
Epoch: 451 	Training Loss: 1.087737 	Validation Loss: 0.281544
Epoch: 452 	Training Loss: 1.084897 	Validation Loss: 0.283131
Epoch: 453 	Training Loss: 1.085416 	Validation Loss: 0.283956
Epoch: 454 	Training Loss: 1.079511 	Validation Loss: 0.284032
Epoch: 455 	Training Loss: 1.081187 	Validation Loss: 0.277546
Epoch: 456 	Training Loss: 1.081564 	Validation Loss: 0.283062
Epoch: 457 	Training Loss: 1.090161 	Validation Loss: 0.277227
Epoch: 458 	Training Loss: 1.082555 	Validation Loss: 0.281654
Epoch: 459 	Training Loss: 1.084783 	Validation Loss: 0.282357
Epoch: 460 	Training Loss: 1.086960 	Validation Loss: 0.283228
Epoch: 461 	Training Loss: 1.088104 	Validation Loss: 0.283043
Epoch: 462 	Training Loss: 1.079098 	Validation Loss: 0.280849
Epoch: 463 	Training Loss: 1.077743 	Validation Loss: 0.279460
Epoch: 464 	Training Loss: 1.080590 	Validation Loss: 0.281254
Epoch: 465 	Training Loss: 1.083514 	Validation Loss: 0.280558
Epoch: 466 	Training Loss: 1.089853 	Validation Loss: 0.277356
Epoch: 467 	Training Loss: 1.080071 	Validation Loss: 0.279764
Epoch: 468 	Training Loss: 1.083149 	Validation Loss: 0.280320
Epoch: 469 	Training Loss: 1.086154 	Validation Loss: 0.278509
Epoch: 470 	Training Loss: 1.075413 	Validation Loss: 0.277589
Epoch: 471 	Training Loss: 1.090838 	Validation Loss: 0.284972
Epoch: 472 	Training Loss: 1.083023 	Validation Loss: 0.280417
Epoch: 473 	Training Loss: 1.078518 	Validation Loss: 0.279890
Epoch: 474 	Training Loss: 1.081342 	Validation Loss: 0.282047
Epoch: 475 	Training Loss: 1.082641 	Validation Loss: 0.277632
Epoch: 476 	Training Loss: 1.077731 	Validation Loss: 0.282896
Epoch: 477 	Training Loss: 1.074824 	Validation Loss: 0.278524
Epoch: 478 	Training Loss: 1.081040 	Validation Loss: 0.282670
Epoch: 479 	Training Loss: 1.078880 	Validation Loss: 0.281313
Epoch: 480 	Training Loss: 1.077215 	Validation Loss: 0.280679
Epoch: 481 	Training Loss: 1.081206 	Validation Loss: 0.278332
Epoch: 482 	Training Loss: 1.084885 	Validation Loss: 0.278158
Epoch: 483 	Training Loss: 1.075072 	Validation Loss: 0.277820
Epoch: 484 	Training Loss: 1.081011 	Validation Loss: 0.284402
Epoch: 485 	Training Loss: 1.081351 	Validation Loss: 0.281961
Epoch: 486 	Training Loss: 1.083745 	Validation Loss: 0.279679
Epoch: 487 	Training Loss: 1.081245 	Validation Loss: 0.280318
Epoch: 488 	Training Loss: 1.075557 	Validation Loss: 0.278577
Epoch: 489 	Training Loss: 1.079408 	Validation Loss: 0.278910
Epoch: 490 	Training Loss: 1.082496 	Validation Loss: 0.280904
Epoch: 491 	Training Loss: 1.078611 	Validation Loss: 0.277847
Epoch: 492 	Training Loss: 1.087269 	Validation Loss: 0.280784
Epoch: 493 	Training Loss: 1.080308 	Validation Loss: 0.280509
Epoch: 494 	Training Loss: 1.079977 	Validation Loss: 0.280467
Epoch: 495 	Training Loss: 1.071035 	Validation Loss: 0.277071
Epoch: 496 	Training Loss: 1.081492 	Validation Loss: 0.279537
Epoch: 497 	Training Loss: 1.076939 	Validation Loss: 0.277763
Epoch: 498 	Training Loss: 1.076834 	Validation Loss: 0.277170
Epoch: 499 	Training Loss: 1.077066 	Validation Loss: 0.281241
Epoch: 500 	Training Loss: 1.078915 	Validation Loss: 0.278007
Elapsed: 1:41:06.408824
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test(model_2)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Loss: 1.336450

Test Accuracy of airplane: 55% (553/1000)
Test Accuracy of automobile: 58% (583/1000)
Test Accuracy of  bird: 23% (234/1000)
Test Accuracy of   cat: 30% (307/1000)
Test Accuracy of  deer: 36% (365/1000)
Test Accuracy of   dog: 25% (257/1000)
Test Accuracy of  frog: 88% (880/1000)
Test Accuracy of horse: 69% (694/1000)
Test Accuracy of  ship: 76% (766/1000)
Test Accuracy of truck: 61% (611/1000)

Test Accuracy (Overall): 52% (5250/10000)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
figure.suptitle("Filter Size 5 Training/Validation Loss", weight="bold")
x = numpy.arange(len(training_loss_2))
axe.plot(x, training_loss_2, label="Training")
axe.plot(x, validation_loss_2, label="Validation")
axe.set_xlabel("Epoch")
axe.set_ylabel("Cross-Entropy Loss")
labeled = False
for improvement in improvements_2:
    label = "_" if labeled else "Model Improved"
    axe.axvline(improvement, color='r', linestyle='--', label=label)
    labeled = True
legend = axe.legend()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/model_2_training.png" alt="model_2_training.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
It looks like the model from the Pytorch tutorial starts to overfit after the 15th epoch (by count, not index).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2bf08e0" class="outline-3"&gt;
&lt;h3 id="org2bf08e0"&gt;Udacity Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2bf08e0"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_1 = CNN(3)
model_1.to(device)
filename_1, training_loss_1, validation_loss_1, improvements_1  = train(model_1, epochs=30, model_number=1)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 1 	Training Loss: 1.764122 	Validation Loss: 0.408952
Validation loss decreased (inf --&amp;gt; 0.408952).  Saving model ...
Epoch: 2 	Training Loss: 1.586364 	Validation Loss: 0.383241
Validation loss decreased (0.408952 --&amp;gt; 0.383241).  Saving model ...
Epoch: 3 	Training Loss: 1.519929 	Validation Loss: 0.371740
Validation loss decreased (0.383241 --&amp;gt; 0.371740).  Saving model ...
Epoch: 4 	Training Loss: 1.488349 	Validation Loss: 0.362653
Validation loss decreased (0.371740 --&amp;gt; 0.362653).  Saving model ...
Epoch: 5 	Training Loss: 1.455125 	Validation Loss: 0.358624
Validation loss decreased (0.362653 --&amp;gt; 0.358624).  Saving model ...
Epoch: 6 	Training Loss: 1.431836 	Validation Loss: 0.353852
Validation loss decreased (0.358624 --&amp;gt; 0.353852).  Saving model ...
Epoch: 7 	Training Loss: 1.406383 	Validation Loss: 0.351643
Validation loss decreased (0.353852 --&amp;gt; 0.351643).  Saving model ...
Epoch: 8 	Training Loss: 1.396167 	Validation Loss: 0.342488
Validation loss decreased (0.351643 --&amp;gt; 0.342488).  Saving model ...
Epoch: 9 	Training Loss: 1.374800 	Validation Loss: 0.344513
Epoch: 10 	Training Loss: 1.365321 	Validation Loss: 0.339705
Validation loss decreased (0.342488 --&amp;gt; 0.339705).  Saving model ...
Epoch: 11 	Training Loss: 1.350646 	Validation Loss: 0.334100
Validation loss decreased (0.339705 --&amp;gt; 0.334100).  Saving model ...
Epoch: 12 	Training Loss: 1.336463 	Validation Loss: 0.342720
Epoch: 13 	Training Loss: 1.327740 	Validation Loss: 0.329569
Validation loss decreased (0.334100 --&amp;gt; 0.329569).  Saving model ...
Epoch: 14 	Training Loss: 1.318054 	Validation Loss: 0.330011
Epoch: 15 	Training Loss: 1.318000 	Validation Loss: 0.331113
Epoch: 16 	Training Loss: 1.307698 	Validation Loss: 0.325177
Validation loss decreased (0.329569 --&amp;gt; 0.325177).  Saving model ...
Epoch: 17 	Training Loss: 1.300564 	Validation Loss: 0.324221
Validation loss decreased (0.325177 --&amp;gt; 0.324221).  Saving model ...
Epoch: 18 	Training Loss: 1.298909 	Validation Loss: 0.323380
Validation loss decreased (0.324221 --&amp;gt; 0.323380).  Saving model ...
Epoch: 19 	Training Loss: 1.284629 	Validation Loss: 0.317989
Validation loss decreased (0.323380 --&amp;gt; 0.317989).  Saving model ...
Epoch: 20 	Training Loss: 1.284566 	Validation Loss: 0.316856
Validation loss decreased (0.317989 --&amp;gt; 0.316856).  Saving model ...
Epoch: 21 	Training Loss: 1.276280 	Validation Loss: 0.320113
Epoch: 22 	Training Loss: 1.274713 	Validation Loss: 0.320777
Epoch: 23 	Training Loss: 1.267952 	Validation Loss: 0.317876
Epoch: 24 	Training Loss: 1.270328 	Validation Loss: 0.311076
Validation loss decreased (0.316856 --&amp;gt; 0.311076).  Saving model ...
Epoch: 25 	Training Loss: 1.258179 	Validation Loss: 0.313508
Epoch: 26 	Training Loss: 1.253091 	Validation Loss: 0.314421
Epoch: 27 	Training Loss: 1.254100 	Validation Loss: 0.312774
Epoch: 28 	Training Loss: 1.244802 	Validation Loss: 0.311225
Epoch: 29 	Training Loss: 1.242637 	Validation Loss: 0.310512
Validation loss decreased (0.311076 --&amp;gt; 0.310512).  Saving model ...
Epoch: 30 	Training Loss: 1.245316 	Validation Loss: 0.311031
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
figure.suptitle("Filter Size 3 Training/Validation Loss", weight="bold")
x = numpy.arange(len(training_loss_1))
axe.plot(x, training_loss_1, label="Training")
axe.plot(x, validation_loss_1, label="Validation")
axe.set_xlabel("Epoch")
axe.set_ylabel("Cross-Entropy Loss")
labeled = False
for improvement in improvements_1:
    label = "_" if labeled else "Model Improved"
    axe.axvline(improvement, color='r', linestyle='--', label=label)
    labeled = True
legend = axe.legend()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/model_1_training.png" alt="model_1_training.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So it looks like there isn't much difference between the models, but the filter size of 3 did slightly better.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org018e3eb" class="outline-2"&gt;
&lt;h2 id="org018e3eb"&gt;Load the Model with the Lowest Validation Loss&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org018e3eb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_2.load_state_dict(torch.load(outcome["hyperparameters_file"]))
best_model = model_2
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0753a1e" class="outline-2"&gt;
&lt;h2 id="org0753a1e"&gt;Test the Trained Network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0753a1e"&gt;
&lt;p&gt;
Test your trained model on previously unseen data! A "good" result will be a CNN that gets around 70% (or more, try your best!) accuracy on these test images.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def test(best_model):
    criterion = nn.CrossEntropyLoss()
    # track test loss
    test_loss = 0.0
    class_correct = list(0. for i in range(10))
    class_total = list(0. for i in range(10))

    best_model.to(device)
    best_model.eval()
    # iterate over test data
    for data, target in test_loader:
	# move tensors to GPU if CUDA is available
	data, target = data.to(device), target.to(device)
	# forward pass: compute predicted outputs by passing inputs to the model
	output = best_model(data)
	# calculate the batch loss
	loss = criterion(output, target)
	# update test loss 
	test_loss += loss.item() * data.size(0)
	# convert output probabilities to predicted class
	_, pred = torch.max(output, 1)    
	# compare predictions to true label
	correct_tensor = pred.eq(target.data.view_as(pred))
	correct = (
	    numpy.squeeze(correct_tensor.numpy())
	    if not train_on_gpu
	    else numpy.squeeze(correct_tensor.cpu().numpy()))
	# calculate test accuracy for each object class
	for i in range(BATCH_SIZE):
	    label = target.data[i]
	    class_correct[label] += correct[i].item()
	    class_total[label] += 1

    # average test loss
    test_loss = test_loss/len(test_loader.dataset)
    print('Test Loss: {:.6f}\n'.format(test_loss))

    for i in range(10):
	if class_total[i] &amp;gt; 0:
	    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
		classes[i], 100 * class_correct[i] / class_total[i],
		numpy.sum(class_correct[i]), numpy.sum(class_total[i])))
	else:
	    print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

    print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
	100. * numpy.sum(class_correct) / numpy.sum(class_total),
	numpy.sum(class_correct), numpy.sum(class_total)))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
dataiter = iter(test_loader)
images, labels = dataiter.next()
images.numpy()
&lt;/p&gt;

&lt;p&gt;
if train_on_gpu:
    images = images.cuda()
&lt;/p&gt;

&lt;p&gt;
output = model(images)
&lt;/p&gt;

&lt;p&gt;
_, preds_tensor = torch.max(output, 1)
preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())
&lt;/p&gt;

&lt;p&gt;
fig = plt.figure(figsize=(25, 4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    imshow(images[idx])
    ax.set_title("{} ({})".format(classes[preds[idx]], classes[labels[idx]]),
                 color=("green" if preds[idx]==labels[idx].item() else "red"))
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbf3c4cc" class="outline-2"&gt;
&lt;h2 id="orgbf3c4cc"&gt;Make it Easier&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgbf3c4cc"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;means = deviations = (0.5, 0.5, 0.5)
train_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(means, deviations)
    ])
test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(means,
			 deviations)])
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_data = datasets.CIFAR10(path.folder, train=True,
			      download=True, transform=train_transform)
test_data = datasets.CIFAR10(path.folder, train=False,
			     download=True, transform=test_transforms)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Files already downloaded and verified
Files already downloaded and verified
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;indices = list(range(len(training_data)))
training_indices, validation_indices = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_sampler = SubsetRandomSampler(training_indices)
valid_sampler = SubsetRandomSampler(validation_indices)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE,
    sampler=train_sampler, num_workers=NUM_WORKERS)
valid_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE, 
    sampler=valid_sampler, num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, 
    num_workers=NUM_WORKERS)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def load_and_train(model_number:int=3, epochs:int=100) -&amp;gt; dict:
    """Load the model using hyperparameters in the dict

    Args:
     model_number: identifier for the model (and its pickles)
     epochs: how many times to repeat training

    Returns:
     outcome: trained model and outcome dict
    """
    model = CNN()
    model = model.to(device)
    start = datetime.now()
    outcome = train_and_pickle(
	model,
	epochs=epochs,
	model_number=model_number)
    model.load_state_dict(torch.load(outcome["hyperparameters_file"],
				     map_location=device))
    test(model)
    ended = datetime.now()
    print("Ended: {}".format(ended))
    print("Elapsed: {}".format(ended - start))
    return outcome
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_3 = CNN()
model_3.to(device)
start = datetime.now()
outcome = train_and_pickle(
    model_3,
    epochs=100,
    model_number=3)
print("Elapsed: {}".format(datetime.now() - start))
model_3.load_state_dict(torch.load(outcome["hyperparameters_file"],
				   map_location=device))
test(model_3)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 404 	Training Loss: 1.766535 	Validation Loss: 0.404297
Validation loss decreased (inf --&amp;gt; 0.404297).  Saving model ...
Epoch: 405 	Training Loss: 1.539740 	Validation Loss: 0.351437
Validation loss decreased (0.404297 --&amp;gt; 0.351437).  Saving model ...
Epoch: 406 	Training Loss: 1.408876 	Validation Loss: 0.327341
Validation loss decreased (0.351437 --&amp;gt; 0.327341).  Saving model ...
Epoch: 407 	Training Loss: 1.325226 	Validation Loss: 0.303032
Validation loss decreased (0.327341 --&amp;gt; 0.303032).  Saving model ...
Epoch: 408 	Training Loss: 1.260864 	Validation Loss: 0.291623
Validation loss decreased (0.303032 --&amp;gt; 0.291623).  Saving model ...
Epoch: 409 	Training Loss: 1.214102 	Validation Loss: 0.283056
Validation loss decreased (0.291623 --&amp;gt; 0.283056).  Saving model ...
Epoch: 410 	Training Loss: 1.178166 	Validation Loss: 0.275751
Validation loss decreased (0.283056 --&amp;gt; 0.275751).  Saving model ...
Epoch: 411 	Training Loss: 1.146879 	Validation Loss: 0.264309
Validation loss decreased (0.275751 --&amp;gt; 0.264309).  Saving model ...
Epoch: 412 	Training Loss: 1.121644 	Validation Loss: 0.258764
Validation loss decreased (0.264309 --&amp;gt; 0.258764).  Saving model ...
Epoch: 413 	Training Loss: 1.097969 	Validation Loss: 0.252846
Validation loss decreased (0.258764 --&amp;gt; 0.252846).  Saving model ...
Epoch: 414 	Training Loss: 1.078815 	Validation Loss: 0.250729
Validation loss decreased (0.252846 --&amp;gt; 0.250729).  Saving model ...
Epoch: 415 	Training Loss: 1.055899 	Validation Loss: 0.241823
Validation loss decreased (0.250729 --&amp;gt; 0.241823).  Saving model ...
Epoch: 416 	Training Loss: 1.041387 	Validation Loss: 0.238933
Validation loss decreased (0.241823 --&amp;gt; 0.238933).  Saving model ...
Epoch: 417 	Training Loss: 1.029270 	Validation Loss: 0.234940
Validation loss decreased (0.238933 --&amp;gt; 0.234940).  Saving model ...
Epoch: 418 	Training Loss: 1.016113 	Validation Loss: 0.232727
Validation loss decreased (0.234940 --&amp;gt; 0.232727).  Saving model ...
Epoch: 419 	Training Loss: 1.005521 	Validation Loss: 0.226466
Validation loss decreased (0.232727 --&amp;gt; 0.226466).  Saving model ...
Epoch: 420 	Training Loss: 0.992684 	Validation Loss: 0.226542
Epoch: 421 	Training Loss: 0.978596 	Validation Loss: 0.225691
Validation loss decreased (0.226466 --&amp;gt; 0.225691).  Saving model ...
Epoch: 422 	Training Loss: 0.976063 	Validation Loss: 0.228258
Epoch: 423 	Training Loss: 0.961974 	Validation Loss: 0.221933
Validation loss decreased (0.225691 --&amp;gt; 0.221933).  Saving model ...
Epoch: 424 	Training Loss: 0.954803 	Validation Loss: 0.220159
Validation loss decreased (0.221933 --&amp;gt; 0.220159).  Saving model ...
Epoch: 425 	Training Loss: 0.948879 	Validation Loss: 0.219641
Validation loss decreased (0.220159 --&amp;gt; 0.219641).  Saving model ...
Epoch: 426 	Training Loss: 0.945494 	Validation Loss: 0.220472
Epoch: 427 	Training Loss: 0.935160 	Validation Loss: 0.215726
Validation loss decreased (0.219641 --&amp;gt; 0.215726).  Saving model ...
Epoch: 428 	Training Loss: 0.928077 	Validation Loss: 0.215445
Validation loss decreased (0.215726 --&amp;gt; 0.215445).  Saving model ...
Epoch: 429 	Training Loss: 0.925603 	Validation Loss: 0.212353
Validation loss decreased (0.215445 --&amp;gt; 0.212353).  Saving model ...
Epoch: 430 	Training Loss: 0.921984 	Validation Loss: 0.208420
Validation loss decreased (0.212353 --&amp;gt; 0.208420).  Saving model ...
Epoch: 431 	Training Loss: 0.912180 	Validation Loss: 0.218620
Epoch: 432 	Training Loss: 0.909916 	Validation Loss: 0.208612
Epoch: 433 	Training Loss: 0.902665 	Validation Loss: 0.208177
Validation loss decreased (0.208420 --&amp;gt; 0.208177).  Saving model ...
Epoch: 434 	Training Loss: 0.899616 	Validation Loss: 0.210920
Epoch: 435 	Training Loss: 0.895718 	Validation Loss: 0.212328
Epoch: 436 	Training Loss: 0.883933 	Validation Loss: 0.204341
Validation loss decreased (0.208177 --&amp;gt; 0.204341).  Saving model ...
Epoch: 437 	Training Loss: 0.888972 	Validation Loss: 0.206792
Epoch: 438 	Training Loss: 0.878481 	Validation Loss: 0.204317
Validation loss decreased (0.204341 --&amp;gt; 0.204317).  Saving model ...
Epoch: 439 	Training Loss: 0.879559 	Validation Loss: 0.204447
Epoch: 440 	Training Loss: 0.871985 	Validation Loss: 0.203039
Validation loss decreased (0.204317 --&amp;gt; 0.203039).  Saving model ...
Epoch: 441 	Training Loss: 0.870123 	Validation Loss: 0.202717
Validation loss decreased (0.203039 --&amp;gt; 0.202717).  Saving model ...
Epoch: 442 	Training Loss: 0.870877 	Validation Loss: 0.201654
Validation loss decreased (0.202717 --&amp;gt; 0.201654).  Saving model ...
Epoch: 443 	Training Loss: 0.863020 	Validation Loss: 0.204858
Epoch: 444 	Training Loss: 0.861419 	Validation Loss: 0.202981
Epoch: 445 	Training Loss: 0.864864 	Validation Loss: 0.200853
Validation loss decreased (0.201654 --&amp;gt; 0.200853).  Saving model ...
Epoch: 446 	Training Loss: 0.859879 	Validation Loss: 0.202888
Epoch: 447 	Training Loss: 0.859062 	Validation Loss: 0.199505
Validation loss decreased (0.200853 --&amp;gt; 0.199505).  Saving model ...
Epoch: 448 	Training Loss: 0.853924 	Validation Loss: 0.196931
Validation loss decreased (0.199505 --&amp;gt; 0.196931).  Saving model ...
Epoch: 449 	Training Loss: 0.849512 	Validation Loss: 0.201266
Epoch: 450 	Training Loss: 0.845482 	Validation Loss: 0.196021
Validation loss decreased (0.196931 --&amp;gt; 0.196021).  Saving model ...
Epoch: 451 	Training Loss: 0.844360 	Validation Loss: 0.195308
Validation loss decreased (0.196021 --&amp;gt; 0.195308).  Saving model ...
Epoch: 452 	Training Loss: 0.844023 	Validation Loss: 0.197164
Epoch: 453 	Training Loss: 0.839186 	Validation Loss: 0.194882
Validation loss decreased (0.195308 --&amp;gt; 0.194882).  Saving model ...
Epoch: 454 	Training Loss: 0.838193 	Validation Loss: 0.198097
Epoch: 455 	Training Loss: 0.837155 	Validation Loss: 0.197095
Epoch: 456 	Training Loss: 0.831614 	Validation Loss: 0.195633
Epoch: 457 	Training Loss: 0.827912 	Validation Loss: 0.195327
Epoch: 458 	Training Loss: 0.830631 	Validation Loss: 0.192197
Validation loss decreased (0.194882 --&amp;gt; 0.192197).  Saving model ...
Epoch: 459 	Training Loss: 0.825767 	Validation Loss: 0.195351
Epoch: 460 	Training Loss: 0.824248 	Validation Loss: 0.192982
Epoch: 461 	Training Loss: 0.822047 	Validation Loss: 0.191864
Validation loss decreased (0.192197 --&amp;gt; 0.191864).  Saving model ...
Epoch: 462 	Training Loss: 0.824057 	Validation Loss: 0.191095
Validation loss decreased (0.191864 --&amp;gt; 0.191095).  Saving model ...
Epoch: 463 	Training Loss: 0.821909 	Validation Loss: 0.190179
Validation loss decreased (0.191095 --&amp;gt; 0.190179).  Saving model ...
Epoch: 464 	Training Loss: 0.820941 	Validation Loss: 0.193425
Epoch: 465 	Training Loss: 0.820359 	Validation Loss: 0.193750
Epoch: 466 	Training Loss: 0.815640 	Validation Loss: 0.194663
Epoch: 467 	Training Loss: 0.818372 	Validation Loss: 0.192682
Epoch: 468 	Training Loss: 0.817113 	Validation Loss: 0.192452
Epoch: 469 	Training Loss: 0.817581 	Validation Loss: 0.196727
Epoch: 470 	Training Loss: 0.809651 	Validation Loss: 0.190927
Epoch: 471 	Training Loss: 0.811329 	Validation Loss: 0.194151
Epoch: 472 	Training Loss: 0.806093 	Validation Loss: 0.192417
Epoch: 473 	Training Loss: 0.806517 	Validation Loss: 0.189602
Validation loss decreased (0.190179 --&amp;gt; 0.189602).  Saving model ...
Epoch: 474 	Training Loss: 0.807954 	Validation Loss: 0.191487
Epoch: 475 	Training Loss: 0.807010 	Validation Loss: 0.191636
Epoch: 476 	Training Loss: 0.801799 	Validation Loss: 0.190896
Epoch: 477 	Training Loss: 0.798797 	Validation Loss: 0.187708
Validation loss decreased (0.189602 --&amp;gt; 0.187708).  Saving model ...
Epoch: 478 	Training Loss: 0.799128 	Validation Loss: 0.189194
Epoch: 479 	Training Loss: 0.799459 	Validation Loss: 0.194036
Epoch: 480 	Training Loss: 0.795995 	Validation Loss: 0.190724
Epoch: 481 	Training Loss: 0.798655 	Validation Loss: 0.190721
Epoch: 482 	Training Loss: 0.792206 	Validation Loss: 0.188309
Epoch: 483 	Training Loss: 0.799025 	Validation Loss: 0.187985
Epoch: 484 	Training Loss: 0.791694 	Validation Loss: 0.186556
Validation loss decreased (0.187708 --&amp;gt; 0.186556).  Saving model ...
Epoch: 485 	Training Loss: 0.784249 	Validation Loss: 0.184879
Validation loss decreased (0.186556 --&amp;gt; 0.184879).  Saving model ...
Epoch: 486 	Training Loss: 0.793165 	Validation Loss: 0.185806
Epoch: 487 	Training Loss: 0.791051 	Validation Loss: 0.189010
Epoch: 488 	Training Loss: 0.787608 	Validation Loss: 0.186931
Epoch: 489 	Training Loss: 0.789344 	Validation Loss: 0.195780
Epoch: 490 	Training Loss: 0.792061 	Validation Loss: 0.191099
Epoch: 491 	Training Loss: 0.786356 	Validation Loss: 0.189476
Epoch: 492 	Training Loss: 0.784223 	Validation Loss: 0.192026
Epoch: 493 	Training Loss: 0.785188 	Validation Loss: 0.189652
Epoch: 494 	Training Loss: 0.782519 	Validation Loss: 0.188833
Epoch: 495 	Training Loss: 0.786059 	Validation Loss: 0.192020
Epoch: 496 	Training Loss: 0.782317 	Validation Loss: 0.187162
Epoch: 497 	Training Loss: 0.785475 	Validation Loss: 0.191352
Epoch: 498 	Training Loss: 0.778186 	Validation Loss: 0.193208
Epoch: 499 	Training Loss: 0.780198 	Validation Loss: 0.190525
Epoch: 500 	Training Loss: 0.778074 	Validation Loss: 0.194126
Epoch: 501 	Training Loss: 0.778832 	Validation Loss: 0.186440
Epoch: 502 	Training Loss: 0.776556 	Validation Loss: 0.188577
Epoch: 503 	Training Loss: 0.774062 	Validation Loss: 0.190385
Epoch: 504 	Training Loss: 0.776408 	Validation Loss: 0.188763
Elapsed: 0:28:16.205032
Test Loss: 0.925722

Test Accuracy of airplane: 70% (703/1000)
Test Accuracy of automobile: 75% (753/1000)
Test Accuracy of  bird: 47% (470/1000)
Test Accuracy of   cat: 56% (562/1000)
Test Accuracy of  deer: 69% (697/1000)
Test Accuracy of   dog: 53% (536/1000)
Test Accuracy of  frog: 80% (803/1000)
Test Accuracy of horse: 67% (670/1000)
Test Accuracy of  ship: 82% (825/1000)
Test Accuracy of truck: 75% (756/1000)

Test Accuracy (Overall): 67% (6775/10000)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test(model_3)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Loss: 0.954966

Test Accuracy of airplane: 62% (629/1000)
Test Accuracy of automobile: 76% (766/1000)
Test Accuracy of  bird: 50% (506/1000)
Test Accuracy of   cat: 44% (449/1000)
Test Accuracy of  deer: 66% (666/1000)
Test Accuracy of   dog: 52% (521/1000)
Test Accuracy of  frog: 82% (827/1000)
Test Accuracy of horse: 72% (721/1000)
Test Accuracy of  ship: 82% (829/1000)
Test Accuracy of truck: 67% (672/1000)

Test Accuracy (Overall): 65% (6586/10000)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;outcome = load_and_train(outcome)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Validation loss decreased (inf --&amp;gt; 0.396734).  Saving model ...
Validation loss decreased (0.396734 --&amp;gt; 0.353406).  Saving model ...
Validation loss decreased (0.353406 --&amp;gt; 0.312288).  Saving model ...
Validation loss decreased (0.312288 --&amp;gt; 0.292421).  Saving model ...
Validation loss decreased (0.292421 --&amp;gt; 0.281344).  Saving model ...
Validation loss decreased (0.281344 --&amp;gt; 0.267129).  Saving model ...
Validation loss decreased (0.267129 --&amp;gt; 0.259950).  Saving model ...
Validation loss decreased (0.259950 --&amp;gt; 0.255096).  Saving model ...
Validation loss decreased (0.255096 --&amp;gt; 0.249626).  Saving model ...
Epoch: 110 	Training Loss: 1.074378 	Validation Loss: 0.241995
Validation loss decreased (0.249626 --&amp;gt; 0.241995).  Saving model ...
Validation loss decreased (0.241995 --&amp;gt; 0.234253).  Saving model ...
Validation loss decreased (0.234253 --&amp;gt; 0.233744).  Saving model ...
Validation loss decreased (0.233744 --&amp;gt; 0.226195).  Saving model ...
Validation loss decreased (0.226195 --&amp;gt; 0.225804).  Saving model ...
Validation loss decreased (0.225804 --&amp;gt; 0.223489).  Saving model ...
Validation loss decreased (0.223489 --&amp;gt; 0.221263).  Saving model ...
Validation loss decreased (0.221263 --&amp;gt; 0.217546).  Saving model ...
Validation loss decreased (0.217546 --&amp;gt; 0.215720).  Saving model ...
Validation loss decreased (0.215720 --&amp;gt; 0.213332).  Saving model ...
Epoch: 120 	Training Loss: 0.952941 	Validation Loss: 0.209708
Validation loss decreased (0.213332 --&amp;gt; 0.209708).  Saving model ...
Validation loss decreased (0.209708 --&amp;gt; 0.207232).  Saving model ...
Validation loss decreased (0.207232 --&amp;gt; 0.205873).  Saving model ...
Validation loss decreased (0.205873 --&amp;gt; 0.199750).  Saving model ...
Epoch: 130 	Training Loss: 0.898597 	Validation Loss: 0.197858
Validation loss decreased (0.199750 --&amp;gt; 0.197858).  Saving model ...
Validation loss decreased (0.197858 --&amp;gt; 0.195818).  Saving model ...
Validation loss decreased (0.195818 --&amp;gt; 0.194920).  Saving model ...
Validation loss decreased (0.194920 --&amp;gt; 0.194267).  Saving model ...
Validation loss decreased (0.194267 --&amp;gt; 0.193904).  Saving model ...
Epoch: 140 	Training Loss: 0.856769 	Validation Loss: 0.203387
Validation loss decreased (0.193904 --&amp;gt; 0.187780).  Saving model ...
Epoch: 150 	Training Loss: 0.842055 	Validation Loss: 0.190620
Validation loss decreased (0.187780 --&amp;gt; 0.186874).  Saving model ...
Validation loss decreased (0.186874 --&amp;gt; 0.183554).  Saving model ...
Epoch: 160 	Training Loss: 0.821771 	Validation Loss: 0.186012
Validation loss decreased (0.183554 --&amp;gt; 0.183435).  Saving model ...
Validation loss decreased (0.183435 --&amp;gt; 0.183237).  Saving model ...
Epoch: 170 	Training Loss: 0.807321 	Validation Loss: 0.185445
Epoch: 180 	Training Loss: 0.796137 	Validation Loss: 0.182606
Validation loss decreased (0.183237 --&amp;gt; 0.182606).  Saving model ...
Validation loss decreased (0.182606 --&amp;gt; 0.180978).  Saving model ...
Validation loss decreased (0.180978 --&amp;gt; 0.179344).  Saving model ...
Epoch: 190 	Training Loss: 0.792454 	Validation Loss: 0.181462
Epoch: 200 	Training Loss: 0.777160 	Validation Loss: 0.187384
Ended: 2018-12-14 15:45:54.887063
Elapsed: 1:09:07.537337
Test Loss: 0.913029

Test Accuracy of airplane: 71% (715/1000)
Test Accuracy of automobile: 80% (803/1000)
Test Accuracy of  bird: 44% (445/1000)
Test Accuracy of   cat: 49% (496/1000)
Test Accuracy of  deer: 73% (733/1000)
Test Accuracy of   dog: 54% (540/1000)
Test Accuracy of  frog: 78% (788/1000)
Test Accuracy of horse: 74% (747/1000)
Test Accuracy of  ship: 84% (840/1000)
Test Accuracy of truck: 75% (750/1000)

Test Accuracy (Overall): 68% (6857/10000)
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;outcome = load_and_train(outcome)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Validation loss decreased (inf --&amp;gt; 0.405255).  Saving model ...
Validation loss decreased (0.405255 --&amp;gt; 0.347472).  Saving model ...
Validation loss decreased (0.347472 --&amp;gt; 0.318586).  Saving model ...
Validation loss decreased (0.318586 --&amp;gt; 0.301050).  Saving model ...
Validation loss decreased (0.301050 --&amp;gt; 0.287208).  Saving model ...
Validation loss decreased (0.287208 --&amp;gt; 0.279541).  Saving model ...
Epoch: 410 	Training Loss: 1.188729 	Validation Loss: 0.269707
Validation loss decreased (0.279541 --&amp;gt; 0.269707).  Saving model ...
Validation loss decreased (0.269707 --&amp;gt; 0.262379).  Saving model ...
Validation loss decreased (0.262379 --&amp;gt; 0.254531).  Saving model ...
Validation loss decreased (0.254531 --&amp;gt; 0.252625).  Saving model ...
Validation loss decreased (0.252625 --&amp;gt; 0.240125).  Saving model ...
Validation loss decreased (0.240125 --&amp;gt; 0.235959).  Saving model ...
Validation loss decreased (0.235959 --&amp;gt; 0.234190).  Saving model ...
Validation loss decreased (0.234190 --&amp;gt; 0.231890).  Saving model ...
Validation loss decreased (0.231890 --&amp;gt; 0.226316).  Saving model ...
Epoch: 420 	Training Loss: 1.003592 	Validation Loss: 0.228944
Validation loss decreased (0.226316 --&amp;gt; 0.224643).  Saving model ...
Validation loss decreased (0.224643 --&amp;gt; 0.222303).  Saving model ...
Validation loss decreased (0.222303 --&amp;gt; 0.221804).  Saving model ...
Validation loss decreased (0.221804 --&amp;gt; 0.219019).  Saving model ...
Validation loss decreased (0.219019 --&amp;gt; 0.211782).  Saving model ...
Epoch: 430 	Training Loss: 0.933949 	Validation Loss: 0.211028
Validation loss decreased (0.211782 --&amp;gt; 0.211028).  Saving model ...
Validation loss decreased (0.211028 --&amp;gt; 0.210736).  Saving model ...
Validation loss decreased (0.210736 --&amp;gt; 0.207784).  Saving model ...
Validation loss decreased (0.207784 --&amp;gt; 0.204068).  Saving model ...
Validation loss decreased (0.204068 --&amp;gt; 0.202933).  Saving model ...
Validation loss decreased (0.202933 --&amp;gt; 0.201327).  Saving model ...
Epoch: 440 	Training Loss: 0.893833 	Validation Loss: 0.201305
Validation loss decreased (0.201327 --&amp;gt; 0.201305).  Saving model ...
Validation loss decreased (0.201305 --&amp;gt; 0.200246).  Saving model ...
Validation loss decreased (0.200246 --&amp;gt; 0.199212).  Saving model ...
Validation loss decreased (0.199212 --&amp;gt; 0.198127).  Saving model ...
Validation loss decreased (0.198127 --&amp;gt; 0.197780).  Saving model ...
Epoch: 450 	Training Loss: 0.869887 	Validation Loss: 0.193194
Validation loss decreased (0.197780 --&amp;gt; 0.193194).  Saving model ...
Validation loss decreased (0.193194 --&amp;gt; 0.192689).  Saving model ...
Validation loss decreased (0.192689 --&amp;gt; 0.191178).  Saving model ...
Epoch: 460 	Training Loss: 0.845976 	Validation Loss: 0.193641
Validation loss decreased (0.191178 --&amp;gt; 0.190434).  Saving model ...
Epoch: 470 	Training Loss: 0.831866 	Validation Loss: 0.190801
Validation loss decreased (0.190434 --&amp;gt; 0.189088).  Saving model ...
Epoch: 480 	Training Loss: 0.814776 	Validation Loss: 0.190064
Validation loss decreased (0.189088 --&amp;gt; 0.189077).  Saving model ...
Validation loss decreased (0.189077 --&amp;gt; 0.188256).  Saving model ...
Validation loss decreased (0.188256 --&amp;gt; 0.185333).  Saving model ...
Epoch: 490 	Training Loss: 0.804873 	Validation Loss: 0.190370
Epoch: 500 	Training Loss: 0.792694 	Validation Loss: 0.188568
Ended: 2018-12-14 22:04:39.786682
Elapsed: 0:28:03.912152
Test Loss: 0.933276

Test Accuracy of airplane: 75% (756/1000)
Test Accuracy of automobile: 74% (744/1000)
Test Accuracy of  bird: 48% (482/1000)
Test Accuracy of   cat: 44% (443/1000)
Test Accuracy of  deer: 68% (686/1000)
Test Accuracy of   dog: 50% (502/1000)
Test Accuracy of  frog: 80% (801/1000)
Test Accuracy of horse: 68% (681/1000)
Test Accuracy of  ship: 82% (823/1000)
Test Accuracy of truck: 77% (770/1000)

Test Accuracy (Overall): 66% (6688/10000)
&lt;/pre&gt;

&lt;p&gt;
The overall test-accuracy is going down - is it overfitting?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with open("model_3_outcomes.pkl", "rb") as reader:
    outcome = pickle.load(reader)

outcome = load_and_train(outcome)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Validation loss decreased (inf --&amp;gt; 0.400317).  Saving model ...
Validation loss decreased (0.400317 --&amp;gt; 0.339392).  Saving model ...
Epoch: 810 	Training Loss: 1.361320 	Validation Loss: 0.310385
Validation loss decreased (0.339392 --&amp;gt; 0.310385).  Saving model ...
Validation loss decreased (0.310385 --&amp;gt; 0.295311).  Saving model ...
Validation loss decreased (0.295311 --&amp;gt; 0.283410).  Saving model ...
Validation loss decreased (0.283410 --&amp;gt; 0.274456).  Saving model ...
Validation loss decreased (0.274456 --&amp;gt; 0.266069).  Saving model ...
Validation loss decreased (0.266069 --&amp;gt; 0.262745).  Saving model ...
Validation loss decreased (0.262745 --&amp;gt; 0.247262).  Saving model ...
Validation loss decreased (0.247262 --&amp;gt; 0.237769).  Saving model ...
Epoch: 820 	Training Loss: 1.028606 	Validation Loss: 0.236005
Validation loss decreased (0.237769 --&amp;gt; 0.236005).  Saving model ...
Validation loss decreased (0.236005 --&amp;gt; 0.230968).  Saving model ...
Validation loss decreased (0.230968 --&amp;gt; 0.228058).  Saving model ...
Validation loss decreased (0.228058 --&amp;gt; 0.224573).  Saving model ...
Validation loss decreased (0.224573 --&amp;gt; 0.223884).  Saving model ...
Validation loss decreased (0.223884 --&amp;gt; 0.219913).  Saving model ...
Validation loss decreased (0.219913 --&amp;gt; 0.217769).  Saving model ...
Epoch: 830 	Training Loss: 0.942998 	Validation Loss: 0.215061
Validation loss decreased (0.217769 --&amp;gt; 0.215061).  Saving model ...
Validation loss decreased (0.215061 --&amp;gt; 0.212656).  Saving model ...
Validation loss decreased (0.212656 --&amp;gt; 0.212616).  Saving model ...
Validation loss decreased (0.212616 --&amp;gt; 0.210596).  Saving model ...
Validation loss decreased (0.210596 --&amp;gt; 0.207554).  Saving model ...
Epoch: 840 	Training Loss: 0.900498 	Validation Loss: 0.208390
Validation loss decreased (0.207554 --&amp;gt; 0.206364).  Saving model ...
Validation loss decreased (0.206364 --&amp;gt; 0.205531).  Saving model ...
Validation loss decreased (0.205531 --&amp;gt; 0.203900).  Saving model ...
Epoch: 850 	Training Loss: 0.872049 	Validation Loss: 0.205466
Validation loss decreased (0.203900 --&amp;gt; 0.198664).  Saving model ...
Validation loss decreased (0.198664 --&amp;gt; 0.196482).  Saving model ...
Validation loss decreased (0.196482 --&amp;gt; 0.195664).  Saving model ...
Epoch: 860 	Training Loss: 0.845757 	Validation Loss: 0.198456
Validation loss decreased (0.195664 --&amp;gt; 0.193952).  Saving model ...
Epoch: 870 	Training Loss: 0.826413 	Validation Loss: 0.195060
Validation loss decreased (0.193952 --&amp;gt; 0.193670).  Saving model ...
Validation loss decreased (0.193670 --&amp;gt; 0.192782).  Saving model ...
Validation loss decreased (0.192782 --&amp;gt; 0.188631).  Saving model ...
Epoch: 880 	Training Loss: 0.818928 	Validation Loss: 0.199424
Epoch: 890 	Training Loss: 0.808009 	Validation Loss: 0.191352
Epoch: 900 	Training Loss: 0.801281 	Validation Loss: 0.196643
Ended: 2018-12-14 22:37:13.843477
Elapsed: 0:29:26.736300
Test Loss: 0.945705

Test Accuracy of airplane: 72% (725/1000)
Test Accuracy of automobile: 78% (782/1000)
Test Accuracy of  bird: 47% (473/1000)
Test Accuracy of   cat: 48% (488/1000)
Test Accuracy of  deer: 70% (705/1000)
Test Accuracy of   dog: 52% (527/1000)
Test Accuracy of  frog: 77% (776/1000)
Test Accuracy of horse: 69% (696/1000)
Test Accuracy of  ship: 84% (844/1000)
Test Accuracy of truck: 70% (703/1000)

Test Accuracy (Overall): 67% (6719/10000)
&lt;/pre&gt;

&lt;p&gt;
It looks like the overall accuracy dropped slightly beacause the best categories (truck, ship, frog) did worse but the worst categories did slightly better - although not bird for some reason.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2829f7d" class="outline-2"&gt;
&lt;h2 id="org2829f7d"&gt;Take two&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2829f7d"&gt;
&lt;p&gt;
It looks like I wasn't loading the model between each round of epochsâ¦
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;outcome = load_and_train(model_number=4, epochs=200)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 0 	Training Loss: 1.784692 	Validation Loss: 0.410727
Validation loss decreased (inf --&amp;gt; 0.410727).  Saving model ...
Validation loss decreased (0.410727 --&amp;gt; 0.360800).  Saving model ...
Validation loss decreased (0.360800 --&amp;gt; 0.314237).  Saving model ...
Validation loss decreased (0.314237 --&amp;gt; 0.293987).  Saving model ...
Validation loss decreased (0.293987 --&amp;gt; 0.283064).  Saving model ...
Validation loss decreased (0.283064 --&amp;gt; 0.275761).  Saving model ...
Validation loss decreased (0.275761 --&amp;gt; 0.270119).  Saving model ...
Validation loss decreased (0.270119 --&amp;gt; 0.261688).  Saving model ...
Validation loss decreased (0.261688 --&amp;gt; 0.254598).  Saving model ...
Epoch: 10 	Training Loss: 1.092668 	Validation Loss: 0.254406
Validation loss decreased (0.254598 --&amp;gt; 0.254406).  Saving model ...
Validation loss decreased (0.254406 --&amp;gt; 0.248653).  Saving model ...
Validation loss decreased (0.248653 --&amp;gt; 0.245797).  Saving model ...
Validation loss decreased (0.245797 --&amp;gt; 0.240849).  Saving model ...
Validation loss decreased (0.240849 --&amp;gt; 0.238558).  Saving model ...
Validation loss decreased (0.238558 --&amp;gt; 0.237812).  Saving model ...
Validation loss decreased (0.237812 --&amp;gt; 0.230956).  Saving model ...
Epoch: 20 	Training Loss: 0.991010 	Validation Loss: 0.225704
Validation loss decreased (0.230956 --&amp;gt; 0.225704).  Saving model ...
Validation loss decreased (0.225704 --&amp;gt; 0.221112).  Saving model ...
Validation loss decreased (0.221112 --&amp;gt; 0.218632).  Saving model ...
Epoch: 30 	Training Loss: 0.938513 	Validation Loss: 0.220019
Validation loss decreased (0.218632 --&amp;gt; 0.216886).  Saving model ...
Validation loss decreased (0.216886 --&amp;gt; 0.215869).  Saving model ...
Validation loss decreased (0.215869 --&amp;gt; 0.214766).  Saving model ...
Validation loss decreased (0.214766 --&amp;gt; 0.212452).  Saving model ...
Epoch: 40 	Training Loss: 0.896510 	Validation Loss: 0.212819
Validation loss decreased (0.212452 --&amp;gt; 0.209142).  Saving model ...
Validation loss decreased (0.209142 --&amp;gt; 0.208595).  Saving model ...
Validation loss decreased (0.208595 --&amp;gt; 0.205967).  Saving model ...
Validation loss decreased (0.205967 --&amp;gt; 0.205484).  Saving model ...
Epoch: 50 	Training Loss: 0.875811 	Validation Loss: 0.207912
Validation loss decreased (0.205484 --&amp;gt; 0.205164).  Saving model ...
Epoch: 60 	Training Loss: 0.856581 	Validation Loss: 0.208312
Validation loss decreased (0.205164 --&amp;gt; 0.204649).  Saving model ...
Validation loss decreased (0.204649 --&amp;gt; 0.203608).  Saving model ...
Epoch: 70 	Training Loss: 0.846062 	Validation Loss: 0.214614
Validation loss decreased (0.203608 --&amp;gt; 0.203064).  Saving model ...
Epoch: 80 	Training Loss: 0.826153 	Validation Loss: 0.212527
Validation loss decreased (0.203064 --&amp;gt; 0.201932).  Saving model ...
Validation loss decreased (0.201932 --&amp;gt; 0.200173).  Saving model ...
Epoch: 90 	Training Loss: 0.823697 	Validation Loss: 0.204494
Validation loss decreased (0.200173 --&amp;gt; 0.199886).  Saving model ...
Validation loss decreased (0.199886 --&amp;gt; 0.198804).  Saving model ...
Epoch: 100 	Training Loss: 0.808043 	Validation Loss: 0.205323
Epoch: 110 	Training Loss: 0.805417 	Validation Loss: 0.201136
Epoch: 120 	Training Loss: 0.805155 	Validation Loss: 0.204370
Epoch: 130 	Training Loss: 0.793174 	Validation Loss: 0.214048
Validation loss decreased (0.198804 --&amp;gt; 0.194650).  Saving model ...
Epoch: 140 	Training Loss: 0.783871 	Validation Loss: 0.200537
Epoch: 150 	Training Loss: 0.781592 	Validation Loss: 0.203295
Epoch: 160 	Training Loss: 0.774657 	Validation Loss: 0.199732
Epoch: 170 	Training Loss: 0.770487 	Validation Loss: 0.205331
Epoch: 180 	Training Loss: 0.767693 	Validation Loss: 0.202990
Epoch: 190 	Training Loss: 0.767225 	Validation Loss: 0.203797
Epoch: 200 	Training Loss: 0.769268 	Validation Loss: 0.196108
Test Loss: 0.974566

Test Accuracy of airplane: 70% (707/1000)
Test Accuracy of automobile: 73% (732/1000)
Test Accuracy of  bird: 45% (453/1000)
Test Accuracy of   cat: 53% (533/1000)
Test Accuracy of  deer: 71% (719/1000)
Test Accuracy of   dog: 42% (429/1000)
Test Accuracy of  frog: 81% (814/1000)
Test Accuracy of horse: 66% (666/1000)
Test Accuracy of  ship: 82% (823/1000)
Test Accuracy of truck: 72% (720/1000)

Test Accuracy (Overall): 65% (6596/10000)
Ended: 2018-12-15 08:33:22.925579
Elapsed: 0:55:24.733457
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;outcome = load_and_train(model_number=4, epochs=200)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Validation loss decreased (inf --&amp;gt; 0.203577).  Saving model ...
Validation loss decreased (0.203577 --&amp;gt; 0.201161).  Saving model ...
Validation loss decreased (0.201161 --&amp;gt; 0.198027).  Saving model ...
Epoch: 210 	Training Loss: 0.785905 	Validation Loss: 0.199885
Epoch: 220 	Training Loss: 0.780148 	Validation Loss: 0.199842
Validation loss decreased (0.198027 --&amp;gt; 0.197471).  Saving model ...
Epoch: 230 	Training Loss: 0.773492 	Validation Loss: 0.206471
Validation loss decreased (0.197471 --&amp;gt; 0.195811).  Saving model ...
Epoch: 240 	Training Loss: 0.777896 	Validation Loss: 0.201046
Epoch: 250 	Training Loss: 0.767602 	Validation Loss: 0.203973
Epoch: 260 	Training Loss: 0.765374 	Validation Loss: 0.205219
Epoch: 270 	Training Loss: 0.764604 	Validation Loss: 0.202613
Epoch: 280 	Training Loss: 0.755534 	Validation Loss: 0.201307
Epoch: 290 	Training Loss: 0.754538 	Validation Loss: 0.199495
Epoch: 300 	Training Loss: 0.759395 	Validation Loss: 0.206451
Epoch: 310 	Training Loss: 0.750621 	Validation Loss: 0.203110
Epoch: 320 	Training Loss: 0.751456 	Validation Loss: 0.206920
Epoch: 330 	Training Loss: 0.747122 	Validation Loss: 0.199856
Epoch: 340 	Training Loss: 0.742640 	Validation Loss: 0.211159
Epoch: 350 	Training Loss: 0.743110 	Validation Loss: 0.214833
Epoch: 360 	Training Loss: 0.741861 	Validation Loss: 0.207520
Epoch: 370 	Training Loss: 0.740826 	Validation Loss: 0.210348
Epoch: 380 	Training Loss: 0.740333 	Validation Loss: 0.207724
Epoch: 390 	Training Loss: 0.739157 	Validation Loss: 0.204985
Epoch: 400 	Training Loss: 0.742582 	Validation Loss: 0.204150
Test Loss: 0.979350

Test Accuracy of airplane: 64% (648/1000)
Test Accuracy of automobile: 75% (751/1000)
Test Accuracy of  bird: 43% (430/1000)
Test Accuracy of   cat: 50% (507/1000)
Test Accuracy of  deer: 76% (766/1000)
Test Accuracy of   dog: 44% (443/1000)
Test Accuracy of  frog: 81% (818/1000)
Test Accuracy of horse: 63% (630/1000)
Test Accuracy of  ship: 86% (868/1000)
Test Accuracy of truck: 68% (680/1000)

Test Accuracy (Overall): 65% (6541/10000)
Ended: 2018-12-15 11:19:36.845565
Elapsed: 0:55:22.008796
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8efe260" class="outline-2"&gt;
&lt;h2 id="org8efe260"&gt;Change the Training and Validation Sets&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8efe260"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;INDICES = list(range(len(training_data)))
DataIterators = (torch.utils.data.dataloader.DataLoader,
		 torch.utils.data.dataloader.DataLoader)

def split_data() -&amp;gt; DataIterators:
    training_indices, validation_indices = train_test_split(
	INDICES,
	test_size=VALIDATION_FRACTION)
    train_sampler = SubsetRandomSampler(training_indices)
    valid_sampler = SubsetRandomSampler(validation_indices)
    train_loader = torch.utils.data.DataLoader(
	training_data, batch_size=BATCH_SIZE,
	sampler=train_sampler, num_workers=NUM_WORKERS)
    valid_loader = torch.utils.data.DataLoader(
	training_data, batch_size=BATCH_SIZE, 
	sampler=valid_sampler, num_workers=NUM_WORKERS)
    return train_loader, valid_loader
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_loader, valid_loader = split_data()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for epoch in range(8):
    outcome = load_and_train(model_number=4, epochs=50)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Validation loss decreased (inf --&amp;gt; 0.178021).  Saving model ...
Validation loss decreased (0.178021 --&amp;gt; 0.164977).  Saving model ...
Epoch: 410 	Training Loss: 0.790843 	Validation Loss: 0.180614
Epoch: 420 	Training Loss: 0.779451 	Validation Loss: 0.184705
Epoch: 430 	Training Loss: 0.776067 	Validation Loss: 0.188225
Epoch: 440 	Training Loss: 0.767443 	Validation Loss: 0.189623
Epoch: 450 	Training Loss: 0.763348 	Validation Loss: 0.190223
Test Loss: 0.994385

Test Accuracy of airplane: 63% (632/1000)
Test Accuracy of automobile: 73% (738/1000)
Test Accuracy of  bird: 43% (432/1000)
Test Accuracy of   cat: 55% (551/1000)
Test Accuracy of  deer: 73% (731/1000)
Test Accuracy of   dog: 38% (384/1000)
Test Accuracy of  frog: 82% (828/1000)
Test Accuracy of horse: 63% (632/1000)
Test Accuracy of  ship: 88% (880/1000)
Test Accuracy of truck: 65% (658/1000)

Test Accuracy (Overall): 64% (6466/10000)
Ended: 2018-12-15 11:57:44.922535
Elapsed: 0:14:05.152783
Validation loss decreased (inf --&amp;gt; 0.170476).  Saving model ...
Epoch: 810 	Training Loss: 0.791785 	Validation Loss: 0.185611
Epoch: 820 	Training Loss: 0.775938 	Validation Loss: 0.185072
Epoch: 830 	Training Loss: 0.776210 	Validation Loss: 0.187146
Epoch: 840 	Training Loss: 0.768063 	Validation Loss: 0.182017
Epoch: 850 	Training Loss: 0.769061 	Validation Loss: 0.196850
Test Loss: 1.012101

Test Accuracy of airplane: 62% (624/1000)
Test Accuracy of automobile: 73% (738/1000)
Test Accuracy of  bird: 42% (429/1000)
Test Accuracy of   cat: 55% (551/1000)
Test Accuracy of  deer: 73% (730/1000)
Test Accuracy of   dog: 42% (420/1000)
Test Accuracy of  frog: 85% (854/1000)
Test Accuracy of horse: 60% (604/1000)
Test Accuracy of  ship: 84% (843/1000)
Test Accuracy of truck: 67% (679/1000)

Test Accuracy (Overall): 64% (6472/10000)
Ended: 2018-12-15 12:12:04.058599
Elapsed: 0:14:19.132241
Validation loss decreased (inf --&amp;gt; 0.174863).  Saving model ...
Epoch: 1610 	Training Loss: 0.797948 	Validation Loss: 0.176395
Validation loss decreased (0.174863 --&amp;gt; 0.172779).  Saving model ...
Validation loss decreased (0.172779 --&amp;gt; 0.170694).  Saving model ...
Epoch: 1620 	Training Loss: 0.789980 	Validation Loss: 0.178468
Epoch: 1630 	Training Loss: 0.772959 	Validation Loss: 0.183980
Epoch: 1640 	Training Loss: 0.776142 	Validation Loss: 0.198711
Epoch: 1650 	Training Loss: 0.767914 	Validation Loss: 0.208851
Test Loss: 0.987713

Test Accuracy of airplane: 62% (624/1000)
Test Accuracy of automobile: 74% (743/1000)
Test Accuracy of  bird: 43% (436/1000)
Test Accuracy of   cat: 52% (525/1000)
Test Accuracy of  deer: 73% (734/1000)
Test Accuracy of   dog: 47% (473/1000)
Test Accuracy of  frog: 83% (831/1000)
Test Accuracy of horse: 63% (631/1000)
Test Accuracy of  ship: 84% (845/1000)
Test Accuracy of truck: 68% (682/1000)

Test Accuracy (Overall): 65% (6524/10000)
Ended: 2018-12-15 12:26:50.701191
Elapsed: 0:14:46.638712
Validation loss decreased (inf --&amp;gt; 0.181906).  Saving model ...
Validation loss decreased (0.181906 --&amp;gt; 0.175381).  Saving model ...
Validation loss decreased (0.175381 --&amp;gt; 0.169833).  Saving model ...
Epoch: 3220 	Training Loss: 0.776567 	Validation Loss: 0.178259
Epoch: 3230 	Training Loss: 0.777072 	Validation Loss: 0.180300
Epoch: 3240 	Training Loss: 0.770289 	Validation Loss: 0.192919
Epoch: 3250 	Training Loss: 0.762633 	Validation Loss: 0.192530
Epoch: 3260 	Training Loss: 0.760599 	Validation Loss: 0.195964
Test Loss: 0.982302

Test Accuracy of airplane: 66% (665/1000)
Test Accuracy of automobile: 75% (756/1000)
Test Accuracy of  bird: 44% (444/1000)
Test Accuracy of   cat: 56% (565/1000)
Test Accuracy of  deer: 68% (686/1000)
Test Accuracy of   dog: 40% (407/1000)
Test Accuracy of  frog: 85% (855/1000)
Test Accuracy of horse: 63% (639/1000)
Test Accuracy of  ship: 84% (844/1000)
Test Accuracy of truck: 68% (683/1000)

Test Accuracy (Overall): 65% (6544/10000)
Ended: 2018-12-15 12:41:47.333383
Elapsed: 0:14:56.629183
Validation loss decreased (inf --&amp;gt; 0.187802).  Saving model ...
Validation loss decreased (0.187802 --&amp;gt; 0.184430).  Saving model ...
Validation loss decreased (0.184430 --&amp;gt; 0.183925).  Saving model ...
Validation loss decreased (0.183925 --&amp;gt; 0.180367).  Saving model ...
Validation loss decreased (0.180367 --&amp;gt; 0.173719).  Saving model ...
Epoch: 6440 	Training Loss: 0.778801 	Validation Loss: 0.190905
Epoch: 6450 	Training Loss: 0.771958 	Validation Loss: 0.182070
Epoch: 6460 	Training Loss: 0.764318 	Validation Loss: 0.190349
Epoch: 6470 	Training Loss: 0.766295 	Validation Loss: 0.192508
Epoch: 6480 	Training Loss: 0.761968 	Validation Loss: 0.189583
Test Loss: 0.987995

Test Accuracy of airplane: 66% (661/1000)
Test Accuracy of automobile: 76% (763/1000)
Test Accuracy of  bird: 44% (443/1000)
Test Accuracy of   cat: 55% (557/1000)
Test Accuracy of  deer: 72% (728/1000)
Test Accuracy of   dog: 41% (415/1000)
Test Accuracy of  frog: 85% (853/1000)
Test Accuracy of horse: 60% (600/1000)
Test Accuracy of  ship: 84% (849/1000)
Test Accuracy of truck: 66% (669/1000)

Test Accuracy (Overall): 65% (6538/10000)
Ended: 2018-12-15 12:56:04.438153
Elapsed: 0:14:17.094202
Validation loss decreased (inf --&amp;gt; 0.191682).  Saving model ...
Validation loss decreased (0.191682 --&amp;gt; 0.182732).  Saving model ...
Validation loss decreased (0.182732 --&amp;gt; 0.181846).  Saving model ...
Epoch: 12870 	Training Loss: 0.770414 	Validation Loss: 0.185177
Validation loss decreased (0.181846 --&amp;gt; 0.179913).  Saving model ...
Epoch: 12880 	Training Loss: 0.772306 	Validation Loss: 0.191702
Epoch: 12890 	Training Loss: 0.768497 	Validation Loss: 0.181795
Epoch: 12900 	Training Loss: 0.760247 	Validation Loss: 0.183884
Epoch: 12910 	Training Loss: 0.757400 	Validation Loss: 0.197759
Test Loss: 0.995634

Test Accuracy of airplane: 64% (648/1000)
Test Accuracy of automobile: 75% (755/1000)
Test Accuracy of  bird: 37% (377/1000)
Test Accuracy of   cat: 55% (557/1000)
Test Accuracy of  deer: 72% (726/1000)
Test Accuracy of   dog: 45% (459/1000)
Test Accuracy of  frog: 85% (857/1000)
Test Accuracy of horse: 59% (590/1000)
Test Accuracy of  ship: 84% (842/1000)
Test Accuracy of truck: 69% (696/1000)

Test Accuracy (Overall): 65% (6507/10000)
Ended: 2018-12-15 13:10:05.720077
Elapsed: 0:14:01.278026
Validation loss decreased (inf --&amp;gt; 0.190403).  Saving model ...
Validation loss decreased (0.190403 --&amp;gt; 0.187068).  Saving model ...
Epoch: 25730 	Training Loss: 0.768132 	Validation Loss: 0.185507
Validation loss decreased (0.187068 --&amp;gt; 0.185507).  Saving model ...
Validation loss decreased (0.185507 --&amp;gt; 0.177258).  Saving model ...
Epoch: 25740 	Training Loss: 0.772002 	Validation Loss: 0.190112
Epoch: 25750 	Training Loss: 0.760312 	Validation Loss: 0.195855
Epoch: 25760 	Training Loss: 0.759808 	Validation Loss: 0.204542
Epoch: 25770 	Training Loss: 0.756103 	Validation Loss: 0.193606
Test Loss: 0.979529

Test Accuracy of airplane: 66% (663/1000)
Test Accuracy of automobile: 76% (769/1000)
Test Accuracy of  bird: 39% (396/1000)
Test Accuracy of   cat: 57% (578/1000)
Test Accuracy of  deer: 74% (749/1000)
Test Accuracy of   dog: 41% (414/1000)
Test Accuracy of  frog: 83% (833/1000)
Test Accuracy of horse: 61% (618/1000)
Test Accuracy of  ship: 84% (844/1000)
Test Accuracy of truck: 68% (687/1000)

Test Accuracy (Overall): 65% (6551/10000)
Ended: 2018-12-15 13:24:12.319440
Elapsed: 0:14:06.595121
Validation loss decreased (inf --&amp;gt; 0.186117).  Saving model ...
Validation loss decreased (0.186117 --&amp;gt; 0.182822).  Saving model ...
Epoch: 51460 	Training Loss: 0.767829 	Validation Loss: 0.189161
Epoch: 51470 	Training Loss: 0.763347 	Validation Loss: 0.194681
Validation loss decreased (0.182822 --&amp;gt; 0.179458).  Saving model ...
Epoch: 51480 	Training Loss: 0.756280 	Validation Loss: 0.187176
Epoch: 51490 	Training Loss: 0.757250 	Validation Loss: 0.198088
Epoch: 51500 	Training Loss: 0.754145 	Validation Loss: 0.204468
Test Loss: 0.973007

Test Accuracy of airplane: 67% (676/1000)
Test Accuracy of automobile: 74% (749/1000)
Test Accuracy of  bird: 41% (415/1000)
Test Accuracy of   cat: 57% (579/1000)
Test Accuracy of  deer: 75% (752/1000)
Test Accuracy of   dog: 41% (412/1000)
Test Accuracy of  frog: 81% (815/1000)
Test Accuracy of horse: 65% (653/1000)
Test Accuracy of  ship: 85% (850/1000)
Test Accuracy of truck: 69% (696/1000)

Test Accuracy (Overall): 65% (6597/10000)
Ended: 2018-12-15 13:38:06.475872
Elapsed: 0:13:54.151685
&lt;/pre&gt;

&lt;p&gt;
So, this model seems pretty much stuck. I cheated and peaked at the lecturer's solution, but this post is getting too long so I'll save that for another one.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
figure.suptitle("Loss")
x = list(range(len(outcome["training_loss"])))
training = numpy.array(outcome["training_loss"])
limit = 500
axe.plot(x[:limit], training[:limit], ".", label="Training")
axe.plot(x[:limit], outcome["validation_loss"][:limit], ".", label="Validation")
legend = axe.legend()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/final_model.png" alt="final_model.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So it looks like there's something wrong with my code. I'll have to figure this out (or just stick with straight epochs).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>exercise</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/cifar-10/</guid><pubDate>Tue, 04 Dec 2018 23:26:15 GMT</pubDate></item><item><title>MNIST Multi-Layer Perceptron with Validation</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org3753cf1"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org4faa131"&gt;Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org779b66a"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#orgad5b5e4"&gt;Visualize a Batch of Training Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org901ebf8"&gt;Define the Network Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org68b435d"&gt;Specify the Loss Function and Optimizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org3e0edfd"&gt;Train the Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org92fb12e"&gt;Testing the Best Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#orgf1baeb7"&gt;Visualize Test Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org61b0919"&gt;Object-Oriented Trainer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3753cf1" class="outline-2"&gt;
&lt;h2 id="org3753cf1"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3753cf1"&gt;
&lt;p&gt;
This is from &lt;a href="https://github.com/udacity/deep-learning-v2-pytorch.git"&gt;Udacity's Deep Learning Repository&lt;/a&gt; which supports their Deep Learning Nanodegree.
&lt;/p&gt;

&lt;p&gt;
We are going to train a &lt;a href="https://en.wikipedia.org/wiki/Multilayer_perceptron"&gt;Multi-Layer Perceptron&lt;/a&gt; to classify images from the &lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST database&lt;/a&gt; of hand-written digits.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4faa131" class="outline-2"&gt;
&lt;h2 id="org4faa131"&gt;Setup&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4faa131"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9bd8781" class="outline-3"&gt;
&lt;h3 id="org9bd8781"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9bd8781"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgee6a7e2" class="outline-4"&gt;
&lt;h4 id="orgee6a7e2"&gt;From Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgee6a7e2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
 &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;typing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Tuple&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;gc&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8ec5f3f" class="outline-4"&gt;
&lt;h4 id="org8ec5f3f"&gt;From PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8ec5f3f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dotenv&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;
 &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
 &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torchvision&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pyplot&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torchvision.transforms&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;transforms&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
 &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org07d89bd" class="outline-4"&gt;
&lt;h4 id="org07d89bd"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org07d89bd"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;neurotic.tangles.data_paths&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org14ff52f" class="outline-3"&gt;
&lt;h3 id="org14ff52f"&gt;Setup the Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org14ff52f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="n"&gt;get_ipython&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_line_magic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'matplotlib'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'inline'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
 &lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"whitegrid"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	     &lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"axes.grid"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		 &lt;span class="s2"&gt;"font.family"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"sans-serif"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
		 &lt;span class="s2"&gt;"font.sans-serif"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Latin Modern Sans"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Lato"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
		 &lt;span class="s2"&gt;"figure.figsize"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)},&lt;/span&gt;
	     &lt;span class="n"&gt;font_scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org062e450" class="outline-3"&gt;
&lt;h3 id="org062e450"&gt;Types&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org062e450"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="n"&gt;Outcome&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Tuple&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org779b66a" class="outline-2"&gt;
&lt;h2 id="org779b66a"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org779b66a"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd4167a8" class="outline-3"&gt;
&lt;h3 id="orgd4167a8"&gt;The Path To the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd4167a8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"MNIST"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/MNIST
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org31c7cf6" class="outline-3"&gt;
&lt;h3 id="org31c7cf6"&gt;Some Settings&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org31c7cf6"&gt;
&lt;p&gt;
Since I downloaded the data earlier for some other exercise forking sub-processes is probably unnecessary, and for the training and testing we'll use a relatively small batch-size of 20.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;WORKERS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;
&lt;span class="n"&gt;VALIDATION_PROPORTION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5b6f1ce" class="outline-3"&gt;
&lt;h3 id="org5b6f1ce"&gt;A Transform&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5b6f1ce"&gt;
&lt;p&gt;
We're just going to convert the images to tensors.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1531256" class="outline-3"&gt;
&lt;h3 id="org1531256"&gt;Split Up the Training and Testing Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1531256"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MNIST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MNIST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			   &lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1885cc6" class="outline-3"&gt;
&lt;h3 id="org1885cc6"&gt;Make a Validation Set&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1885cc6"&gt;
&lt;p&gt;
Now we're going to re-split the training-data into training and  validation data. First we're going to generate indices for each set using sklearn's &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"&gt;&lt;code&gt;train_test_split&lt;/code&gt;&lt;/a&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;training_indices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;VALIDATION_PROPORTION&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_indices&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_indices&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;VALIDATION_PROPORTION&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
48000
12000
&lt;/pre&gt;


&lt;p&gt;
Now that we have our indices we need to create some samplers that can be passed to the Data Loaders. We need them to create the batches from our data.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;training_sampler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SubsetRandomSampler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;validation_sampler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SubsetRandomSampler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_indices&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgba89f63" class="outline-3"&gt;
&lt;h3 id="orgba89f63"&gt;Create The Data Loaders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgba89f63"&gt;
&lt;p&gt;
Now we will create the batch-iterators.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;training_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sampler&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;training_sampler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;WORKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
For the validation batch we pass in the training data and use the validation-sampler to create a separate set of batches.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;validation_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;training_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sampler&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;validation_sampler&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;WORKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Since we're not splitting the testing data it doesn't get a sampler.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;WORKERS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgad5b5e4" class="outline-2"&gt;
&lt;h2 id="orgad5b5e4"&gt;Visualize a Batch of Training Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgad5b5e4"&gt;
&lt;p&gt;
Our first step is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org15ee57b" class="outline-3"&gt;
&lt;h3 id="org15ee57b"&gt;Grab a batch&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org15ee57b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now that we have a batch we're going to plot the images in the batch, along with the corresponding labels.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;font_scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"First Batch"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[])&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# print out the correct label for each image&lt;/span&gt;
    &lt;span class="c1"&gt;# .item() gets the value contained in a Tensor&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/batch.png" alt="batch.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org91ec5ab" class="outline-3"&gt;
&lt;h3 id="org91ec5ab"&gt;View a Single Image&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org91ec5ab"&gt;
&lt;p&gt;
Now we're going to take a closer look at the second image in the batch.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;font_scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"white"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; 
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"xx-large"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;111&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;
&lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;2.5&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
	&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;xy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
		    &lt;span class="n"&gt;horizontalalignment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'center'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		    &lt;span class="n"&gt;verticalalignment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'center'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		    &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'white'&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s1"&gt;'black'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/image.png" alt="image.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
We're looking at a single image with the normalized values for each pixel superimposed on it. It looks like black is 0 and white is 1, although for this image most of the 'white' pixels are just a little less than one.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org901ebf8" class="outline-2"&gt;
&lt;h2 id="org901ebf8"&gt;Define the Network &lt;a href="http://pytorch.org/docs/stable/nn.html"&gt;Architecture&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org901ebf8"&gt;
&lt;p&gt;
The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting.
&lt;/p&gt;

&lt;p&gt;
These values are based on the &lt;a href="https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py"&gt;keras&lt;/a&gt; example implementation.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;INPUT_NODES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;
&lt;span class="n"&gt;HIDDEN_NODES_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;HIDDEN_NODES_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;512&lt;/span&gt;
&lt;span class="n"&gt;DROPOUT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
&lt;span class="n"&gt;CLASSES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MultiLayerPerceptron&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""A Multi-Layer Perceptron&lt;/span&gt;

&lt;span class="sd"&gt;    This is a network with 2 hidden layers&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;        
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fully_connected_layer_1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INPUT_NODES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HIDDEN_NODES_1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fully_connected_layer_2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;HIDDEN_NODES_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HIDDEN_NODES_2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;HIDDEN_NODES_2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CLASSES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DROPOUT&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="sd"&gt;"""One feed-forward through the network&lt;/span&gt;

&lt;span class="sd"&gt;	Args:&lt;/span&gt;
&lt;span class="sd"&gt;	 x: a 28 x 28 tensor&lt;/span&gt;

&lt;span class="sd"&gt;	Returns:&lt;/span&gt;
&lt;span class="sd"&gt;	 tensor: output of the network without activation&lt;/span&gt;
&lt;span class="sd"&gt;	"""&lt;/span&gt;
	&lt;span class="c1"&gt;# flatten image input&lt;/span&gt;
	&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;INPUT_NODES&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fully_connected_layer_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
	&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fully_connected_layer_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;        
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb2ecca5" class="outline-3"&gt;
&lt;h3 id="orgb2ecca5"&gt;Initialize the Neural Network&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb2ecca5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MultiLayerPerceptron&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
MultiLayerPerceptron(
  (fully_connected_layer_1): Linear(in_features=784, out_features=512, bias=True)
  (fully_connected_layer_2): Linear(in_features=512, out_features=512, bias=True)
  (output): Linear(in_features=512, out_features=10, bias=True)
  (dropout): Dropout(p=0.2)
)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7111497" class="outline-3"&gt;
&lt;h3 id="org7111497"&gt;A Little CUDA&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7111497"&gt;
&lt;p&gt;
This sets it up to use CUDA (if available).
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"cuda"&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s2"&gt;"cpu"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device_count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Using &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt; GPUs"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device_count&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataParallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Only 1 GPU available"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Only 1 GPU available
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org68b435d" class="outline-2"&gt;
&lt;h2 id="org68b435d"&gt;Specify the &lt;a href="http://pytorch.org/docs/stable/nn.html#loss-functions"&gt;Loss Function&lt;/a&gt; and &lt;a href="http://pytorch.org/docs/stable/optim.html"&gt;Optimizer&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org68b435d"&gt;
&lt;p&gt;
We're going to use &lt;a href="http://pytorch.org/docs/stable/nn.html#loss-functions"&gt;cross-entropy loss&lt;/a&gt; for classification. PyTorch's cross entropy function applies a softmax function to the output layer &lt;b&gt;and&lt;/b&gt; then calculates the log loss (so you don't want to do softmax as part of the model output).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SGD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3e0edfd" class="outline-2"&gt;
&lt;h2 id="org3e0edfd"&gt;Train the Network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3e0edfd"&gt;
&lt;p&gt;
We're going to do a quasi-search by optimizing over 50 epochs and keeping the model that has the best validation score.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# number of epochs to train the model&lt;/span&gt;
&lt;span class="n"&gt;EPOCHS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;
&lt;span class="n"&gt;SAVED_MODEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;'multilayer_perceptron.pt'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;process_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		   &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Outcome&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
     &lt;span class="sd"&gt;"""process one batch of the data&lt;/span&gt;

&lt;span class="sd"&gt;     Args:&lt;/span&gt;
&lt;span class="sd"&gt;      model: model to predict target&lt;/span&gt;
&lt;span class="sd"&gt;      data: data to use to predict target&lt;/span&gt;
&lt;span class="sd"&gt;      target: what we're trying to predict&lt;/span&gt;
&lt;span class="sd"&gt;      device: cpu or gpu&lt;/span&gt;

&lt;span class="sd"&gt;     Returns:&lt;/span&gt;
&lt;span class="sd"&gt;      outcome: loss and correct count&lt;/span&gt;
&lt;span class="sd"&gt;     """&lt;/span&gt;
     &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predicted&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	  &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	  &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Outcome&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""Perform one forward pass through the batches&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     model: thing to train&lt;/span&gt;
&lt;span class="sd"&gt;     batches: batch-iterator of training data&lt;/span&gt;
&lt;span class="sd"&gt;     device: cpu or cuda device&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     outcome: cumulative loss, accuracy for the batches&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;total_correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;process_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;total_correct&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt;
	&lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
	&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;total_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_correct&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;validate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	     &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Outcome&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""Calculate the loss for the model&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     model: the model to validate&lt;/span&gt;
&lt;span class="sd"&gt;     batches: the batch-iterator of validation data&lt;/span&gt;
&lt;span class="sd"&gt;     device: cuda or cpu&lt;/span&gt;

&lt;span class="sd"&gt;    Returns:&lt;/span&gt;
&lt;span class="sd"&gt;     Outcome: Cumulative loss, Accuracy over batches&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="n"&gt;total_correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;process_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;total_correct&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt;
	&lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;total_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;total_correct&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# initialize tracker for minimum validation loss&lt;/span&gt;
&lt;span class="n"&gt;lowest_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Inf&lt;/span&gt;
&lt;span class="n"&gt;training_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;validation_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;training_accuracies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;validation_accuracies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;EPOCHS&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;training_losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mean_training_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;training_accuracies&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;validate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;validation_losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;mean_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;validation_batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;validation_accuracies&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mean_validation_loss&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;lowest_validation_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Epoch &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;: Validation loss decreased (&lt;/span&gt;&lt;span class="si"&gt;{:.6f}&lt;/span&gt;&lt;span class="s1"&gt; --&amp;gt; &lt;/span&gt;&lt;span class="si"&gt;{:.6f}&lt;/span&gt;&lt;span class="s1"&gt;).  Saving model ...'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="n"&gt;lowest_validation_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="n"&gt;mean_validation_loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
	&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;SAVED_MODEL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="n"&gt;lowest_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_validation_loss&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch 1: Validation loss decreased (inf --&amp;gt; 0.076556).  Saving model ...
Epoch 2: Validation loss decreased (0.076556 --&amp;gt; 0.058478).  Saving model ...
Epoch 3: Validation loss decreased (0.058478 --&amp;gt; 0.049405).  Saving model ...
Epoch 4: Validation loss decreased (0.049405 --&amp;gt; 0.043155).  Saving model ...
Epoch 5: Validation loss decreased (0.043155 --&amp;gt; 0.037079).  Saving model ...
Epoch 6: Validation loss decreased (0.037079 --&amp;gt; 0.032932).  Saving model ...
Epoch 7: Validation loss decreased (0.032932 --&amp;gt; 0.029682).  Saving model ...
Epoch 8: Validation loss decreased (0.029682 --&amp;gt; 0.028046).  Saving model ...
Epoch 9: Validation loss decreased (0.028046 --&amp;gt; 0.025318).  Saving model ...
Epoch 10: Validation loss decreased (0.025318 --&amp;gt; 0.023867).  Saving model ...
Epoch 11: Validation loss decreased (0.023867 --&amp;gt; 0.022447).  Saving model ...
Epoch 12: Validation loss decreased (0.022447 --&amp;gt; 0.021411).  Saving model ...
Epoch 13: Validation loss decreased (0.021411 --&amp;gt; 0.020793).  Saving model ...
Epoch 14: Validation loss decreased (0.020793 --&amp;gt; 0.019830).  Saving model ...
Epoch 15: Validation loss decreased (0.019830 --&amp;gt; 0.018676).  Saving model ...
Epoch 16: Validation loss decreased (0.018676 --&amp;gt; 0.018644).  Saving model ...
Epoch 17: Validation loss decreased (0.018644 --&amp;gt; 0.017666).  Saving model ...
Epoch 18: Validation loss decreased (0.017666 --&amp;gt; 0.017635).  Saving model ...
Epoch 20: Validation loss decreased (0.017635 --&amp;gt; 0.016688).  Saving model ...
Epoch 21: Validation loss decreased (0.016688 --&amp;gt; 0.016489).  Saving model ...
Epoch 22: Validation loss decreased (0.016489 --&amp;gt; 0.016364).  Saving model ...
Epoch 23: Validation loss decreased (0.016364 --&amp;gt; 0.015944).  Saving model ...
Epoch 24: Validation loss decreased (0.015944 --&amp;gt; 0.015633).  Saving model ...
Epoch 26: Validation loss decreased (0.015633 --&amp;gt; 0.015446).  Saving model ...
Epoch 27: Validation loss decreased (0.015446 --&amp;gt; 0.015257).  Saving model ...
Epoch 30: Validation loss decreased (0.015257 --&amp;gt; 0.015216).  Saving model ...
Epoch 31: Validation loss decreased (0.015216 --&amp;gt; 0.015175).  Saving model ...
Epoch 34: Validation loss decreased (0.015175 --&amp;gt; 0.014866).  Saving model ...
Epoch 36: Validation loss decreased (0.014866 --&amp;gt; 0.014530).  Saving model ...
&lt;/pre&gt;

&lt;p&gt;
The training and validation loss seems surprisingly good.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_losses&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Loss Per Batch"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;training_losses&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Training"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_losses&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Validation"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;legend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/losses.png" alt="losses.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So it looks like it improves fairly quickly then after 36 epochs the model stops improving.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org92fb12e" class="outline-2"&gt;
&lt;h2 id="org92fb12e"&gt;Testing the Best Model&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org92fb12e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SAVED_MODEL&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;span class="n"&gt;class_correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;class_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# calculate the loss&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# update test loss &lt;/span&gt;
    &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# convert output probabilities to predicted class&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# compare predictions to true label&lt;/span&gt;
    &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view_as&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="c1"&gt;# calculate test accuracy for each object class&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
	&lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="c1"&gt;# calculate and print avg test loss&lt;/span&gt;
&lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Loss: &lt;/span&gt;&lt;span class="si"&gt;{:.6f}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Accuracy of &lt;/span&gt;&lt;span class="si"&gt;%5s&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%2d%%&lt;/span&gt;&lt;span class="s1"&gt; (&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;)'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	    &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Accuracy of &lt;/span&gt;&lt;span class="si"&gt;%5s&lt;/span&gt;&lt;span class="s1"&gt;: N/A (no training examples)'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Loss: 0.059497

Test Accuracy of     0: 99% (974/980)
Test Accuracy of     1: 99% (1127/1135)
Test Accuracy of     2: 97% (1009/1032)
Test Accuracy of     3: 98% (994/1010)
Test Accuracy of     4: 97% (960/982)
Test Accuracy of     5: 97% (867/892)
Test Accuracy of     6: 98% (941/958)
Test Accuracy of     7: 98% (1008/1028)
Test Accuracy of     8: 97% (947/974)
Test Accuracy of     9: 97% (986/1009)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf1baeb7" class="outline-2"&gt;
&lt;h2 id="orgf1baeb7"&gt;Visualize Test Results&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf1baeb7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;# matplotlib doesn't like the CUDA and the model doesn't like the CPU... too bad for the model.&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"cpu"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# prep images for display&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# plot the images in the batch, along with predicted and true labels&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Test Predictions"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;position&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[])&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt; (&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;)"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;())),&lt;/span&gt;
		 &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"green"&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s2"&gt;"red"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tight_layout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/test_results.png" alt="test_results.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org61b0919" class="outline-2"&gt;
&lt;h2 id="org61b0919"&gt;Object-Oriented Trainer&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org61b0919"&gt;
&lt;p&gt;
This just bundles up the earlier stuff.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Trainer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
     &lt;span class="sd"&gt;"""Train-test-validate the model&lt;/span&gt;

&lt;span class="sd"&gt;     Args:&lt;/span&gt;
&lt;span class="sd"&gt;      train: training batches&lt;/span&gt;
&lt;span class="sd"&gt;      validate: validation batches&lt;/span&gt;
&lt;span class="sd"&gt;      test: testing batches&lt;/span&gt;
&lt;span class="sd"&gt;      epochs: number of times to repeat training over the batches&lt;/span&gt;
&lt;span class="sd"&gt;      model_filename: name to save the hyperparameters of best model&lt;/span&gt;
&lt;span class="sd"&gt;      learning_rate: how much to update the weights&lt;/span&gt;
&lt;span class="sd"&gt;     """&lt;/span&gt;
     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		  &lt;span class="n"&gt;validate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		  &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		  &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		  &lt;span class="n"&gt;model_filename&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"multilayer_perceptron.pth"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		  &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;validate&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epochs&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_as&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_filename&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_accuracies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_accuracies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt;

     &lt;span class="nd"&gt;@property&lt;/span&gt;
     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	 &lt;span class="sd"&gt;"""The Multi-Layer Perceptron"""&lt;/span&gt;
	 &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_model&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MultiLayerPerceptron&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_model&lt;/span&gt;

     &lt;span class="nd"&gt;@property&lt;/span&gt;
     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	 &lt;span class="sd"&gt;"""The Loss Measurer"""&lt;/span&gt;
	 &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_criterion&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_criterion&lt;/span&gt;

     &lt;span class="nd"&gt;@property&lt;/span&gt;
     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	 &lt;span class="sd"&gt;"""The gradient descent"""&lt;/span&gt;
	 &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_optimizer&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SGD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
					       &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_optimizer&lt;/span&gt;

     &lt;span class="nd"&gt;@property&lt;/span&gt;
     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	 &lt;span class="sd"&gt;"""The CPU or GPU"""&lt;/span&gt;
	 &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_device&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"cuda"&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s2"&gt;"cpu"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_device&lt;/span&gt;


     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;process_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Outcome&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	 &lt;span class="sd"&gt;"""process one batch of the data&lt;/span&gt;

&lt;span class="sd"&gt;	 Args:&lt;/span&gt;
&lt;span class="sd"&gt;	  data: data to use to predict target&lt;/span&gt;
&lt;span class="sd"&gt;	  target: what we're trying to predict&lt;/span&gt;
&lt;span class="sd"&gt;	  device: cpu or gpu&lt;/span&gt;

&lt;span class="sd"&gt;	 Returns:&lt;/span&gt;
&lt;span class="sd"&gt;	  outcome: loss and correct count&lt;/span&gt;
&lt;span class="sd"&gt;	 """&lt;/span&gt;
	 &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predicted&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Outcome&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	 &lt;span class="sd"&gt;"""Perform one forward pass through the batches&lt;/span&gt;

&lt;span class="sd"&gt;	 Returns:&lt;/span&gt;
&lt;span class="sd"&gt;	  outcome: cumulative loss, accuracy for the batches&lt;/span&gt;
&lt;span class="sd"&gt;	 """&lt;/span&gt;
	 &lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
	 &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
	 &lt;span class="n"&gt;total_correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	     &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;process_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;total_correct&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt;
	     &lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
	     &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	     &lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_correct&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;validate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Outcome&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	 &lt;span class="sd"&gt;"""Calculate the loss for the model&lt;/span&gt;

&lt;span class="sd"&gt;	 Returns:&lt;/span&gt;
&lt;span class="sd"&gt;	  Outcome: Cumulative loss, Accuracy over batches&lt;/span&gt;
&lt;span class="sd"&gt;	 """&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	 &lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
	 &lt;span class="n"&gt;total_correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
	 &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
	 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	     &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;process_batch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;total_correct&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt;
	     &lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="k"&gt;del&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_loss&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_correct&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run_training&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	 &lt;span class="sd"&gt;"""Runs the training and validation"""&lt;/span&gt;
	 &lt;span class="n"&gt;lowest_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Inf&lt;/span&gt;
	 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	     &lt;span class="n"&gt;gc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;collect&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	     &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;mean_training_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_accuracies&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;accuracy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validate&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_losses&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;mean_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_accuracies&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	     &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;mean_validation_loss&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;lowest_validation_loss&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
		 &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Epoch &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;: Validation loss decreased (&lt;/span&gt;&lt;span class="si"&gt;{:.6f}&lt;/span&gt;&lt;span class="s1"&gt; --&amp;gt; &lt;/span&gt;&lt;span class="si"&gt;{:.6f}&lt;/span&gt;&lt;span class="s1"&gt;).  Saving model ...'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
		     &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		     &lt;span class="n"&gt;lowest_validation_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		     &lt;span class="n"&gt;mean_validation_loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
		 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_parameters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
		 &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_parameters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_as&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
		 &lt;span class="n"&gt;lowest_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_validation_loss&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt;

     &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	 &lt;span class="sd"&gt;"""Test Our Model"""&lt;/span&gt;
	 &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_parameters&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	     &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"call ``run_training`` or set ``best_parameters"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_parameters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
	 &lt;span class="n"&gt;digits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
	 &lt;span class="n"&gt;class_correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;
	 &lt;span class="n"&gt;class_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt;
	 &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

	 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	     &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

	     &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	     &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
		 &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view_as&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
	     &lt;span class="c1"&gt;# calculate test accuracy for each object class&lt;/span&gt;
	     &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
		 &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
		 &lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
		 &lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

	 &lt;span class="c1"&gt;# calculate and print avg test loss&lt;/span&gt;
	 &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	 &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Loss: &lt;/span&gt;&lt;span class="si"&gt;{:.6f}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

	 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;digit&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	     &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;digit&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
		 &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Accuracy of &lt;/span&gt;&lt;span class="si"&gt;%5s&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;%2d%%&lt;/span&gt;&lt;span class="s1"&gt; (&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;)'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
		     &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;digit&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;digit&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;digit&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
		     &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;digit&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;digit&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
	     &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
		 &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Accuracy of &lt;/span&gt;&lt;span class="si"&gt;%5s&lt;/span&gt;&lt;span class="s1"&gt;: N/A (no training examples)'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;digit&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
	 &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
For some reason, this raises an error when the backward propagation step is run.
&lt;/p&gt;

&lt;pre class="example"&gt;
RuntimeError: CUDA error: out of memory
&lt;/pre&gt;

&lt;p&gt;
So I can't run it until I figure out what's going on. &lt;b&gt;Update&lt;/b&gt; - it looks like casting the outputs of the functions to floats solved the problem. Apparently even they look like floats, whatever the &lt;code&gt;item()&lt;/code&gt; method returns prevents the freeing up of the memory, so casting them to floats fixes the memory problem.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; &lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Trainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;validation_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
 &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_training&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch 1: Validation loss decreased (inf --&amp;gt; 0.077417).  Saving model ...
Epoch 2: Validation loss decreased (0.077417 --&amp;gt; 0.058746).  Saving model ...
Epoch 3: Validation loss decreased (0.058746 --&amp;gt; 0.048325).  Saving model ...
Epoch 4: Validation loss decreased (0.048325 --&amp;gt; 0.040851).  Saving model ...
Epoch 5: Validation loss decreased (0.040851 --&amp;gt; 0.036083).  Saving model ...
Epoch 6: Validation loss decreased (0.036083 --&amp;gt; 0.032722).  Saving model ...
Epoch 7: Validation loss decreased (0.032722 --&amp;gt; 0.028545).  Saving model ...
Epoch 8: Validation loss decreased (0.028545 --&amp;gt; 0.026376).  Saving model ...
Epoch 9: Validation loss decreased (0.026376 --&amp;gt; 0.024063).  Saving model ...
Epoch 10: Validation loss decreased (0.024063 --&amp;gt; 0.023637).  Saving model ...
Epoch 11: Validation loss decreased (0.023637 --&amp;gt; 0.021980).  Saving model ...
Epoch 12: Validation loss decreased (0.021980 --&amp;gt; 0.020723).  Saving model ...
Epoch 13: Validation loss decreased (0.020723 --&amp;gt; 0.019802).  Saving model ...
Epoch 14: Validation loss decreased (0.019802 --&amp;gt; 0.019013).  Saving model ...
Epoch 15: Validation loss decreased (0.019013 --&amp;gt; 0.018458).  Saving model ...
Epoch 16: Validation loss decreased (0.018458 --&amp;gt; 0.017919).  Saving model ...
Epoch 17: Validation loss decreased (0.017919 --&amp;gt; 0.017918).  Saving model ...
Epoch 18: Validation loss decreased (0.017918 --&amp;gt; 0.017127).  Saving model ...
Epoch 19: Validation loss decreased (0.017127 --&amp;gt; 0.016704).  Saving model ...
Epoch 20: Validation loss decreased (0.016704 --&amp;gt; 0.016167).  Saving model ...
Epoch 22: Validation loss decreased (0.016167 --&amp;gt; 0.016154).  Saving model ...
Epoch 23: Validation loss decreased (0.016154 --&amp;gt; 0.015817).  Saving model ...
Epoch 24: Validation loss decreased (0.015817 --&amp;gt; 0.015352).  Saving model ...
Epoch 25: Validation loss decreased (0.015352 --&amp;gt; 0.015075).  Saving model ...
Epoch 27: Validation loss decreased (0.015075 --&amp;gt; 0.015059).  Saving model ...
Epoch 28: Validation loss decreased (0.015059 --&amp;gt; 0.014940).  Saving model ...
Epoch 32: Validation loss decreased (0.014940 --&amp;gt; 0.014644).  Saving model ...
Epoch 34: Validation loss decreased (0.014644 --&amp;gt; 0.014383).  Saving model ...
Epoch 46: Validation loss decreased (0.014383 --&amp;gt; 0.014357).  Saving model ...
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_accuracies&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Model Accuracy"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_accuracies&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Training"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_accuracies&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Validation"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;legend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/accuracy.png" alt="accuracy.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Although the validation loss decreases for a while, it nearly reaches its peak accuracy around 10 epochs. The training worked out a little differently this time, so here's the losses again.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_losses&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Loss Per Batch"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training_losses&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Training"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation_losses&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Validation"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;legend&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/losses_2.png" alt="losses_2.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>classification</category><category>cnn</category><category>exercise</category><category>validation</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/</guid><pubDate>Tue, 27 Nov 2018 20:02:56 GMT</pubDate></item><item><title>MNIST MLP</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#org28bd910"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#org364e3f4"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#orgc34826a"&gt;Setup the Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#orgf204f78"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#org85fc7ec"&gt;Visualize a Batch of Training Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#org49c904b"&gt;Define the Network Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#org4382971"&gt;Specify the Loss Function and Optimizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#org9a5f81e"&gt;Train the Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/#org625da8d"&gt;Test the Trained Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org28bd910" class="outline-2"&gt;
&lt;h2 id="org28bd910"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org28bd910"&gt;
&lt;p&gt;
This is from &lt;a href="https://github.com/udacity/deep-learning-v2-pytorch.git"&gt;Udacity's Deep Learning Repository&lt;/a&gt; which supports their Deep Learning Nanodegree.
&lt;/p&gt;

&lt;p&gt;
We are going to train a &lt;a href="https://en.wikipedia.org/wiki/Multilayer_perceptron"&gt;Multi-Layer Perceptron&lt;/a&gt; to classify images from the &lt;a href="http://yann.lecun.com/exdb/mnist/"&gt;MNIST database&lt;/a&gt; of hand-written digits.
&lt;/p&gt;

&lt;p&gt;
We're going to do it using the following steps.
&lt;/p&gt;


&lt;ol class="org-ol"&gt;
&lt;li&gt;Load and visualize the data&lt;/li&gt;
&lt;li&gt;Define a neural network&lt;/li&gt;
&lt;li&gt;Train the model&lt;/li&gt;
&lt;li&gt;Evaluate the performance of our trained model on a test dataset&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org364e3f4" class="outline-2"&gt;
&lt;h2 id="org364e3f4"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org364e3f4"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org610fbc4" class="outline-3"&gt;
&lt;h3 id="org610fbc4"&gt;From Python&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org610fbc4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from datetime import datetime
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd3f6250" class="outline-3"&gt;
&lt;h3 id="orgd3f6250"&gt;From PyPi&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd3f6250"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from torchvision import datasets
import matplotlib.pyplot as pyplot
import seaborn
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torch
import numpy
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org621ca35" class="outline-3"&gt;
&lt;h3 id="org621ca35"&gt;This Project&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org621ca35"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from neurotic.tangles.data_paths import DataPathTwo
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc34826a" class="outline-2"&gt;
&lt;h2 id="orgc34826a"&gt;Setup the Plotting&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc34826a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=3)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf204f78" class="outline-2"&gt;
&lt;h2 id="orgf204f78"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf204f78"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9d17b14" class="outline-3"&gt;
&lt;h3 id="org9d17b14"&gt;The Path To the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9d17b14"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
path = DataPathTwo(folder_key="MNIST")
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(path.folder)
print(path.folder.exists())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/MNIST
True
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org50679be" class="outline-3"&gt;
&lt;h3 id="org50679be"&gt;Some Settings&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org50679be"&gt;
&lt;p&gt;
Since I downloaded the data earlier for some other exercise forking sub-processes is probably unnecessary, and for the training and testing we'll use a relatively small batch-size of 20.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;WORKERS = 0
BATCH_SIZE = 20
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb83fd29" class="outline-3"&gt;
&lt;h3 id="orgb83fd29"&gt;A Transform&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb83fd29"&gt;
&lt;p&gt;
We're just going to convert the images to tensors.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;transform = transforms.ToTensor()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf790d31" class="outline-3"&gt;
&lt;h3 id="orgf790d31"&gt;Split Up the Training and Testing Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf790d31"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_data = datasets.MNIST(root=path.folder, train=True,
			    download=True, transform=transform)
test_data = datasets.MNIST(root=path.folder, train=False,
			   download=True, transform=transform)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org140dab4" class="outline-3"&gt;
&lt;h3 id="org140dab4"&gt;Create the Batch Loaders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org140dab4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_batches = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
					    num_workers=WORKERS)
test_batches = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, 
					   num_workers=WORKERS)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org85fc7ec" class="outline-2"&gt;
&lt;h2 id="org85fc7ec"&gt;Visualize a Batch of Training Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org85fc7ec"&gt;
&lt;p&gt;
The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdf1d862" class="outline-3"&gt;
&lt;h3 id="orgdf1d862"&gt;Grab a batch&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdf1d862"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;images, labels = iter(train_batches).next()
images = images.numpy()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now that we have a batch we're going to plot the images in the batch, along with the corresponding labels.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure = pyplot.figure(figsize=(25, 4))
figure.suptitle("First Batch", weight="bold")
for index in numpy.arange(BATCH_SIZE):
    ax = figure.add_subplot(2, BATCH_SIZE/2, index+1, xticks=[], yticks=[])
    ax.imshow(numpy.squeeze(images[index]), cmap='gray')
    # print out the correct label for each image
    # .item() gets the value contained in a Tensor
    ax.set_title(str(labels[index].item()))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/batch.png" alt="batch.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org06ac88e" class="outline-3"&gt;
&lt;h3 id="org06ac88e"&gt;View a Single Image&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org06ac88e"&gt;
&lt;p&gt;
Now we're going to take a closer look at the second image in the batch.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;image = numpy.squeeze(images[1])

figure = pyplot.figure(figsize = (12,12)) 
ax = figure.add_subplot(111)
ax.imshow(image, cmap='gray')
width, height = image.shape
threshold = image.max()/2.5
for x in range(width):
    for y in range(height):
	val = round(image[x][y],2) if image[x][y] !=0 else 0
	ax.annotate(str(val), xy=(y,x),
		    horizontalalignment='center',
		    verticalalignment='center',
		    color='white' if image[x][y]&amp;lt;threshold else 'black')
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/image.png" alt="image.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
We're looking at a single image with the normalized values for each pixel superimposed on it. It looks like black is 0 and white is 1, although for this image most of the 'white' pixels are just a little less than one.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org49c904b" class="outline-2"&gt;
&lt;h2 id="org49c904b"&gt;Define the Network &lt;a href="http://pytorch.org/docs/stable/nn.html"&gt;Architecture&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org49c904b"&gt;
&lt;p&gt;
The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting.
&lt;/p&gt;

&lt;p&gt;
These values are based on the &lt;a href="https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py"&gt;keras&lt;/a&gt; example implementation.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;INPUT_NODES = 28 * 28
HIDDEN_NODES = 512
DROPOUT = 0.2
CLASSES = 10
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Net(nn.Module):
    def __init__(self):
	super().__init__()        
	self.fully_connected_layer_1 = nn.Linear(INPUT_NODES, HIDDEN_NODES)
	self.fully_connected_layer_2 = nn.Linear(HIDDEN_NODES, HIDDEN_NODES)
	self.output = nn.Linear(HIDDEN_NODES, CLASSES)
	self.dropout = nn.Dropout(p=DROPOUT)
	return

    def forward(self, x):
	# flatten image input
	x = x.view(-1, 28 * 28)
	# add hidden layer, with relu activation function
	x = self.dropout(F.relu(self.fully_connected_layer_1(x)))
	x = self.dropout(F.relu(self.fully_connected_layer_2(x)))        
	return self.output(x)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge8a1e46" class="outline-3"&gt;
&lt;h3 id="orge8a1e46"&gt;Initialize the NN&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge8a1e46"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = Net()
print(model)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Net(
  (fully_connected_layer_1): Linear(in_features=784, out_features=512, bias=True)
  (fully_connected_layer_2): Linear(in_features=512, out_features=512, bias=True)
  (output): Linear(in_features=512, out_features=10, bias=True)
  (dropout): Dropout(p=0.2)
)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1069f8a" class="outline-3"&gt;
&lt;h3 id="org1069f8a"&gt;A Little CUDA&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1069f8a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4382971" class="outline-2"&gt;
&lt;h2 id="org4382971"&gt;Specify the &lt;a href="http://pytorch.org/docs/stable/nn.html#loss-functions"&gt;Loss Function&lt;/a&gt; and &lt;a href="http://pytorch.org/docs/stable/optim.html"&gt;Optimizer&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4382971"&gt;
&lt;p&gt;
It's recommended that you use &lt;a href="http://pytorch.org/docs/stable/nn.html#loss-functions"&gt;cross-entropy loss&lt;/a&gt; for classification. If you look at the documentation you can see that PyTorch's cross entropy function applies a softmax function to the output layer &lt;b&gt;and&lt;/b&gt; then calculates the log loss (so you don't want to do softmax as part of the model output).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9a5f81e" class="outline-2"&gt;
&lt;h2 id="org9a5f81e"&gt;Train the Network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9a5f81e"&gt;
&lt;p&gt;
The steps for training/learning from a batch of data are:
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Clear the gradients of all optimized variables&lt;/li&gt;
&lt;li&gt;Forward pass: compute predicted outputs by passing inputs to the model&lt;/li&gt;
&lt;li&gt;Calculate the loss&lt;/li&gt;
&lt;li&gt;Backward pass: compute gradient of the loss with respect to model parameters&lt;/li&gt;
&lt;li&gt;Perform a single optimization step (parameter update)&lt;/li&gt;
&lt;li&gt;Update average training loss&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
The following loop trains for 30 epochs; feel free to change this number. For now, we suggest somewhere between 20-50 epochs. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;EPOCHS = 30
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
model.train() # prep model for training

for epoch in range(EPOCHS):
    # monitor training loss
    train_loss = 0.0
    train_losses = []
    # train the model
    for data, target in train_batches:
	# move it to the GPU or CPU
	data, target = data.to(device), target.to(device)
	# clear the gradients of all optimized variables
	optimizer.zero_grad()
	# forward pass: compute predicted outputs by passing inputs to the model
	output = model(data)
	# calculate the loss
	loss = criterion(output, target)
	# backward pass: compute gradient of the loss with respect to model parameters
	loss.backward()
	# perform a single optimization step (parameter update)
	optimizer.step()
	# update running training loss
	train_loss += loss.item() * data.size(0)

	# print training statistics 
	# calculate average loss over an epoch
    train_loss = train_loss/len(train_batches.dataset)
    train_losses.append(train_loss)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(
	epoch+1, 
	train_loss
	))
print("Training Time: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Epoch: 1 	Training Loss: 0.826836
Epoch: 2 	Training Loss: 0.324859
Epoch: 3 	Training Loss: 0.251608
Epoch: 4 	Training Loss: 0.202294
Epoch: 5 	Training Loss: 0.170231
Epoch: 6 	Training Loss: 0.146775
Epoch: 7 	Training Loss: 0.127352
Epoch: 8 	Training Loss: 0.115026
Epoch: 9 	Training Loss: 0.104332
Epoch: 10 	Training Loss: 0.093575
Epoch: 11 	Training Loss: 0.084913
Epoch: 12 	Training Loss: 0.077826
Epoch: 13 	Training Loss: 0.071506
Epoch: 14 	Training Loss: 0.067273
Epoch: 15 	Training Loss: 0.063749
Epoch: 16 	Training Loss: 0.058150
Epoch: 17 	Training Loss: 0.054770
Epoch: 18 	Training Loss: 0.051584
Epoch: 19 	Training Loss: 0.047762
Epoch: 20 	Training Loss: 0.045219
Epoch: 21 	Training Loss: 0.041732
Epoch: 22 	Training Loss: 0.040526
Epoch: 23 	Training Loss: 0.038247
Epoch: 24 	Training Loss: 0.035713
Epoch: 25 	Training Loss: 0.033801
Epoch: 26 	Training Loss: 0.031963
Epoch: 27 	Training Loss: 0.031082
Epoch: 28 	Training Loss: 0.028971
Epoch: 29 	Training Loss: 0.027500
Epoch: 30 	Training Loss: 0.026876
Training Time: 0:05:59.808071
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org625da8d" class="outline-2"&gt;
&lt;h2 id="org625da8d"&gt;Test the Trained Network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org625da8d"&gt;
&lt;p&gt;
Finally, we test our best model on previously unseen &lt;b&gt;&lt;b&gt;test data&lt;/b&gt;&lt;/b&gt; and evaluate it's performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4c8f7ec" class="outline-3"&gt;
&lt;h3 id="org4c8f7ec"&gt;&lt;code&gt;model.eval()&lt;/code&gt;&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4c8f7ec"&gt;
&lt;p&gt;
&lt;code&gt;model.eval(&lt;/code&gt;) will set all the layers in your model to evaluation mode. This affects layers like dropout layers that turn "off" nodes during training with some probability, but should allow every node to be "on" for evaluation!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgec52a3e" class="outline-3"&gt;
&lt;h3 id="orgec52a3e"&gt;Set Up the Testing&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgec52a3e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
&lt;span class="n"&gt;class_correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;class_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# forward pass: compute predicted outputs by passing inputs to the model&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# calculate the loss&lt;/span&gt;
    &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# update test loss &lt;/span&gt;
    &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# convert output probabilities to predicted class&lt;/span&gt;
    &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# compare predictions to true label&lt;/span&gt;
    &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view_as&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="c1"&gt;# calculate test accuracy for each object class&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
	&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
	&lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
	&lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Test Time: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Time: 0:00:01.860151
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org06b67f5" class="outline-3"&gt;
&lt;h3 id="org06b67f5"&gt;Calculate and Print Average Test Loss&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org06b67f5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Loss: &lt;/span&gt;&lt;span class="si"&gt;{:.6f}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Accuracy of Batch &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{:.2f}&lt;/span&gt;&lt;span class="s1"&gt; (&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;)'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	    &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])))&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'Test Accuracy of &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s1"&gt;: N/A (no training examples)'&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;Test Accuracy (Overall): &lt;/span&gt;&lt;span class="si"&gt;%2d%%&lt;/span&gt;&lt;span class="s1"&gt; (&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;/&lt;/span&gt;&lt;span class="si"&gt;%2d&lt;/span&gt;&lt;span class="s1"&gt;)'&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="mf"&gt;100.&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_correct&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_total&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Loss: 0.056054

Test Accuracy of Batch 0: 99.18 (972.0/980.0)
Test Accuracy of Batch 1: 99.21 (1126.0/1135.0)
Test Accuracy of Batch 2: 98.16 (1013.0/1032.0)
Test Accuracy of Batch 3: 98.02 (990.0/1010.0)
Test Accuracy of Batch 4: 98.47 (967.0/982.0)
Test Accuracy of Batch 5: 98.43 (878.0/892.0)
Test Accuracy of Batch 6: 98.12 (940.0/958.0)
Test Accuracy of Batch 7: 97.47 (1002.0/1028.0)
Test Accuracy of Batch 8: 97.13 (946.0/974.0)
Test Accuracy of Batch 9: 98.12 (990.0/1009.0)

Test Accuracy (Overall): 98% (9824/10000)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org83cfef1" class="outline-3"&gt;
&lt;h3 id="org83cfef1"&gt;Visualize Sample Test Results&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org83cfef1"&gt;
&lt;p&gt;
This cell displays test images and their labels in this format: &lt;code&gt;predicted (ground-truth)&lt;/code&gt;. The text will be green for accurately classified examples and red for incorrect predictions.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3f75b32" class="outline-4"&gt;
&lt;h4 id="org3f75b32"&gt;Obtain One Batch of Test Images&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3f75b32"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;dataiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataiter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# get sample outputs&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# convert output probabilities to predicted class&lt;/span&gt;
&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# prep images for display&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# plot the images in the batch, along with predicted and true labels&lt;/span&gt;
&lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="n"&gt;yticks&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[])&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'gray'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt; (&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;)"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;())),&lt;/span&gt;
		 &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"green"&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;preds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s2"&gt;"red"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/test.png" alt="test.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
This model is surprisingly accurate. I say surprising, even though we created a very accurate model previously, because in my original implementation I used &lt;code&gt;RMSprop&lt;/code&gt; as the optimizer, because that's what the Keras implementation used, but then I only got 11%. I'm guessing that there's some extra tuning you need to do to the parameters for &lt;code&gt;RMSprop&lt;/code&gt; but I just naively used the defaults. In any case, it semms that SGD is still the champ.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>classification</category><category>cnn</category><category>exercise</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/cnn/mnist-mlp/</guid><pubDate>Mon, 26 Nov 2018 01:29:13 GMT</pubDate></item></channel></rss>