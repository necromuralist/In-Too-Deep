<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neurotic Networking (Posts about Face Detection)</title><link>https://necromuralist.github.io/Neurotic-Networking/</link><description></description><atom:link href="https://necromuralist.github.io/Neurotic-Networking/categories/cat_face-detection.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2021 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; &lt;a rel="license" href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;img id="license-image" alt="Creative Commons License" style="border-width:0" src="https://licensebuttons.net/l/by/4.0/80x15.png" /&gt;&lt;/a&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.</copyright><lastBuildDate>Tue, 09 Mar 2021 06:02:55 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Human Face Detection</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#orgd02edce"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#org5e48c6d"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#org75675e9"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#org6034b8d"&gt;OpenCV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#orgdce6fc7"&gt;DLIB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd02edce" class="outline-2"&gt;
&lt;h2 id="orgd02edce"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd02edce"&gt;
&lt;p&gt;
In this post, I'll use two libraries to detect human faces in images - &lt;a href="https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html"&gt;OpenCV&lt;/a&gt; and a python interface to &lt;a href="http://dlib.net/"&gt;dlib&lt;/a&gt; called &lt;a href="https://github.com/ageitgey/face_recognition"&gt;&lt;code&gt;face_recognition&lt;/code&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5e48c6d" class="outline-2"&gt;
&lt;h2 id="org5e48c6d"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5e48c6d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6986c8b" class="outline-3"&gt;
&lt;h3 id="org6986c8b"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6986c8b"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org84a2418" class="outline-4"&gt;
&lt;h4 id="org84a2418"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org84a2418"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
import os
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgbb20bf7" class="outline-4"&gt;
&lt;h4 id="orgbb20bf7"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgbb20bf7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from PIL import Image
import cv2
import face_recognition
import matplotlib
import matplotlib.image as matplotlib_image
import matplotlib.patches as patches
import matplotlib.pyplot as pyplot
import numpy
import seaborn
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge66de1e" class="outline-4"&gt;
&lt;h4 id="orge66de1e"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge66de1e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from neurotic.tangles.data_paths import DataPathTwo
from neurotic.tangles.f1_scorer import F1Scorer
from neurotic.tangles.timer import Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org789e1b7" class="outline-3"&gt;
&lt;h3 id="org789e1b7"&gt;Set Up the Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org789e1b7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=1)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3744c09" class="outline-3"&gt;
&lt;h3 id="org3744c09"&gt;Build the Timer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3744c09"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org734540f" class="outline-3"&gt;
&lt;h3 id="org734540f"&gt;Helpers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org734540f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def first_prediction(source: list, start:int=0) -&amp;gt; int:
    """Gets the index of the first True prediction

    Args:
     source: list of True/False predictions
     start: index to start the search from

    Returns:
     index of first True prediction found
    """
    for index, prediction in enumerate(source[start:]):
	if prediction:
	    print("{}: {}".format(start + index, prediction))
	    break
    return start + index
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9ce12ac" class="outline-3"&gt;
&lt;h3 id="org9ce12ac"&gt;Set the Random Seed&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9ce12ac"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;numpy.random.seed(2019)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org75675e9" class="outline-2"&gt;
&lt;h2 id="org75675e9"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org75675e9"&gt;
&lt;p&gt;
Download the &lt;a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip"&gt;human dataset&lt;/a&gt; (this is a download link), unzip the folder, and place it in a folder named &lt;code&gt;/lfw&lt;/code&gt;.  
&lt;/p&gt;

&lt;p&gt;
The &lt;a href="http://vis-www.cs.umass.edu/lfw/lfw.tgz"&gt;human dataset&lt;/a&gt; is the &lt;a href="http://vis-www.cs.umass.edu/lfw/"&gt;Labeled Faces in the Wild&lt;/a&gt; data set which was built to study the problem of facial recognition. It's made up of real photos of people taken from the web. Each photo sits in a sub-folder that was given the name of the person (e.g. &lt;a href="https://en.wikipedia.org/wiki/Michelle_Yeoh"&gt;Michelle_Yeoh&lt;/a&gt;). The folder hasn't been split inte train-test-validiation folders the way the dog dataset was.
&lt;/p&gt;

&lt;p&gt;
The &lt;a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"&gt;dog dataset&lt;/a&gt; (this is also a download link) is in a zip-file hosted on Amazon Web Services. The folder should contain three folders (&lt;code&gt;test&lt;/code&gt;, &lt;code&gt;train&lt;/code&gt;, and &lt;code&gt;valid&lt;/code&gt;) and each of these folders should have 133 folders, one for each dog-breed. It looks like the &lt;a href="http://vision.stanford.edu/aditya86/ImageNetDogs/"&gt;Stanford Dogs Dataset&lt;/a&gt;, but the Stanford data set has 120 breeds, so I don't know the actual source.
&lt;/p&gt;

&lt;p&gt;
You might be thinking &lt;i&gt;Why are we loading dog images if this is about detecting human faces?&lt;/i&gt; but our goal is to discern human images from dog images so the dog images will act as our negative data set (the one we don't want to detect faces in).
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5d47f4f" class="outline-3"&gt;
&lt;h3 id="org5d47f4f"&gt;The Paths to the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5d47f4f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
dog_path = DataPathTwo(folder_key="DOG_PATH")
print(dog_path.folder)
assert dog_path.folder.is_dir()
for folder in dog_path.folder.iterdir():
    print("Dog: {}".format(folder))
human_path = DataPathTwo(folder_key="HUMAN_PATH")
print(human_path.folder)
assert human_path.folder.is_dir()

for name in human_path.folder.glob("Gina*"):
    print(name)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/dog-breed-classification/dogImages
Dog: /home/hades/datasets/dog-breed-classification/dogImages/valid
Dog: /home/hades/datasets/dog-breed-classification/dogImages/train
Dog: /home/hades/datasets/dog-breed-classification/dogImages/test
/home/hades/datasets/dog-breed-classification/lfw
/home/hades/datasets/dog-breed-classification/lfw/Gina_Torres
/home/hades/datasets/dog-breed-classification/lfw/Gina_Centrello
/home/hades/datasets/dog-breed-classification/lfw/Gina_Gershon
/home/hades/datasets/dog-breed-classification/lfw/Gina_Lollobrigida
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer.start()
people = len(set(human_path.folder.iterdir()))
images = len(set(human_path.folder.glob("*/*")))
print("People Count: {:,}".format(people))
print("Image Count: {:,}".format(images))
print("Images Per Person: {:.2f}".format(images/people))
timer.end()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
People Count: 5,749
Image Count: 13,233
Images Per Person: 2.30
Ended: 2019-01-02 19:28:11.529962
Elapsed: 0:00:00.550351
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org29297ae" class="outline-3"&gt;
&lt;h3 id="org29297ae"&gt;Load All the Files&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org29297ae"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer.start()
human_files = numpy.array(list(human_path.folder.glob("*/*")))
dog_files = numpy.array(list(dog_path.folder.glob("*/*/*")))
print('There are {:,} total human images.'.format(len(human_files)))
print('There are {:,} total dog images.'.format(len(dog_files)))
timer.end()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
There are 13,233 total human images.
There are 8,351 total dog images.
Ended: 2019-01-02 19:28:20.426379
Elapsed: 0:00:00.816752
&lt;/pre&gt;


&lt;p&gt;
The &lt;code&gt;human_files&lt;/code&gt; and &lt;code&gt;dog_files&lt;/code&gt; are numpy arrays of python &lt;code&gt;Path&lt;/code&gt; objects pointing to image files. Note that at this point we've thrown away all the dog-breed information as well as the names of the people in the images. We're only going for a binary split - human or not human.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org02e1eb0" class="outline-3"&gt;
&lt;h3 id="org02e1eb0"&gt;Test Sets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org02e1eb0"&gt;
&lt;p&gt;
The models we're going to use are pre-trained so we're just going to choose 100 images from each set to see how well they do.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;human_files_short = numpy.random.choice(human_files, 100)
dog_files_short = numpy.random.choice(dog_files, 100)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org04a4c7a" class="outline-3"&gt;
&lt;h3 id="org04a4c7a"&gt;The Scorer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org04a4c7a"&gt;
&lt;p&gt;
The &lt;code&gt;human_scorer&lt;/code&gt; will score how well the detectors did on our data sets. The only thing that needs to be passed into it is the detector/predictor that decides if an image has a human in it. Calling it will produce an org-table with some metrics about how well it did.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;human_scorer = partial(F1Scorer,
		       true_images=human_files_short,
		       false_images=dog_files_short)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6034b8d" class="outline-2"&gt;
&lt;h2 id="org6034b8d"&gt;OpenCV&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6034b8d"&gt;
&lt;p&gt;
We're going to use OpenCV's implementation of &lt;a href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html"&gt;Haar feature-based cascade classifiers&lt;/a&gt; to detect human faces in images.  
&lt;/p&gt;

&lt;p&gt;
OpenCV provides pre-trained face detectors stored as XML files on &lt;a href="https://github.com/opencv/opencv/tree/master/data/haarcascades"&gt;github&lt;/a&gt;. The detector I'm going to use is stored in a directory named &lt;code&gt;haarcascades&lt;/code&gt;. Here's a demonstration of how to use this face detector to find a human face in an image.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org19b0162" class="outline-3"&gt;
&lt;h3 id="org19b0162"&gt;Extract the Pre-Trained Face Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org19b0162"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;haar_path = DataPathTwo("haarcascade_frontalface_alt.xml", folder_key="HAAR_CASCADES")
assert haar_path.from_folder.is_file()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Ended: 2019-01-02 19:28:33.152747
Elapsed: 0:00:00.000933
&lt;/pre&gt;


&lt;p&gt;
As you can see from the file-name this detector is tuned for faces looking at the camera (as opposed to, say, a face in profile). Now we need to build the classifier using the XML file.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class OpenCVFaceDetector:
    """OpenCV Face Detector

    Args:
     path: path to the model's XML file
    """
    def __init__(self, path: str) -&amp;gt; None:
	self.path = path
	self._classifier = None
	return

    @property
    def classifier(self) -&amp;gt; cv2.CascadeClassifier:
	"""Face Classifier"""
	if self._classifier is None:
	    self._classifier = cv2.CascadeClassifier(self.path)
	return self._classifier

    def detect_faces(self, image_path: str) -&amp;gt; numpy.ndarray:
	"""Find faces in an image

	Args:
	 image_path: path to the image

	Returns:
	 array of bounding boxes
	"""
	# this creates a Matplotlib Image
	image = cv2.imread(str(image_path))
	# the classifier needs a grayscale image
	grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
	return self.classifier.detectMultiScale(grayscale)

    def add_bounding_boxes(self, image_path: str) -&amp;gt; numpy.ndarray:
	"""Adds bounding boxes to the image

	Args:
	 image: path to the image

	Returns:
	 RGB image with faces boxed in
	"""
	faces = self.detect_faces(image_path)
	# this is redundant, but it's only for troubleshooting
	image = cv2.imread(str(image_path))

	# The arguments to the ``cv2.rectangle`` call are
	#  - image
	#  - the top-left coordinates of the rectangle
	#  - the bottom-right coordinates of the rectangle
	#  - the color
	#  - the thickness of the line.
	for top_left_x, top_left_y ,width, height in faces:
	    cv2.rectangle(image,
		  (top_left_x, top_left_y),
		  (top_left_x + width, top_left_y + height),
		  (255,0,0), 2)
	# the image is BGR, so the triplet setting the color =(200, 0, 0)=
	# is setting the rectangle to blue.
	# before we convert it to RGB
	return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    def has_face(self, image_path: str) -&amp;gt; bool:
	"""Checks if the image contains faces

	Args:
	 image_path: path to the image file

	Returns:
	 True if there is at least one face in the image
	"""
	return len(self.detect_faces(image_path)) &amp;gt; 0
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;open_cv_detector = OpenCVFaceDetector(str(haar_path.from_folder))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8804f6c" class="outline-3"&gt;
&lt;h3 id="org8804f6c"&gt;Check Out How It Works On An Image&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8804f6c"&gt;
&lt;p&gt;
Before trying to use it, let's see how it does on one of our faces. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
figure.suptitle("OpenCV Face-Detection Bounding Box", weight="bold")
image = axe.imshow(open_cv_detector.add_bounding_boxes(human))
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="orgdb53ce4" class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/opencv_face_bounded.png" alt="opencv_face_bounded.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Seems like it did a reasonable job. If you run this enough times you'll note that it draws the tightest box when the person is facing the camera directly and grabs more negative space when the person angles their head away from the camera.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7ddfdaa" class="outline-3"&gt;
&lt;h3 id="org7ddfdaa"&gt;Face Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7ddfdaa"&gt;
&lt;p&gt;
Now that we have something that will draw bounding boxes for any faces it finds in photographs we can create a face-detector that just returns &lt;code&gt;True&lt;/code&gt; if there is a face or &lt;code&gt;False&lt;/code&gt; if there isn't one.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge18a15a" class="outline-3"&gt;
&lt;h3 id="orge18a15a"&gt;Testing the Face Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge18a15a"&gt;
&lt;p&gt;
Here we're going to see how well the face detector does at detecting human faces and not mistaking dogs for humans.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;open_cv_scorer = human_scorer(open_cv_detector.has_face)
open_cv_scorer()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Accuracy&lt;/td&gt;
&lt;td class="org-right"&gt;0.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Precision&lt;/td&gt;
&lt;td class="org-right"&gt;0.85&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Recall&lt;/td&gt;
&lt;td class="org-right"&gt;1.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Specificity&lt;/td&gt;
&lt;td class="org-right"&gt;0.83&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;F1&lt;/td&gt;
&lt;td class="org-right"&gt;0.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Ended&lt;/td&gt;
&lt;td class="org-right"&gt;2019-01-03 14:01:49.321416&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Elapsed&lt;/td&gt;
&lt;td class="org-right"&gt;0:00:17.670546&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It did pretty well, but was penalized for some false-positives. What did a false positive look like?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgeb7230a" class="outline-3"&gt;
&lt;h3 id="orgeb7230a"&gt;Looking at the False Positives&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgeb7230a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dogman_index = first_prediction(open_cv_scorer.false_image_predictions)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
1: True
&lt;/pre&gt;


&lt;p&gt;
So the image at index 1 was a dog that the OpenCV detector thought was a human.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
source = dog_files_short[dogman_index]
name = " ".join(
    os.path.splitext(
	os.path.basename(source))[0].split("_")[:-1]).title()
figure.suptitle("Dog-Human OpenCV Prediction ({})".format(
    name), weight="bold")
image = Image.open(source)
image = axe.imshow(image)
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="org6eacc2e" class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/files/posts/nano/dog-breed-classifier/human-face-detection/opencv_dog_man.png" alt="opencv_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div id="org555f646" class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/opencv_dog_man.png" alt="opencv_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
This doesn't really look like a human, but I don't think the detector is specifically trained for &lt;i&gt;humans&lt;/i&gt; so much as &lt;i&gt;features&lt;/i&gt; that human have when looking straight at the camera, so I'm guessing straight-on views will create false positives. Although the mouth seems to be kind of inhuman.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdce6fc7" class="outline-2"&gt;
&lt;h2 id="orgdce6fc7"&gt;DLIB&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdce6fc7"&gt;
&lt;p&gt;
Now for another face-detector, this time using &lt;a href="https://github.com/ageitgey/face_recognition"&gt;&lt;code&gt;face_recognition&lt;/code&gt;&lt;/a&gt;, a python interface to &lt;a href="http://dlib.net/"&gt;dlib's&lt;/a&gt; facial recognition code.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga35350d" class="outline-3"&gt;
&lt;h3 id="orga35350d"&gt;Testing It With an Image&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga35350d"&gt;
&lt;p&gt;
Let's see how the bounding box it produces looks given the same image that the &lt;code&gt;OpenCV&lt;/code&gt; detector was given.
&lt;/p&gt;

&lt;p&gt;
The face-recognition code is much simpler, but to make it consistent I'll add a class that matches the &lt;code&gt;OpenCVFaceDetector&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DlibFaceDetector:
    """DLIB (via face_detector) face detector"""
    def detect_faces(self, image_path: str) -&amp;gt; numpy.ndarray:
	"""Finds the locations of faces

	Args:
	 image_path: path to the image

	Returns:
	 array of bounding box coordinates for the face(s)
	"""
	image = face_recognition.load_image_file(str(image_path))
	return face_recognition.face_locations(image)

    def add_bounding_boxes(self, image_path: str,
			   axe: matplotlib.axes.Axes) -&amp;gt; None:
	"""Adds patches to the current matplotlib figure

	Args:
	 image_path: path to the image file
	 axe: axes to add the rectangle to
	"""
	for (top, right, bottom, left) in self.detect_faces(image_path):
	    width = right - left
	    height = top - bottom
	    rectangle = patches.Rectangle((top, right), width, height,
					  fill=False)
	    axe.add_patch(rectangle)
	return

    def has_face(self, image_path: str) -&amp;gt; bool:
	"""Checks if there is at least one face in the image

	Args:
	 image_path: path to the image file

	Returns:
	 True if there's at least one face in the image
	"""
	return len(self.detect_faces(image_path)) &amp;gt; 0
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dlib_detector = DlibFaceDetector()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
image = matplotlib_image.imread(str(human))
figure.suptitle("dlib Face Recognition Bounding-Box", weight='bold')
dlib_detector.add_bounding_boxes(str(human), axe)
plot = axe.imshow(image)
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="orga74419e" class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/files/posts/nano/dog-breed-classifier/human-face-detection/dlib_box.png" alt="dlib_box.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div id="org38c2d53" class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/dlib_box.png" alt="dlib_box.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
It seems pretty comparable to what the &lt;code&gt;OpenCV&lt;/code&gt; detector came up with.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2550f3c" class="outline-3"&gt;
&lt;h3 id="org2550f3c"&gt;Measuring Performance&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2550f3c"&gt;
&lt;p&gt;
Once again I'll run it through the FI scorer to see what's what.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dlib_scorer = human_scorer(dlib_detector.has_face)
dlib_scorer()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Accuracy&lt;/td&gt;
&lt;td class="org-right"&gt;0.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Precision&lt;/td&gt;
&lt;td class="org-right"&gt;0.86&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Recall&lt;/td&gt;
&lt;td class="org-right"&gt;1.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Specificity&lt;/td&gt;
&lt;td class="org-right"&gt;0.84&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;F1&lt;/td&gt;
&lt;td class="org-right"&gt;0.93&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Ended&lt;/td&gt;
&lt;td class="org-right"&gt;2019-01-03 14:31:36.848015&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Elapsed&lt;/td&gt;
&lt;td class="org-right"&gt;0:00:47.395556&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
The dlib model did &lt;i&gt;slightly&lt;/i&gt; better with its avoidance of false positives, but it might not be enough to justify the extra time.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge32c802" class="outline-3"&gt;
&lt;h3 id="orge32c802"&gt;False Humans&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge32c802"&gt;
&lt;p&gt;
What kind of image did the DLib Classifier classify as human when it came from the dog images?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dlib_dog_human_index = first_prediction(dlib_scorer.false_image_predictions)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
11: True
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
source = dog_files_short[dlib_dog_human_index]
name = " ".join(
    os.path.splitext(
	os.path.basename(source))[0].split("_")[:-1]).title()
figure.suptitle("Dog-Human DLib Prediction ({})".format(
    name), weight="bold")
image = Image.open(source)
image = axe.imshow(image)
&lt;/pre&gt;&lt;/div&gt;


&lt;div id="org1785deb" class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/files/posts/nano/dog-breed-classifier/human-face-detection/dlib_dog_man.png" alt="dlib_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div id="org1373bab" class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/dlib_dog_man.png" alt="dlib_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Well, this was a bit of a surprise. I don't know that it's really fair to be using this type of image, but what can you do?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>dlib</category><category>face detection</category><category>opencv</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/</guid><pubDate>Wed, 02 Jan 2019 21:51:55 GMT</pubDate></item></channel></rss>