<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neurotic Networking (Posts about topic modeling)</title><link>https://necromuralist.github.io/Neurotic-Networking/</link><description></description><atom:link href="https://necromuralist.github.io/Neurotic-Networking/categories/topic-modeling.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2021 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img id="license-image" alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /&gt;&lt;/a&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.</copyright><lastBuildDate>Fri, 01 Jan 2021 00:38:18 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Topic Modeling With Matrix Decomposition</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org495dcde"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org47a9bf9"&gt;Related Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org33b1c3d"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org44ee8e6"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org7b8563e"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org543be76"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#orgf0cf954"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org66479bf"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#orgcaa9df7"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#orge3671b6"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org23397b8"&gt;The Dataset&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org15ef0dd"&gt;Vectorizing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#orgcdea0f6"&gt;Singular Value Decomposition (SVD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#orgef54610"&gt;Looking At Some Topics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#org4541996"&gt;Non-negative Matrix Factorization (NMF)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/#orgc5a8909"&gt;Term-Frequency/Inverse Document Frequency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org495dcde" class="outline-2"&gt;
&lt;h2 id="org495dcde"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org495dcde"&gt;
&lt;p&gt;
This is part of a walk-through of the &lt;a href="https://github.com/fastai/course-nlp"&gt;fastai Code-First Introduction to NLP&lt;/a&gt;. In this post I'll be using &lt;a href="https://www.wikiwand.com/en/Singular_value_decomposition"&gt;Singular Value Decomposition (SVD)&lt;/a&gt; and &lt;a href="https://www.wikiwand.com/en/Non-negative_matrix_factorization"&gt;Non-Negative Matrix Factorization (NMF)&lt;/a&gt; to group &lt;a href="https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset"&gt;newsgroup posts&lt;/a&gt;. Both of these methods are statistical approaches that use the word-counts within documents to decide how similar they are (while ignoring things like word order).
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org47a9bf9" class="outline-3"&gt;
&lt;h3 id="org47a9bf9"&gt;Related Tutorials&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org47a9bf9"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html"&gt;Out-Of-Core Text Classification with sklearn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://de.dariah.eu/tatom/index.html)"&gt;Text Analysis with Topic Models for the Humanities and Social Sciences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org33b1c3d" class="outline-3"&gt;
&lt;h3 id="org33b1c3d"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org33b1c3d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org44ee8e6" class="outline-4"&gt;
&lt;h4 id="org44ee8e6"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org44ee8e6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
import random
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7b8563e" class="outline-4"&gt;
&lt;h4 id="org7b8563e"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7b8563e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from scipy import linalg
from sklearn import decomposition
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import hvplot.pandas
import matplotlib.pyplot as pyplot
import numpy
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org543be76" class="outline-4"&gt;
&lt;h4 id="org543be76"&gt;Others&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org543be76"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import EmbedHoloviews, Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf0cf954" class="outline-3"&gt;
&lt;h3 id="orgf0cf954"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf0cf954"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org66479bf" class="outline-4"&gt;
&lt;h4 id="org66479bf"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org66479bf"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcaa9df7" class="outline-4"&gt;
&lt;h4 id="orgcaa9df7"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgcaa9df7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Embed = partial(
    EmbedHoloviews,
    folder_path="../../files/posts/fastai/topic-modeling-with-matrix-decomposition")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge3671b6" class="outline-2"&gt;
&lt;h2 id="orge3671b6"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge3671b6"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org23397b8" class="outline-3"&gt;
&lt;h3 id="org23397b8"&gt;The Dataset&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org23397b8"&gt;
&lt;p&gt;
The dataset consists of ~18,000 newsgroup posts with 20 topics. To keep the computation down I'll only use a subset of the categories. I'm also going to only use the body of the posts.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;keep = ("alt.atheism", "comp.graphics", "misc.forsale", "sci.crypt", "talk.politics.guns")
remove = ("headers", "footers", "quotes")
training = fetch_20newsgroups(subset="train", categories=keep, remove=remove)
testing = fetch_20newsgroups(subset="test", categories=keep, remove=remove)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
I've run this more than once so there's no output, but the first time you run the &lt;code&gt;fetch_20newsgroups&lt;/code&gt; function it downloads the dataset and you'll see some output mentioning this fact.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(f"{training.filenames.shape[0]:,}")
print(f"{training.target.shape[0]:,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2,790
2,790
&lt;/pre&gt;


&lt;p&gt;
So, although the entire dataset has over 18,000 entries, our sub-set has fewer than 3,000.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(numpy.unique(training.target))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[0 1 2 3 4]
&lt;/pre&gt;


&lt;p&gt;
So the categories don't seem to be preserved (I'm assuming that the five I kept weren't the first five in the original set) so you have to check anytime you pull a subset out of the data.
&lt;/p&gt;

&lt;p&gt;
Lets see what one of the posts looks like.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(random.choice(training.data))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example" id="orgd5f6148"&gt;
      Just a question. 
      As a provider of a public BBS service - aren't you bound by law to gurantee
      intelligble access to the data of the users on the BBS, if police comes
      with sufficent authorisation ? I guessed this would be  a basic condition
      for such systems. (I did run a bbs some time ago, but that was in Switzerland)

The US doesn't yet have many laws covering BBSs - they're not common carriers,
they're not phone companies, they're just private machines or services
operated by businesses.  There's no obligation to keep records.
As Perry Metzger points out, if the police come with a search warrant,
you have to let them see what the warrant demands, if it exists,
and they generally can confiscate the equipment as "evidence"
(which is not Constitutionally valid, but we're only beginning to
develop court cases supporting us).  A court MAY be able to compel
you to tell them information you know, such as the encryption password
for the disk - there aren't any definitive cases yet, since it's a new
situation, and there probably aren't laws specifically covering it.
But the court can't force you to *know* the keys, and there are no
laws preventing you from allowing your users to have their own keys
for their own files without giving them to you.

Even in areas that do have established law, there is uncertainty.
There was a guy in Idaho a few years ago who had his business records
subpoenaed as evidence for taxes or some other business-restriction law,
so he gave the court the records.  Which were in Hebrew.
The US doesn't have laws forcing you to keep your records in English,
and these were the originals of the records.  HE didn't speak Hebrew,
and neither did anybody in the court organization.  Don't think they
were able to do much about it.

It might be illegal for your BBS to deny access to potential customers
based on race, religion, national origin, gender, or sexual preference;
it probably hasn't been tested in court, but it seems like a plausible
extension of anti-discrimination laws affecting other businesses.
&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-org15ef0dd" class="outline-4"&gt;
&lt;h4 id="org15ef0dd"&gt;Vectorizing&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org15ef0dd"&gt;
&lt;p&gt;
Here we'll convert the text to a matrix using &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"&gt;sklearn's CountVectorizer&lt;/a&gt;.
Interestingly, the &lt;a href="https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html"&gt;Introduction to Information Retrieval&lt;/a&gt; book says the the trend has been towards not removing the most common words (&lt;i&gt;stop word&lt;/i&gt;) but we'll be dropping them. There's a paper called &lt;a href="https://www.aclweb.org/anthology/W18-2502/"&gt;Stop Word Lists in Free Open-source Software Packages&lt;/a&gt; which points out some problems with stop-word lists in general, but sklearn's list in particular. I don't know if sklearn has done anything to address their concerns since the paper came out, but the sklearn documentation includes a link to the paper so I would assume the problems are still there. Nonetheless, the fastai examples uses them so I will too.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vectorizer = CountVectorizer(stop_words="english")
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The function we'le going to use doesn't accept the sparse matrices that are output by default so we'll make it a dense matrix after it's fit.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    vectors = vectorizer.fit_transform(training.data).todense()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2020-01-01 16:26:48,048 graeae.timers.timer start: Started: 2020-01-01 16:26:48.047927
2020-01-01 16:26:48,466 graeae.timers.timer end: Ended: 2020-01-01 16:26:48.466285
2020-01-01 16:26:48,466 graeae.timers.timer end: Elapsed: 0:00:00.418358
&lt;/pre&gt;


&lt;p&gt;
That was much quicker than I thought it would be, probably because our dataset is so small.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vocabulary = vectorizer.get_feature_names()
print(f"{len(vocabulary):,}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
34,632
&lt;/pre&gt;


&lt;p&gt;
So our "vocabulary" is around 35,000 tokens.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcdea0f6" class="outline-3"&gt;
&lt;h3 id="orgcdea0f6"&gt;Singular Value Decomposition (SVD)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcdea0f6"&gt;
&lt;p&gt;
Singular Value Decomposition is a linear algebra method to factor a matrix. The math is beyond me at this point, so I'll just try using it as a black box.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    U, s, V = linalg.svd(vectors, full_matrices=False)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2020-01-01 16:26:50,508 graeae.timers.timer start: Started: 2020-01-01 16:26:50.508003
2020-01-01 16:27:23,979 graeae.timers.timer end: Ended: 2020-01-01 16:27:23.978988
2020-01-01 16:27:23,980 graeae.timers.timer end: Elapsed: 0:00:33.470985
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;s_frame = pandas.Series(s)
plot = s_frame.hvplot().opts(title="Diagonal Matrix S", width=1000, height=800)
Embed(plot=plot, file_name="s_values")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/s_values.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgef54610" class="outline-3"&gt;
&lt;h3 id="orgef54610"&gt;Looking At Some Topics&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgef54610"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;top_words_count = 8

def top_words(token):
    return [vocabulary[index] for index in numpy.argsort(token)[: -top_words_count - 1: -1]]

def show_topics(array):
    topic_words = ([top_words(topic) for topic in array])
    return [' '.join(topic) for topic in topic_words]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;topics = show_topics(V[:10])
for index, topic in enumerate(topics):
    print(f"{index}: {topic}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example" id="org2458576"&gt;
0: propagandist heliocentric galacticentric surname sandvik 400included wovy imaginative
1: file jpeg image edu pub ftp use graphics
2: file gun congress firearms control mr states rkba
3: privacy internet anonymous pub email information eff mail
4: graphics edu 128 3d ray pub data ftp
5: 00 50 40 appears dos 10 art 25
6: privacy internet 00 jpeg eff pub email electronic
7: key data image encryption des chip available law
8: pub key jesus jpeg eff graphics encryption ripem
9: key encryption edu des anonymous posting chip graphics
&lt;/pre&gt;

&lt;p&gt;
So what we're showing is the most significant words for the top-ten most strongly grouped "topics". It takes a little bit of interpretation to figure out how to map them to the newsgroups we used, and there probably could have been some clean-up of the texts (entry 5 looks suspect) but it's interesting that this linear algebra decomposition method could find these similar groups without any kind of prompting as to what groups might even exist in the first place (this is an unsupervised method, not a supervised method).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4541996" class="outline-3"&gt;
&lt;h3 id="org4541996"&gt;Non-negative Matrix Factorization (NMF)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4541996"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;number_of_topics = 5
classifier = decomposition.NMF(n_components=number_of_topics, random_state=1)
weights = classifier.fit_transform(vectors)
classified = classifier.components_
for index, topic in enumerate(show_topics(classified)):
    print(f"{index}: {topic}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
0: db mov bh si cs byte al bl
1: privacy internet anonymous information email eff use pub
2: file gun congress control firearms states mr united
3: jpeg image gif file color images format quality
4: edu graphics pub image data ftp mail available
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc5a8909" class="outline-3"&gt;
&lt;h3 id="orgc5a8909"&gt;Term-Frequency/Inverse Document Frequency&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc5a8909"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tfidf_vectorizer = TfidfVectorizer(stop_words="english")
tfidf_vectors = tfidf_vectorizer.fit_transform(training.data)
weights = classifier.fit_transform(tfidf_vectors)
classified = classifier.components_

for index, topic in enumerate(show_topics(classified)):
    print(f"{index}: {topic}")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
0: people gun don think just guns right government
1: 00 sale offer shipping new drive price condition
2: key chip encryption clipper keys escrow government algorithm
3: graphics thanks file files image program know windows
4: god atheism believe does atheists belief said exist
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>matrices</category><category>nlp</category><category>topic modeling</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/fastai/topic-modeling-with-matrix-decomposition/</guid><pubDate>Sun, 29 Dec 2019 01:11:58 GMT</pubDate></item></channel></rss>