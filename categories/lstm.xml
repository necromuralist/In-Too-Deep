<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neurotic Networking (Posts about lstm)</title><link>https://necromuralist.github.io/Neurotic-Networking/</link><description></description><atom:link href="https://necromuralist.github.io/Neurotic-Networking/categories/lstm.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;&lt;img id="license-image" alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/80x15.png" /&gt;&lt;/a&gt;This work is licensed under a &lt;a rel="license" href="http://creativecommons.org/licenses/by/4.0/"&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.</copyright><lastBuildDate>Fri, 23 Oct 2020 01:36:49 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Multi-Layer LSTM</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org485ff21"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org13450af"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org6149368"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org92e8116"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org05d83e6"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org74ca1ef"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org59e7d08"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org13b5ba1"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org17f93b1"&gt;The Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org37b0680"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org23cc260"&gt;Set Up the Datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org9f4f22b"&gt;The Model&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#orgcfa9795"&gt;Embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#orged0566f"&gt;Bidirectional&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#orgb45fd28"&gt;LSTM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#orgc304783"&gt;Compile It&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#org308a09e"&gt;Train the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/#orge5aabfd"&gt;Looking at the Performance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org485ff21" class="outline-2"&gt;
&lt;h2 id="org485ff21"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org485ff21"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org13450af" class="outline-3"&gt;
&lt;h3 id="org13450af"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org13450af"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6149368" class="outline-4"&gt;
&lt;h4 id="org6149368"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6149368"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
from pathlib import Path
import pickle
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org92e8116" class="outline-4"&gt;
&lt;h4 id="org92e8116"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org92e8116"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import holoviews
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org05d83e6" class="outline-4"&gt;
&lt;h4 id="org05d83e6"&gt;Others&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org05d83e6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import Timer, EmbedHoloviews
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org74ca1ef" class="outline-3"&gt;
&lt;h3 id="org74ca1ef"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org74ca1ef"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org59e7d08" class="outline-4"&gt;
&lt;h4 id="org59e7d08"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org59e7d08"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org13b5ba1" class="outline-4"&gt;
&lt;h4 id="org13b5ba1"&gt;Plotting&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org13b5ba1"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Embed = partial(EmbedHoloviews,
		folder_path="../../files/posts/keras/multi-layer-lstm/")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org17f93b1" class="outline-4"&gt;
&lt;h4 id="org17f93b1"&gt;The Dataset&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org17f93b1"&gt;
&lt;p&gt;
This once again uses the &lt;a href="https://www.tensorflow.org/datasets/catalog/imdb_reviews"&gt;IMDB dataset&lt;/a&gt; with 50,000 reviews. It has already been converted from strings to integers - each word is encoded as its own integer. Adding &lt;code&gt;with_info=True&lt;/code&gt; returns an object that contains the dictionary with the word to integer mapping. Passing in &lt;code&gt;imdb_reviews/subwords8k&lt;/code&gt; limits the vocabulary to 8,000 words.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Note:&lt;/b&gt;&lt;/b&gt; The first time you run this it will download a fairly large dataset so it might appear to hang, but after the first time it is fairly quick.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dataset, info = tensorflow_datasets.load("imdb_reviews/subwords8k",
					 with_info=True,
					 as_supervised=True)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org37b0680" class="outline-2"&gt;
&lt;h2 id="org37b0680"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org37b0680"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org23cc260" class="outline-3"&gt;
&lt;h3 id="org23cc260"&gt;Set Up the Datasets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org23cc260"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_dataset, test_dataset = dataset["train"], dataset["test"]
tokenizer = info.features['text'].encoder
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now we're going to shuffle and padd the data. The &lt;code&gt;BUFFER_SIZE&lt;/code&gt; argument sets the size of the data to sample from. In this case 10,000 entries in the training set will be selected to be put in the buffer and then the "shuffle" is created by randomly selecting items from the buffer, replacing each item as it's selected until all the data has been through the buffer. The &lt;code&gt;padded_batch&lt;/code&gt; method creates batches of consecutive data and pads them so that they are all the same shape.
&lt;/p&gt;

&lt;p&gt;
The BATCH_SIZE needs to be tuned a little. If it's too big the amount of memory needed might keep the GPU from being able to use it (and it might not generalize), and if it's too small, you will take a long time to train, so you have to do a little tuning. If you train it and the GPU process percentage stays at 0, try reducing the Batch Size.
&lt;/p&gt;

&lt;p&gt;
Also note that if you change the batch-size you have to go back to the previous step and re-define &lt;code&gt;train_dataset&lt;/code&gt; and &lt;code&gt;test_dataset&lt;/code&gt; because we alter them in the next step and re-altering them makes the shape wrong somehow.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;BUFFER_SIZE = 10000
# if the batch size is too big it will run out of memory on the GPU 
# so you might have to experiment with this
BATCH_SIZE = 32

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9f4f22b" class="outline-3"&gt;
&lt;h3 id="org9f4f22b"&gt;The Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9f4f22b"&gt;
&lt;p&gt;
The previous model had one Bidirectional layer, this will add a second one.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcfa9795" class="outline-4"&gt;
&lt;h4 id="orgcfa9795"&gt;Embedding&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgcfa9795"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/guide/embedding"&gt;Embedding layer&lt;/a&gt; converts our inputs of integers and converts them to vectors of real-numbers, which is a better input for a neural network.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orged0566f" class="outline-4"&gt;
&lt;h4 id="orged0566f"&gt;Bidirectional&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orged0566f"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional"&gt;Bidirectional layer&lt;/a&gt; is a wrapper for Recurrent Neural Networks.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb45fd28" class="outline-4"&gt;
&lt;h4 id="orgb45fd28"&gt;LSTM&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb45fd28"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM"&gt;LSTM layer&lt;/a&gt; implements Long-Short-Term Memory. The first argument is the size of the outputs. This is similar to the model that we ran previously on the same data, but it has an extra layer (so it uses more memory).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tensorflow.keras.layers.Bidirectional(
	tensorflow.keras.layers.LSTM(64, return_sequences=True)),
    tensorflow.keras.layers.Bidirectional(
	tensorflow.keras.layers.LSTM(32)),
    tensorflow.keras.layers.Dense(64, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(model.summary())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         66048     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 64)                41216     
_________________________________________________________________
dense (Dense)                (None, 64)                4160      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 635,329
Trainable params: 635,329
Non-trainable params: 0
_________________________________________________________________
None
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc304783" class="outline-4"&gt;
&lt;h4 id="orgc304783"&gt;Compile It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc304783"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model.compile(loss='binary_crossentropy',
	      optimizer="adam",
	      metrics=['accuracy'])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org308a09e" class="outline-3"&gt;
&lt;h3 id="org308a09e"&gt;Train the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org308a09e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ONCE_PER_EPOCH = 2
NUM_EPOCHS = 10
with TIMER:
    history = model.fit(train_dataset,
			epochs=NUM_EPOCHS,
			validation_data=test_dataset,
			verbose=ONCE_PER_EPOCH)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-21 17:26:50,395 graeae.timers.timer start: Started: 2019-09-21 17:26:50.394797
I0921 17:26:50.395130 140275698915136 timer.py:70] Started: 2019-09-21 17:26:50.394797
Epoch 1/10
W0921 17:26:51.400280 140275698915136 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
782/782 - 224s - loss: 0.6486 - accuracy: 0.6039 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/10
782/782 - 214s - loss: 0.4941 - accuracy: 0.7661 - val_loss: 0.6706 - val_accuracy: 0.6744
Epoch 3/10
782/782 - 216s - loss: 0.4087 - accuracy: 0.8266 - val_loss: 0.4024 - val_accuracy: 0.8222
Epoch 4/10
782/782 - 217s - loss: 0.2855 - accuracy: 0.8865 - val_loss: 0.3343 - val_accuracy: 0.8645
Epoch 5/10
782/782 - 216s - loss: 0.2097 - accuracy: 0.9217 - val_loss: 0.2936 - val_accuracy: 0.8837
Epoch 6/10
782/782 - 217s - loss: 0.1526 - accuracy: 0.9467 - val_loss: 0.3188 - val_accuracy: 0.8771
Epoch 7/10
782/782 - 215s - loss: 0.1048 - accuracy: 0.9657 - val_loss: 0.3750 - val_accuracy: 0.8710
Epoch 8/10
782/782 - 216s - loss: 0.0764 - accuracy: 0.9757 - val_loss: 0.3821 - val_accuracy: 0.8762
Epoch 9/10
782/782 - 216s - loss: 0.0585 - accuracy: 0.9832 - val_loss: 0.4747 - val_accuracy: 0.8683
Epoch 10/10
782/782 - 216s - loss: 0.0438 - accuracy: 0.9883 - val_loss: 0.4441 - val_accuracy: 0.8704
2019-09-21 18:02:56,353 graeae.timers.timer end: Ended: 2019-09-21 18:02:56.353722
I0921 18:02:56.353781 140275698915136 timer.py:77] Ended: 2019-09-21 18:02:56.353722
2019-09-21 18:02:56,356 graeae.timers.timer end: Elapsed: 0:36:05.958925
I0921 18:02:56.356238 140275698915136 timer.py:78] Elapsed: 0:36:05.958925
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge5aabfd" class="outline-3"&gt;
&lt;h3 id="orge5aabfd"&gt;Looking at the Performance&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge5aabfd"&gt;
&lt;p&gt;
To get the history I had to pickle it and then copy it over to the machine with this org-notebook, so you can't just run this notebook and make it work unless everything is run on the same machine (which it wasn't).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path("~/history.pkl").expanduser()
with path.open("wb") as writer:
    pickle.dump(history.history, writer)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = Path("~/history.pkl").expanduser()
with path.open("rb") as reader:
    history = pickle.load(reader)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame(history)
best = data.val_loss.idxmin()
best_line = holoviews.VLine(best)
plot = (data.hvplot() * best_line).opts(
    title="Two-Layer LSTM Model",
    width=1000,
    height=800)
Embed(plot=plot, file_name="lstm_training")()
&lt;/pre&gt;&lt;/div&gt;

&lt;object type="text/html" data="https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/lstm_training.html" style="width:100%" height="800"&gt;
  &lt;p&gt;Figure Missing&lt;/p&gt;
&lt;/object&gt;

&lt;p&gt;
It looks like the best epoch was the fifth one, with a validation loss of 0.29 and a validation accuracy of 0.88, after that it looks like it overfits. It seems that text might be a harder problem than images.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>lstm</category><category>nlp</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/keras/multi-layer-lstm/</guid><pubDate>Thu, 19 Sep 2019 23:07:27 GMT</pubDate></item></channel></rss>