<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>In Too Deep (Posts about transfer learning)</title><link>https://necromuralist.github.io/In-Too-Deep/</link><description></description><atom:link href="https://necromuralist.github.io/In-Too-Deep/categories/transfer-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Sun, 06 Jan 2019 23:56:21 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Transfer Learning Exercise</title><link>https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#orgfdd5c7d"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#org61de970"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#orgd469f63"&gt;Flower power&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#org06b789c"&gt;Download the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#orgd700eb6"&gt;Transforming the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#orgbb263f2"&gt;DataLoaders and Data Visualization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#org96ae6b0"&gt;Visualize some sample data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#orgba439a5"&gt;Plot The Images In The Batch, Along With The Corresponding Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#org2324f7f"&gt;Define the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#org52ef690"&gt;Final Classifier Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#org9c83d0e"&gt;Specify Loss Function and Optimizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/#org73d4f36"&gt;Training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfdd5c7d" class="outline-2"&gt;
&lt;h2 id="orgfdd5c7d"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfdd5c7d"&gt;
&lt;p&gt;
Most of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. Instead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune.
&lt;/p&gt;

&lt;p&gt;
In this notebook, you'll be using &lt;a href="https://arxiv.org/pdf/1409.1556.pdf"&gt;VGGNet&lt;/a&gt; trained on the &lt;a href="http://www.image-net.org/"&gt;ImageNet dataset&lt;/a&gt; as a feature extractor. 
&lt;/p&gt;

&lt;p&gt;
VGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but &lt;b&gt;&lt;b&gt;replace the final fully-connected layer&lt;/b&gt;&lt;/b&gt; with our own classifier. This way we can use VGGNet as a &lt;b&gt;fixed feature extractor&lt;/b&gt; for our images then easily train a simple classifier on top of that. 
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;Use all but the last fully-connected layer as a fixed feature extractor.&lt;/li&gt;
&lt;li&gt;Define a new, final classification layer and apply it to a task of our choice!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
You can read more about transfer learning from &lt;a href="http://cs231n.github.io/transfer-learning/"&gt;the CS231n Stanford course notes&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org61de970" class="outline-2"&gt;
&lt;h2 id="org61de970"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org61de970"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# python
from collections import OrderedDict
from datetime import datetime
import os

# pypi
from dotenv import load_dotenv
from torch import nn
from sklearn.model_selection import train_test_split
from torch.utils.data.sampler import SubsetRandomSampler

import matplotlib
import numpy
import seaborn
import torch
import torch.optim as optimize
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as pyplot

# this project
from neurotic.tangles.data_paths import DataPathTwo
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgda26d41" class="outline-3"&gt;
&lt;h3 id="orgda26d41"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgda26d41"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.size": 8,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=3)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd469f63" class="outline-2"&gt;
&lt;h2 id="orgd469f63"&gt;Flower power&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd469f63"&gt;
&lt;p&gt;
Here we'll be using VGGNet to classify images of flowers. We'll start, as usual, by importing our usual resources. And checking if we can train our model on the GPU.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org06b789c" class="outline-2"&gt;
&lt;h2 id="org06b789c"&gt;Download the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org06b789c"&gt;
&lt;p&gt;
Download the flower data from &lt;a href="https://s3.amazonaws.com/video.udacity-data.com/topher/2018/September/5baa60a0_flower-photos/flower-photos.zip"&gt;this link&lt;/a&gt;, save it in the home directory of this notebook and extract the zip file to get the directory &lt;code&gt;flower_photos/&lt;/code&gt;. &lt;b&gt;&lt;b&gt;Make sure the directory has this exact name for accessing data: flower_photos&lt;/b&gt;&lt;/b&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
path = DataPathTwo(folder_key="FLOWERS")
print(path.folder)
for target in path.folder.iterdir():
    print(target)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/flower_photos
/home/hades/datasets/flower_photos/.DS_Store
/home/hades/datasets/flower_photos/train
/home/hades/datasets/flower_photos/test
/home/hades/datasets/flower_photos/LICENSE.txt

&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-org330eae2" class="outline-3"&gt;
&lt;h3 id="org330eae2"&gt;Check If CUDA Is Available&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org330eae2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(device)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
cuda:0

&lt;/pre&gt;

&lt;p&gt;
CUDA is running out of memory and crashing so don't use CUDA.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;device = "cpu"
print(device)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
cpu

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org060cec8" class="outline-3"&gt;
&lt;h3 id="org060cec8"&gt;Load and Transform our Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org060cec8"&gt;
&lt;p&gt;
We'll be using PyTorch's &lt;a href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder"&gt;ImageFolder&lt;/a&gt; class which makes is very easy to load data from a directory. For example, the training images are all stored in a directory path that looks like this:
&lt;/p&gt;

&lt;pre class="example"&gt;
root/class_1/xxx.png
root/class_1/xxy.png
root/class_1/xxz.png

root/class_2/123.png
root/class_2/nsdf3.png
root/class_2/asd932_.png
&lt;/pre&gt;

&lt;p&gt;
Where, in this case, the root folder for training is &lt;code&gt;flower_photos/train/&lt;/code&gt; and the classes are the names of flower types.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org013542e" class="outline-3"&gt;
&lt;h3 id="org013542e"&gt;Define Training and Test Data Directories&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org013542e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_dir = path.folder.joinpath('train/')
test_dir = path.folder.joinpath('test/')
print(train_dir)
print(test_dir)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/flower_photos/train
/home/hades/datasets/flower_photos/test

&lt;/pre&gt;

&lt;p&gt;
&lt;i&gt;Classes&lt;/i&gt; are folders in each directory with these names:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']
CLASS_COUNT = len(classes)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd700eb6" class="outline-2"&gt;
&lt;h2 id="orgd700eb6"&gt;Transforming the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd700eb6"&gt;
&lt;p&gt;
When we perform transfer learning, we have to shape our input data into the shape that the pre-trained model expects. VGG16 expects `224`-dim square images as input and so, we resize each flower image to fit this mold.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org85725cf" class="outline-3"&gt;
&lt;h3 id="org85725cf"&gt;Load And Transform Data Using ImageFolder&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org85725cf"&gt;
&lt;p&gt;
VGG-16 Takes 224x224 images as input, so we resize all of them.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data_transform = transforms.Compose([transforms.RandomResizedCrop(224), 
				      transforms.ToTensor()])

train_data = datasets.ImageFolder(train_dir, transform=data_transform)
test_data = datasets.ImageFolder(test_dir, transform=data_transform)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfcbad1f" class="outline-3"&gt;
&lt;h3 id="orgfcbad1f"&gt;Print Out Some Data Stats&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfcbad1f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print('Num training images: ', len(train_data))
print('Num test images: ', len(test_data))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Num training images:  3130
Num test images:  540

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;VALIDATION_FRACTION = 0.2
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;indices = list(range(len(train_data)))
training_indices, validation_indices = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbb263f2" class="outline-2"&gt;
&lt;h2 id="orgbb263f2"&gt;DataLoaders and Data Visualization&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgbb263f2"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga909a1e" class="outline-3"&gt;
&lt;h3 id="orga909a1e"&gt;Define Dataloader Parameters&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga909a1e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;BATCH_SIZE = 20
NUM_WORKERS=4
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_sampler = SubsetRandomSampler(training_indices)
valid_sampler = SubsetRandomSampler(validation_indices)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org084bd6a" class="outline-3"&gt;
&lt;h3 id="org084bd6a"&gt;Prepare Data Loaders&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org084bd6a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, 
					   sampler=train_sampler,
					   num_workers=NUM_WORKERS)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, 
					   sampler=valid_sampler, num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, 
					  num_workers=num_workers, shuffle=True)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org96ae6b0" class="outline-2"&gt;
&lt;h2 id="org96ae6b0"&gt;Visualize some sample data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org96ae6b0"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf68c8a7" class="outline-3"&gt;
&lt;h3 id="orgf68c8a7"&gt;obtain one batch of training images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf68c8a7"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy() # convert images to numpy for display
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgba439a5" class="outline-2"&gt;
&lt;h2 id="orgba439a5"&gt;Plot The Images In The Batch, Along With The Corresponding Labels&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgba439a5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fig = pyplot.figure(figsize=(12, 10))
pyplot.rc("axes", titlesize=10)
for idx in numpy.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    pyplot.imshow(numpy.transpose(images[idx], (1, 2, 0)))
    ax.set_title(classes[labels[idx]])
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/sample_batches.png" alt="sample_batches.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2324f7f" class="outline-2"&gt;
&lt;h2 id="org2324f7f"&gt;Define the Model&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2324f7f"&gt;
&lt;p&gt;
To define a model for training we'll follow these steps:
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Load in a pre-trained VGG16 model&lt;/li&gt;
&lt;li&gt;"Freeze" all the parameters, so the net acts as a fixed feature extractor&lt;/li&gt;
&lt;li&gt;Remove the last layer&lt;/li&gt;
&lt;li&gt;Replace the last layer with a linear classifier of our own&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
/Freezing simply means that the parameters in the pre-trained model will &lt;b&gt;not&lt;/b&gt; change during training.**
&lt;/p&gt;

&lt;p&gt;
Load the pretrained model from pytorch
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vgg16 = models.vgg16(pretrained=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Print Out The Model Structure
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(vgg16)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace)
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace)
    (5): Dropout(p=0.5)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
&lt;/pre&gt;

&lt;p&gt;
Since we're only going to change the last (classification) layer, it might be helpful to see how many inputs and outpts it has.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(vgg16.classifier[6].in_features) 
print(vgg16.classifier[6].out_features) 
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
4096
1000

&lt;/pre&gt;

&lt;p&gt;
So, the original model output 1,000 classes - we're going to need to change that to our five classes (eventually).
&lt;/p&gt;

&lt;p&gt;
Freeze training for all "features" layers
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for param in vgg16.features.parameters():
    param.requires_grad = False
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org52ef690" class="outline-2"&gt;
&lt;h2 id="org52ef690"&gt;Final Classifier Layer&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org52ef690"&gt;
&lt;p&gt;
Once you have the pre-trained feature extractor, you just need to modify and/or add to the final, fully-connected classifier layers. In this case, we suggest that you replace the last layer in the vgg classifier group of layers. 
&lt;/p&gt;

&lt;p&gt;
This layer should see as input the number of features produced by the portion of the network that you are not changing, and produce an appropriate number of outputs for the flower classification task.
&lt;/p&gt;

&lt;p&gt;
You can access any layer in a pretrained network by name and (sometimes) number, i.e. &lt;code&gt;vgg16.classifier[6]&lt;/code&gt; is the sixth layer in a group of layers named "classifier".
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;classifier = nn.Sequential(OrderedDict([
    ("Fullly Connected Classifier", nn.Linear(in_features=4096, out_features=CLASS_COUNT, bias=True)),
]))
vgg16.classifier[6] = classifier
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
after completing your model, if GPU is available, move the model to GPU
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vgg16.to(device)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9c83d0e" class="outline-2"&gt;
&lt;h2 id="org9c83d0e"&gt;Specify &lt;a href="http://pytorch.org/docs/stable/nn.html#loss-functions"&gt;Loss Function&lt;/a&gt; and &lt;a href="http://pytorch.org/docs/stable/optim.html"&gt;Optimizer&lt;/a&gt;&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9c83d0e"&gt;
&lt;p&gt;
Below we'll use cross-entropy loss and stochastic gradient descent with a small learning rate. Note that the optimizer accepts as input &lt;i&gt;only&lt;/i&gt; the trainable parameters &lt;code&gt;vgg.classifier.parameters()&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org934dd12" class="outline-3"&gt;
&lt;h3 id="org934dd12"&gt;Specify Loss Function (Categorical Cross-Entropy)&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org934dd12"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;criterion = nn.CrossEntropyLoss()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
specify optimizer (stochastic gradient descent) and learning rate = 0.001
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;optimizer = optimize.SGD(vgg16.classifier.parameters(), lr=0.001)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org73d4f36" class="outline-2"&gt;
&lt;h2 id="org73d4f36"&gt;Training&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org73d4f36"&gt;
&lt;p&gt;
Here, we'll train the network.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Exercise:&lt;/b&gt;&lt;/b&gt; So far we've been providing the training code for you. Here, I'm going to give you a bit more of a challenge and have you write the code to train the network. Of course, you'll be able to see my solution if you need help.
&lt;/p&gt;

&lt;p&gt;
number of epochs to train the model
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;n_epochs = EPOCHS = 2
def train(model: nn.Module, epochs: int=EPOCHS, model_number: int=0,
	  epoch_offset: int=1, print_every: int=10) -&amp;gt; tuple:
    """Train, validate, and save the model
    This trains the model and validates it, saving the best 
    (based on validation loss) as =model_&amp;lt;number&amp;gt;_cifar.pth=

    Args:
     model: the network to train
     epochs: number of times to repeat training
     model_number: an identifier for the saved hyperparameters file
     epoch_offset: amount of epochs that have occurred previously
     print_every: how often to print output
    Returns:
     filename, training-loss, validation-loss, improvements: the outcomes for the training
    """
    optimizer = optimize.SGD(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    output_file = "model_{}_vgg.pth".format(model_number)
    training_losses = []
    validation_losses = []
    improvements = []
    valid_loss_min = numpy.Inf # track change in validation loss
    epoch_start = epoch_offset
    last_epoch = epoch_start + epochs + 1
    for epoch in range(epoch_start, last_epoch):

	# keep track of training and validation loss
	train_loss = 0.0
	valid_loss = 0.0

	model.train()
	for data, target in train_loader:
	    # move tensors to GPU if CUDA is available            
	    data, target = data.to(device), target.to(device)
	    # clear the gradients of all optimized variables
	    optimizer.zero_grad()
	    # forward pass: compute predicted outputs by passing inputs to the model
	    output = model(data)
	    # calculate the batch loss
	    loss = criterion(output, target)
	    # backward pass: compute gradient of the loss with respect to model parameters
	    loss.backward()
	    # perform a single optimization step (parameter update)
	    optimizer.step()
	    # update training loss
	    train_loss += loss.item() * data.size(0)

	model.eval()
	for data, target in valid_loader:
	    # move tensors to GPU if CUDA is available
	    data, target = data.to(device), target.to(device)
	    # forward pass: compute predicted outputs by passing inputs to the model
	    output = model(data)
	    # calculate the batch loss
	    loss = criterion(output, target)
	    # update total validation loss 
	    valid_loss += loss.item() * data.size(0)

	# calculate average losses
	train_loss = train_loss/len(train_loader.dataset)
	valid_loss = valid_loss/len(valid_loader.dataset)

	# print training/validation statistics 
	if not (epoch % print_every):
	    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
		epoch, train_loss, valid_loss))
	training_losses.append(train_loss)
	validation_losses.append(valid_loss)
	# save model if validation loss has decreased
	if valid_loss &amp;lt;= valid_loss_min:
	    print('Validation loss decreased ({:.6f} --&amp;gt; {:.6f}).  Saving model ...'.format(
	    valid_loss_min,
	    valid_loss))
	    torch.save(model.state_dict(), output_file)
	    valid_loss_min = valid_loss
	    improvements.append(epoch - 1)
    return output_file, training_losses, validation_losses, improvements
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def test(best_model):
    criterion = nn.CrossEntropyLoss()
    # track test loss
    test_loss = 0.0
    class_correct = list(0. for i in range(10))
    class_total = list(0. for i in range(10))

    best_model.to(device)
    best_model.eval()
    # iterate over test data
    for data, target in test_loader:
	# move tensors to GPU if CUDA is available
	data, target = data.to(device), target.to(device)
	# forward pass: compute predicted outputs by passing inputs to the model
	output = best_model(data)
	# calculate the batch loss
	loss = criterion(output, target)
	# update test loss 
	test_loss += loss.item() * data.size(0)
	# convert output probabilities to predicted class
	_, pred = torch.max(output, 1)    
	# compare predictions to true label
	correct_tensor = pred.eq(target.data.view_as(pred))
	correct = (
	    numpy.squeeze(correct_tensor.numpy())
	    if not train_on_gpu
	    else numpy.squeeze(correct_tensor.cpu().numpy()))
	# calculate test accuracy for each object class
	for i in range(BATCH_SIZE):
	    label = target.data[i]
	    class_correct[label] += correct[i].item()
	    class_total[label] += 1

    # average test loss
    test_loss = test_loss/len(test_loader.dataset)
    print('Test Loss: {:.6f}\n'.format(test_loss))

    for i in range(10):
	if class_total[i] &amp;gt; 0:
	    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
		classes[i], 100 * class_correct[i] / class_total[i],
		numpy.sum(class_correct[i]), numpy.sum(class_total[i])))
	else:
	    print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

    print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
	100. * numpy.sum(class_correct) / numpy.sum(class_total),
	numpy.sum(class_correct), numpy.sum(class_total)))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;output_file, training_losses, validation_losses, improvements = train(vgg16, print_every=1)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_losses = []
validation_losses = []
improvements = []
valid_loss_min = numpy.Inf # track change in validation loss
for epoch in range(1, 3):

    # keep track of training and validation loss
    train_loss = 0.0
    valid_loss = 0.0

    vgg16.train()
    for data, target in train_loader:
	# move tensors to GPU if CUDA is available            
	data, target = data.to(device), target.to(device)
	# clear the gradients of all optimized variables
	optimizer.zero_grad()
	# forward pass: compute predicted outputs by passing inputs to the model
	output = vgg16(data)
	# calculate the batch loss
	loss = criterion(output, target)
	# backward pass: compute gradient of the loss with respect to model parameters
	loss.backward()
	# perform a single optimization step (parameter update)
	optimizer.step()
	# update training loss
	train_loss += loss.item() * data.size(0)

    vgg16.eval()
    for data, target in valid_loader:
	# move tensors to GPU if CUDA is available
	data, target = data.to(device), target.to(device)
	# forward pass: compute predicted outputs by passing inputs to the model
	output = vgg16(data)
	# calculate the batch loss
	loss = criterion(output, target)
	# update total validation loss 
	valid_loss += loss.item() * data.size(0)

    # calculate average losses
    train_loss = train_loss/len(train_loader.dataset)
    valid_loss = valid_loss/len(valid_loader.dataset)

    # print training/validation statistics 
    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
	epoch, train_loss, valid_loss))
    training_losses.append(train_loss)
    validation_losses.append(valid_loss)
    # save model if validation loss has decreased
    if valid_loss &amp;lt;= valid_loss_min:
	print('Validation loss decreased ({:.6f} --&amp;gt; {:.6f}).  Saving model ...'.format(
	valid_loss_min,
	valid_loss))
	torch.save(vgg16.state_dict(), output_file)
	valid_loss_min = valid_loss
	improvements.append(epoch - 1)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;
test_loss = 0.0
class_correct = list(0. for i in range(5))
class_total = list(0. for i in range(5))
&lt;/p&gt;

&lt;p&gt;
vgg16.eval() # eval mode
&lt;/p&gt;

&lt;p&gt;
for data, target in test_loader:
&lt;/p&gt;

&lt;p&gt;
if train_on_gpu:
    data, target = data.cuda(), target.cuda()
&lt;/p&gt;

&lt;p&gt;
output = vgg16(data)
&lt;/p&gt;

&lt;p&gt;
loss = criterion(output, target)
&lt;/p&gt;

&lt;p&gt;
test_loss += loss.item()*data.size(0)
&lt;/p&gt;

&lt;p&gt;
_, pred = torch.max(output, 1)    
&lt;/p&gt;

&lt;p&gt;
correct_tensor = pred.eq(target.data.view_as(pred))
correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())
&lt;/p&gt;

&lt;p&gt;
for i in range(batch_size):
    label = target.data[i]
    class_correct[label] += correct[i].item()
    class_total[label] += 1
&lt;/p&gt;

&lt;p&gt;
test_loss = test_loss/len(test_loader.dataset)
print('Test Loss: {:.6f}\n'.format(test_loss))
&lt;/p&gt;

&lt;p&gt;
for i in range(5):
    if class_total[i] &amp;gt; 0:
        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
            classes[i], 100 * class_correct[i] / class_total[i],
            np.sum(class_correct[i]), np.sum(class_total[i])))
    else:
        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))
&lt;/p&gt;

&lt;p&gt;
print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;* np.sum(class_correct) / np.sum(class_total),&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
np.sum(class_correct), np.sum(class_total)))
&lt;/p&gt;


&lt;p&gt;
dataiter = iter(test_loader)
images, labels = dataiter.next()
images.numpy()
&lt;/p&gt;

&lt;p&gt;
if train_on_gpu:
    images = images.cuda()
&lt;/p&gt;

&lt;p&gt;
output = vgg16(images)
&lt;/p&gt;

&lt;p&gt;
_, preds_tensor = torch.max(output, 1)
preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())
&lt;/p&gt;

&lt;p&gt;
fig = plt.figure(figsize=(25, 4))
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    plt.imshow(np.transpose(images[idx], (1, 2, 0)))
    ax.set_title("{} ({})".format(classes[preds[idx]], classes[labels[idx]]),
                 color=("green" if preds[idx]==labels[idx].item() else "red"))
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>exercise</category><category>transfer learning</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/transfer-learning-exercise/</guid><pubDate>Sat, 15 Dec 2018 22:50:47 GMT</pubDate></item><item><title>Transfer Learning One More Time</title><link>https://necromuralist.github.io/In-Too-Deep/posts/nano/pytorch/transfer-learning-one-more-time/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/pytorch/transfer-learning-one-more-time/#org3b1f845"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/pytorch/transfer-learning-one-more-time/#org4339404"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/pytorch/transfer-learning-one-more-time/#org81cbe64"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/pytorch/transfer-learning-one-more-time/#org32a4be3"&gt;The DenseNet Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/pytorch/transfer-learning-one-more-time/#orgc95a1a3"&gt;Add Some CUDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/pytorch/transfer-learning-one-more-time/#org45831e3"&gt;Train It&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
I spent so much time debugging the original post that I though I'd re-do it without all the flailing around.
&lt;/p&gt;

&lt;div id="outline-container-org3b1f845" class="outline-2"&gt;
&lt;h2 id="org3b1f845"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3b1f845"&gt;
&lt;p&gt;
This is from &lt;a href="https://github.com/udacity/deep-learning-v2-pytorch.git"&gt;Udacity's Deep Learning Repository&lt;/a&gt; which supports their Deep Learning Nanodegree.
&lt;/p&gt;

&lt;p&gt;
This uses a model trained on &lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt; (&lt;a href="http://pytorch.org/docs/0.3.0/torchvision/models.html"&gt;available from torchvision&lt;/a&gt;) to classify the &lt;a href="https://www.kaggle.com/c/dogs-vs-cats"&gt;dataset of cat and dog photos&lt;/a&gt; that we used earlier. We're going to use a method called &lt;a href="https://en.wikipedia.org/wiki/Transfer_learning"&gt;transfer learning&lt;/a&gt; where we will use the layers of the pretrained model all the way up until the final classifier which we will define ourselves and train on our new data-set. This way we can take advantage of what the model has already learned for image detection and only train a few layers.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4339404" class="outline-2"&gt;
&lt;h2 id="org4339404"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4339404"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgfa57da7" class="outline-3"&gt;
&lt;h3 id="orgfa57da7"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfa57da7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2b00468" class="outline-4"&gt;
&lt;h4 id="org2b00468"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2b00468"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;OrderedDict&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd7b56de" class="outline-4"&gt;
&lt;h4 id="orgd7b56de"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd7b56de"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torchvision&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn.functional&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;F&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org01df1bb" class="outline-4"&gt;
&lt;h4 id="org01df1bb"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org01df1bb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;neurotic.tangles.data_paths&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;neurotic.models.fashion&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;train_only&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;test_only&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcea48f4" class="outline-3"&gt;
&lt;h3 id="orgcea48f4"&gt;Dotenv&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcea48f4"&gt;
&lt;p&gt;
For some reason dotenv has stopped working unless it's called in the notebook. Maybe this will fix it
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org81cbe64" class="outline-2"&gt;
&lt;h2 id="org81cbe64"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org81cbe64"&gt;
&lt;p&gt;
We're going to have to resize the images to be 224x224 to work with the pre-trained models and match the means (&lt;code&gt;[0.485, 0.456, 0.406]&lt;/code&gt;) and the standard deviations (&lt;code&gt;[0.229, 0.224, 0.225]&lt;/code&gt;) that were used to normalize the original data set.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.485&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.456&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.406&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;deviations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.229&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.224&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.225&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;PIXELS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;224&lt;/span&gt;

&lt;span class="n"&gt;train_transforms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Compose&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RandomRotation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
				       &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RandomResizedCrop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PIXELS&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
				       &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RandomHorizontalFlip&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
				       &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
				       &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
							    &lt;span class="n"&gt;deviations&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;

&lt;span class="n"&gt;test_transforms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Compose&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Resize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
				      &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CenterCrop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PIXELS&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
				      &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
				      &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normalize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
							   &lt;span class="n"&gt;deviations&lt;/span&gt;&lt;span class="p"&gt;)])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgce86459" class="outline-3"&gt;
&lt;h3 id="orgce86459"&gt;Load the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgce86459"&gt;
&lt;p&gt;
As I mentioned we're using the same Cat and Dog images as before. So first I make my path-setter (which maybe isn't as useful as it was when I had dotenv working better).
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"CAT_DOG_TRAIN"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"CAT_DOG_TEST"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
So now we set up the testing and training data sets.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ImageFolder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				  &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_transforms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ImageFolder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
				 &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_transforms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
And create the batch-iterators with a batch-size of 64.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;
&lt;span class="n"&gt;train_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					    &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;BATCH_SIZE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org32a4be3" class="outline-2"&gt;
&lt;h2 id="org32a4be3"&gt;The DenseNet Model&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org32a4be3"&gt;
&lt;p&gt;
I'm going to load the &lt;a href="http://pytorch.org/docs/0.3.0/torchvision/models.html#id5"&gt;DenseNet&lt;/a&gt; model.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;densenet121&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pretrained&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
It actually emits a warning that the code is using an incorrect method call somewhere, but I'll ignore that.
&lt;/p&gt;

&lt;pre class="example"&gt;
UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
 nn.init.kaiming_normal(m.weight.data)
&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb1c7818" class="outline-3"&gt;
&lt;h3 id="orgb1c7818"&gt;Freeze The Model Parameters&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb1c7818"&gt;
&lt;p&gt;
We need to freeze the parameters before training so we don't end up trying to re-train our pre-trained network.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd8f1478" class="outline-3"&gt;
&lt;h3 id="orgd8f1478"&gt;The Classifier&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd8f1478"&gt;
&lt;p&gt;
So this is the part where we add our own classifier at the end so that we can train it on cats and dogs. I'll use the original 500 fully connected nodes instead of the 256 I ended up with in my previous attempt.
&lt;/p&gt;

&lt;p&gt;
To figure out the inputs to the layer we can just look at the original &lt;code&gt;classifier&lt;/code&gt; layer in the model.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Linear(in_features=1024, out_features=1000, bias=True)

&lt;/pre&gt;

&lt;p&gt;
So we need to make sure we have 1,024 inputs to our classification layer and change the number of outputs to 2 (since we have only dogs and cats). We're also going to use two layers, the first one will have a ReLU activation and the second (the output) will have a &lt;a href="https://pytorch.org/docs/stable/nn.html?highlight=logsoftmax#torch.nn.LogSoftmax"&gt;Log-Softmax&lt;/a&gt; activation.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;HIDDEN_NODES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;
&lt;span class="n"&gt;INPUT_NODES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1024&lt;/span&gt;
&lt;span class="n"&gt;OUTPUT_NODES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OrderedDict&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
			  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fully_connected_layer'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			   &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;INPUT_NODES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;HIDDEN_NODES&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
			  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'relu'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;
			  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"dropout"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
			  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'fully_connected_layer_2'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			   &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;HIDDEN_NODES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;OUTPUT_NODES&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
			  &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'output'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LogSoftmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
			  &lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;classifier&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
So we now have a (mostly) pre-trained deep neural network with an untrained classifier.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc95a1a3" class="outline-2"&gt;
&lt;h2 id="orgc95a1a3"&gt;Add Some CUDA&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc95a1a3"&gt;
&lt;p&gt;
To speed this up somewhat I'll add (if it's available) a little cuda.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"cuda"&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s2"&gt;"cpu"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd552106" class="outline-3"&gt;
&lt;h3 id="orgd552106"&gt;Add some more CUDA&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd552106"&gt;
&lt;p&gt;
This next bit doesn't work on any of my machines, but maybe someday.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device_count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Using {} GPUs"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device_count&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataParallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Only 1 GPU available"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Only 1 GPU available

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org45831e3" class="outline-2"&gt;
&lt;h2 id="org45831e3"&gt;Train It&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org45831e3"&gt;
&lt;p&gt;
First we'll set up our criterion - Negative Log Likelihood Loss (&lt;a href="https://pytorch.org/docs/stable/nn.html?highlight=nllloss#torch.nn.NLLLoss"&gt;NLLLoss&lt;/a&gt;) and optimizer - &lt;a href="https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam"&gt;Adam&lt;/a&gt; Optimization. Amazingly this only needs one pass through the data set. There's 352 batches in the training data-set so I won't print out each of the outcomes for the epochs.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.003&lt;/span&gt;
&lt;span class="n"&gt;EPOCHS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;criterion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NLLLoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classifier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;LEARNING_RATE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;outcome&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_only&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		     &lt;span class="n"&gt;train_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		     &lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;EPOCHS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;emit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Training Time: {}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Training Time: 0:10:35.847469

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;test_outcome&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_only&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"Test Time: {}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Time: 0:00:46.695136

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_outcome&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
0.9788

&lt;/pre&gt;

&lt;p&gt;
The key bit here was that I was earlier forgetting to add dropout, dropping the accuracy to between .5 and .6.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>classification</category><category>exercise</category><category>pytorch</category><category>transfer learning</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/nano/pytorch/transfer-learning-one-more-time/</guid><pubDate>Sun, 25 Nov 2018 22:55:58 GMT</pubDate></item></channel></rss>