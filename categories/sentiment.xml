<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>In Too Deep (Posts about sentiment)</title><link>https://necromuralist.github.io/In-Too-Deep/</link><description></description><atom:link href="https://necromuralist.github.io/In-Too-Deep/categories/sentiment.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Tue, 10 Sep 2019 00:03:33 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>IMDB Reviews Tensorflow Dataset</title><link>https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org9e58ef8"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org0760cc7"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org9cc75e4"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org4f54300"&gt;Middle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#org8e22346"&gt;End&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/#orgcd17ba0"&gt;Raw&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9e58ef8" class="outline-2"&gt;
&lt;h2 id="org9e58ef8"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9e58ef8"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0760cc7" class="outline-3"&gt;
&lt;h3 id="org0760cc7"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0760cc7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9cc75e4" class="outline-4"&gt;
&lt;h4 id="org9cc75e4"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9cc75e4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4f54300" class="outline-2"&gt;
&lt;h2 id="org4f54300"&gt;Middle&lt;/h2&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8e22346" class="outline-2"&gt;
&lt;h2 id="org8e22346"&gt;End&lt;/h2&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcd17ba0" class="outline-2"&gt;
&lt;h2 id="orgcd17ba0"&gt;Raw&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgcd17ba0"&gt;
&lt;div class="org-center"&gt;
&lt;p&gt;
#!/usr/bin/env python
&lt;/p&gt;


&lt;p&gt;
from &lt;span class="underline"&gt;&lt;span class="underline"&gt;future&lt;/span&gt;&lt;/span&gt; import absolute_import, division, print_function, unicode_literals
&lt;/p&gt;


&lt;p&gt;
import tensorflow_datasets as tfds
import tensorflow as tf
print(tf.__version__)
&lt;/p&gt;


&lt;p&gt;
#!pip install tensorflow==2.0.0-beta0
&lt;/p&gt;


&lt;p&gt;
import tensorflow_datasets as tfds
import tensorflow as tf
print(tf.__version__)
&lt;/p&gt;


&lt;p&gt;
dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)
train_dataset, test_dataset = dataset['train'], dataset['test']
&lt;/p&gt;


&lt;p&gt;
tokenizer = info.features['text'].encoder
&lt;/p&gt;


&lt;p&gt;
BUFFER_SIZE = 10000
BATCH_SIZE = 64
&lt;/p&gt;

&lt;p&gt;
train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)
&lt;/p&gt;


&lt;p&gt;
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
&lt;/p&gt;


&lt;p&gt;
model.summary()
&lt;/p&gt;


&lt;p&gt;
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
&lt;/p&gt;


&lt;p&gt;
NUM_EPOCHS = 10
history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)
&lt;/p&gt;


&lt;p&gt;
import matplotlib.pyplot as plt
&lt;/p&gt;


&lt;p&gt;
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
&lt;/p&gt;


&lt;p&gt;
plot_graphs(history, 'accuracy')
&lt;/p&gt;


&lt;p&gt;
plot_graphs(history, 'loss')
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>nlp</category><category>sentiment</category><category>tensorflow</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/keras/imdb-reviews-tensorflow-dataset/</guid><pubDate>Mon, 09 Sep 2019 23:24:46 GMT</pubDate></item></channel></rss>