<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neurotic Networking (Posts about project)</title><link>https://necromuralist.github.io/Neurotic-Networking/</link><description></description><atom:link href="https://necromuralist.github.io/Neurotic-Networking/categories/project.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2020 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Tue, 18 Aug 2020 13:39:53 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>TV Script Generation</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org13cc80c"&gt;Act I - The Call To Adventure&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org1457c6b"&gt;What is this about, then?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#orgb1cb438"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org01aed4f"&gt;Get the Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#orgcc5ac1f"&gt;Act II - The Departure&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org5744d12"&gt;Build the Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org9186c93"&gt;Input&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#orgad0fc6d"&gt;Batching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org10e5af3"&gt;Test your dataloader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org91d556c"&gt;Sizes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#orge04a365"&gt;Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org069bdfe"&gt;Build the Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org4cb77bf"&gt;Act III - The Final Battle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#orgab8ceb9"&gt;Generate TV Script&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org3a1ed0b"&gt;Generate Text&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org2f4a8b6"&gt;Generate a New Script&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org8c27e89"&gt;Save your favorite scripts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org39a2925"&gt;The TV Script is Not Perfect&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org2e07ae5"&gt;Example generated script&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/#org33ed17e"&gt;Submitting This Project&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org13cc80c" class="outline-2"&gt;
&lt;h2 id="org13cc80c"&gt;Act I - The Call To Adventure&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org13cc80c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1457c6b" class="outline-3"&gt;
&lt;h3 id="org1457c6b"&gt;What is this about, then?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1457c6b"&gt;
&lt;p&gt;
We want to create a model that can generate scripts for you. To do I'll use part of the &lt;a href="https://en.wikipedia.org/wiki/Seinfeld"&gt;Seinfeld&lt;/a&gt;  &lt;a href="https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv"&gt;dataset of scripts hosted on kaggle&lt;/a&gt; to create an RNN to create "fake" TV scripts that emulate the Seinfeld ones.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb1cb438" class="outline-3"&gt;
&lt;h3 id="orgb1cb438"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb1cb438"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge1bbc5c" class="outline-4"&gt;
&lt;h4 id="orge1bbc5c"&gt;Imports&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge1bbc5c"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="orga088e6f"&gt;&lt;/a&gt;Python&lt;br&gt;
&lt;div class="outline-text-5" id="text-orga088e6f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from collections import Counter
from functools import partial
from pathlib import Path
from typing import Collection
import os
import pickle
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org1b5e2b2"&gt;&lt;/a&gt;PyPi&lt;br&gt;
&lt;div class="outline-text-5" id="text-org1b5e2b2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from tabulate import tabulate
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
import hvplot.pandas
import numpy
import pandas
import torch
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="orge473205"&gt;&lt;/a&gt;This Project&lt;br&gt;
&lt;div class="outline-text-5" id="text-orge473205"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from bartleby_the_penguin.tangles.embed_bokeh import EmbedBokeh
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org213749c"&gt;&lt;/a&gt;Support Code&lt;br&gt;
&lt;div class="outline-text-5" id="text-org213749c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from udacity.project_tv_script_generation import helper
import udacity.project_tv_script_generation.problem_unittests as unittests
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org945718c" class="outline-4"&gt;
&lt;h4 id="org945718c"&gt;Load Dotenv&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org945718c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge1aaf71" class="outline-4"&gt;
&lt;h4 id="orge1aaf71"&gt;The Folder Path&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge1aaf71"&gt;
&lt;p&gt;
This is the path for saving files for this post.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;FOLDER_PATH = Path("../../../files/posts/nano/tv-script-generation/"
		   "tv-script-generation/")
if not FOLDER_PATH.is_dir():
    FOLDER_PATH.mkdir(parents=True)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org55af688" class="outline-4"&gt;
&lt;h4 id="org55af688"&gt;The Bokeh Embedder&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org55af688"&gt;
&lt;p&gt;
This sets up the bokeh files and HTML.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Embed = partial(EmbedBokeh, folder_path=FOLDER_PATH)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgec8c494" class="outline-4"&gt;
&lt;h4 id="orgec8c494"&gt;Check CUDA&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgec8c494"&gt;
&lt;p&gt;
Make sure that we can use CUDA.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
assert device.type == "cuda", 'No GPU found. Please use a GPU to train your neural network.'
print("Using {}".format(device))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga1b489f" class="outline-4"&gt;
&lt;h4 id="orga1b489f"&gt;Some Types&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga1b489f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;WordIndices = Collection[int]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org01aed4f" class="outline-3"&gt;
&lt;h3 id="org01aed4f"&gt;Get the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org01aed4f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge5ac991" class="outline-4"&gt;
&lt;h4 id="orge5ac991"&gt;Scripts&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge5ac991"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Scripts:
    """Seinfeld Scripts

    Args:
     environment_key: environment variable with the source location
     dialog_only: remove descriptive columns
    """
    def __init__(self, environment_key: str="SCRIPTS", dialog_only: bool=True) -&amp;gt; None:
	self.environment_key = environment_key
	self.dialog_only = dialog_only
	self._script_blob = None
	self._path = None
	self._lines = None
	self._tokens = None
	self._line_tokens = None
	return

    @property
    def path(self) -&amp;gt; Path:
	"""The path to the file"""
	if self._path is None:
	    load_dotenv(".env")
	    self._path = Path(os.environ.get("SCRIPTS")).expanduser()
	    assert self._path.is_file()
	return self._path

    @property
    def script_blob(self) -&amp;gt; str:
	"""The input file as a string"""
	if self._script_blob is None:
	    with open(self.path) as reader:
		self._script_blob = reader.read()
	return self._script_blob

    @property
    def line_tokens(self) -&amp;gt; list:
	"""list of tokens for each line"""
	if self._line_tokens is None:
	    self._line_tokens = [line.split(" ") for line in self.lines]
	return self._line_tokens

    @property
    def lines(self) -&amp;gt; list:
	"""The lines of the script"""
	if self._lines is None:
	    lines = self.script_blob.split("\n")
	    if self.dialog_only:
		lines = lines[1:]
		lines = [(",").join(line.split(",")[2:-3]) for line in lines]
	    self._lines = lines
	return self._lines

    @property
    def tokens(self) -&amp;gt; Counter:
	"""The tokens and their counts"""
	if self._tokens is None:
	    self._tokens = Counter()
	    for token in self.script_blob.split():
		self._tokens[token] += 1
	return self._tokens
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org471ea9a" class="outline-4"&gt;
&lt;h4 id="org471ea9a"&gt;Script Inspector&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org471ea9a"&gt;
&lt;p&gt;
This is just to help with some preliminary exploratory data analysis.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class ScriptInspector:
    """gets some basic counts

    Args:
     scripts: object with the scripts
    """
    def __init__(self, scripts: Scripts=None) -&amp;gt; None:
	self._scripts = scripts
	self._line_count = None
	self._count_per_line = None
	self._mean_words_per_line = None
	self._median_words_per_line = None
	self._max_words_per_line = None
	self._min_words_per_line = None
	self._token_count = None
	return

    @property
    def scripts(self) -&amp;gt; Scripts:
	"""The scripts object"""
	if self._scripts is None:
	    self._scripts = Scripts()
	return self._scripts

    @property
    def line_count(self) -&amp;gt; int:
	"""Number of lines in the source"""
	if self._line_count is None:
	    self._line_count = len(self.scripts.lines)
	return self._line_count

    @property
    def count_per_line(self) -&amp;gt; list:
	"""tokens per line"""
	if self._count_per_line is None:
	    self._count_per_line = [len(tokens)
				    for tokens in self.scripts.line_tokens]
	return self._count_per_line

    @property
    def mean_words_per_line(self) -&amp;gt; float:
	"""Average number of words per line"""
	if self._mean_words_per_line is None:
	    self._mean_words_per_line = (sum(self.count_per_line)
					 /self.line_count)
	return self._mean_words_per_line

    @property
    def median_words_per_line(self) -&amp;gt; float:
	"""Median words per line in the scripts"""
	if self._median_words_per_line is None:
	    self._median_words_per_line = numpy.median(self.count_per_line)
	return self._median_words_per_line

    @property
    def max_words_per_line(self) -&amp;gt; int:
	"""Count of words in longest line"""
	if self._max_words_per_line is None:
	    self._max_words_per_line = max(self.count_per_line)
	return self._max_words_per_line

    @property
    def min_words_per_line(self) -&amp;gt; int:
	"""Count of words in shortest line"""
	if self._min_words_per_line is None:
	    self._min_words_per_line = min(self.count_per_line)
	return self._min_words_per_line

    @property
    def token_count(self) -&amp;gt; int:
	"""Number of tokens in the text"""
	if self._token_count is None:
	    self._token_count = sum(self.scripts.tokens.values())
	return self._token_count

    def most_common_tokens(self, count: int=10) -&amp;gt; list:
	"""token, count tuples in descending rank

	Args:
	 count: number of tuples to return in the list
	"""
	if count &amp;gt; 0:
	    return self.scripts.tokens.most_common(count)
	return self.scripts.tokens.most_common()[count:]

    def line_range(self, start: int=0, stop: int=10) -&amp;gt; list:
	"""lines within range

	Args:
	 start: index of first line
	 stop: upper bound for last line
	"""
	return self.scripts.lines[start:stop]
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The scripts aren't really in a format that is optimized for pandas, at least not for this initial look, so we'll just load it as text.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;inspector = ScriptInspector()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgab71c7a" class="outline-4"&gt;
&lt;h4 id="orgab71c7a"&gt;Explore the Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgab71c7a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;view_line_range = (0, 10)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;words_per_line = pandas.DataFrame(inspector.count_per_line,
				  columns=["line_counts"])
print(words_per_line.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
(54617, 1)

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb56f889" class="outline-4"&gt;
&lt;h4 id="orgb56f889"&gt;Dataset Statistics&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgb56f889"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;lines = (("Number of unique tokens", "{:,}".format(inspector.token_count)),
	 ("Number of lines", "{:,}".format(inspector.line_count)),
	 ("Words in longest line", "{:,}".format(inspector.max_words_per_line)),
	 ("Average number of words in each line", "{:.2f}".format(
	     inspector.mean_words_per_line)),
	 ("Median Words Per Line", "{:.2f}".format(
	     inspector.median_words_per_line)),
	 ("Words in shortest line", "{}".format(inspector.min_words_per_line))
)
print(tabulate(lines, headers="Statistic Value".split(), tablefmt="orgtbl"))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Statistic&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Number of unique tokens&lt;/td&gt;
&lt;td class="org-right"&gt;550,996&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Number of lines&lt;/td&gt;
&lt;td class="org-right"&gt;54,617&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Words in longest line&lt;/td&gt;
&lt;td class="org-right"&gt;363&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Average number of words in each line&lt;/td&gt;
&lt;td class="org-right"&gt;10.01&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Median Words Per Line&lt;/td&gt;
&lt;td class="org-right"&gt;7.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Words in shortest line&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Why would a line have 363 words?
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;index = words_per_line.line_counts.idxmax()
print(inspector.count_per_line[index])
print(inspector.scripts.lines[index])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
363
"The dating world is not a fun world...its a pressure world, its a world of tension, its a world of pain...and you know, if a woman comes over to my house, I gotta get that bathroom ready, cause she needs things. Women need equipment. I dont know what they need. I know I dont have it, I know that- You know what they need, women seem to need a lot of cotton-balls. This is the one Im- always has been one of the amazing things to me...I have no cotton-balls, were all human beings, what is the story? Ive never had one...I never bought one, I never needed one, Ive never been in a situation, when I thought to myself I could use a cotton-ball right now. I can certainly get out of this mess. Women need them and they dont need one or two, they need thousands of them, they need bags, theyre like peat moss bags, have you ever seen these giant bags? Theyre huge and two days later, theyre out, theyre gone, the, the bag is empty, where are the cotton-balls, ladies? What are you doin with them? The only time I ever see em is in the bottom of your little waste basket, theres two or three, that look like theyve been through some horrible experience... tortured, interrogated, I dont know what happened to them. I once went out with a girl whos left a little zip-lock-baggy of cotton-balls over at my house. I dont know what to do with them, I took them out, I put them on my kitchen floor like little tumbleweeds. I thought maybe the cockroaches would see it, figure this is a dead town. Lets move on. The dating world is a world of pressure. Lets face it a date is a job interview that lasts all night. The only difference between a date and a job interview is not many job interviews is there a chance youll end up naked at the end of it. You know? Well, Bill, the boss thinks youre the man for the position, why dont you strip down and meet some of the people youll be workin with?"

&lt;/pre&gt;

&lt;p&gt;
This is one of Seinfeld's stand up routines, so I don't think it's, strictly speaking, a line, or at least not a line of dialog.
&lt;/p&gt;

&lt;p&gt;
What about one word?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(inspector.scripts.lines[words_per_line.line_counts.idxmin()])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Ha.

&lt;/pre&gt;

&lt;p&gt;
There's probably a lot of one word lines ("Yes", "No", etc.).
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org95d619f" class="outline-4"&gt;
&lt;h4 id="org95d619f"&gt;Plot the Words Per Line&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org95d619f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;plot = words_per_line.line_counts.hvplot.kde(title="Word Counts Per Line Distribution")
plotter = plot.opts(width=600, height=600, tools=["hover"])
Embed(plotter, "line_counts.js")()
&lt;/pre&gt;&lt;/div&gt;

&lt;script src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/line_counts.js" id="250c8a54-0b89-45b6-8d90-75202b719f48"&gt;&lt;/script&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;plot = words_per_line.line_counts.hvplot.box(title="Words Per Line")
plot = plot.opts(tools=["hover"])
Embed(plot, "line_counts_boxplot.js")()
&lt;/pre&gt;&lt;/div&gt;

&lt;script src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/line_counts_boxplot.js" id="f60e5282-ccfb-4fe1-bd48-8578c8761f21"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org12d344e" class="outline-4"&gt;
&lt;h4 id="org12d344e"&gt;Most Used Words&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org12d344e"&gt;
&lt;p&gt;
&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; d51aea0b1ff0725156523a28363e1f7bc18d91e0
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;lines = ((token, "{:,}".format(count))
	 for token, count in inspector.most_common_tokens())
print(tabulate(lines,
	       tablefmt="orgtbl", headers=["Token", "Count"]))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;16,373&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;I&lt;/td&gt;
&lt;td class="org-left"&gt;13,911&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;you&lt;/td&gt;
&lt;td class="org-left"&gt;12,831&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;a&lt;/td&gt;
&lt;td class="org-left"&gt;12,096&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;to&lt;/td&gt;
&lt;td class="org-left"&gt;11,594&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;5,490&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;and&lt;/td&gt;
&lt;td class="org-left"&gt;5,210&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;in&lt;/td&gt;
&lt;td class="org-left"&gt;4,741&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;is&lt;/td&gt;
&lt;td class="org-left"&gt;4,283&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;that&lt;/td&gt;
&lt;td class="org-left"&gt;4,047&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
So it looks like the stop words are the most common, as you might expect.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;words, counts = zip(*inspector.most_common_tokens(20))
top_twenty = pandas.DataFrame([counts], columns=words).T.reset_index()
top_twenty.columns = ["Word", "Count"]
layout = top_twenty.hvplot.bar(x="Word", y="Count",
			       title="Twenty Most Used Words",
			       colormap="Category20")
layout.opts(height=500, width=600)
Embed(layout, "top_twenty.js")()
&lt;/pre&gt;&lt;/div&gt;

&lt;script src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/top_twenty.js" id="4ad3d0ad-3f73-4d1d-9707-73e2f3a80ee7"&gt;&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org05ae2c1" class="outline-4"&gt;
&lt;h4 id="org05ae2c1"&gt;The First five Lines&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org05ae2c1"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for line in inspector.line_range(stop=5):
    print(line)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
"Do you know what this is all about? Do you know, why were here? To be out, this is out...and out is one of the single most enjoyable experiences of life. People...did you ever hear people talking about We should go out? This is what theyre talking about...this whole thing, were all out now, no one is home. Not one person here is home, were all out! There are people tryin to find us, they dont know where we are. (on an imaginary phone) Did you ring?, I cant find him. Where did he go? He didnt tell me where he was going. He must have gone out. You wanna go out you get ready, you pick out the clothes, right? You take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...Then youre standing around, whatta you do? You go We gotta be getting back. Once youre out, you wanna get back! You wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? Where ever you are in life, its my feeling, youve gotta go."
"(pointing at Georges shirt) See, to me, that button is in the worst possible spot. The second button literally makes or breaks the shirt, look at it. Its too high! Its in no-mans-land. You look like you live with your mother."
Are you through?
"You do of course try on, when you buy?"
"Yes, it was purple, I liked it, I dont actually recall considering the buttons."

&lt;/pre&gt;

&lt;p&gt;
I took out the header and the identifying columns so this is just the dialog part of the data. It looks like they left in all the punctuation except for apostrophes for some reason.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcc6bb50" class="outline-4"&gt;
&lt;h4 id="orgcc6bb50"&gt;Pre-Processing the Text&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgcc6bb50"&gt;
&lt;p&gt;
The first thing to do to any dataset is pre-processing.  Implement the following pre-processing functions below:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Lookup Table&lt;/li&gt;
&lt;li&gt;Tokenize Punctuation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org976e445" class="outline-4"&gt;
&lt;h4 id="org976e445"&gt;Lookup Table&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org976e445"&gt;
&lt;p&gt;
To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Dictionary to go from the &lt;i&gt;words&lt;/i&gt; to an &lt;i&gt;ID&lt;/i&gt;, we'll call it &lt;code&gt;vocab_to_int&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Dictionary to go from the &lt;i&gt;ID&lt;/i&gt; to &lt;i&gt;word&lt;/i&gt;, we'll call it &lt;code&gt;int_to_vocab&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Return these dictionaries in the following &lt;b&gt;&lt;b&gt;tuple&lt;/b&gt;&lt;/b&gt; &lt;code&gt;(vocab_to_int, int_to_vocab)&lt;/code&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def create_lookup_tables(text: list) -&amp;gt; tuple:
    """
    Create lookup tables for vocabulary

    Args:
     text The text of tv scripts split into words

    Returns: 
     A tuple of dicts (vocab_to_int, int_to_vocab)
    """
    text = set(text)
    vocabulary_to_index = {token: index for index, token in enumerate(text)}
    index_to_vocabulary = {index: token for index, token in enumerate(text)}
    return vocabulary_to_index, index_to_vocabulary
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test_text = '''
Moe_Szyslak Moe's Tavern Where the elite meet to drink
Bart_Simpson Eh yeah hello is Mike there Last name Rotch
Moe_Szyslak Hold on I'll check Mike Rotch Mike Rotch Hey has anybody seen Mike Rotch lately
Moe_Szyslak Listen you little puke One of these days I'm gonna catch you and I'm gonna carve my name on your back with an ice pick
Moe_Szyslak Whats the matter Homer You're not your normal effervescent self
Homer_Simpson I got my problems Moe Give me another one
Moe_Szyslak Homer hey you should not drink to forget your problems
Barney_Gumble Yeah you should only drink to enhance your social skills'''
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;unittests.test_create_lookup_tables(create_lookup_tables)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Tests Passed

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcda3409" class="outline-4"&gt;
&lt;h4 id="orgcda3409"&gt;Tokenize Punctuation&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgcda3409"&gt;
&lt;p&gt;
We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, "bye" and "bye!" would generate two different word ids.
&lt;/p&gt;

&lt;p&gt;
Implement the function &lt;code&gt;token_lookup&lt;/code&gt; to return a dict that will be used to tokenize symbols like "!" into "||Exclamation_Mark||".  Create a dictionary for the following symbols where the symbol is the key and value is the token:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Period ( &lt;b&gt;&lt;b&gt;.&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Comma ( &lt;b&gt;&lt;b&gt;,&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Quotation Mark ( &lt;b&gt;&lt;b&gt;"&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Semicolon ( &lt;b&gt;&lt;b&gt;;&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Exclamation mark ( &lt;b&gt;&lt;b&gt;!&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Question mark ( &lt;b&gt;&lt;b&gt;?&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Left Parentheses ( &lt;b&gt;&lt;b&gt;(&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Right Parentheses ( &lt;b&gt;&lt;b&gt;)&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Dash ( &lt;b&gt;&lt;b&gt;-&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;li&gt;Return ( &lt;b&gt;&lt;b&gt;\n&lt;/b&gt;&lt;/b&gt; )&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word. Make sure you don't use a value that could be confused as a word; for example, instead of using the value "dash", try using something like "||dash||".
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def token_lookup():
    """
    Generate a dict to turn punctuation into a token.

    Returns:
     Tokenized dictionary where the key is the punctuation and the value is the token
    """
    tokens = {'.': "period",
	      ',': 'comma',
	      '"': 'quotation',
	      ';': 'semicolon',
	      '!': 'exclamation',
	      '?': 'question',
	      '(': 'leftparenthesis',
	      ')': 'rightparenthesis',
	      '-': 'dash',
	      '\n': 'newline'}
    return {token: '**{}**'.format(coded) for token,coded in tokens.items()}
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;unittests.test_tokenize(token_lookup)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2e0d354" class="outline-4"&gt;
&lt;h4 id="org2e0d354"&gt;Pre-process all the data and save it&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2e0d354"&gt;
&lt;p&gt;
Running the code cell below will pre-process all the data and save it to file. You're encouraged to look at the code for &lt;code&gt;preprocess_and_save_data&lt;/code&gt; in the &lt;code&gt;helpers.py&lt;/code&gt; file to see what it's doing in detail, but you do not need to change this code.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;text = helper.load_data(inspector.scripts.path)
text = text[81:]
token_dict = token_lookup()
for key, token in token_dict.items():
    text = text.replace(key, ' {} '.format(token))
text = text.lower()
text = text.split()
vocab_to_int, int_to_vocab = create_lookup_tables(text + list(helper.SPECIAL_WORDS.values()))
int_text = [vocab_to_int[word] for word in text]
pre_processed = inspector.scripts.path.parent.joinpath('preprocess.pkl')
with pre_processed.open("wb") as writer:
    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), writer)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9722bc9" class="outline-4"&gt;
&lt;h4 id="org9722bc9"&gt;Check Point&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9722bc9"&gt;
&lt;p&gt;
This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pre_processed = inspector.scripts.path.parent.joinpath('preprocess.pkl')
with pre_processed.open("rb") as reader:
    int_text, vocab_to_int, int_to_vocab, token_dict = pickle.load(reader)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcc5ac1f" class="outline-2"&gt;
&lt;h2 id="orgcc5ac1f"&gt;Act II - The Departure&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgcc5ac1f"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5744d12" class="outline-3"&gt;
&lt;h3 id="org5744d12"&gt;Build the Neural Network&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5744d12"&gt;
&lt;p&gt;
In this section, you'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9186c93" class="outline-3"&gt;
&lt;h3 id="org9186c93"&gt;Input&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9186c93"&gt;
&lt;p&gt;
Let's start with the preprocessed input data. We'll use &lt;a href="http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset"&gt;TensorDataset&lt;/a&gt; to provide a known format to our dataset; in combination with &lt;a href="http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader"&gt;DataLoader&lt;/a&gt;, it will handle batching, shuffling, and other dataset iteration functions.
&lt;/p&gt;

&lt;p&gt;
You can create data with TensorDataset by passing in feature and target tensors. Then create a DataLoader as usual.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TensorDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature_tensors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_tensors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;data_loader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
					  &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgad0fc6d" class="outline-3"&gt;
&lt;h3 id="orgad0fc6d"&gt;Batching&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgad0fc6d"&gt;
&lt;p&gt;
Implement the &lt;code&gt;batch_data&lt;/code&gt; function to batch &lt;code&gt;words&lt;/code&gt; data into chunks of size &lt;code&gt;batch_size&lt;/code&gt; using the &lt;code&gt;TensorDataset&lt;/code&gt; and &lt;code&gt;DataLoader&lt;/code&gt; classes.
&lt;/p&gt;

&lt;p&gt;
You can batch words using the DataLoader, but it will be up to you to create &lt;code&gt;feature_tensors&lt;/code&gt; and &lt;code&gt;target_tensors&lt;/code&gt; of the correct size and content for a given &lt;code&gt;sequence_length&lt;/code&gt;.
&lt;/p&gt;

&lt;p&gt;
For example, say we have these as input:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;sequence_length&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Your first &lt;code&gt;feature_tensor&lt;/code&gt; should contain the values:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
And the corresponding &lt;code&gt;target_tensor&lt;/code&gt; should just be the next "word"/tokenized word value:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This should continue with the second &lt;code&gt;feature_tensor&lt;/code&gt;, &lt;code&gt;target_tensor&lt;/code&gt; being:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# features&lt;/span&gt;
&lt;span class="mi"&gt;6&lt;/span&gt;             &lt;span class="c1"&gt;# target&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def train_test_split(words: WordIndices, sequence_length: int) -&amp;gt; tuple:
    """Breaks the words into a training and a test set

    Args:
     words: the IDs of the TV scripts
     sequence_length: the sequence length of each training instance

    Returns:
     tuple of training tensors, target tensors
    """
    training, testing = [], []
    for start in range(len(words) - sequence_length):
	training.append(words[start:start+sequence_length])
	testing.append(words[start + sequence_length])
    return torch.Tensor(training), torch.Tensor(testing)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;words = list(range(1, 8))
sequence_length = 4
training, testing = train_test_split(words, sequence_length)
assert training[0] == torch.Tensor([1, 2, 3, 4])
assert testing[0] == torch.Tensor(5)
assert training[1] == torch.Tensor([2, 3, 4, 5])
assert testing[1] == torch.Tensor(6)
assert training[2] == torch.Tensor([3, 4, 5, 6])
assert testing[2] == torch.Tensor(7)
assert len(training) == torch.Tensor(3)
assert len(testing) == torch.Tensor(3)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def batch_data(words: WordIndices, sequence_length: int, batch_size: int) -&amp;gt; DataLoader:
    """
    Batch the neural network data using DataLoader

    Args:
     - words: The word ids of the TV scripts
     - sequence_length: The sequence length of each batch
     - batch_size: The size of each batch; the number of sequences in a batch
    Returns: 
     DataLoader with batched data
    """
    training, target = train_test_split(words, sequence_length)
    data = TensorDataset(training, target)
    return DataLoader(data)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
There is no test for this function, but you are encouraged to create tests of your own.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org10e5af3" class="outline-3"&gt;
&lt;h3 id="org10e5af3"&gt;Test your dataloader&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org10e5af3"&gt;
&lt;p&gt;
You'll have to modify this code to test a batching function, but it should look fairly similar.
&lt;/p&gt;

&lt;p&gt;
Below, we're generating some test text data and defining a dataloader using the function you defined, above. Then, we are getting some sample batch of inputs `sample_x` and targets `sample_y` from our dataloader.
&lt;/p&gt;

&lt;p&gt;
Your code should return something like the following (likely in a different order, if you shuffled your data):
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;29&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;34&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;36&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;37&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;38&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;27&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;38&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;39&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;41&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;27&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;29&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
	&lt;span class="p"&gt;[&lt;/span&gt;  &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;   &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Size&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt; &lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;39&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;43&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org91d556c" class="outline-3"&gt;
&lt;h3 id="org91d556c"&gt;Sizes&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org91d556c"&gt;
&lt;p&gt;
Your sample_x should be of size `(batch_size, sequence_length)` or (10, 5) in this case and sample_y should just have one dimension: batch_size (10). 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge04a365" class="outline-3"&gt;
&lt;h3 id="orge04a365"&gt;Values&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge04a365"&gt;
&lt;p&gt;
You should also notice that the targets, sample_y, are the &lt;b&gt;next&lt;/b&gt; value in the ordered test_text data. So, for an input sequence `[ 28,  29,  30,  31,  32]` that ends with the value `32`, the corresponding output should be `33`.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test_text = range(50)
t_loader = batch_data(test_text, sequence_length=5, batch_size=10)

data_iter = iter(t_loader)
sample_x, sample_y = data_iter.next()

print(sample_x.shape)
print(sample_x)
print()
print(sample_y.shape)
print(sample_y)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org069bdfe" class="outline-3"&gt;
&lt;h3 id="org069bdfe"&gt;Build the Neural Network&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org069bdfe"&gt;
&lt;p&gt;
Implement an RNN using PyTorch's [Module class](&lt;a href="http://pytorch.org/docs/master/nn.html#torch.nn.Module"&gt;http://pytorch.org/docs/master/nn.html#torch.nn.Module&lt;/a&gt;). You may choose to use a GRU or an LSTM. To complete the RNN, you'll have to implement the following functions for the class:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;`__init__` - The initialize function.&lt;/li&gt;
&lt;li&gt;`init_hidden` - The initialization function for an LSTM/GRU hidden state&lt;/li&gt;
&lt;li&gt;`forward` - Forward propagation function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;*The output of this model should be the *last&lt;/b&gt; batch of word scores** after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org639a694" class="outline-4"&gt;
&lt;h4 id="org639a694"&gt;Hints&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org639a694"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Make sure to stack the outputs of the lstm to pass to your fully-connected layer, you can do this with `lstm_output = lstm_output.contiguous().view(-1, self.hidden_dim)`&lt;/li&gt;
&lt;li&gt;You can get the last batch of word scores by shaping the output of the final, fully-connected layer like so:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# reshape into (batch_size, seq_length, output_size)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# get last batch&lt;/span&gt;
&lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import torch.nn as nn

class RNN(nn.Module):

    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):
	"""
	Initialize the PyTorch RNN Module
	:param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)
	:param output_size: The number of output dimensions of the neural network
	:param embedding_dim: The size of embeddings, should you choose to use them        
	:param hidden_dim: The size of the hidden layer outputs
	:param dropout: dropout to add in between LSTM/GRU layers
	"""
	super(RNN, self).__init__()
	# TODO: Implement function

	# set class variables

	# define model layers


    def forward(self, nn_input, hidden):
	"""
	Forward propagation of the neural network
	:param nn_input: The input to the neural network
	:param hidden: The hidden state        
	:return: Two Tensors, the output of the neural network and the latest hidden state
	"""
	# TODO: Implement function   

	# return one batch of output word scores and the hidden state
	return None, None


    def init_hidden(self, batch_size):
	'''
	Initialize the hidden state of an LSTM/GRU
	:param batch_size: The batch_size of the hidden state
	:return: hidden state of dims (n_layers, batch_size, hidden_dim)
	'''
	# Implement function

	# initialize hidden state with zero weights, and move to GPU if available

	return None

tests.test_rnn(RNN, train_on_gpu)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge4bebfa" class="outline-4"&gt;
&lt;h4 id="orge4bebfa"&gt;Define forward and backpropagation&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge4bebfa"&gt;
&lt;p&gt;
Use the RNN class you implemented to apply forward and back propagation. This function will be called, iteratively, in the training loop as follows:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;forward_back_prop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;decoder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;decoder_optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
And it should return the average loss over a batch and the hidden state returned by a call to `RNN(inp, hidden)`. Recall that you can get this loss by computing it, as usual, and calling `loss.item()`.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;If a GPU is available, you should move your data to that GPU device, here.&lt;/b&gt;&lt;/b&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):
    """
    Forward and backward propagation on the neural network
    :param decoder: The PyTorch Module that holds the neural network
    :param decoder_optimizer: The PyTorch optimizer for the neural network
    :param criterion: The PyTorch loss function
    :param inp: A batch of input to the neural network
    :param target: The target output for the batch of input
    :return: The loss and the latest hidden state Tensor
    """

    # TODO: Implement Function

    # move data to GPU, if available

    # perform backpropagation and optimization

    # return the loss over a batch and the hidden state produced by our model
    return None, None

# Note that these tests aren't completely extensive.
# they are here to act as general checks on the expected outputs of your functions
"""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
"""
tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9184833" class="outline-4"&gt;
&lt;h4 id="org9184833"&gt;Neural Network Training&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9184833"&gt;
&lt;p&gt;
With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.
&lt;/p&gt;
&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="orgc060888"&gt;&lt;/a&gt;Train Loop&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgc060888"&gt;
&lt;p&gt;
The training loop is implemented for you in the `train_decoder` function. This function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter. You'll set this parameter along with other parameters in the next section.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):
    batch_losses = []

    rnn.train()

    print("Training for %d epoch(s)..." % n_epochs)
    for epoch_i in range(1, n_epochs + 1):

	# initialize hidden state
	hidden = rnn.init_hidden(batch_size)

	for batch_i, (inputs, labels) in enumerate(train_loader, 1):

	    # make sure you iterate over completely full batches, only
	    n_batches = len(train_loader.dataset)//batch_size
	    if(batch_i &amp;gt; n_batches):
		break

	    # forward, back prop
	    loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          
	    # record loss
	    batch_losses.append(loss)

	    # printing loss stats
	    if batch_i % show_every_n_batches == 0:
		print('Epoch: {:&amp;gt;4}/{:&amp;lt;4}  Loss: {}\n'.format(
		    epoch_i, n_epochs, np.average(batch_losses)))
		batch_losses = []

    # returns a trained rnn
    return rnn
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5f201e2" class="outline-4"&gt;
&lt;h4 id="org5f201e2"&gt;Hyperparameters&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5f201e2"&gt;
&lt;p&gt;
Set and train the neural network with the following parameters:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Set `sequence_length` to the length of a sequence.&lt;/li&gt;
&lt;li&gt;Set `batch_size` to the batch size.&lt;/li&gt;
&lt;li&gt;Set `num_epochs` to the number of epochs to train for.&lt;/li&gt;
&lt;li&gt;Set `learning_rate` to the learning rate for an Adam optimizer.&lt;/li&gt;
&lt;li&gt;Set `vocab_size` to the number of unique tokens in our vocabulary.&lt;/li&gt;
&lt;li&gt;Set `output_size` to the desired size of the output.&lt;/li&gt;
&lt;li&gt;Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.&lt;/li&gt;
&lt;li&gt;Set `hidden_dim` to the hidden dimension of your RNN.&lt;/li&gt;
&lt;li&gt;Set `n_layers` to the number of layers/cells in your RNN.&lt;/li&gt;
&lt;li&gt;Set `show_every_n_batches` to the number of batches at which the neural network should print progress.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
If the network isn't getting the desired results, tweak these parameters and/or the layers in the `RNN` class.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Data params
# Sequence Length
sequence_length =   # of words in a sequence
# Batch Size
batch_size = 

# data loader - do not change
train_loader = batch_data(int_text, sequence_length, batch_size)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Training parameters
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Number of Epochs
num_epochs = 
# Learning Rate
learning_rate = 

# Model parameters
# Vocab size
vocab_size = 
# Output size
output_size = 
# Embedding Dimension
embedding_dim = 
# Hidden Dimension
hidden_dim = 
# Number of RNN Layers
n_layers = 

# Show stats for every n number of batches
show_every_n_batches = 500
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgff92f98" class="outline-4"&gt;
&lt;h4 id="orgff92f98"&gt;Train&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgff92f98"&gt;
&lt;p&gt;
In the next cell, you'll train the neural network on the pre-processed data.  If you have a hard time getting a good loss, you may consider changing your hyperparameters. In general, you may get better results with larger hidden and n_layer dimensions, but larger models take a longer time to train. 
 &amp;gt; &lt;b&gt;&lt;b&gt;You should aim for a loss less than 3.5.&lt;/b&gt;&lt;/b&gt; 
&lt;/p&gt;

&lt;p&gt;
You should also experiment with different sequence lengths, which determine the size of the long range dependencies that a model can learn.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# create model and move to gpu if available
rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)
if train_on_gpu:
    rnn.cuda()

# defining loss and optimization functions for training
optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()

# training the model
trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)

# saving the trained model
helper.save_model('./save/trained_rnn', trained_rnn)
print('Model Trained and Saved')
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgad25c9e" class="outline-4"&gt;
&lt;h4 id="orgad25c9e"&gt;Question: How did you decide on your model hyperparameters?&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgad25c9e"&gt;
&lt;p&gt;
For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Answer:&lt;/b&gt;&lt;/b&gt; (Write answer, here)
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-orgd5d93f8" class="outline-4"&gt;
&lt;h4 id="orgd5d93f8"&gt;Checkpoint&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd5d93f8"&gt;
&lt;p&gt;
After running the above training cell, your model will be saved by name, `trained_rnn`, and if you save your notebook progress, &lt;b&gt;&lt;b&gt;you can pause here and come back to this code at another time&lt;/b&gt;&lt;/b&gt;. You can resume your progress by running the next cell, which will load in our word:id dictionaries &lt;span class="underline"&gt;and&lt;/span&gt; load in your saved model by name!
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import torch
import helper
import problem_unittests as tests

_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()
trained_rnn = helper.load_model('./save/trained_rnn')
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4cb77bf" class="outline-2"&gt;
&lt;h2 id="org4cb77bf"&gt;Act III - The Final Battle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4cb77bf"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgab8ceb9" class="outline-3"&gt;
&lt;h3 id="orgab8ceb9"&gt;Generate TV Script&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgab8ceb9"&gt;
&lt;p&gt;
With the network trained and saved, you'll use it to generate a new, "fake" Seinfeld TV script in this section.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3a1ed0b" class="outline-3"&gt;
&lt;h3 id="org3a1ed0b"&gt;Generate Text&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3a1ed0b"&gt;
&lt;p&gt;
To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. You'll be using the `generate` function to do this. It takes a word id to start with, `prime_id`, and generates a set length of text, `predict_len`. Also note that it uses topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import torch.nn.functional as F

def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):
    """
    Generate text using the neural network
    :param decoder: The PyTorch Module that holds the trained neural network
    :param prime_id: The word id to start the first prediction
    :param int_to_vocab: Dict of word id keys to word values
    :param token_dict: Dict of puncuation tokens keys to puncuation values
    :param pad_value: The value used to pad a sequence
    :param predict_len: The length of text to generate
    :return: The generated text
    """
    rnn.eval()

    # create a sequence (batch_size=1) with the prime_id
    current_seq = np.full((1, sequence_length), pad_value)
    current_seq[-1][-1] = prime_id
    predicted = [int_to_vocab[prime_id]]

    for _ in range(predict_len):
	if train_on_gpu:
	    current_seq = torch.LongTensor(current_seq).cuda()
	else:
	    current_seq = torch.LongTensor(current_seq)

	# initialize the hidden state
	hidden = rnn.init_hidden(current_seq.size(0))

	# get the output of the rnn
	output, _ = rnn(current_seq, hidden)

	# get the next word probabilities
	p = F.softmax(output, dim=1).data
	if(train_on_gpu):
	    p = p.cpu() # move to cpu

	# use top_k sampling to get the index of the next word
	top_k = 5
	p, top_i = p.topk(top_k)
	top_i = top_i.numpy().squeeze()

	# select the likely next word index with some element of randomness
	p = p.numpy().squeeze()
	word_i = np.random.choice(top_i, p=p/p.sum())

	# retrieve that word from the dictionary
	word = int_to_vocab[word_i]
	predicted.append(word)     

	# the generated word becomes the next "current sequence" and the cycle can continue
	current_seq = np.roll(current_seq, -1, 1)
	current_seq[-1][-1] = word_i

    gen_sentences = ' '.join(predicted)

    # Replace punctuation tokens
    for key, token in token_dict.items():
	ending = ' ' if key in ['\n', '(', '"'] else ''
	gen_sentences = gen_sentences.replace(' ' + token.lower(), key)
    gen_sentences = gen_sentences.replace('\n ', '\n')
    gen_sentences = gen_sentences.replace('( ', '(')

    # return all the sentences
    return gen_sentences
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2f4a8b6" class="outline-3"&gt;
&lt;h3 id="org2f4a8b6"&gt;Generate a New Script&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2f4a8b6"&gt;
&lt;p&gt;
It's time to generate the text. Set `gen_length` to the length of TV script you want to generate and set `prime_word` to one of the following to start the prediction:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;"jerry"&lt;/li&gt;
&lt;li&gt;"elaine"&lt;/li&gt;
&lt;li&gt;"george"&lt;/li&gt;
&lt;li&gt;"kramer"&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
You can set the prime word to &lt;span class="underline"&gt;any word&lt;/span&gt; in our dictionary, but it's best to start with a name for generating a TV script. (You can also start with any other names you find in the original text file!)
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# run the cell multiple times to get different results!
gen_length = 400 # modify the length to your preference
prime_word = 'jerry' # name for starting the script

"""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
"""
pad_word = helper.SPECIAL_WORDS['PADDING']
generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)
print(generated_script)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8c27e89" class="outline-3"&gt;
&lt;h3 id="org8c27e89"&gt;Save your favorite scripts&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8c27e89"&gt;
&lt;p&gt;
Once you have a script that you like (or find interesting), save it to a text file!
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# save script to a text file
f =  open("generated_script_1.txt","w")
f.write(generated_script)
f.close()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org39a2925" class="outline-3"&gt;
&lt;h3 id="org39a2925"&gt;The TV Script is Not Perfect&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org39a2925"&gt;
&lt;p&gt;
It's ok if the TV script doesn't make perfect sense. It should look like alternating lines of dialogue, here is one such example of a few generated lines.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2e07ae5" class="outline-3"&gt;
&lt;h3 id="org2e07ae5"&gt;Example generated script&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2e07ae5"&gt;
&lt;p&gt;
&amp;gt;jerry: what about me?
&amp;gt;
&amp;gt;jerry: i don't have to wait.
&amp;gt;
&amp;gt;kramer:(to the sales table)
&amp;gt;
&amp;gt;elaine:(to jerry) hey, look at this, i'm a good doctor.
&amp;gt;
&amp;gt;newman:(to elaine) you think i have no idea of thisâ¦
&amp;gt;
&amp;gt;elaine: oh, you better take the phone, and he was a little nervous.
&amp;gt;
&amp;gt;kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.
&amp;gt;
&amp;gt;jerry: oh, yeah. i don't even know, i know.
&amp;gt;
&amp;gt;jerry:(to the phone) oh, i know.
&amp;gt;
&amp;gt;kramer:(laughing) you knowâ¦(to jerry) you don't know.
&lt;/p&gt;

&lt;p&gt;
You can see that there are multiple characters that say (somewhat) complete sentences, but it doesn't have to be perfect! It takes quite a while to get good results, and often, you'll have to use a smaller vocabulary (and discard uncommon words), or get more data.  The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes; for script generation you'll want more than 1 MB of text, generally. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org33ed17e" class="outline-3"&gt;
&lt;h3 id="org33ed17e"&gt;Submitting This Project&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org33ed17e"&gt;
&lt;p&gt;
When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as "dlnd_tv_script_generation.ipynb" and save another copy as an HTML file by clicking "File" -&amp;gt; "Download as.."-&amp;gt;"html". Include the "helper.py" and "problem_unittests.py" files in your submission. Once you download these files, compress them into one zip file for submission.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>project</category><category>rnn</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/tv-script-generation/tv-script-generation/</guid><pubDate>Tue, 05 Feb 2019 23:29:20 GMT</pubDate></item><item><title>Dermatologist Mini-Project</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/#org3bbc466"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/#org38509f4"&gt;Data Sources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/#org4338644"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/#org9752816"&gt;The Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/#org45d06af"&gt;The Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/#org0597cee"&gt;Prepping The Test File&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/#org8e182f0"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3bbc466" class="outline-2"&gt;
&lt;h2 id="org3bbc466"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3bbc466"&gt;
&lt;p&gt;
This is an exercise in using transfer learning to diagnose melanoma based on images of skin legions. There are three diseases to be detected:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Melanoma&lt;/li&gt;
&lt;li&gt;Nevus&lt;/li&gt;
&lt;li&gt;Sebhorrheic Keratosis&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
There is a paper online &lt;a href="https://arxiv.org/pdf/1710.05006.pdf"&gt;here&lt;/a&gt; (PDF link) that describes the approaches that did best in the competition.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org38509f4" class="outline-2"&gt;
&lt;h2 id="org38509f4"&gt;Data Sources&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org38509f4"&gt;
&lt;p&gt;
The data is taken from the &lt;a href="https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a"&gt;ISIC 2017: Skin Lesion Analysis Towards Melanoma Detection&lt;/a&gt; challenge.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip"&gt;Training Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/valid.zip"&gt;Validation Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/test.zip"&gt;Test Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Each folder contains three sub-folders:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;code&gt;melanoma/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nevus/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;seborrheic_keratosis/&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4338644" class="outline-2"&gt;
&lt;h2 id="org4338644"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4338644"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdc38958" class="outline-3"&gt;
&lt;h3 id="orgdc38958"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdc38958"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org90225c3" class="outline-4"&gt;
&lt;h4 id="org90225c3"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org90225c3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path
import warnings
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8018a5f" class="outline-4"&gt;
&lt;h4 id="org8018a5f"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8018a5f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from PIL import Image, ImageFile
from torchvision import datasets
import matplotlib
warnings.filterwarnings("ignore", category=matplotlib.cbook.mplDeprecation)
import matplotlib.pyplot as pyplot
import matplotlib.image as mpimage
import matplotlib.patches as patches
import numpy
import pyttsx3
import seaborn
import torch
import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optimizer
import torchvision.transforms as transforms
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org513b400" class="outline-4"&gt;
&lt;h4 id="org513b400"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org513b400"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from neurotic.tangles.data_paths import (Batches, DataPathTwo, DataSets,
					 TrainingTestingValidationPaths,
					 Transformer)
from neurotic.tangles.models import Inception
from neurotic.tangles.timer import Timer
from neurotic.tangles.trainer import Trainer
from neurotic.tangles.logging import Tee
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc021de4" class="outline-3"&gt;
&lt;h3 id="orgc021de4"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc021de4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=1)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgab543bb" class="outline-3"&gt;
&lt;h3 id="orgab543bb"&gt;Set the Random Seed&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgab543bb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;numpy.random.seed(seed=2019)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdb1d1f8" class="outline-3"&gt;
&lt;h3 id="orgdb1d1f8"&gt;Handle Truncated Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdb1d1f8"&gt;
&lt;p&gt;
There seems to be at least one image that is truncated which will cause an exception when it's loaded so this next setting lets us ignore the error and keep working.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ImageFile.LOAD_TRUNCATED_IMAGES = True
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org147dde5" class="outline-3"&gt;
&lt;h3 id="org147dde5"&gt;Constants&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org147dde5"&gt;
&lt;p&gt;
These are some global constants
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org402e57a" class="outline-3"&gt;
&lt;h3 id="org402e57a"&gt;Load Dotenv&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org402e57a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org61efd66" class="outline-4"&gt;
&lt;h4 id="org61efd66"&gt;Model Path&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org61efd66"&gt;
&lt;p&gt;
This is where to save the best model.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;MODEL_PATH = DataPathTwo(folder_key="MODELS")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9752816" class="outline-2"&gt;
&lt;h2 id="org9752816"&gt;The Model&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9752816"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org68536a3" class="outline-3"&gt;
&lt;h3 id="org68536a3"&gt;The Training&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org68536a3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
EPOCHS = 100
transfer_path = MODEL_PATH.folder.joinpath("model_transfer.pt")
directory = "~/logs/dermatalogist"
training_log = Tee(log_name="inception_train.log", directory_name=directory)
testing_log = Tee(log_name="inception_test.log", directory_name=directory)
data_sets = DataSets()
inception = Inception(data_sets.class_count)
batches = Batches(data_sets)
trainer = Trainer(training_batches=batches.training,
		  validation_batches=batches.validation,
		  testing_batches=batches.testing,
		  model=inception.model,
		  model_path=transfer_path,
		  optimizer=inception.optimizer,
		  criterion=inception.criterion ,
		  device=inception.device,
		  epochs=EPOCHS,
		  epoch_start=1,
		  is_inception=True,
		  load_model=False,
		  training_log=training_log,
		  testing_log=testing_log,
		  beep=True,
)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;trainer()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Started: 2019-01-26 13:59:40.249210
Started: 2019-01-26 13:59:40.249398
Ended: 2019-01-26 14:16:25.675136
Elapsed: 0:16:45.425738
Epoch: 1	Training - Loss: 0.85	Accuracy: 0.67	Validation - Loss: 0.97	Accuracy: 0.53
Validation loss decreased (inf --&amp;gt; 0.973706). Saving model ...
Started: 2019-01-26 14:16:26.913182
Ended: 2019-01-26 14:33:23.108155
Elapsed: 0:16:56.194973
Epoch: 2	Training - Loss: 0.78	Accuracy: 0.68	Validation - Loss: 0.93	Accuracy: 0.56
Validation loss decreased (0.973706 --&amp;gt; 0.934509). Saving model ...
Ended: 2019-01-26 14:33:23.997547
Elapsed: 0:16:57.084365

Starting Testing
Started: 2019-01-26 14:33:24.706175
Test Loss: 0.697
Test Accuracy: 70.95 (1419.0/2000)
Ended: 2019-01-26 14:47:30.356073
Elapsed: 0:14:05.649898
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org45d06af" class="outline-2"&gt;
&lt;h2 id="org45d06af"&gt;The Testing&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org45d06af"&gt;
&lt;p&gt;
The remote session died so I'll just load the test output.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;testing_log = Tee(log_name="inception_test.log", directory_name="~/logs/dermatologist")
with testing_log.path.open() as reader:
    for line in reader:
	print(line.rstrip())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;

Starting Testing
Test Loss: 0.620
Test Accuracy: 74.80 (1496.0/2000)

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0597cee" class="outline-2"&gt;
&lt;h2 id="org0597cee"&gt;Prepping The Test File&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0597cee"&gt;
&lt;p&gt;
To check the model you need to create a CSV file with three columns.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Column&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Description&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;&lt;code&gt;Id&lt;/code&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;Path to the file&lt;/td&gt;
&lt;td class="org-left"&gt;&lt;code&gt;data/test/melanoma/ISIC_0012258.jpg&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;&lt;code&gt;task_1&lt;/code&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;Is melanoma&lt;/td&gt;
&lt;td class="org-left"&gt;&lt;code&gt;0&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;&lt;code&gt;task_2&lt;/code&gt;&lt;/td&gt;
&lt;td class="org-left"&gt;Is seborrheic keratosis&lt;/td&gt;
&lt;td class="org-left"&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Predictions:
    """Maps the test data to a predictions file

    Args:
     model_path: path to the stored model parameters
     device: processor to use
     output_path: path to the CSV to output
     test_path: path to the test folder
     inception: object with the model
    """
    def __init__(self, model_path: Path,
		 device: torch.device,
		 output_path: Path,
		 test_path: Path,
		 data_sets: DataSets=None,                 
		 inception: Inception=None) -&amp;gt; None:
	self.model_path = model_path
	self.output_path = output_path
	self.test_path = test_path
	self._device = device
	self._data_sets = data_sets
	self._activation = None
	self.inception = inception
	return

    @property
    def data_sets(self) -&amp;gt; DataSets:
	"""the data-sets"""
	if self._data_sets is None:
	    self._data_sets = DataSets()
	return self._data_sets

    @property
    def device(self):
	"""The processor to use"""
	if self._device is None:
	    self._device = torch.device("cuda"
					if torch.cuda.is_available()
					else "cpu")
	return self._device

    @property
    def inception(self) -&amp;gt; Inception:
	"""The Inception Object"""
	if self._inception is None:
	    self._inception = Inception(
		classel= self.data_sets.class_count,
		model_path=self.model_path,
		device=self.device)
	    self._inception.model.eval()
	return self._inception

    @property
    def activation(self) -&amp;gt; nn.Sigmoid:
	"""The non-linear activation"""
	if self._activation is None:
	    self._activation = nn.Sigmoid()
	return self._activation

    @inception.setter
    def inception(self, new_inception: Inception) -&amp;gt; None:
	"""Sets the inception model to eval only"""
	self._inception = new_inception
	self._inception.model.eval()
	return

    def prediction(self, image_path: Path) -&amp;gt; numpy.ndarray:
	"""Calculate predicted class for an image

	Args:
	 image_path: path to an inmage file
	Returns:
	 array with the probabilities for each disease
	"""
	model = self.inception.model        
	image = Image.open(image_path)
	tensor = self.data_sets.transformer.testing(image)
	# add a batch number
	tensor = tensor.unsqueeze_(0)
	tensor = tensor.to(self.inception.device)
	x = torch.autograd.Variable(tensor)
	output = torch.exp(model(x))
	_, top_class = output.topk(1, dim=1)
	return top_class.item()

    def __call__(self) -&amp;gt; None:
	"""Creates CSV of predictions"""
	with self.output_path.open("w") as writer:
	    writer.write("Id,task_1,task_2\n")
	    for category in self.test_path.iterdir():
		for path in category.iterdir():
		    identifier = 'data/' + str(path).split("/dermatologist/")[-1]
		    guess = self.prediction(path)
		    first = 0 if guess else 1
		    second = 1 if guess == 2 else 0
		    writer.write("{},{},{}\n".format(identifier,
						     first,
						     second))
	return
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
test_path = DataPathTwo(folder_key="TEST").folder
csv_output = Path("~/documents/pcloud_drive/outcomes/dermatologist/predictions.csv").expanduser()

predictions = Predictions(model_path=transfer_path,
			  device=inception.device,
			  output_path=csv_output,
			  test_path=test_path,
			  data_sets=data_sets,
			  inception=inception)
with TIMER:
    predictions()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Started: 2019-01-29 22:36:10.975682
Ended: 2019-01-29 22:46:47.190355
Elapsed: 0:10:36.214673

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8e182f0" class="outline-2"&gt;
&lt;h2 id="org8e182f0"&gt;References&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8e182f0"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://github.com/udacity/dermatologist-ai"&gt;Github Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>dermatologist</category><category>project</category><category>transfer learning</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dermatologist/dermatologist-mini-project/</guid><pubDate>Thu, 17 Jan 2019 05:17:45 GMT</pubDate></item><item><title>Dog Detector</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-detector/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-detector/#orgf157e6d"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-detector/#orgc9c521e"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-detector/#org95be9e6"&gt;VGG-16&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf157e6d" class="outline-2"&gt;
&lt;h2 id="orgf157e6d"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf157e6d"&gt;
&lt;p&gt;
As part of the Dog-Breed Classification application I want to be able to detect whether an image has a dog or a human. This post will use pre-trained models to detect dogs in images.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc9c521e" class="outline-2"&gt;
&lt;h2 id="orgc9c521e"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc9c521e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcfe0755" class="outline-3"&gt;
&lt;h3 id="orgcfe0755"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcfe0755"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge375759" class="outline-4"&gt;
&lt;h4 id="orge375759"&gt;From PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge375759"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import torchvision.models as models
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org95be9e6" class="outline-2"&gt;
&lt;h2 id="org95be9e6"&gt;VGG-16&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org95be9e6"&gt;
&lt;p&gt;
My first model will be a pre-trained VGG-16 model that has weights that wer trained on the &lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt; data set.  ImageNet contains over 10 million URLs which link to an image containing an object from one of &lt;a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"&gt;1000 categories&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7549185" class="outline-3"&gt;
&lt;h3 id="org7549185"&gt;Build the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7549185"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;VGG16 = models.vgg16(pretrained=True)
VGG16.eval()
VGG16.to(device)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-detector/</guid><pubDate>Mon, 07 Jan 2019 00:32:48 GMT</pubDate></item><item><title>Human Face Detection</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#org26b85ef"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#org36cef70"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#orgaec76b8"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#org3e18b8f"&gt;OpenCV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/#org9d623ae"&gt;DLIB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org26b85ef" class="outline-2"&gt;
&lt;h2 id="org26b85ef"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org26b85ef"&gt;
&lt;p&gt;
In this post, I'll use two libraries to detect human faces in images - &lt;a href="https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html"&gt;OpenCV&lt;/a&gt; and a python interface to &lt;a href="http://dlib.net/"&gt;dlib&lt;/a&gt; called &lt;a href="https://github.com/ageitgey/face_recognition"&gt;&lt;code&gt;face_recognition&lt;/code&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org36cef70" class="outline-2"&gt;
&lt;h2 id="org36cef70"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org36cef70"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9d05118" class="outline-3"&gt;
&lt;h3 id="org9d05118"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9d05118"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org676d499" class="outline-4"&gt;
&lt;h4 id="org676d499"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org676d499"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
import os
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org18588ab" class="outline-4"&gt;
&lt;h4 id="org18588ab"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org18588ab"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from PIL import Image
import cv2
import face_recognition
import matplotlib
import matplotlib.image as matplotlib_image
import matplotlib.patches as patches
import matplotlib.pyplot as pyplot
import numpy
import seaborn
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8edde4c" class="outline-4"&gt;
&lt;h4 id="org8edde4c"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8edde4c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from neurotic.tangles.data_paths import DataPathTwo
from neurotic.tangles.f1_scorer import F1Scorer
from neurotic.tangles.timer import Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3479d45" class="outline-3"&gt;
&lt;h3 id="org3479d45"&gt;Set Up the Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3479d45"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=1)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8893b9e" class="outline-3"&gt;
&lt;h3 id="org8893b9e"&gt;Build the Timer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8893b9e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga92c7fd" class="outline-3"&gt;
&lt;h3 id="orga92c7fd"&gt;Helpers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga92c7fd"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def first_prediction(source: list, start:int=0) -&amp;gt; int:
    """Gets the index of the first True prediction

    Args:
     source: list of True/False predictions
     start: index to start the search from

    Returns:
     index of first True prediction found
    """
    for index, prediction in enumerate(source[start:]):
	if prediction:
	    print("{}: {}".format(start + index, prediction))
	    break
    return start + index
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org581349f" class="outline-3"&gt;
&lt;h3 id="org581349f"&gt;Set the Random Seed&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org581349f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;numpy.random.seed(2019)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgaec76b8" class="outline-2"&gt;
&lt;h2 id="orgaec76b8"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgaec76b8"&gt;
&lt;p&gt;
Download the &lt;a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip"&gt;human dataset&lt;/a&gt; (this is a download link), unzip the folder, and place it in a folder named &lt;code&gt;/lfw&lt;/code&gt;.  
&lt;/p&gt;

&lt;p&gt;
The &lt;a href="http://vis-www.cs.umass.edu/lfw/lfw.tgz"&gt;human dataset&lt;/a&gt; is the &lt;a href="http://vis-www.cs.umass.edu/lfw/"&gt;Labeled Faces in the Wild&lt;/a&gt; data set which was built to study the problem of facial recognition. It's made up of real photos of people taken from the web. Each photo sits in a sub-folder that was given the name of the person (e.g. &lt;a href="https://en.wikipedia.org/wiki/Michelle_Yeoh"&gt;Michelle_Yeoh&lt;/a&gt;). The folder hasn't been split inte train-test-validiation folders the way the dog dataset was.
&lt;/p&gt;

&lt;p&gt;
The &lt;a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"&gt;dog dataset&lt;/a&gt; (this is also a download link) is in a zip-file hosted on Amazon Web Services. The folder should contain three folders (&lt;code&gt;test&lt;/code&gt;, &lt;code&gt;train&lt;/code&gt;, and &lt;code&gt;valid&lt;/code&gt;) and each of these folders should have 133 folders, one for each dog-breed. It looks like the &lt;a href="http://vision.stanford.edu/aditya86/ImageNetDogs/"&gt;Stanford Dogs Dataset&lt;/a&gt;, but the Stanford data set has 120 breeds, so I don't know the actual source.
&lt;/p&gt;

&lt;p&gt;
You might be thinking &lt;i&gt;Why are we loading dog images if this is about detecting human faces?&lt;/i&gt; but our goal is to discern human images from dog images so the dog images will act as our negative data set (the one we don't want to detect faces in).
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4f361f9" class="outline-3"&gt;
&lt;h3 id="org4f361f9"&gt;The Paths to the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4f361f9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
dog_path = DataPathTwo(folder_key="DOG_PATH")
print(dog_path.folder)
assert dog_path.folder.is_dir()
for folder in dog_path.folder.iterdir():
    print("Dog: {}".format(folder))
human_path = DataPathTwo(folder_key="HUMAN_PATH")
print(human_path.folder)
assert human_path.folder.is_dir()

for name in human_path.folder.glob("Gina*"):
    print(name)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/dog-breed-classification/dogImages
Dog: /home/hades/datasets/dog-breed-classification/dogImages/valid
Dog: /home/hades/datasets/dog-breed-classification/dogImages/train
Dog: /home/hades/datasets/dog-breed-classification/dogImages/test
/home/hades/datasets/dog-breed-classification/lfw
/home/hades/datasets/dog-breed-classification/lfw/Gina_Torres
/home/hades/datasets/dog-breed-classification/lfw/Gina_Centrello
/home/hades/datasets/dog-breed-classification/lfw/Gina_Gershon
/home/hades/datasets/dog-breed-classification/lfw/Gina_Lollobrigida

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer.start()
people = len(set(human_path.folder.iterdir()))
images = len(set(human_path.folder.glob("*/*")))
print("People Count: {:,}".format(people))
print("Image Count: {:,}".format(images))
print("Images Per Person: {:.2f}".format(images/people))
timer.end()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
People Count: 5,749
Image Count: 13,233
Images Per Person: 2.30
Ended: 2019-01-02 19:28:11.529962
Elapsed: 0:00:00.550351

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcbd7ac2" class="outline-3"&gt;
&lt;h3 id="orgcbd7ac2"&gt;Load All the Files&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcbd7ac2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer.start()
human_files = numpy.array(list(human_path.folder.glob("*/*")))
dog_files = numpy.array(list(dog_path.folder.glob("*/*/*")))
print('There are {:,} total human images.'.format(len(human_files)))
print('There are {:,} total dog images.'.format(len(dog_files)))
timer.end()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
There are 13,233 total human images.
There are 8,351 total dog images.
Ended: 2019-01-02 19:28:20.426379
Elapsed: 0:00:00.816752

&lt;/pre&gt;

&lt;p&gt;
The &lt;code&gt;human_files&lt;/code&gt; and &lt;code&gt;dog_files&lt;/code&gt; are numpy arrays of python &lt;code&gt;Path&lt;/code&gt; objects pointing to image files. Note that at this point we've thrown away all the dog-breed information as well as the names of the people in the images. We're only going for a binary split - human or not human.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org860ca93" class="outline-3"&gt;
&lt;h3 id="org860ca93"&gt;Test Sets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org860ca93"&gt;
&lt;p&gt;
The models we're going to use are pre-trained so we're just going to choose 100 images from each set to see how well they do.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;human_files_short = numpy.random.choice(human_files, 100)
dog_files_short = numpy.random.choice(dog_files, 100)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7de4408" class="outline-3"&gt;
&lt;h3 id="org7de4408"&gt;The Scorer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org7de4408"&gt;
&lt;p&gt;
The &lt;code&gt;human_scorer&lt;/code&gt; will score how well the detectors did on our data sets. The only thing that needs to be passed into it is the detector/predictor that decides if an image has a human in it. Calling it will produce an org-table with some metrics about how well it did.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;human_scorer = partial(F1Scorer,
		       true_images=human_files_short,
		       false_images=dog_files_short)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3e18b8f" class="outline-2"&gt;
&lt;h2 id="org3e18b8f"&gt;OpenCV&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3e18b8f"&gt;
&lt;p&gt;
We're going to use OpenCV's implementation of &lt;a href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html"&gt;Haar feature-based cascade classifiers&lt;/a&gt; to detect human faces in images.  
&lt;/p&gt;

&lt;p&gt;
OpenCV provides pre-trained face detectors stored as XML files on &lt;a href="https://github.com/opencv/opencv/tree/master/data/haarcascades"&gt;github&lt;/a&gt;. The detector I'm going to use is stored in a directory named &lt;code&gt;haarcascades&lt;/code&gt;. Here's a demonstration of how to use this face detector to find a human face in an image.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb3c0254" class="outline-3"&gt;
&lt;h3 id="orgb3c0254"&gt;Extract the Pre-Trained Face Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb3c0254"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;haar_path = DataPathTwo("haarcascade_frontalface_alt.xml", folder_key="HAAR_CASCADES")
assert haar_path.from_folder.is_file()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Ended: 2019-01-02 19:28:33.152747
Elapsed: 0:00:00.000933

&lt;/pre&gt;

&lt;p&gt;
As you can see from the file-name this detector is tuned for faces looking at the camera (as opposed to, say, a face in profile). Now we need to build the classifier using the XML file.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class OpenCVFaceDetector:
    """OpenCV Face Detector

    Args:
     path: path to the model's XML file
    """
    def __init__(self, path: str) -&amp;gt; None:
	self.path = path
	self._classifier = None
	return

    @property
    def classifier(self) -&amp;gt; cv2.CascadeClassifier:
	"""Face Classifier"""
	if self._classifier is None:
	    self._classifier = cv2.CascadeClassifier(self.path)
	return self._classifier

    def detect_faces(self, image_path: str) -&amp;gt; numpy.ndarray:
	"""Find faces in an image

	Args:
	 image_path: path to the image

	Returns:
	 array of bounding boxes
	"""
	# this creates a Matplotlib Image
	image = cv2.imread(str(image_path))
	# the classifier needs a grayscale image
	grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
	return self.classifier.detectMultiScale(grayscale)

    def add_bounding_boxes(self, image_path: str) -&amp;gt; numpy.ndarray:
	"""Adds bounding boxes to the image

	Args:
	 image: path to the image

	Returns:
	 RGB image with faces boxed in
	"""
	faces = self.detect_faces(image_path)
	# this is redundant, but it's only for troubleshooting
	image = cv2.imread(str(image_path))

	# The arguments to the ``cv2.rectangle`` call are
	#  - image
	#  - the top-left coordinates of the rectangle
	#  - the bottom-right coordinates of the rectangle
	#  - the color
	#  - the thickness of the line.
	for top_left_x, top_left_y ,width, height in faces:
	    cv2.rectangle(image,
		  (top_left_x, top_left_y),
		  (top_left_x + width, top_left_y + height),
		  (255,0,0), 2)
	# the image is BGR, so the triplet setting the color =(200, 0, 0)=
	# is setting the rectangle to blue.
	# before we convert it to RGB
	return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    def has_face(self, image_path: str) -&amp;gt; bool:
	"""Checks if the image contains faces

	Args:
	 image_path: path to the image file

	Returns:
	 True if there is at least one face in the image
	"""
	return len(self.detect_faces(image_path)) &amp;gt; 0
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;open_cv_detector = OpenCVFaceDetector(str(haar_path.from_folder))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org984510e" class="outline-3"&gt;
&lt;h3 id="org984510e"&gt;Check Out How It Works On An Image&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org984510e"&gt;
&lt;p&gt;
Before trying to use it, let's see how it does on one of our faces. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
figure.suptitle("OpenCV Face-Detection Bounding Box", weight="bold")
image = axe.imshow(open_cv_detector.add_bounding_boxes(human))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/opencv_face_bounded.png" alt="opencv_face_bounded.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Seems like it did a reasonable job. If you run this enough times you'll note that it draws the tightest box when the person is facing the camera directly and grabs more negative space when the person angles their head away from the camera.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3f69b5d" class="outline-3"&gt;
&lt;h3 id="org3f69b5d"&gt;Face Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3f69b5d"&gt;
&lt;p&gt;
Now that we have something that will draw bounding boxes for any faces it finds in photographs we can create a face-detector that just returns &lt;code&gt;True&lt;/code&gt; if there is a face or &lt;code&gt;False&lt;/code&gt; if there isn't one.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbf82758" class="outline-3"&gt;
&lt;h3 id="orgbf82758"&gt;Testing the Face Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgbf82758"&gt;
&lt;p&gt;
Here we're going to see how well the face detector does at detecting human faces and not mistaking dogs for humans.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;open_cv_scorer = human_scorer(open_cv_detector.has_face)
open_cv_scorer()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Accuracy&lt;/td&gt;
&lt;td class="org-right"&gt;0.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Precision&lt;/td&gt;
&lt;td class="org-right"&gt;0.85&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Recall&lt;/td&gt;
&lt;td class="org-right"&gt;1.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Specificity&lt;/td&gt;
&lt;td class="org-right"&gt;0.83&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;F1&lt;/td&gt;
&lt;td class="org-right"&gt;0.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Ended&lt;/td&gt;
&lt;td class="org-right"&gt;2019-01-03 14:01:49.321416&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Elapsed&lt;/td&gt;
&lt;td class="org-right"&gt;0:00:17.670546&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
It did pretty well, but was penalized for some false-positives. What did a false positive look like?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org935d7a1" class="outline-3"&gt;
&lt;h3 id="org935d7a1"&gt;Looking at the False Positives&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org935d7a1"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dogman_index = first_prediction(open_cv_scorer.false_image_predictions)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
1: True

&lt;/pre&gt;

&lt;p&gt;
So the image at index 1 was a dog that the OpenCV detector thought was a human.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
source = dog_files_short[dogman_index]
name = " ".join(
    os.path.splitext(
	os.path.basename(source))[0].split("_")[:-1]).title()
figure.suptitle("Dog-Human OpenCV Prediction ({})".format(
    name), weight="bold")
image = Image.open(source)
image = axe.imshow(image)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/files/posts/nano/dog-breed-classifier/human-face-detection/opencv_dog_man.png" alt="opencv_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/opencv_dog_man.png" alt="opencv_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
This doesn't really look like a human, but I don't think the detector is specifically trained for &lt;i&gt;humans&lt;/i&gt; so much as &lt;i&gt;features&lt;/i&gt; that human have when looking straight at the camera, so I'm guessing straight-on views will create false positives. Although the mouth seems to be kind of inhuman.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9d623ae" class="outline-2"&gt;
&lt;h2 id="org9d623ae"&gt;DLIB&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9d623ae"&gt;
&lt;p&gt;
Now for another face-detector, this time using &lt;a href="https://github.com/ageitgey/face_recognition"&gt;&lt;code&gt;face_recognition&lt;/code&gt;&lt;/a&gt;, a python interface to &lt;a href="http://dlib.net/"&gt;dlib's&lt;/a&gt; facial recognition code.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3a9f4c2" class="outline-3"&gt;
&lt;h3 id="org3a9f4c2"&gt;Testing It With an Image&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3a9f4c2"&gt;
&lt;p&gt;
Let's see how the bounding box it produces looks given the same image that the &lt;code&gt;OpenCV&lt;/code&gt; detector was given.
&lt;/p&gt;

&lt;p&gt;
The face-recognition code is much simpler, but to make it consistent I'll add a class that matches the &lt;code&gt;OpenCVFaceDetector&lt;/code&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DlibFaceDetector:
    """DLIB (via face_detector) face detector"""
    def detect_faces(self, image_path: str) -&amp;gt; numpy.ndarray:
	"""Finds the locations of faces

	Args:
	 image_path: path to the image

	Returns:
	 array of bounding box coordinates for the face(s)
	"""
	image = face_recognition.load_image_file(str(image_path))
	return face_recognition.face_locations(image)

    def add_bounding_boxes(self, image_path: str,
			   axe: matplotlib.axes.Axes) -&amp;gt; None:
	"""Adds patches to the current matplotlib figure

	Args:
	 image_path: path to the image file
	 axe: axes to add the rectangle to
	"""
	for (top, right, bottom, left) in self.detect_faces(image_path):
	    width = right - left
	    height = top - bottom
	    rectangle = patches.Rectangle((top, right), width, height,
					  fill=False)
	    axe.add_patch(rectangle)
	return

    def has_face(self, image_path: str) -&amp;gt; bool:
	"""Checks if there is at least one face in the image

	Args:
	 image_path: path to the image file

	Returns:
	 True if there's at least one face in the image
	"""
	return len(self.detect_faces(image_path)) &amp;gt; 0
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dlib_detector = DlibFaceDetector()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
image = matplotlib_image.imread(str(human))
figure.suptitle("dlib Face Recognition Bounding-Box", weight='bold')
dlib_detector.add_bounding_boxes(str(human), axe)
plot = axe.imshow(image)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/files/posts/nano/dog-breed-classifier/human-face-detection/dlib_box.png" alt="dlib_box.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/dlib_box.png" alt="dlib_box.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
It seems pretty comparable to what the &lt;code&gt;OpenCV&lt;/code&gt; detector came up with.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd15cdd2" class="outline-3"&gt;
&lt;h3 id="orgd15cdd2"&gt;Measuring Performance&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd15cdd2"&gt;
&lt;p&gt;
Once again I'll run it through the FI scorer to see what's what.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dlib_scorer = human_scorer(dlib_detector.has_face)
dlib_scorer()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Accuracy&lt;/td&gt;
&lt;td class="org-right"&gt;0.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Precision&lt;/td&gt;
&lt;td class="org-right"&gt;0.86&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Recall&lt;/td&gt;
&lt;td class="org-right"&gt;1.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Specificity&lt;/td&gt;
&lt;td class="org-right"&gt;0.84&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;F1&lt;/td&gt;
&lt;td class="org-right"&gt;0.93&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Ended&lt;/td&gt;
&lt;td class="org-right"&gt;2019-01-03 14:31:36.848015&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Elapsed&lt;/td&gt;
&lt;td class="org-right"&gt;0:00:47.395556&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
The dlib model did &lt;i&gt;slightly&lt;/i&gt; better with its avoidance of false positives, but it might not be enough to justify the extra time.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org34dc53e" class="outline-3"&gt;
&lt;h3 id="org34dc53e"&gt;False Humans&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org34dc53e"&gt;
&lt;p&gt;
What kind of image did the DLib Classifier classify as human when it came from the dog images?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dlib_dog_human_index = first_prediction(dlib_scorer.false_image_predictions)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
11: True

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
source = dog_files_short[dlib_dog_human_index]
name = " ".join(
    os.path.splitext(
	os.path.basename(source))[0].split("_")[:-1]).title()
figure.suptitle("Dog-Human DLib Prediction ({})".format(
    name), weight="bold")
image = Image.open(source)
image = axe.imshow(image)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/files/posts/nano/dog-breed-classifier/human-face-detection/dlib_dog_man.png" alt="dlib_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/dlib_dog_man.png" alt="dlib_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Well, this was a bit of a surprise. I don't know that it's really fair to be using this type of image, but what can you do?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>dlib</category><category>face detection</category><category>opencv</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/human-face-detection/</guid><pubDate>Wed, 02 Jan 2019 21:51:55 GMT</pubDate></item><item><title>Custom Data Loader</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/custom-data-loader/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/custom-data-loader/#org21d7387"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/custom-data-loader/#orge21415b"&gt;The Data Set&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/custom-data-loader/#org4583461"&gt;Put It All Together&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/custom-data-loader/#orgea758e9"&gt;Double-Check the Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/custom-data-loader/#org7ec17f9"&gt;Once Again With Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org21d7387" class="outline-2"&gt;
&lt;h2 id="org21d7387"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org21d7387"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0fdf21e" class="outline-3"&gt;
&lt;h3 id="org0fdf21e"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0fdf21e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org91f4393" class="outline-4"&gt;
&lt;h4 id="org91f4393"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org91f4393"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge54d445" class="outline-4"&gt;
&lt;h4 id="orge54d445"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge54d445"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dotenv&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torchvision&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pyplot&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;seaborn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torchvision.transforms&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;transforms&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga84f647" class="outline-4"&gt;
&lt;h4 id="orga84f647"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga84f647"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;neurotic.tangles.data_paths&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8dbf948" class="outline-3"&gt;
&lt;h3 id="org8dbf948"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8dbf948"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;get_ipython&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_line_magic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'matplotlib'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;'inline'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;get_ipython&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run_line_magic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'config'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"InlineBackend.figure_format = 'retina'"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;seaborn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"whitegrid"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"axes.grid"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		&lt;span class="s2"&gt;"xtick.labelsize"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		&lt;span class="s2"&gt;"ytick.labelsize"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		&lt;span class="s2"&gt;"font.size"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		&lt;span class="s2"&gt;"font.family"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"sans-serif"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
		&lt;span class="s2"&gt;"font.sans-serif"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"Open Sans"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Latin Modern Sans"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"Lato"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
		&lt;span class="s2"&gt;"figure.figsize"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)},&lt;/span&gt;
	    &lt;span class="n"&gt;font_scale&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge21415b" class="outline-2"&gt;
&lt;h2 id="orge21415b"&gt;The Data Set&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge21415b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;train_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"DOG_TRAIN"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_dir&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/datasets/dog-breed-classification/dogImages/train

&lt;/pre&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgac5ab5f" class="outline-3"&gt;
&lt;h3 id="orgac5ab5f"&gt;The Breeds&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgac5ab5f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;folders&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;directory&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;directory&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterdir&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folders&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['024.Bichon_frise', '022.Belgian_tervuren', '100.Lowchen', '028.Bluetick_coonhound', '128.Smooth_fox_terrier']

&lt;/pre&gt;

&lt;p&gt;
The folder-name structure appears to be &lt;code&gt;&amp;lt;index&amp;gt;.&amp;lt;breed&amp;gt;&lt;/code&gt;. One thing to note is that it isn't ordered by the leading index.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;breeds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"."&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;folder&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folders&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['Affenpinscher', 'Afghan_hound', 'Airedale_terrier', 'Akita', 'Alaskan_malamute']

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org951945a" class="outline-3"&gt;
&lt;h3 id="org951945a"&gt;The Files&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org951945a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;bichon_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;joinpath&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folders&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;bichon_files&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;bichon_folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"*"&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bichon_files&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['Bichon_frise_01735.jpg', 'Bichon_frise_01701.jpg', 'Bichon_frise_01697.jpg', 'Bichon_frise_01771.jpg', 'Bichon_frise_01716.jpg']

&lt;/pre&gt;

&lt;p&gt;
So the file structure appears to be &lt;code&gt;&amp;lt;breed&amp;gt;_&amp;lt;index&amp;gt;.jpg&lt;/code&gt;. I checked by hand (&lt;code&gt;ls -R train/ | grep "jpg" | wc -l&lt;/code&gt;) and there are 6,680 images in the training set.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"*/*"&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;6680&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[PosixPath('/home/hades/datasets/dog-breed-classification/dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'), PosixPath('/home/hades/datasets/dog-breed-classification/dogImages/train/001.Affenpinscher/Affenpinscher_00002.jpg'), PosixPath('/home/hades/datasets/dog-breed-classification/dogImages/train/001.Affenpinscher/Affenpinscher_00004.jpg'), PosixPath('/home/hades/datasets/dog-breed-classification/dogImages/train/001.Affenpinscher/Affenpinscher_00005.jpg'), PosixPath('/home/hades/datasets/dog-breed-classification/dogImages/train/001.Affenpinscher/Affenpinscher_00006.jpg')]
6680

&lt;/pre&gt;

&lt;p&gt;
In this case I don't think we need the paths to be sorted, since we're going to look them up by index, but why not?
&lt;/p&gt;

&lt;p&gt;
So, &lt;code&gt;training&lt;/code&gt; holds the paths to all the training images. We need a way to look up the images and labels by index.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"_"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"_"&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
['Pharaoh_hound', 'Irish_water_spaniel', 'Xoloitzcuintli', 'Border_collie', 'Lakeland_terrier']

&lt;/pre&gt;

&lt;p&gt;
So we have the path to each training file and the breed for each, now we need a list of indices to look it up. Now that I think about it, there really wasn't a reason for making the breeds from the foldersâ¦ maybe I'll make a pretty-name lookup from them instead.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;indices&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
6680

&lt;/pre&gt;

&lt;p&gt;
Now the name lookup.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;breed_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;breed&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;" "&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;breed&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"_"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;breed&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;breed&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"{}: {}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;breed&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;breed_map&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;breed&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
American_eskimo_dog: American Eskimo Dog
Bull_terrier: Bull Terrier
Boxer: Boxer
Xoloitzcuintli: Xoloitzcuintli
Bullmastiff: Bullmastiff

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4583461" class="outline-2"&gt;
&lt;h2 id="org4583461"&gt;Put It All Together&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4583461"&gt;
&lt;p&gt;
I'll make a class to build it up.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;DogFiles&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""Builds up the lists for the data-files&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     path: path to the top (train, test, validate) folder&lt;/span&gt;
&lt;span class="sd"&gt;     glob: glob to grab the files in the path&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"*/*"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;glob&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_breeds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_breeds_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_file_breeds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_file_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_paths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="sd"&gt;"""Breed names"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_breeds&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="n"&gt;folders&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;directory&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;directory&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterdir&lt;/span&gt;&lt;span class="p"&gt;()]&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_breeds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format_breed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"."&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
			    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;folder&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folders&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_breeds&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;breeds_labels&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="sd"&gt;"""maps the breed name to an index for the breed"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_breeds_labels&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_breeds_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
		&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_breeds_labels&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;file_breeds&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="sd"&gt;"""Breed for each file"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_file_breeds&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_file_breeds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format_breed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"_"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"_"&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
				 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_file_breeds&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;file_labels&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="sd"&gt;"""Breed-labels for each file"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_file_labels&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_file_labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;breeds_labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;breed&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
				 &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;breed&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;file_breeds&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_file_labels&lt;/span&gt;

    &lt;span class="nd"&gt;@property&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="sd"&gt;"""Paths to files&lt;/span&gt;

&lt;span class="sd"&gt;	Assumes there is a list of folders in the path and we want all their files&lt;/span&gt;
&lt;span class="sd"&gt;	"""&lt;/span&gt;
	&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_paths&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_paths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_paths&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;format_breed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="sd"&gt;"""remove underscore and caps-case&lt;/span&gt;

&lt;span class="sd"&gt;	Args:&lt;/span&gt;
&lt;span class="sd"&gt;	 token: the breed-name portion of the file or folder&lt;/span&gt;
&lt;span class="sd"&gt;	"""&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="s2"&gt;" "&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"_"&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;filer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DogFiles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;133&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;6680&lt;/span&gt;
&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;paths&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;file_labels&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;file_breeds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;file_breeds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2704
/home/hades/datasets/dog-breed-classification/dogImages/train/047.Chesapeake_bay_retriever/Chesapeake_bay_retriever_03378.jpg
46
Chesapeake Bay Retriever
Chesapeake Bay Retriever

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgea758e9" class="outline-2"&gt;
&lt;h2 id="orgea758e9"&gt;Double-Check the Labels&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgea758e9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;transform&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;folder_key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"MNIST"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MNIST&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			    &lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_loader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					   &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
					   &lt;span class="n"&gt;num_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dataiter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataiter&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
tensor([5])

&lt;/pre&gt;

&lt;p&gt;
So, when actually building the data-loader I'd have to return a tensor - or does the dataloader do that?
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7ec17f9" class="outline-2"&gt;
&lt;h2 id="org7ec17f9"&gt;Once Again With Pytorch&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7ec17f9"&gt;
&lt;p&gt;
According to &lt;a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"&gt;the data loading tutorial&lt;/a&gt; I don't actually have to do this - I thought I did because they bury how to actually do it for images at the bottom of the page, but it says that as long as the folders group the images by classification it will automatically create the labels for them and load the imagesâ¦
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;transformer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transforms&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ToTensor&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ImageFolder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;root&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transformer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;utils&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;iter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;images&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pyplot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;suptitle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"First Image ({})"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;breeds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()]),&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"bold"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axe_image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;axe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;imshow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/custom-data-loader/first_image.png" alt="first_image.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So it looks like that's all that I really neededâ¦
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>data</category><category>exploration</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/custom-data-loader/</guid><pubDate>Wed, 26 Dec 2018 02:51:06 GMT</pubDate></item><item><title>Dog Breed Classification</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/#orga2d1d6c"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/#org24f5732"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/#org50dbfe1"&gt;A Human Face Detector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/#org5b4622c"&gt;A Dog Detector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/#orgbce73cb"&gt;Combine The Detectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/#org3c5ba58"&gt;A Dog Breed Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/#org2c63c5c"&gt;The Dog Breed Classifier&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/#org8e6e688"&gt;Some Sample applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga2d1d6c" class="outline-2"&gt;
&lt;h2 id="orga2d1d6c"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga2d1d6c"&gt;
&lt;p&gt;
This application is a dog-breed classifier. It takes as input an image and detects if it's an image of either a human or a dog and if it's either one of those then it finds the dog-breed classification that the subject of the image most resembles. If it's neither a human or a dog then it emits an error message. To do this I'm going to try two libraries for each of the human face-detectors and dog detectors and I'm also going to try three Neural Networks to try and classify the dog breeds.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org24f5732" class="outline-2"&gt;
&lt;h2 id="org24f5732"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org24f5732"&gt;
&lt;p&gt;
This section does some preliminary set-up for the code that comes later.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3bbffed" class="outline-3"&gt;
&lt;h3 id="org3bbffed"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3bbffed"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4fe25d9" class="outline-4"&gt;
&lt;h4 id="org4fe25d9"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4fe25d9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from functools import partial
from pathlib import Path
import os
import warnings
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4ba0b36" class="outline-4"&gt;
&lt;h4 id="org4ba0b36"&gt;From Pypi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4ba0b36"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from dotenv import load_dotenv
from PIL import Image, ImageFile
from torchvision import datasets
import cv2
import face_recognition
import matplotlib.cbook
warnings.filterwarnings("ignore", category=matplotlib.cbook.mplDeprecation)
import matplotlib.pyplot as pyplot
import matplotlib.image as mpimage
import matplotlib.patches as patches
import numpy
try:
    import pyttsx3
    SPEAKABLE = True
except ImportError:
    print("pyttsx3 not available")
    SPEAKABLE = False
import seaborn
import torch
import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optimizer
import torchvision.transforms as transforms
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org281fe44" class="outline-4"&gt;
&lt;h4 id="org281fe44"&gt;This Project&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org281fe44"&gt;
&lt;p&gt;
This is code that I wrote to maybe make it easier to work with.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from neurotic.tangles.data_paths import DataPathTwo
from neurotic.tangles.timer import Timer
from neurotic.constants.imagenet_map import imagenet
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org37fffb3" class="outline-3"&gt;
&lt;h3 id="org37fffb3"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org37fffb3"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
	    rc={"axes.grid": False,
		"font.family": ["sans-serif"],
		"font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
		"figure.figsize": (8, 6)},
	    font_scale=1)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org273bddd" class="outline-3"&gt;
&lt;h3 id="org273bddd"&gt;Set the Random Seed&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org273bddd"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;numpy.random.seed(seed=2019)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga1032b8" class="outline-3"&gt;
&lt;h3 id="orga1032b8"&gt;Check If CUDA Is Available&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga1032b8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
cuda

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb316609" class="outline-3"&gt;
&lt;h3 id="orgb316609"&gt;Handle Truncated Images&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb316609"&gt;
&lt;p&gt;
There seems to be at least one image that is truncated which will cause an exception when it's loaded so this next setting lets us ignore the error and keep working.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ImageFile.LOAD_TRUNCATED_IMAGES = True
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org948aac1" class="outline-3"&gt;
&lt;h3 id="org948aac1"&gt;Build the Timer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org948aac1"&gt;
&lt;p&gt;
The timer times how long a code-block takes to run so that if I run it more than once I'll know if it will take a while.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer = Timer(beep=SPEAKABLE)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga7f9ddf" class="outline-3"&gt;
&lt;h3 id="orga7f9ddf"&gt;The Data Paths&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orga7f9ddf"&gt;
&lt;p&gt;
The data-sets are hosted online and need to be downloaded.
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;This is a download link for the &lt;a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"&gt;dog dataset&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;This is a download link for the the &lt;a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip"&gt;human dataset&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
I've already downloaded them and put the path to the folders in a &lt;code&gt;.env&lt;/code&gt; file so this next block gets the paths so we can load the data later.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9a68d5c" class="outline-4"&gt;
&lt;h4 id="org9a68d5c"&gt;The Model Path&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9a68d5c"&gt;
&lt;p&gt;
The models turn out to take up a lot of space so I'm saving them outside of the repository.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;MODEL_PATH = DataPathTwo(folder_key="MODELS")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc4778f9" class="outline-4"&gt;
&lt;h4 id="orgc4778f9"&gt;Dog Paths&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc4778f9"&gt;
&lt;p&gt;
This is a class to hold the paths for the dog Images
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DogPaths:
    """holds the paths to the dog images"""
    def __init__(self) -&amp;gt; None:
	self._main = None
	self._training = None
	self._testing = None
	self._validation = None
	self._breed_count = None
	load_dotenv()
	return

    @property
    def main(self) -&amp;gt; DataPathTwo:
	"""The path to the main folder"""
	if self._main is None:
	    self._main = DataPathTwo(folder_key="DOG_PATH")
	return self._main

    @property
    def training(self) -&amp;gt; DataPathTwo:
	"""Path to the training images"""
	if self._training is None:
	    self._training = DataPathTwo(folder_key="DOG_TRAIN")
	return self._training

    @property
    def validation(self) -&amp;gt; DataPathTwo:
	"""Path to the validation images"""
	if self._validation is None:
	    self._validation = DataPathTwo(folder_key="DOG_VALIDATE")
	return self._validation

    @property
    def testing(self) -&amp;gt; DataPathTwo:
	"""Path to the testing images"""
	if self._testing is None:
	    self._testing = DataPathTwo(folder_key="DOG_TEST")
	return self._testing

    @property
    def breed_count(self) -&amp;gt; int:
	"""Counts the number of dog breeds

	This assumes that the training folder has all the breeds
	"""
	if self._breed_count is None:
	    self._breed_count = len(set(self.training.folder.iterdir()))
	return self._breed_count

    def check(self) -&amp;gt; None:
	"""Checks that the folders are valid

	Raises: 
	 AssertionError: folder doesn't exist
	"""
	self.main.check_folder()
	self.training.check_folder()
	self.validation.check_folder()
	self.testing.check_folder()
	return
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now I'll build the dog-paths.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dog_paths = DogPaths()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge76b265" class="outline-4"&gt;
&lt;h4 id="orge76b265"&gt;Human Path&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge76b265"&gt;
&lt;p&gt;
This is the path to the downloaded &lt;a href="http://vis-www.cs.umass.edu/lfw/"&gt;Labeled Faces in the Wild&lt;/a&gt; data set.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;human_path = DataPathTwo(folder_key="HUMAN_PATH")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org55b75bb" class="outline-4"&gt;
&lt;h4 id="org55b75bb"&gt;Check the Paths&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org55b75bb"&gt;
&lt;p&gt;
This makes sure that the folders exist and shows where they are.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(dog_paths.main.folder)
print(dog_paths.training.folder)
print(dog_paths.testing.folder)
print(dog_paths.validation.folder)
dog_paths.check()
print(human_path.folder)
human_path.check_folder()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/data/datasets/dog-breed-classification/dogImages
/home/hades/data/datasets/dog-breed-classification/dogImages/train
/home/hades/data/datasets/dog-breed-classification/dogImages/test
/home/hades/data/datasets/dog-breed-classification/dogImages/valid
/home/hades/data/datasets/dog-breed-classification/lfw

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1d72b4b" class="outline-3"&gt;
&lt;h3 id="org1d72b4b"&gt;Count The Breeds&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1d72b4b"&gt;
&lt;p&gt;
To build the neural network I'll need to know how many dog breeds there are. I made it an attribute of the &lt;code&gt;DogPath&lt;/code&gt; class and I'll just inspect it here.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("Number of Dog Breeds: {}".format(dog_paths.breed_count))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Number of Dog Breeds: 133

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org55c34b9" class="outline-3"&gt;
&lt;h3 id="org55c34b9"&gt;Load the Files&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org55c34b9"&gt;
&lt;p&gt;
For this first part we're going to load in all the files and ignore the train-validation-test split for the dog-images.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer.start()
human_files = numpy.array(list(human_path.folder.glob("*/*")))
dog_files = numpy.array(list(dog_paths.main.folder.glob("*/*/*")))
timer.end()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Started: 2019-01-13 14:05:09.566221
Ended: 2019-01-13 14:05:42.932863
Elapsed: 0:00:33.366642

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print('There are {:,} total human images.'.format(len(human_files)))
print('There are {:,} total dog images.'.format(len(dog_files)))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
There are 13,233 total human images.
There are 8,351 total dog images.

&lt;/pre&gt;

&lt;p&gt;
So we have a bit more human images than dog images.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0792098" class="outline-3"&gt;
&lt;h3 id="org0792098"&gt;Some Helper Code&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0792098"&gt;
&lt;p&gt;
This is code meant to help with the other code.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge0fea57" class="outline-4"&gt;
&lt;h4 id="orge0fea57"&gt;Tee&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge0fea57"&gt;
&lt;p&gt;
I wrote this for the jupyter notebook because it loses the output if the server disconnects. I think it will also make it easier to use multiproccessing so I can train things in parallel. But I don't think I'm using it right now.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Tee:
    """Save the input to a file and print it

    Args:
     log_name: name to give the log    
     directory_path: path to the directory for the file
    """
    def __init__(self, log_name: str, 
		 directory_name: str="../../../logs/dog-breed-classifier") -&amp;gt; None:
	self.directory_name = directory_name
	self.log_name = log_name
	self._path = None
	self._log = None
	return

    @property
    def path(self) -&amp;gt; Path:
	"""path to the log-file"""
	if self._path is None:
	    self._path = Path(self.directory_name).expanduser()
	    assert self._path.is_dir()
	    self._path = self._path.joinpath(self.log_name)
	return self._path

    @property
    def log(self):
	"""File object to write log to"""
	if self._log is None:
	    self._log = self.path.open("w", buffering=1)
	return self._log

    def __call__(self, line: str) -&amp;gt; None:
	"""Writes to the file and stdout

	Args:
	 line: text to emit
	"""
	self.log.write("{}\n".format(line))
	print(line)
	return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0f6dc8a" class="outline-4"&gt;
&lt;h4 id="org0f6dc8a"&gt;F1 Scorer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0f6dc8a"&gt;
&lt;p&gt;
I'm going to be comparing two models for both the humans and dogs, this scorer will focus on the F1 score, but will emit some other information as well.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; class F1Scorer:
     """Calculates the F1 and other scores

     Args:
      predictor: callable that gets passed and image and outputs boolean
      true_images: images that should be predicted as True
      false_images: images that shouldn't be matched by the predictor
      done_message: what to announce when done
     """
     def __init__(self, predictor: callable, true_images:list,
		  false_images: list,
		  done_message: str="Scoring Done") -&amp;gt; None:
	 self.predictor = predictor
	 self.true_images = true_images
	 self.false_images = false_images
	 self.done_message = done_message
	 self._timer = None
	 self._false_image_predictions = None
	 self._true_image_predictions = None
	 self._false_positives = None
	 self._false_negatives = None
	 self._true_positives = None
	 self._true_negatives = None
	 self._false_positive_rate = None
	 self._precision = None
	 self._recall = None
	 self._f1 = None
	 self._accuracy = None
	 self._specificity = None
	 return

     @property
     def timer(self) -&amp;gt; Timer:
	 if self._timer is None:
	     self._timer = Timer(message=self.done_message, emit=False)
	 return self._timer

     @property
     def false_image_predictions(self) -&amp;gt; list:
	 """Predictions made on the false-images"""
	 if self._false_image_predictions is None:
	     self._false_image_predictions = [self.predictor(str(image))
					      for image in self.false_images]
	 return self._false_image_predictions

     @property
     def true_image_predictions(self) -&amp;gt; list:
	 """Predictions on the true-images"""
	 if self._true_image_predictions is None:
	     self._true_image_predictions = [self.predictor(str(image))
					     for image in self.true_images]
	 return self._true_image_predictions

     @property
     def true_positives(self) -&amp;gt; int:
	 """count of correct positive predictions"""
	 if self._true_positives is None:
	     self._true_positives = sum(self.true_image_predictions)
	 return self._true_positives

     @property
     def false_positives(self) -&amp;gt; int:
	 """Count of incorrect positive predictions"""
	 if self._false_positives is None:
	     self._false_positives = sum(self.false_image_predictions)
	 return self._false_positives

     @property
     def false_negatives(self) -&amp;gt; int:
	 """Count of images that were incorrectly classified as negative"""
	 if self._false_negatives is None:
	     self._false_negatives = len(self.true_images) - self.true_positives
	 return self._false_negatives

     @property
     def true_negatives(self) -&amp;gt; int:
	 """Count of images that were correctly ignored"""
	 if self._true_negatives is None:
	     self._true_negatives = len(self.false_images) - self.false_positives
	 return self._true_negatives

     @property
     def accuracy(self) -&amp;gt; float:
	 """fraction of correct predictions"""
	 if self._accuracy is None:
	     self._accuracy = (
		 (self.true_positives + self.true_negatives)
		 /(len(self.true_images) + len(self.false_images)))
	 return self._accuracy

     @property
     def precision(self) -&amp;gt; float:
	 """True-Positive with penalty for false positives"""
	 if self._precision is None:
	     self._precision = self.true_positives/(
		 self.true_positives + self.false_positives)
	 return self._precision

     @property
     def recall(self) -&amp;gt; float:
	 """fraction of correct images correctly predicted"""
	 if self._recall is None:
	     self._recall = (
		 self.true_positives/len(self.true_images))
	 return self._recall

     @property
     def false_positive_rate(self) -&amp;gt; float:
	 """fraction of incorrect images predicted as positive"""
	 if self._false_positive_rate is None:
	     self._false_positive_rate = (
		 self.false_positives/len(self.false_images))
	 return self._false_positive_rate

     @property
     def specificity(self) -&amp;gt; float:
	 """metric for how much to believe a negative prediction

	 Specificity is 1 - false positive rate so you only need one or the other
	 """
	 if self._specificity is None:
	     self._specificity = self.true_negatives/(self.true_negatives
						      + self.false_positives)
	 return self._specificity

     @property
     def f1(self) -&amp;gt; float:
	 """Harmonic Mean of the precision and recall"""
	 if self._f1 is None:
	     TP = 2 * self.true_positives
	     self._f1 = (TP)/(TP + self.false_negatives + self.false_positives)
	 return self._f1

     def __call__(self) -&amp;gt; None:
	 """Emits the F1 and other scores as an org-table
	 """
	 self.timer.start()
	 print("|Metric|Value|")
	 print("|-+-|")
	 print("|Accuracy|{:.2f}|".format(self.accuracy))
	 print("|Precision|{:.2f}|".format(self.precision))
	 print("|Recall|{:.2f}|".format(self.recall))
	 print("|Specificity|{:.2f}".format(self.specificity))
	 # print("|False Positive Rate|{:.2f}|".format(self.false_positive_rate))
	 print("|F1|{:.2f}|".format(self.f1))
	 self.timer.end()
	 print("|Elapsed|{}|".format(self.timer.ended - self.timer.started))
	 return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf026dfb" class="outline-4"&gt;
&lt;h4 id="orgf026dfb"&gt;Get Human&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf026dfb"&gt;
&lt;p&gt;
This will grab the name of the person in an image file (based on the file name).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def get_name(path: Path) -&amp;gt; str:
    """Extracts the name of the person from the file name

    Args:
     path: path to the file

    Returns:
     the name extracted from the file name
    """
    return " ".join(path.name.split("_")[:-1]).title()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc485e46" class="outline-4"&gt;
&lt;h4 id="orgc485e46"&gt;Display Image&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgc485e46"&gt;
&lt;p&gt;
A little matplotlib helper.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def display_image(image: Path, title: str, is_file: bool=True) -&amp;gt; tuple:
    """Plot the image

    Args:
     image: path to the image file or image
     title: title for the image
     is_file: first argument is a file name, not an array

    Returns:
     figure, axe
    """
    figure, axe = pyplot.subplots()
    figure.suptitle(title, weight="bold")
    axe.tick_params(dict(axis="both",
			 which="both",
			 bottom=False,
			 top=False))
    axe.get_xaxis().set_ticks([])
    axe.get_yaxis().set_ticks([])
    if is_file:
	image = Image.open(image)
    image = axe.imshow(image)
    return figure, axe
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org19eaf12" class="outline-4"&gt;
&lt;h4 id="org19eaf12"&gt;First Prediction&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org19eaf12"&gt;
&lt;p&gt;
This function is used to grab images that register as false-positives.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def first_prediction(source: list, start:int=0) -&amp;gt; int:
    """Gets the index of the first True prediction

    Args:
     source: list of True/False predictions
     start: index to start the search from

    Returns:
     index of first True prediction found
    """
    for index, prediction in enumerate(source[start:]):
	if prediction:
	    print("{}: {}".format(start + index, prediction))
	    break
    return start + index
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2adb687" class="outline-3"&gt;
&lt;h3 id="org2adb687"&gt;Some Constants&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org2adb687"&gt;
&lt;p&gt;
The pre-trained models need to be normalized using the following means and standard deviations.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;MEANS = [0.485, 0.456, 0.406]
DEVIATIONS = [0.229, 0.224, 0.225]
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
I'm going to offload the models that I move to the GPU while exploring before doing the final implementation so this list is to keep track of all of them.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;MODELS = []
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org50dbfe1" class="outline-2"&gt;
&lt;h2 id="org50dbfe1"&gt;A Human Face Detector&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org50dbfe1"&gt;
&lt;p&gt;
I'm going to need a way to tell if an image has a human in it (or not), so I'll build two versions of a detector, one using &lt;a href="https://opencv.org/"&gt;OpenCV&lt;/a&gt;, and one using &lt;a href="http://dlib.net/"&gt;dlib&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
For each detector I'm going to look at an example image before running an assessment of how well it did so I'll select one at random here. 
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sample_face = numpy.random.choice(human_files, 1)[0]
sample_name = get_name(sample_face)
print(sample_name)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
David Anderson

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = display_image(sample_face, sample_name)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/sample_human.png" alt="sample_human.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgec53c1a" class="outline-3"&gt;
&lt;h3 id="orgec53c1a"&gt;The Data Sets&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgec53c1a"&gt;
&lt;p&gt;
To save some time I'm going to assess the detectors using random images from the data sets.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;count = int(.1 * len(human_files))
human_files_short = numpy.random.choice(human_files, count)
dog_files_short = numpy.random.choice(dog_files, count)
print("{:,}".format(count))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
1323

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0e3cc0e" class="outline-3"&gt;
&lt;h3 id="org0e3cc0e"&gt;The Scorer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0e3cc0e"&gt;
&lt;p&gt;
I'm going to re-use the same scorer for the dlib face-detector so to make it simpler I'll attach the correct images to the &lt;code&gt;F1Scorer&lt;/code&gt; class.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;human_scorer = partial(F1Scorer,
		       true_images=human_files_short,
		       false_images=dog_files_short)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org002acef" class="outline-3"&gt;
&lt;h3 id="org002acef"&gt;OpenCV&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org002acef"&gt;
&lt;p&gt;
Here I'll use OpenCV's implementation of &lt;a href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html"&gt;Haar feature-based cascade classifiers&lt;/a&gt; (which you can grab from &lt;a href="https://github.com/opencv/opencv/tree/master/data/haarcascades"&gt;github&lt;/a&gt;) to detect human faces in images.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org12e5988" class="outline-4"&gt;
&lt;h4 id="org12e5988"&gt;Extract the Pre-Trained Face Detector&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org12e5988"&gt;
&lt;p&gt;
First I'll grab the path to the XML file that defines the classifier.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;haar_path = DataPathTwo("haarcascade_frontalface_alt.xml", folder_key="HAAR_CASCADES")
print(haar_path.from_folder)
assert haar_path.from_folder.is_file()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/data/datasets/dog-breed-classification/haarcascades/haarcascade_frontalface_alt.xml

&lt;/pre&gt;

&lt;p&gt;
Now we can load it.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;face_cascade = cv2.CascadeClassifier(str(haar_path.from_folder))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9f8c558" class="outline-4"&gt;
&lt;h4 id="org9f8c558"&gt;Inspect An Image&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9f8c558"&gt;
&lt;p&gt;
First let's see what the face detector detects by looking at a single image.
&lt;/p&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="orgddde77b"&gt;&lt;/a&gt;Load a Color (BGR) Image&lt;br&gt;
&lt;div class="outline-text-5" id="text-orgddde77b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;image = cv2.imread(str(sample_face))
print(image.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
(250, 250, 3)

&lt;/pre&gt;

&lt;p&gt;
So the image is a 250x250 pixel image with three channels. Since we're loading it with &lt;code&gt;cv2&lt;/code&gt; the three channels are Blue, Green, and Red.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org18be239" class="outline-4"&gt;
&lt;h4 id="org18be239"&gt;Convert the BGR Image To Grayscale&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org18be239"&gt;
&lt;p&gt;
To do the face-detection we need to convert the image to a grayscale image.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8568346" class="outline-4"&gt;
&lt;h4 id="org8568346"&gt;Find Some Faces In the Image&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8568346"&gt;
&lt;p&gt;
Now we can find the coordinates for bounding boxes for any faces that OpenCV finds in the image. 
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;faces = face_cascade.detectMultiScale(gray)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print('Number of faces detected:', len(faces))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Number of faces detected: 1

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org969834d" class="outline-4"&gt;
&lt;h4 id="org969834d"&gt;Show Us the Box&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org969834d"&gt;
&lt;p&gt;
The boxes are defined using a four-tuple with the &lt;i&gt;x&lt;/i&gt; and &lt;i&gt;y&lt;/i&gt; coordinates of the top-left corner of the box first followed by the width and height of the box. This next block adds the box to the image.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for (x,y,w,h) in faces:
    # add bounding box to color image
    cv2.rectangle(image, (x,y), (x+w,y+h), (255,0,0), 2)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
To display the image we need to convert it to RGB.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cv_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now we can display the image with the bounding box.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = display_image(cv_rgb, "OpenCV Face-Detection Bounding Box", False)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/face_bounded.png" alt="face_bounded.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7868790" class="outline-4"&gt;
&lt;h4 id="org7868790"&gt;Write a Human Face Detector&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7868790"&gt;
&lt;p&gt;
Now that we know how it works, we can use the OpenCV face-recognizer to tell us if the image has a human in it (because there will be at least one bounding-box).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# returns "True" if face is detected in image stored at img_path
def face_detector(image_path: str) -&amp;gt; bool:
    """Detects human faces in an image

    Args:
     image_path: path to the image to check

    Returns:
     True if there was at least one face in the image
    """
    image = cv2.imread(image_path)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray)
    return len(faces) &amp;gt; 0
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9e48c9f" class="outline-4"&gt;
&lt;h4 id="org9e48c9f"&gt;Assess the Human Face Detector&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9e48c9f"&gt;
&lt;p&gt;
Here I'll check how well the face detector does using an F1 score. I'll also show some other metrics, but F1 is the single-value that I'll be focused on.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;open_cv_scorer = human_scorer(face_detector)
open_cv_scorer()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Accuracy&lt;/td&gt;
&lt;td class="org-right"&gt;0.94&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Precision&lt;/td&gt;
&lt;td class="org-right"&gt;0.90&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Recall&lt;/td&gt;
&lt;td class="org-right"&gt;0.99&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Specificity&lt;/td&gt;
&lt;td class="org-right"&gt;0.89&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;F1&lt;/td&gt;
&lt;td class="org-right"&gt;0.94&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Elapsed&lt;/td&gt;
&lt;td class="org-right"&gt;0:02:42.880287&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Overall the model seems to have done quite well. It was better at &lt;i&gt;recall&lt;/i&gt; than &lt;i&gt;specificity&lt;/i&gt; so it tended to classify some dogs as humans (around 11 %).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dogman_index = first_prediction(open_cv_scorer.false_image_predictions)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2: True

&lt;/pre&gt;

&lt;p&gt;
It looks like the third dog image was classified as a human by OpenCV.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;source = dog_files_short[dogman_index]
name = get_name(source)
figure, axe = display_image(source,
			    "Dog-Human OpenCV Prediction ({})".format(name))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/dog_man.png" alt="dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
I guess I can see where this might look like a human face. Maybe.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgb8c06b0" class="outline-3"&gt;
&lt;h3 id="orgb8c06b0"&gt;DLIB&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgb8c06b0"&gt;
&lt;p&gt;
I'm also going to test &lt;a href="https://github.com/ageitgey/face_recognition"&gt;&lt;code&gt;face_recognition&lt;/code&gt;&lt;/a&gt;, a python interface to &lt;a href="http://dlib.net/"&gt;dlib's&lt;/a&gt; facial recognition code. Unlike &lt;code&gt;OpenCV&lt;/code&gt;, &lt;code&gt;face_recognition&lt;/code&gt; doesn't require you to do the image-conversions before looking for faces.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org29b5c71" class="outline-4"&gt;
&lt;h4 id="org29b5c71"&gt;Inspect an Image&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org29b5c71"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;image = face_recognition.load_image_file(sample_face)
locations = face_recognition.face_locations(image)
image = mpimage.imread(sample_face)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = display_image(image, "dlib Face Recognition Bounding-Box", False)
top, right, bottom, left = locations[0]
width = right - left
height = top - bottom
rectangle = patches.Rectangle((top, right), width, height, fill=False)
patch = axe.add_patch(rectangle)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/dlib_box.png" alt="dlib_box.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
This box seems to be more tightly cropped than the Open CV version.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2908ab4" class="outline-4"&gt;
&lt;h4 id="org2908ab4"&gt;The Face Detecor&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2908ab4"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def face_recognition_check(image_path: str) -&amp;gt; bool:
    """This decides if an image has a face in it

    Args:
     image_path: path to an image
    Returns:
     True if there's at least one face in the image
    """
    image = face_recognition.load_image_file(str(image_path))
    locations = face_recognition.face_locations(image)
    return len(locations) &amp;gt; 0
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org90cd6c5" class="outline-4"&gt;
&lt;h4 id="org90cd6c5"&gt;Assess the Face Detector&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org90cd6c5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dlib_dog_humans = human_scorer(face_recognition_check)
dlib_dog_humans()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Accuracy&lt;/td&gt;
&lt;td class="org-right"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Precision&lt;/td&gt;
&lt;td class="org-right"&gt;0.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Recall&lt;/td&gt;
&lt;td class="org-right"&gt;1.00&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Specificity&lt;/td&gt;
&lt;td class="org-right"&gt;0.91&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;F1&lt;/td&gt;
&lt;td class="org-right"&gt;0.96&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Elapsed&lt;/td&gt;
&lt;td class="org-right"&gt;0:09:28.752909&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Dlib took around four times as long to run as OpenCV did, but did better overall.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dlib_dog_human_index = first_prediction(dlib_dog_humans.false_image_predictions)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
5: True

&lt;/pre&gt;

&lt;p&gt;
The dlib model didn't have a false positive for the third image like the OpenCV model did, but it did get the sixth image wrong.
&lt;/p&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;source = dog_files_short[dlib_dog_human_index]
name = get_name(source)
figure, axe = display_image(source,
			    "Dog-Human DLib Prediction ({})".format(name))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/dlib_dog_man.png" alt="dlib_dog_man.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
These photos with humans and dogs in them seem problematic.
&lt;/p&gt;

&lt;p&gt;
&lt;code&gt;face_recognition&lt;/code&gt; provides another model based on a CNN that I wanted to try but it gives me out-of-memory errors so I'll have to save that for later.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5b4622c" class="outline-2"&gt;
&lt;h2 id="org5b4622c"&gt;A Dog Detector&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5b4622c"&gt;
&lt;p&gt;
Now I'll take two pre-trained CNNs and use &lt;a href="https://en.wikipedia.org/wiki/Transfer_learning"&gt;transfer learning&lt;/a&gt; to have them detect dogs in images.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd671407" class="outline-3"&gt;
&lt;h3 id="orgd671407"&gt;A Dog Detector Function&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd671407"&gt;
&lt;p&gt;
If you look at the imagenet &lt;a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"&gt;dictionary&lt;/a&gt;, you'll see that the categories for dogs have indices from 151 to 268, so without altering our models we can check if an image is a dog by seeing if they classify the image within this range of values.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DOG_LOWER, DOG_UPPER = 150, 260
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def dog_detector(img_path: Path, predictor: object):
    """Predicts if the image is a dog

    Args:
     img_path: path to image file
     predictor: callable that maps the image to an ID

    Returns:
     is-dog: True if the image contains a dog
    """
    return DOG_LOWER &amp;lt; predictor(img_path) &amp;lt; DOG_UPPER
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6834866" class="outline-3"&gt;
&lt;h3 id="org6834866"&gt;The VGG-16 Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6834866"&gt;
&lt;p&gt;
I'm going to use a VGG-16 model, along with weights that have been trained on &lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt;, a data set containing objects from one of &lt;a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a"&gt;1000 categories&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Pytorch comes with a VGG 16 model built-in so we just have to declare it with the &lt;code&gt;pretrained=True&lt;/code&gt; argument to download and load it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer.start()
VGG16 = models.vgg16(pretrained=True)
VGG16.eval()
VGG16.to(device)
MODELS.append(VGG16)
timer.end()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Started: 2019-01-13 14:43:39.512124
Ended: 2019-01-13 14:44:07.819057
Elapsed: 0:00:28.306933

&lt;/pre&gt;

&lt;p&gt;
&lt;b&gt;Note:&lt;/b&gt; The first time you run this it has to download the state dictionary so it will take much longer than it would once you've run it at least once.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org945390c" class="outline-4"&gt;
&lt;h4 id="org945390c"&gt;Making Predictions With the VGG 16 Model&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org945390c"&gt;
&lt;p&gt;
In order to use the images with our model we have to run them through a transform. Even then, the forward-pass expects you to pass it a batch, not a single image, so you have to add an extra (fourth) dimension to the images to represent the batch. I found out how to fix the dimensions (using &lt;a href="https://pytorch.org/docs/stable/tensors.html?highlight=unsqueeze#torch.Tensor.unsqueeze"&gt;unsqueeze&lt;/a&gt; to add an empty dimension) from &lt;a href="http://blog.outcome.io/pytorch-quick-start-classifying-an-image/"&gt;this blog post&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
This next block sets up the transforms. Each pre-trained model expects a specific image-size for the inputs. In this case the &lt;code&gt;VGG16&lt;/code&gt; model expects a 224 x 224 image (which is why I set the &lt;code&gt;IMAGE_SIZE&lt;/code&gt; to 224).
&lt;/p&gt;

&lt;p&gt;
The images also have to be normalized using a specific set of means and standard deviations, but since pytorch uses the same ones for all the models I defined them at the top of this document because I'll be using them later for the inception model as well.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;IMAGE_SIZE = 224
IMAGE_HALF_SIZE = IMAGE_SIZE//2

vgg_transform = transforms.Compose([transforms.Resize(255),
				    transforms.CenterCrop(IMAGE_SIZE),
				    transforms.ToTensor(),
				    transforms.Normalize(MEANS,
							 DEVIATIONS)])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org50770d0" class="outline-4"&gt;
&lt;h4 id="org50770d0"&gt;VGG16 Predict&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org50770d0"&gt;
&lt;p&gt;
This is a function to predict what class an image is.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def VGG16_predict(img_path: str) -&amp;gt; int:
    '''
    Uses a pre-trained VGG-16 model to obtain the index corresponding to 
    predicted ImageNet class for image at specified path

    Args:
	img_path: path to an image

    Returns:
	Index corresponding to VGG-16 model's prediction
    '''
    image = Image.open(str(img_path))
    image = vgg_transform(image).unsqueeze(0).to(device)
    output = VGG16(image)
    probabilities = torch.exp(output)
    top_probability, top_class = probabilities.topk(1, dim=1)
    return top_class.item()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Let's see what the model predicts for an image.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = numpy.random.choice(dog_files_short)
print(path)
classification = VGG16_predict(path)
print(imagenet[classification])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
/home/hades/data/datasets/dog-breed-classification/dogImages/valid/044.Cane_corso/Cane_corso_03122.jpg
American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier

&lt;/pre&gt;

&lt;p&gt;
Our classifier recognizes that the image is a dog, but thinks that it's a Terrire, not a Cane Corso. Here's what it saw.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;name = get_name(path)
figure, axe = display_image(path, name)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/vgg_misclassified.png" alt="vgg_misclassified.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
And this is what it thought it was (a bull-mastiff).
&lt;/p&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/american_staffordshire_terrier.jpg" alt="american_staffordshire_terrier.jpg"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1c56aad" class="outline-3"&gt;
&lt;h3 id="org1c56aad"&gt;Assess the Dog Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1c56aad"&gt;
&lt;p&gt;
Now, as with the human face-detectors, I'll calculate some metrics to see how the VGG16 dog-detector does.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dog_scorer = partial(F1Scorer, true_images=dog_files_short,
		     false_images=human_files_short)
vgg_predictor = partial(dog_detector, predictor=VGG16_predict)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; vgg_scorer = dog_scorer(vgg_predictor)
 vgg_scorer()
&lt;/pre&gt;&lt;/div&gt;
&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Accuracy&lt;/td&gt;
&lt;td class="org-right"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Precision&lt;/td&gt;
&lt;td class="org-right"&gt;0.99&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Recall&lt;/td&gt;
&lt;td class="org-right"&gt;0.92&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Specificity&lt;/td&gt;
&lt;td class="org-right"&gt;0.99&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;F1&lt;/td&gt;
&lt;td class="org-right"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Elapsed&lt;/td&gt;
&lt;td class="org-right"&gt;0:02:37.257690&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Unlike the face-detectors, the VGG16 dog detector did better at avoiding false-positives than it did at detecting dogs.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3812eb1" class="outline-3"&gt;
&lt;h3 id="org3812eb1"&gt;Inception&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3812eb1"&gt;
&lt;p&gt;
The previous detector used the VGG 16 model, but now I'll try the &lt;a href="http://pytorch.org/docs/master/torchvision/models.html#inception-v3"&gt;Inception-v3&lt;/a&gt; model, which was designed to use less resources than the VGG model, to do some dog-detection.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; timer.start()
 inception = models.inception_v3(pretrained=True)
 inception.to(device)
 inception.eval()
 MODELS.append(inception)
 timer.end()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Started: 2019-01-13 18:45:27.709998
Ended: 2019-01-13 18:45:31.775443
Elapsed: 0:00:04.065445

&lt;/pre&gt;
&lt;/div&gt;

&lt;div id="outline-container-org96d85db" class="outline-4"&gt;
&lt;h4 id="org96d85db"&gt;Making a Prediction&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org96d85db"&gt;
&lt;p&gt;
This was my original dog detector using the Inception model, but when I tried it out it raised an error. See the next section for more information and the fix.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; def inception_predicts(image_path: str) -&amp;gt; int:
     """Predicts the category of the image

     Args:
      image_path: path to the image file

     Returns:
      classification: the resnet ID for the image
     """
     image = Image.open(str(image_path))
     image = vgg_transform(image).unsqueeze(0).to(device)
     output = inception(image)
     probabilities = torch.exp(output)
     top_probability, top_class = probabilities.topk(1, dim=1)
     return top_class.item()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7818cd5" class="outline-4"&gt;
&lt;h4 id="org7818cd5"&gt;Troubleshooting the Error&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7818cd5"&gt;
&lt;p&gt;
The &lt;code&gt;inception_predicts&lt;/code&gt; is throwing a Runtime Error saying that the sizes must be non-negative. I'll grab a file here to check it out.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; for path in dog_files_short:
     try:
	 prediction = inception_predicts(path)
     except RuntimeError as error:
	 print(error)
	 print(path)
	 break
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Given input size: (2048x5x5). Calculated output size: (2048x0x0). Output size is too small at /pytorch/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63
/home/hades/data/datasets/dog-breed-classification/dogImages/valid/044.Cane_corso/Cane_corso_03122.jpg

&lt;/pre&gt;

&lt;p&gt;
So this dog raised an error, let's see what it looks like.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; name = get_name(path)
 figure, axe = display_image(path, "Error-Producing Image ({})".format(name))
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/inception_error.png" alt="inception_error.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org9b1be7e"&gt;&lt;/a&gt;Why did this raise an error?&lt;br&gt;
&lt;div class="outline-text-5" id="text-org9b1be7e"&gt;
&lt;p&gt;
I couldn't find anyplace where pytorch documents it, but if you look at &lt;a href="https://pytorch.org/docs/stable/_modules/torchvision/models/inception.html#inception_v3"&gt;the source code&lt;/a&gt; you can see that they are expecting an image size of 299 pixels, so we need a diferent transform from that used by the VGG model.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; INCEPTION_IMAGE_SIZE = 299
 inception_transforms = transforms.Compose([
     transforms.Resize(INCEPTION_IMAGE_SIZE),
     transforms.CenterCrop(INCEPTION_IMAGE_SIZE),
     transforms.ToTensor(),
     transforms.Normalize(MEANS,
			  DEVIATIONS)])
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now try it again with the new transforms.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def inception_predicts_two(image_path: str) -&amp;gt; int:
    """Predicts the category of the image

    Args:
     image_path: path to the image file

    Returns:
     classification: the resnet ID for the image
    """
    image = Image.open(str(image_path))
    image = inception_transforms(image).unsqueeze(0).to(device)
    output = inception(image)
    probabilities = torch.exp(output)
    top_probability, top_class = probabilities.topk(1, dim=1)
    return top_class.item()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Does this fix it?
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9b8e3b2" class="outline-4"&gt;
&lt;h4 id="org9b8e3b2"&gt;The Score&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9b8e3b2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;inception_predictor = partial(dog_detector, predictor=inception_predicts_two)
inception_scorer = dog_scorer(inception_predictor)
inception_scorer()
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Metric&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Accuracy&lt;/td&gt;
&lt;td class="org-right"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Precision&lt;/td&gt;
&lt;td class="org-right"&gt;0.99&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Recall&lt;/td&gt;
&lt;td class="org-right"&gt;0.91&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Specificity&lt;/td&gt;
&lt;td class="org-right"&gt;0.99&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;F1&lt;/td&gt;
&lt;td class="org-right"&gt;0.95&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Elapsed&lt;/td&gt;
&lt;td class="org-right"&gt;0:03:00.836240&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
The inception had a little more false positives but also more true positives so in the end it came up about the same on the F1 score as the VGG 16 model. They both took about the same amount of time.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;inception_human_dog = first_prediction(inception_scorer.false_image_predictions)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
34: True

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
source = human_files_short[inception_human_dog]
name = " ".join(
    os.path.splitext(
	os.path.basename(source))[0].split("_")[:-1]).title()
figure.suptitle("Human-Dog Inception Prediction ({})".format(
    name), weight="bold")
image = Image.open(source)
image = axe.imshow(image)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/inception_man_dog.png" alt="inception_man_dog.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbce73cb" class="outline-2"&gt;
&lt;h2 id="orgbce73cb"&gt;Combine The Detectors&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgbce73cb"&gt;
&lt;p&gt;
Since jupyter (or org-babel) lets you run cells out of sequence I've spent too much time chasing bugs that weren't really bugs, I just hadn't run the right cell. To try and ameliorate that I'm going to use class-based code for the actual implementations.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5526060" class="outline-3"&gt;
&lt;h3 id="org5526060"&gt;The Dog Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org5526060"&gt;
&lt;p&gt;
The Dog Detector builds the parts of the deep learning model that are needed to check if there are dogs in the image.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DogDetector:
    """Detects dogs

    Args:
     model_definition: definition for the model
     device: where to run the model (CPU or CUDA)
     image_size: what to resize the file to (depends on the model-definition)
     means: mean for each channel
     deviations: standard deviation for each channel
     dog_lower_bound: index below where dogs start
     dog_upper_bound: index above where dogs end
    """
    def __init__(self,
		 model_definition: nn.Module=models.inception_v3,
		 image_size: int=INCEPTION_IMAGE_SIZE,
		 means: list=MEANS,
		 deviations = DEVIATIONS,
		 dog_lower_bound: int=DOG_LOWER,
		 dog_upper_bound: int=DOG_UPPER,
		 device: torch.device=None) -&amp;gt; None:
	self.model_definition = model_definition
	self.image_size = image_size
	self.means = means
	self.deviations = deviations
	self.dog_lower_bound = dog_lower_bound
	self.dog_upper_bound = dog_upper_bound
	self._device = device
	self._model = None
	self._transform = None
	return

    @property
    def device(self) -&amp;gt; torch.device:
	"""The device to add the model to"""
	if self._device is None:
	    self._device = torch.device("cuda"
					if torch.cuda.is_available()
					else "cpu")
	return self._device

    @property
    def model(self) -&amp;gt; nn.Module:
	"""Build the model"""
	if self._model is None:
	    self._model = self.model_definition(pretrained=True)
	    self._model.to(self.device)
	    self._model.eval()
	return self._model

    @property
    def transform(self) -&amp;gt; transforms.Compose:
	"""The transformer for the image data"""
	if self._transform is None:
	    self._transform = transforms.Compose([
		transforms.Resize(self.image_size),
		transforms.CenterCrop(self.image_size),
		transforms.ToTensor(),
		transforms.Normalize(self.means,
				     self.deviations)])
	return self._transform

    def __call__(self, image_path: str) -&amp;gt; bool:
	"""Checks if there is a dog in the image"""
	image = Image.open(str(image_path))
	image = self.transform(image).unsqueeze(0).to(self.device)
	output = self.model(image)
	probabilities = torch.exp(output)
	_, top_class = probabilities.topk(1, dim=1)
	return self.dog_lower_bound &amp;lt; top_class.item() &amp;lt; self.dog_upper_bound
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge31ad43" class="outline-3"&gt;
&lt;h3 id="orge31ad43"&gt;The Species Detector&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge31ad43"&gt;
&lt;p&gt;
The Species Detector holds the human and dog detectors.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class SpeciesDetector:
    """Detect dogs and humans

    Args:
     device: where to put the dog-detecting model
    """
    def __init__(self, device: torch.device=None) -&amp;gt; None:
	self.device = device
	self._dog_detector = None
	return

    @property
    def dog_detector(self) -&amp;gt; DogDetector:
	"""Neural Network dog-detector"""
	if self._dog_detector is None:
	    self._dog_detector = DogDetector(device=self.device)
	return self._dog_detector

    def is_human(self, image_path: str) -&amp;gt; bool:
	"""Checks if the image is a human

	Args:
	 image_path: path to the image

	Returns:
	 True if there is a human face in the image
	"""
	image = face_recognition.load_image_file(str(image_path))
	faces = face_recognition.face_locations(image)
	return len(faces) &amp;gt; 0

    def is_dog(self, image_path: str) -&amp;gt; bool:        
	"""Checks if there is a dog in the image"""
	return self.dog_detector(image_path)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3c5ba58" class="outline-2"&gt;
&lt;h2 id="org3c5ba58"&gt;A Dog Breed Classifier&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3c5ba58"&gt;
&lt;p&gt;
Although the Inception model does do some classification of dogs, we want an even more fine-tuned model. First I'm going to try to build a naive CNN from scratch, then I'm going to use the Inception model and transfer learning to build a better classifier.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd50470e" class="outline-3"&gt;
&lt;h3 id="orgd50470e"&gt;A Naive Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd50470e"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9c7a3f5" class="outline-4"&gt;
&lt;h4 id="org9c7a3f5"&gt;The Data Transformers&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9c7a3f5"&gt;
&lt;p&gt;
For the naive model I'm going to use the image-size the  VGG model uses (&lt;a href="https://arxiv.org/abs/1409.1556"&gt;the original VGG paper&lt;/a&gt; describes the input as being 224 x 224). No particular reason except I've worked with that size before so I think it might make troubleshooting a little easier. The &lt;code&gt;Resize&lt;/code&gt; transform scales the image so that the smaller edge matches the size we give it. I found out the hard way that not all the input images are square so we need to then crop them back to the right size after scaling.
&lt;/p&gt;

&lt;p&gt;
Here's the training tranforms:
&lt;/p&gt;

&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomRotation"&gt;RandomRotation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomResizedCrop"&gt;RandomResizedCrop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip"&gt;RandomHorizontalFlip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
For testing and using:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize"&gt;Resize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.CenterCrop"&gt;CenterCrop&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
For both:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor"&gt;ToTensor&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize"&gt;Normalize&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;IMAGE_SIZE = 224
IMAGE_HALF_SIZE = IMAGE_SIZE//2

train_transform = transforms.Compose([
    transforms.RandomRotation(30),
    transforms.RandomResizedCrop(IMAGE_SIZE),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(MEANS,
			 DEVIATIONS)])

test_transform = transforms.Compose([transforms.Resize(255),
				      transforms.CenterCrop(IMAGE_SIZE),
				      transforms.ToTensor(),
				      transforms.Normalize(MEANS,
							   DEVIATIONS)])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge718946" class="outline-4"&gt;
&lt;h4 id="orge718946"&gt;Load the Data&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge718946"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training = datasets.ImageFolder(root=str(dog_paths.training.folder),
				transform=train_transform)
validation = datasets.ImageFolder(root=str(dog_paths.validation.folder),
				  transform=test_transform)
testing = datasets.ImageFolder(root=str(dog_paths.testing.folder),
			       transform=test_transform)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge99bfda" class="outline-4"&gt;
&lt;h4 id="orge99bfda"&gt;Build the Batch Loaders&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orge99bfda"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;BATCH_SIZE = 35
WORKERS = 0

train_batches = torch.utils.data.DataLoader(training, batch_size=BATCH_SIZE,
					    shuffle=True, num_workers=WORKERS)
validation_batches = torch.utils.data.DataLoader(
    validation, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)
test_batches = torch.utils.data.DataLoader(
    testing, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)

loaders_scratch = dict(train=train_batches,
		       validate=validation_batches,
		       test=test_batches)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org81c24fe" class="outline-4"&gt;
&lt;h4 id="org81c24fe"&gt;The Network&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org81c24fe"&gt;
&lt;p&gt;
This is only going to be a three-layer model. I started out trying to make a really big one but between the computation time and running out of memory I decided to limit the scope since the transfer model is the real one I want anyway, this is just for practice. The first block defines the parameters for the network.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LAYER_ONE_OUT = 16
LAYER_TWO_OUT = LAYER_ONE_OUT * 2
LAYER_THREE_OUT = LAYER_TWO_OUT * 2

KERNEL = 3
PADDING = 1
FULLY_CONNECTED_OUT = 500
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This next block does one pass through what the network is going to be doing so I can make sure the inputs and outputs are the correct size.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;conv_1 = nn.Conv2d(3, LAYER_ONE_OUT, KERNEL, padding=PADDING)
conv_2 = nn.Conv2d(LAYER_ONE_OUT, LAYER_TWO_OUT, KERNEL, padding=PADDING)
conv_3 = nn.Conv2d(LAYER_TWO_OUT, LAYER_THREE_OUT, KERNEL, padding=PADDING)

pool = nn.MaxPool2d(2, 2)
dropout = nn.Dropout(0.25)

fully_connected_1 = nn.Linear((IMAGE_HALF_SIZE//4)**2 * LAYER_THREE_OUT, FULLY_CONNECTED_OUT)
fully_connected_2 = nn.Linear(FULLY_CONNECTED_OUT, dog_paths.breed_count)

dataiter = iter(loaders_scratch['train'])
images, labels = dataiter.next()

x = pool(F.relu(conv_1(images)))
print(x.shape)
assert x.shape == torch.Size([BATCH_SIZE, 16, IMAGE_HALF_SIZE, IMAGE_HALF_SIZE])

x = pool(F.relu(conv_2(x)))
print(x.shape)
assert x.shape == torch.Size([BATCH_SIZE, LAYER_TWO_OUT, IMAGE_HALF_SIZE//2, IMAGE_HALF_SIZE//2])

x = pool(F.relu(conv_3(x)))
print(x.shape)
assert x.shape == torch.Size([BATCH_SIZE, LAYER_THREE_OUT, IMAGE_HALF_SIZE//4, IMAGE_HALF_SIZE//4])

x = x.view(-1, ((IMAGE_HALF_SIZE//4)**2) * LAYER_THREE_OUT)
print(x.shape)
x = fully_connected_1(x)
print(x.shape)
x = fully_connected_2(x)
print(x.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
torch.Size([10, 16, 112, 112])
torch.Size([10, 32, 56, 56])
torch.Size([10, 64, 28, 28])
torch.Size([10, 50176])
torch.Size([10, 500])
torch.Size([10, 133])

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3ec5a51" class="outline-4"&gt;
&lt;h4 id="org3ec5a51"&gt;The Class&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org3ec5a51"&gt;
&lt;p&gt;
This is the actual implementation based on the previous code.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class NaiveNet(nn.Module):
    """Naive Neural Network to classify dog breeds"""
    def __init__(self) -&amp;gt; None:
	super().__init__()
	self.conv1 = nn.Conv2d(3, LAYER_ONE_OUT,
			       KERNEL, padding=PADDING)
	self.conv2 = nn.Conv2d(LAYER_ONE_OUT, LAYER_TWO_OUT,
			       KERNEL, padding=PADDING)
	self.conv3 = nn.Conv2d(LAYER_TWO_OUT, LAYER_THREE_OUT,
			       KERNEL, padding=PADDING)
	# max pooling layer
	self.pool = nn.MaxPool2d(2, 2)
	# linear layer
	self.fc1 = nn.Linear((IMAGE_HALF_SIZE//4)**2 * LAYER_THREE_OUT, FULLY_CONNECTED_OUT)
	self.fc2 = nn.Linear(FULLY_CONNECTED_OUT, BREEDS)
	# dropout layer (p=0.25)
	self.dropout = nn.Dropout(0.25)
	return


    def forward(self, x: torch.Tensor) -&amp;gt; torch.Tensor:
	"""The forward pass method

	Args:
	 x: a n x 224 x 224 x 3 tensor

	Returns:
	 tensor of probabilities
	"""
	x = self.pool(F.relu(self.conv1(x)))
	x = self.pool(F.relu(self.conv2(x)))
	x = self.pool(F.relu(self.conv3(x)))

	x = x.view(-1, (IMAGE_HALF_SIZE//4)**2 * LAYER_THREE_OUT)
	x = self.dropout(x)

	x = self.dropout(F.relu(self.fc1(x)))
	x = self.fc2(x)        
	return x
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;naive_model = NaiveNet()
naive_model.to(device)
MODELS.append(naive_model)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4b73ed6" class="outline-4"&gt;
&lt;h4 id="org4b73ed6"&gt;The Loss Function and Optimizer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4b73ed6"&gt;
&lt;p&gt;
For loss measurement I'm going to use &lt;a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss"&gt;Cross Entropy Loss&lt;/a&gt; and &lt;a href="https://pytorch.org/docs/stable/optim.html#torch.optim.SGD"&gt;Stochastic Gradient Descent&lt;/a&gt; for backward propagation.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;criterion_scratch = nn.CrossEntropyLoss()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;optimizer_scratch = optimizer.SGD(naive_model.parameters(),
				  lr=0.001,
				  momentum=0.9)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org67471d1" class="outline-4"&gt;
&lt;h4 id="org67471d1"&gt;Train and Validate the Model&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org67471d1"&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org5070299"&gt;&lt;/a&gt;The Trainer&lt;br&gt;
&lt;div class="outline-text-5" id="text-org5070299"&gt;
&lt;p&gt;
Another class to try and get everything bundled into one place.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Trainer:
    """Trains, validates, and tests the model

    Args:
     training_batches: batch-loaders for training
     validation_batches: batch-loaders for validation
     testing_batches: batch-loaders for testing
     model: the network to train
     model_path: where to save the best model
     optimizer: the gradient descent object
     criterion: object to do backwards propagation
     device: where to put the data (cuda or cpu)
     epochs: number of times to train on the data set
     epoch_start: number to start the epoch count with
     load_model: whether to load the model from a file
     beep: whether timer should emit sounds
     is_inception: expecte two outputs in training
    """
    def __init__(self,
		 training_batches: torch.utils.data.DataLoader,
		 validation_batches: torch.utils.data.DataLoader,
		 testing_batches: torch.utils.data.DataLoader,
		 model: nn.Module,
		 model_path: Path,
		 optimizer: optimizer.SGD,
		 criterion: nn.CrossEntropyLoss,
		 device: torch.device=None,
		 epochs: int=10,
		 epoch_start: int=1,
		 is_inception: bool=False,
		 load_model: bool=False,
		 beep: bool=False) -&amp;gt; None:
	self.training_batches = training_batches
	self.validation_batches = validation_batches
	self.testing_batches = testing_batches
	self.model = model
	self.model_path = model_path
	self.optimizer = optimizer
	self.criterion = criterion
	self.epochs = epochs
	self.is_inception = is_inception
	self.beep = beep
	self._epoch_start = None
	self.epoch_start = epoch_start
	self.load_model = load_model
	self._timer = None
	self._epoch_end = None
	self._device = device
	return

    @property
    def epoch_start(self) -&amp;gt; int:
	"""The number to start the epoch count"""
	return self._epoch_start

    @epoch_start.setter
    def epoch_start(self, new_start: int) -&amp;gt; None:
	"""Sets the epoch start, removes the epoch end"""
	self._epoch_start = new_start
	self._epoch_end = None
	return

    @property
    def device(self) -&amp;gt; torch.device:
	"""The device to put the data on"""
	if self._device is None:
	    self._device = torch.device("cuda" if torch.cuda.is_available()
					else "cpu")
	return self._device

    @property
    def epoch_end(self) -&amp;gt; int:
	"""the end of the epochs (not inclusive)"""
	if self._epoch_end is None:
	    self._epoch_end = self.epoch_start + self.epochs
	return self._epoch_end

    @property
    def timer(self) -&amp;gt; Timer:
	"""something to emit times"""
	if self._timer is None:
	    self._timer = Timer(beep=self.beep)
	return self._timer

    def forward(self, batches: torch.utils.data.DataLoader,
		training: bool) -&amp;gt; tuple:
	"""runs the forward pass

	Args:
	 batches: data-loader
	 training: if true, runs the training, otherwise validates
	Returns:
	 tuple: loss, correct, total
	"""
	forward_loss = 0
	correct = 0

	if training:
	    self.model.train()
	else:
	    self.model.eval()
	for data, target in batches:
	    data, target = data.to(self.device), target.to(self.device)
	    if training:
		self.optimizer.zero_grad()
	    if training and self.is_inception:
		# throw away the auxiliary output
		output, _ = self.model(data)
	    output = self.model(data)
	    loss = self.criterion(output, target)
	    if training:
		loss.backward()
		self.optimizer.step()
	    forward_loss += loss.item() * data.size(0)

	    predictions = output.data.max(1, keepdim=True)[1]
	    correct += numpy.sum(
		numpy.squeeze(
		    predictions.eq(
			target.data.view_as(predictions))).cpu().numpy())
	forward_loss /= len(batches.dataset)
	return forward_loss, correct, len(batches.dataset)

    def train(self) -&amp;gt; tuple:
	"""Runs the training

	Returns:
	 training loss, correct, count
	"""
	return self.forward(batches=self.training_batches, training=True)

    def validate(self) -&amp;gt; tuple:
	"""Runs the validation

	Returns:
	 validation loss, correct, count
	"""
	return self.forward(batches=self.validation_batches, training=False)

    def test(self) -&amp;gt; None:
	"""Runs the testing

	"""
	self.timer.start()
	self.model.load_state_dict(torch.load(self.model_path))
	loss, correct, total = self.forward(batches=self.testing_batches,
					    training=False)
	print("Test Loss: {:.3f}".format(loss))
	print("Test Accuracy: {:.2f} ({}/{})".format(100 * correct/total,
						     correct, total))
	self.timer.end()
	return

    def train_and_validate(self):
	"""Trains and Validates the model
	"""
	validation_loss_min = numpy.Inf
	for epoch in range(self.epoch_start, self.epoch_end):
	    self.timer.start()
	    training_loss, training_correct, training_count = self.train()
	    (validation_loss, validation_correct,
	     validation_count) = self.validate()
	    self.timer.end()
	    print(("Epoch: {}\t"
		   "Training - Loss: {:.2f}\t"
		   "Accuracy: {:.2f}\t"
		   "Validation - Loss: {:.2f}\t"
		   "Accuracy: {:.2f}").format(
		       epoch,
		       training_loss,
		       training_correct/training_count,
		       validation_loss,
		       validation_correct/validation_count,
		))

	    if validation_loss &amp;lt; validation_loss_min:
		print(
		    ("Validation loss decreased ({:.6f} --&amp;gt; {:.6f}). "
		     "Saving model ...").format(
			 validation_loss_min,
			 validation_loss))
		torch.save(self.model.state_dict(), self.model_path)
		validation_loss_min = validation_loss
	return

    def __call__(self) -&amp;gt; None:
	"""Trains, Validates, and Tests the model"""
	if self.load_model and self.model_path.is_file():
	    self.model.load_state_dict(torch.load(self.model_path))
	print("Starting Training")
	self.timer.start()
	self.train_and_validate()
	self.timer.end()
	print("\nStarting Testing")
	self.test()
	return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4d0472e" class="outline-4"&gt;
&lt;h4 id="org4d0472e"&gt;Broken Images&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4d0472e"&gt;
&lt;p&gt;
I noted at the beginning of the notebook that at least one of the images is raising an OSError:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ne"&gt;OSError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="n"&gt;truncated&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt; &lt;span class="nb"&gt;bytes&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;processed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
This is the part of the notebook where I originally found out what was going on (because it kept crashing during training).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timer.start()
broken = None
for image in dog_files:
    try:
	opened = Image.open(image)
	opened.convert("RGB")
    except OSError as error:
	print("{}: {}".format(error, image))
	broken = image
timer.end()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
image file is truncated (150 bytes not processed): /home/hades/datasets/dog-breed-classification/dogImages/train/098.Leonberger/Leonberger_06571.jpg
Ended: 2018-12-30 15:10:19.141003
Elapsed: 0:02:29.804925

&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots()
name = " ".join(broken.name.split("_")[:-1]).title()
figure.suptitle("Truncated Image ({})".format(name), weight="bold")
image = Image.open(broken)
axe_image = axe.imshow(image)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/truncated_dog.png" alt="truncated_dog.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
I got the solution from &lt;a href="https://stackoverflow.com/questions/12984426/python-pil-ioerror-image-file-truncated-with-big-images"&gt;this Stack Overflow post&lt;/a&gt;, I don't know why but the image seems to be missing some pixels or something. Oh, well. The key to making it work:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ImageFile&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LOAD_TRUNCATED_IMAGES&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org75091f9" class="outline-4"&gt;
&lt;h4 id="org75091f9"&gt;Train the Model&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org75091f9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;NAIVE_PATH = MODEL_PATH.folder.joinpath("model_scratch.pt")
scratch_log = Tee(log_name="scratch_train.log")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0910818" class="outline-4"&gt;
&lt;h4 id="org0910818"&gt;Test the Model&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0910818"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def test(test_batches: torch.utils.data.DataLoader,
	 model: nn.Module,
	 criterion: nn.CrossEntropyLoss) -&amp;gt; None:
    """Test the model

    Args:
     test_batches: batch loader of test images
     model: the network to test
     criterion: calculator for the loss
    """
    test_loss = 0.
    correct = 0.
    total = 0.

    model.eval()
    for data, target in test_batches:
	data, target = data.to(device), target.to(device)
	output = model(data)
	loss = criterion(output, target)
	test_loss += loss.item() * data.size(0)
	# convert output probabilities to predicted class
	predictions = output.data.max(1, keepdim=True)[1]
	# compare predictions to true label
	correct += numpy.sum(
	    numpy.squeeze(
		predictions.eq(
		    target.data.view_as(predictions))).cpu().numpy())
	total += data.size(0)
    test_loss /= len(test_batches.dataset)
    print('Test Loss: {:.6f}\n'.format(test_loss))
    print('\nTest Accuracy: %2d%% (%2d/%2d)' % (
	100. * correct / total, correct, total))
    return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdb904dd" class="outline-3"&gt;
&lt;h3 id="orgdb904dd"&gt;Train and Test&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdb904dd"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def train_and_test(train_batches: torch.utils.data.DataLoader,
		   validate_batches: torch.utils.data.DataLoader,
		   test_batches: torch.utils.data.DataLoader,
		   model: nn.Module,
		   model_path: Path,
		   optimizer: optimizer.SGD,
		   criterion: nn.CrossEntropyLoss,
		   epochs: int=10,
		   epoch_start: int=1,
		   load_model: bool=False) -&amp;gt; None:
    """Trains and Tests the Model

    Args:
     train_batches: batch-loaders for training
     validate_batches: batch-loaders for validation
     test_batches: batch-loaders for testing
     model: the network to train
     model_path: where to save the best model
     optimizer: the gradient descent object
     criterion: object to do backwards propagation
     epochs: number of times to train on the data set
     epoch_start: number to start the epoch count with
     load_model: whether to load the model from a file
    """
    if load_model and model_path.is_file():
	model.load_state_dict(torch.load(model_path))
    print("Starting Training")
    timer.start()
    model_scratch = train(epochs=epochs,
			  epoch_start=epoch_start,
			  train_batches=train_batches,
			  validation_batches=validate_batches,
			  model=model,
			  optimizer=optimizer, 
			  criterion=criterion,
			  save_path=model_path)
    timer.end()
    # load the best model
    model.load_state_dict(torch.load(model_path))
    print("Starting Testing")
    timer.start()
    test(test_batches, model, criterion)
    timer.end()
    return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgaff5ee7" class="outline-3"&gt;
&lt;h3 id="orgaff5ee7"&gt;Train the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgaff5ee7"&gt;
&lt;p&gt;
When I originally wrote this I was using this functional-style of training and testing, which was hard to use, but since it's so expensive to train the model (in terms of time, and to some degree server cost) I'm not going to re-do it so the code here looks a little different from the one I used for the transfer model.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_path = DataPathTwo(
    folder_key="MODELS",
    filename="model_scratch.pt")
assert model_path.folder.is_dir()
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_and_test(epochs=10,
	       train_batches=loaders_scratch["train"],
	       validate_batches=loaders_scratch["validate"],
	       test_batches=loaders_scratch["test"],
	       model=model_scratch,
	       optimizer=optimizer_scratch, 
	       criterion=criterion_scratch,
	       epoch_start=0,
	       model_path=model_path.from_folder,
	       load_model=False)
next_start = 11
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Ended: 2019-01-01 16:35:14.192989
Elapsed: 0:03:23.778459
Epoch: 0 	Training Loss: 3.946975 	Validation Loss: 3.758706
Validation loss decreased (inf --&amp;gt; 3.758706). Saving model ...
Ended: 2019-01-01 16:38:39.497147
Elapsed: 0:03:24.517456
Epoch: 1 	Training Loss: 3.880984 	Validation Loss: 3.695643
Validation loss decreased (3.758706 --&amp;gt; 3.695643). Saving model ...
Ended: 2019-01-01 16:42:04.190248
Elapsed: 0:03:23.903292
Epoch: 2 	Training Loss: 3.870710 	Validation Loss: 3.718353
Ended: 2019-01-01 16:45:28.479552
Elapsed: 0:03:23.718292
Epoch: 3 	Training Loss: 3.836664 	Validation Loss: 3.740289
Ended: 2019-01-01 16:48:53.605419
Elapsed: 0:03:24.555708
Epoch: 4 	Training Loss: 3.819701 	Validation Loss: 3.659244
Validation loss decreased (3.695643 --&amp;gt; 3.659244). Saving model ...
Ended: 2019-01-01 16:52:33.198097
Elapsed: 0:03:38.805586
Epoch: 5 	Training Loss: 3.778872 	Validation Loss: 3.756706
Ended: 2019-01-01 16:56:16.822584
Elapsed: 0:03:43.055469
Epoch: 6 	Training Loss: 3.752981 	Validation Loss: 3.679196
Ended: 2019-01-01 16:59:42.861936
Elapsed: 0:03:25.469331
Epoch: 7 	Training Loss: 3.730930 	Validation Loss: 3.608311
Validation loss decreased (3.659244 --&amp;gt; 3.608311). Saving model ...
Ended: 2019-01-01 17:03:10.958002
Elapsed: 0:03:27.305644
Epoch: 8 	Training Loss: 3.705110 	Validation Loss: 3.636201
Ended: 2019-01-01 17:06:38.939991
Elapsed: 0:03:27.412824
Epoch: 9 	Training Loss: 3.665519 	Validation Loss: 3.595410
Validation loss decreased (3.608311 --&amp;gt; 3.595410). Saving model ...
Ended: 2019-01-01 17:06:39.733176
Elapsed: 0:03:28.206009
Starting Testing
Test Loss: 3.642843


Test Accuracy: 14% (125/836)
Ended: 2019-01-01 17:07:11.142926
Elapsed: 0:00:30.815650
&lt;/pre&gt;

&lt;p&gt;
Hmm, seems suspiciously good all of a sudden. It looks like my GPU is faster than paper space's, too..
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_and_test(epochs=10,
	       train_batches=loaders_scratch["train"],
	       validate_batches=loaders_scratch["validate"],
	       test_batches=loaders_scratch["test"],
	       model=model_scratch,
	       optimizer=optimizer_scratch, 
	       criterion=criterion_scratch,
	       epoch_start=next_start,
	       model_path=model_path.from_folder,
	       load_model=True)
next_start = 21
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Ended: 2019-01-01 17:29:46.425198
Elapsed: 0:03:40.954699
Epoch: 0 	Training Loss: 3.662736 	Validation Loss: 3.631118
Validation loss decreased (inf --&amp;gt; 3.631118). Saving model ...
Ended: 2019-01-01 17:33:12.797754
Elapsed: 0:03:25.528229
Epoch: 1 	Training Loss: 3.612436 	Validation Loss: 3.610919
Validation loss decreased (3.631118 --&amp;gt; 3.610919). Saving model ...
Ended: 2019-01-01 17:36:49.466848
Elapsed: 0:03:35.831733
Epoch: 2 	Training Loss: 3.612902 	Validation Loss: 3.590953
Validation loss decreased (3.610919 --&amp;gt; 3.590953). Saving model ...
Ended: 2019-01-01 17:40:17.511898
Elapsed: 0:03:27.192943
Epoch: 3 	Training Loss: 3.564542 	Validation Loss: 3.566365
Validation loss decreased (3.590953 --&amp;gt; 3.566365). Saving model ...
Ended: 2019-01-01 17:43:45.639219
Elapsed: 0:03:27.309572
Epoch: 4 	Training Loss: 3.551703 	Validation Loss: 3.608934
Ended: 2019-01-01 17:47:32.854824
Elapsed: 0:03:46.646159
Epoch: 5 	Training Loss: 3.542706 	Validation Loss: 3.533696
Validation loss decreased (3.566365 --&amp;gt; 3.533696). Saving model ...
Ended: 2019-01-01 17:51:02.330525
Elapsed: 0:03:28.506819
Epoch: 6 	Training Loss: 3.532894 	Validation Loss: 3.531388
Validation loss decreased (3.533696 --&amp;gt; 3.531388). Saving model ...
Ended: 2019-01-01 17:54:25.844725
Elapsed: 0:03:22.697779
Epoch: 7 	Training Loss: 3.482241 	Validation Loss: 3.564429
Ended: 2019-01-01 17:57:48.563069
Elapsed: 0:03:22.148237
Epoch: 8 	Training Loss: 3.485189 	Validation Loss: 3.624133
Ended: 2019-01-01 18:01:11.755236
Elapsed: 0:03:22.621310
Epoch: 9 	Training Loss: 3.461059 	Validation Loss: 3.594314
Ended: 2019-01-01 18:01:12.326268
Elapsed: 0:03:23.192342
Starting Testing
Test Loss: 3.537503


Test Accuracy: 16% (138/836)
Ended: 2019-01-01 18:01:42.764907
Elapsed: 0:00:29.747148
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_and_test(epochs=10,
	       train_batches=loaders_scratch["train"],
	       validate_batches=loaders_scratch["validate"],
	       test_batches=loaders_scratch["test"],
	       model=model_scratch,
	       optimizer=optimizer_scratch, 
	       criterion=criterion_scratch,
	       epoch_start=next_start,
	       model_path=model_path.from_folder,
	       load_model=True)
next_start = 31
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Ended: 2019-01-01 18:45:17.404562
Elapsed: 0:03:23.081286
Epoch: 21 	Training Loss: 3.510303 	Validation Loss: 3.555182
Validation loss decreased (inf --&amp;gt; 3.555182). Saving model ...
Ended: 2019-01-01 18:48:41.215171
Elapsed: 0:03:22.949288
Epoch: 22 	Training Loss: 3.485824 	Validation Loss: 3.570289
Ended: 2019-01-01 18:52:04.635395
Elapsed: 0:03:22.849569
Epoch: 23 	Training Loss: 3.438656 	Validation Loss: 3.543221
Validation loss decreased (3.555182 --&amp;gt; 3.543221). Saving model ...
Ended: 2019-01-01 18:55:28.409018
Elapsed: 0:03:22.980693
Epoch: 24 	Training Loss: 3.387092 	Validation Loss: 3.649569
Ended: 2019-01-01 18:58:51.555922
Elapsed: 0:03:22.576946
Epoch: 25 	Training Loss: 3.381217 	Validation Loss: 3.529994
Validation loss decreased (3.543221 --&amp;gt; 3.529994). Saving model ...
Ended: 2019-01-01 19:02:15.743200
Elapsed: 0:03:23.359857
Epoch: 26 	Training Loss: 3.379801 	Validation Loss: 3.514583
Validation loss decreased (3.529994 --&amp;gt; 3.514583). Saving model ...
Ended: 2019-01-01 19:05:40.243125
Elapsed: 0:03:23.700481
Epoch: 27 	Training Loss: 3.334058 	Validation Loss: 3.469988
Validation loss decreased (3.514583 --&amp;gt; 3.469988). Saving model ...
Ended: 2019-01-01 19:09:04.218270
Elapsed: 0:03:23.150903
Epoch: 28 	Training Loss: 3.347201 	Validation Loss: 3.456167
Validation loss decreased (3.469988 --&amp;gt; 3.456167). Saving model ...
Ended: 2019-01-01 19:12:27.711756
Elapsed: 0:03:22.677622
Epoch: 29 	Training Loss: 3.320286 	Validation Loss: 3.444669
Validation loss decreased (3.456167 --&amp;gt; 3.444669). Saving model ...
Ended: 2019-01-01 19:15:51.375887
Elapsed: 0:03:22.875358
Epoch: 30 	Training Loss: 3.314001 	Validation Loss: 3.460704
Ended: 2019-01-01 19:15:51.946497
Elapsed: 0:03:23.445968
Starting Testing
Test Loss: 3.492875


Test Accuracy: 17% (146/836)
Ended: 2019-01-01 19:16:10.729405
Elapsed: 0:00:18.109680
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_and_test(epochs=10,
	       train_batches=loaders_scratch["train"],
	       validate_batches=loaders_scratch["validate"],
	       test_batches=loaders_scratch["test"],
	       model=model_scratch,
	       optimizer=optimizer_scratch, 
	       criterion=criterion_scratch,
	       epoch_start=next_start,
	       model_path=model_path.from_folder,
	       load_model=True)
next_start = 41
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Ended: 2019-01-01 20:15:25.906348
Elapsed: 0:05:12.167322
Epoch: 31 	Training Loss: 3.311046 	Validation Loss: 3.446478
Validation loss decreased (inf --&amp;gt; 3.446478). Saving model ...
Ended: 2019-01-01 20:19:13.168084
Elapsed: 0:03:46.461085
Epoch: 32 	Training Loss: 3.270769 	Validation Loss: 3.550049
Ended: 2019-01-01 20:22:38.973465
Elapsed: 0:03:25.195274
Epoch: 33 	Training Loss: 3.221883 	Validation Loss: 3.489280
Ended: 2019-01-01 20:26:02.049299
Elapsed: 0:03:22.483931
Epoch: 34 	Training Loss: 3.271723 	Validation Loss: 3.507546
Ended: 2019-01-01 20:29:24.932614
Elapsed: 0:03:22.292605
Epoch: 35 	Training Loss: 3.197156 	Validation Loss: 3.475409
Ended: 2019-01-01 20:32:47.569786
Elapsed: 0:03:22.046763
Epoch: 36 	Training Loss: 3.210177 	Validation Loss: 3.477707
Ended: 2019-01-01 20:36:09.752175
Elapsed: 0:03:21.592504
Epoch: 37 	Training Loss: 3.199346 	Validation Loss: 3.577469
Ended: 2019-01-01 20:39:32.831340
Elapsed: 0:03:22.489048
Epoch: 38 	Training Loss: 3.158563 	Validation Loss: 3.442629
Validation loss decreased (3.446478 --&amp;gt; 3.442629). Saving model ...
Ended: 2019-01-01 20:42:56.293868
Elapsed: 0:03:22.664005
Epoch: 39 	Training Loss: 3.152231 	Validation Loss: 3.470943
Ended: 2019-01-01 20:46:18.983529
Elapsed: 0:03:22.098438
Epoch: 40 	Training Loss: 3.124298 	Validation Loss: 3.429367
Validation loss decreased (3.442629 --&amp;gt; 3.429367). Saving model ...
Ended: 2019-01-01 20:46:19.801009
Elapsed: 0:03:22.915918
Starting Testing
Test Loss: 3.348011


Test Accuracy: 21% (179/836)
Ended: 2019-01-01 20:46:42.494502
Elapsed: 0:00:22.094465
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_and_test(epochs=10,
	       train_batches=loaders_scratch["train"],
	       validate_batches=loaders_scratch["validate"],
	       test_batches=loaders_scratch["test"],
	       model=model_scratch,
	       optimizer=optimizer_scratch, 
	       criterion=criterion_scratch,
	       epoch_start=next_start,
	       model_path=model_path.from_folder,
	       load_model=True)
next_start = 51
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Ended: 2019-01-01 22:01:17.285699
Elapsed: 0:03:24.381614
Epoch: 41 	Training Loss: 3.095166 	Validation Loss: 3.418227
Validation loss decreased (inf --&amp;gt; 3.418227). Saving model ...
Ended: 2019-01-01 22:04:43.173252
Elapsed: 0:03:25.033381
Epoch: 42 	Training Loss: 3.089258 	Validation Loss: 3.419117
Ended: 2019-01-01 22:08:07.709900
Elapsed: 0:03:23.945667
Epoch: 43 	Training Loss: 3.071535 	Validation Loss: 3.433646
Ended: 2019-01-01 22:11:33.153513
Elapsed: 0:03:24.853880
Epoch: 44 	Training Loss: 3.058665 	Validation Loss: 3.454817
Ended: 2019-01-01 22:14:59.899762
Elapsed: 0:03:26.156530
Epoch: 45 	Training Loss: 3.072674 	Validation Loss: 3.494963
Ended: 2019-01-01 22:18:26.207188
Elapsed: 0:03:25.746042
Epoch: 46 	Training Loss: 3.043788 	Validation Loss: 3.430311
Ended: 2019-01-01 22:21:51.975083
Elapsed: 0:03:25.177310
Epoch: 47 	Training Loss: 3.015571 	Validation Loss: 3.382248
Validation loss decreased (3.418227 --&amp;gt; 3.382248). Saving model ...
Ended: 2019-01-01 22:25:18.237087
Elapsed: 0:03:25.403639
Epoch: 48 	Training Loss: 2.972451 	Validation Loss: 3.449296
Ended: 2019-01-01 22:28:44.315967
Elapsed: 0:03:25.498810
Epoch: 49 	Training Loss: 2.989183 	Validation Loss: 3.428347
Ended: 2019-01-01 22:32:10.738134
Elapsed: 0:03:25.832058
Epoch: 50 	Training Loss: 2.966034 	Validation Loss: 3.501775
Ended: 2019-01-01 22:32:11.326703
Elapsed: 0:03:26.420627
Starting Testing
Test Loss: 3.485910


Test Accuracy: 18% (156/836)
Ended: 2019-01-01 22:32:41.884173
Elapsed: 0:00:29.644028
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_and_test(epochs=10,
	       train_batches=loaders_scratch["train"],
	       validate_batches=loaders_scratch["validate"],
	       test_batches=loaders_scratch["test"],
	       model=model_scratch,
	       optimizer=optimizer_scratch, 
	       criterion=criterion_scratch,
	       epoch_start=next_start,
	       model_path=model_path.from_folder,
	       load_model=True)
next_start = 61
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Ended: 2019-01-01 22:39:53.821378
Elapsed: 0:04:15.535643
Epoch: 51 	Training Loss: 3.024161 	Validation Loss: 3.409968
Validation loss decreased (inf --&amp;gt; 3.409968). Saving model ...
Ended: 2019-01-01 22:43:47.462698
Elapsed: 0:03:52.776151
Epoch: 52 	Training Loss: 2.979377 	Validation Loss: 3.512004
Ended: 2019-01-01 22:47:35.580770
Elapsed: 0:03:47.528679
Epoch: 53 	Training Loss: 2.983352 	Validation Loss: 3.499196
Ended: 2019-01-01 22:50:58.662565
Elapsed: 0:03:22.501398
Epoch: 54 	Training Loss: 2.944738 	Validation Loss: 3.458440
Ended: 2019-01-01 22:54:21.531858
Elapsed: 0:03:22.279749
Epoch: 55 	Training Loss: 2.921185 	Validation Loss: 3.581930
Ended: 2019-01-01 22:57:44.017339
Elapsed: 0:03:21.925483
Epoch: 56 	Training Loss: 2.928508 	Validation Loss: 3.449956
Ended: 2019-01-01 23:01:06.668710
Elapsed: 0:03:22.061753
Epoch: 57 	Training Loss: 2.887215 	Validation Loss: 3.559204
Ended: 2019-01-01 23:04:29.439919
Elapsed: 0:03:22.181396
Epoch: 58 	Training Loss: 2.909253 	Validation Loss: 3.458249
Ended: 2019-01-01 23:07:51.804139
Elapsed: 0:03:21.803807
Epoch: 59 	Training Loss: 2.864969 	Validation Loss: 3.599446
Ended: 2019-01-01 23:11:14.184534
Elapsed: 0:03:21.789954
Epoch: 60 	Training Loss: 2.820693 	Validation Loss: 3.432991
Ended: 2019-01-01 23:11:14.775507
Elapsed: 0:03:22.380927
Starting Testing
Test Loss: 3.370016


Test Accuracy: 21% (176/836)
Ended: 2019-01-01 23:11:44.949942
Elapsed: 0:00:29.259563
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;next_start = 61
train_and_test(epochs=10,
	       train_batches=loaders_scratch["train"],
	       validate_batches=loaders_scratch["validate"],
	       test_batches=loaders_scratch["test"],
	       model=model_scratch,
	       optimizer=optimizer_scratch, 
	       criterion=criterion_scratch,
	       epoch_start=next_start,
	       model_path=model_path.from_folder,
	       load_model=True)
next_start = 71
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Ended: 2019-01-01 23:31:00.034455
Elapsed: 0:03:21.658811
Epoch: 61 	Training Loss: 2.968425 	Validation Loss: 3.469985
Validation loss decreased (inf --&amp;gt; 3.469985). Saving model ...
Ended: 2019-01-01 23:34:24.012685
Elapsed: 0:03:22.630721
Epoch: 62 	Training Loss: 2.980103 	Validation Loss: 3.449017
Validation loss decreased (3.469985 --&amp;gt; 3.449017). Saving model ...
Ended: 2019-01-01 23:37:47.137370
Elapsed: 0:03:22.315870
Epoch: 63 	Training Loss: 2.945722 	Validation Loss: 3.497296
Ended: 2019-01-01 23:41:09.932696
Elapsed: 0:03:22.226620
Epoch: 64 	Training Loss: 2.940117 	Validation Loss: 3.398626
Validation loss decreased (3.449017 --&amp;gt; 3.398626). Saving model ...
Ended: 2019-01-01 23:44:33.204607
Elapsed: 0:03:22.484337
Epoch: 65 	Training Loss: 2.913762 	Validation Loss: 3.465828
Ended: 2019-01-01 23:47:55.682608
Elapsed: 0:03:21.909285
Epoch: 66 	Training Loss: 2.877373 	Validation Loss: 3.525525
Ended: 2019-01-01 23:51:18.110150
Elapsed: 0:03:21.859021
Epoch: 67 	Training Loss: 2.889807 	Validation Loss: 3.499459
Ended: 2019-01-01 23:54:40.142934
Elapsed: 0:03:21.464199
Epoch: 68 	Training Loss: 2.882748 	Validation Loss: 3.364801
Validation loss decreased (3.398626 --&amp;gt; 3.364801). Saving model ...
Ended: 2019-01-01 23:58:02.359285
Elapsed: 0:03:21.435096
Epoch: 69 	Training Loss: 2.886337 	Validation Loss: 3.488435
Ended: 2019-01-02 00:01:26.616419
Elapsed: 0:03:23.688341
Epoch: 70 	Training Loss: 2.867836 	Validation Loss: 3.417904
Ended: 2019-01-02 00:01:27.309412
Elapsed: 0:03:24.381334
Starting Testing
Test Loss: 3.359312


Test Accuracy: 22% (191/836)
Ended: 2019-01-02 00:02:29.963462
Elapsed: 0:01:01.964477
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;train_and_test(epochs=10,
	       train_batches=loaders_scratch["train"],
	       validate_batches=loaders_scratch["validate"],
	       test_batches=loaders_scratch["test"],
	       model=model_scratch,
	       optimizer=optimizer_scratch, 
	       criterion=criterion_scratch,
	       epoch_start=next_start,
	       model_path=model_path.from_folder,
	       load_model=True)
next_start = 81
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Starting Training
Ended: 2019-01-02 00:13:59.560043
Elapsed: 0:09:26.402859
Epoch: 71 	Training Loss: 2.847764 	Validation Loss: 3.462033
Validation loss decreased (inf --&amp;gt; 3.462033). Saving model ...
Ended: 2019-01-02 00:21:40.896206
Elapsed: 0:07:40.511212
Epoch: 72 	Training Loss: 2.852644 	Validation Loss: 3.469687
Ended: 2019-01-02 00:29:05.309753
Elapsed: 0:07:23.845532
Epoch: 73 	Training Loss: 2.840424 	Validation Loss: 3.545896
Ended: 2019-01-02 00:33:46.928392
Elapsed: 0:04:41.026761
Epoch: 74 	Training Loss: 2.813888 	Validation Loss: 3.552435
Ended: 2019-01-02 00:37:18.057707
Elapsed: 0:03:30.560704
Epoch: 75 	Training Loss: 2.807452 	Validation Loss: 3.491534
Ended: 2019-01-02 00:40:41.064242
Elapsed: 0:03:22.438088
Epoch: 76 	Training Loss: 2.802119 	Validation Loss: 3.429099
Validation loss decreased (3.462033 --&amp;gt; 3.429099). Saving model ...
Ended: 2019-01-02 00:44:04.191818
Elapsed: 0:03:22.138587
Epoch: 77 	Training Loss: 2.809226 	Validation Loss: 3.482573
Ended: 2019-01-02 00:47:26.187167
Elapsed: 0:03:21.427162
Epoch: 78 	Training Loss: 2.767340 	Validation Loss: 3.473212
Ended: 2019-01-02 00:50:48.717819
Elapsed: 0:03:21.962244
Epoch: 79 	Training Loss: 2.750881 	Validation Loss: 3.435359
Ended: 2019-01-02 00:54:11.744891
Elapsed: 0:03:22.458406
Epoch: 80 	Training Loss: 2.739076 	Validation Loss: 3.466524
Ended: 2019-01-02 00:54:12.313860
Elapsed: 0:03:23.027375
Starting Testing
Test Loss: 3.505263


Test Accuracy: 21% (183/836)
Ended: 2019-01-02 00:54:42.938753
Elapsed: 0:00:29.924658
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfc4f2ec" class="outline-3"&gt;
&lt;h3 id="orgfc4f2ec"&gt;Debug the CUDA Error&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgfc4f2ec"&gt;
&lt;p&gt;
The previous blocks of code raised an exception when I first ran it.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ne"&gt;RuntimeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;cuda&lt;/span&gt; &lt;span class="n"&gt;runtime&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;59&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;side&lt;/span&gt; &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;triggered&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;pytorch&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;aten&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;src&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;THC&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;generic&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;THCTensorMath&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cu&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
And points to this line as the point where it crashes.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Re-running it gives a similar but different error.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ne"&gt;RuntimeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;CUDA&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;side&lt;/span&gt; &lt;span class="k"&gt;assert&lt;/span&gt; &lt;span class="n"&gt;triggered&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Happening here:
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
According to &lt;a href="https://github.com/pytorch/pytorch/issues/1010"&gt;this bug report&lt;/a&gt; on GitHub, there's two things happening. One is that once the exception happens the CUDA session is dead so trying to move the data to CUDA raises an error just because we are trying to use it (and you can't until you restart the python session). In that same thread they note that the original exception indicates something wrong with the classes being output by the network. One error they list is if there's a negative label, another if the label is out of range for the number of categories, but In my case it might be that I was only outputting 10 classes (I copied the CIFAR model), not the 133 you need for the dog-breeds.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga80c34d" class="outline-4"&gt;
&lt;h4 id="orga80c34d"&gt;Load The Best Model&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga80c34d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_scratch.load_state_dict(torch.load('model_scratch.pt'))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4b5dcc8" class="outline-4"&gt;
&lt;h4 id="org4b5dcc8"&gt;Test It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org4b5dcc8"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test(loaders_scratch["test"], model_scratch, criterion_scratch)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Loss: 3.492875


Test Accuracy: 17% (146/836)

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6f36f06" class="outline-3"&gt;
&lt;h3 id="org6f36f06"&gt;Transfer Learning Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6f36f06"&gt;
&lt;p&gt;
Now I'm going to use transfer learning to make a model to classify dog images by breed.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1bf2e97" class="outline-4"&gt;
&lt;h4 id="org1bf2e97"&gt;The Data Transformer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1bf2e97"&gt;
&lt;p&gt;
As I noted earlier, the &lt;code&gt;Inception V3&lt;/code&gt; model expects a different image size so we can't re-use the previous data-transforms.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Transformer:
    """builds the data-sets

    Args:
     means: list of means for each channel
     deviations: list of standard deviations for each channel
     image_size: size to crop the image to
    """
    def __init__(self,
		 means: list=[0.485, 0.456, 0.406],
		 deviations: list=[0.229, 0.224, 0.225],
		 image_size: int=299) -&amp;gt; None:
	self.means = means
	self.deviations = deviations
	self.image_size = image_size
	self._training = None
	self._testing = None
	return

    @property
    def training(self) -&amp;gt; transforms.Compose:
	"""The image transformers for the training"""
	if self._training is None:
	    self._training = transforms.Compose([
		transforms.RandomRotation(30),
		transforms.RandomResizedCrop(self.image_size),
		transforms.RandomHorizontalFlip(),
		transforms.ToTensor(),
		transforms.Normalize(self.means,
				     self.deviations)])
	return self._training

    @property
    def testing(self) -&amp;gt; transforms.Compose:
	"""Image transforms for the testing"""
	if self._testing is None:
	    self._testing = transforms.Compose(
		[transforms.Resize(350),
		 transforms.CenterCrop(self.image_size),
		 transforms.ToTensor(),
		 transforms.Normalize(self.means,
				      self.deviations)])
	return self._testing
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf168ba5" class="outline-4"&gt;
&lt;h4 id="orgf168ba5"&gt;The Data Set Loader&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgf168ba5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DataSets:
    """Builds the data-sets

    Args:
     paths: object with the paths to the data-sets
    """
    def __init__(self, paths: DogPaths=None, transformer: Transformer=None) -&amp;gt; None:
	self._paths = paths
	self._transformer = transformer
	self._training = None
	self._validation = None
	self._testing = None
	return

    @property
    def paths(self) -&amp;gt; DogPaths:
	"""Object with the paths to the image files"""
	if self._paths is None:
	    self._paths = DogPaths()
	return self._paths

    @property
    def transformer(self) -&amp;gt; Transformer:
	"""Object with the image transforms"""
	if self._transformer is None:
	    self._transformer = Transformer()
	return self._transformer

    @property
    def training(self) -&amp;gt; datasets.ImageFolder:
	"""The training data set"""
	if self._training is None:
	    self._training = datasets.ImageFolder(
		root=self.paths.training.folder,
		transform=self.transformer.training)
	return self._training

    @property
    def validation(self) -&amp;gt; datasets.ImageFolder:
	"""The validation dataset"""
	if self._validation is None:
	    self._validation = datasets.ImageFolder(
		root=self.paths.validation.folder,
		transform=self.transformer.testing)
	return self._validation

    @property
    def testing(self) -&amp;gt; datasets.ImageFolder:
	"""The test set"""
	if self._testing is None:
	    self._testing = datasets.ImageFolder(
		root=self.paths.testing.folder,
		transform=self.transformer.testing)
	return self._testing
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org8dda7dc" class="outline-4"&gt;
&lt;h4 id="org8dda7dc"&gt;The Batch Loader&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org8dda7dc"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Batches:
    """The data batch loaders

    Args:
     datasets: a data-set builder
     batch_size: the size of each batch loaded
     workers: the number of processes to use
    """
    def __init__(self, datasets: DataSets,
		 batch_size: int=20,
		 workers: int=0) -&amp;gt; None:
	self.datasets = datasets
	self.batch_size = batch_size
	self.workers = workers
	self._training = None
	self._validation = None
	self._testing = None
	return

    @property
    def training(self) -&amp;gt; torch.utils.data.DataLoader:
	"""The training batches"""
	if self._training is None:
	    self._training = torch.utils.data.DataLoader(
		self.datasets.training,
		batch_size=self.batch_size,
		shuffle=True, num_workers=self.workers)
	return self._training

    @property
    def validation(self) -&amp;gt; torch.utils.data.DataLoader:
	"""The validation batches"""
	if self._validation is None:
	    self._validation = torch.utils.data.DataLoader(
		self.datasets.validation,
		batch_size=self.batch_size,
		shuffle=True, num_workers=self.workers)
	return self._validation

    @property
    def testing(self) -&amp;gt; torch.utils.data.DataLoader:
	"""The testing batches"""
	if self._testing is None:
	    self._testing = torch.utils.data.DataLoader(
		self.datasets.testing,
		batch_size=self.batch_size,
		shuffle=True, num_workers=self.workers)
	return self._testing
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6fca933" class="outline-4"&gt;
&lt;h4 id="org6fca933"&gt;The Inception Dog Classifier&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6fca933"&gt;
&lt;p&gt;
Although the constructor for the pytorch Inception model takes an &lt;code&gt;aux_logits&lt;/code&gt; parameter, if you set it to false then it will raise an error saying there are unexpected keys in the state dict. But if you don't set it False it will return a tuple from the &lt;code&gt;forward&lt;/code&gt; method so either set it to False after the constructor or catch a tuple as the output &lt;code&gt;(x, aux)&lt;/code&gt; and throw away the second part (or figure out how to combine them). I decided to leave it set because it is supposed to help with training and changed the training function to handle it. But I don't really show that in this notebook. I'll have to re-write things later.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Inception:
    """Sets up the model, criterion, and optimizer for the transfer learning

    Args:
     classes: number of outputs for the final layer
     device: processor to use
     model_path: path to a saved model
     learning_rate: learning rate for the optimizer
     momentum: momentum for the optimizer
    """
    def __init__(self, classes: int,
		 device: torch.device=None,
		 model_path: str=None,
		 learning_rate: float=0.001, momentum: float=0.9) -&amp;gt; None:
	self.classes = classes
	self.model_path = model_path
	self.learning_rate = learning_rate
	self.momentum = momentum
	self._device = device
	self._model = None
	self._classifier_inputs = None
	self._criterion = None
	self._optimizer = None
	return

    @property
    def device(self) -&amp;gt; torch.device:
	"""Processor to use (cpu or cuda)"""
	if self._device is None:
	    self._device = torch.device(
		"cuda" if torch.cuda.is_available() else "cpu")
	return self._device

    @property
    def model(self) -&amp;gt; models.inception_v3:
	"""The inception model"""
	if self._model is None:
	    self._model = models.inception_v3(pretrained=True)
	    for parameter in self._model.parameters():
		parameter.requires_grad = False
	    classifier_inputs = self._model.fc.in_features
	    self._model.fc = nn.Linear(in_features=classifier_inputs,
				       out_features=self.classes,
				       bias=True)
	    self._model.to(self.device)
	    if self.model_path:
		self._model.load_state_dict(torch.load(self.model_path))
	return self._model

    @property
    def criterion(self) -&amp;gt; nn.CrossEntropyLoss:
	"""The loss callable"""
	if self._criterion is None:
	    self._criterion = nn.CrossEntropyLoss()
	return self._criterion

    @property
    def optimizer(self) -&amp;gt; optimizer.SGD:
	"""The Gradient Descent object"""
	if self._optimizer is None:
	    self._optimizer = optimizer.SGD(
		self.model.parameters(),
		lr=self.learning_rate,
		momentum=self.momentum)
	return self._optimizer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1e8d6d8" class="outline-4"&gt;
&lt;h4 id="org1e8d6d8"&gt;Disecting the Inception Class&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org1e8d6d8"&gt;
&lt;p&gt;
The &lt;code&gt;Inception&lt;/code&gt; class bundles together a bunch of stuff that was originally being done in separate cells. Rather than putting comments all over it I'm going to show what it's doing by describing how I was doing it before I created the class.
&lt;/p&gt;
&lt;/div&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;a id="org108cb22"&gt;&lt;/a&gt;The Model Property&lt;br&gt;
&lt;div class="outline-text-5" id="text-org108cb22"&gt;
&lt;p&gt;
The last layer of the classifier in the &lt;code&gt;Inception.model&lt;/code&gt; property is the only layer of the pre-trained model that I change. In the case of the &lt;code&gt;Inception V3&lt;/code&gt; model there is a single layer called &lt;i&gt;fc&lt;/i&gt;, as opposed to multiple layers called &lt;i&gt;classifier&lt;/i&gt; as with the &lt;code&gt;VGG16&lt;/code&gt; model, so I just re-assign it to a fully-connected layer with the number of outputs that matches the number of dog breeds.
&lt;/p&gt;

&lt;p&gt;
Here's a little inspection to show what it's doing.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_transfer = models.inception_v3(pretrained=True)
print(model_transfer.fc)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Linear(in_features=2048, out_features=1000, bias=True)

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;CLASSIFIER_INPUTS = model_transfer.fc.in_features
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(CLASSIFIER_INPUTS) 
print(model_transfer.fc.out_features)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2048
1000

&lt;/pre&gt;

&lt;p&gt;
The layer we're going to replace has 2,048 inputs and 1,000 outputs. We'll have to match the number of inputs and change it to our 133.
&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;

&lt;li&gt;&lt;a id="org2136423"&gt;&lt;/a&gt;Freeze the Features Layers&lt;br&gt;
&lt;div class="outline-text-5" id="text-org2136423"&gt;
&lt;p&gt;
In the &lt;code&gt;model&lt;/code&gt; property I'm also freezing the parameters so that the pre-trained parameters don't change when training the last layer.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for parameter in model_transfer.parameters():
    parameter.requires_grad = False
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a id="org1257c28"&gt;&lt;/a&gt;The New Classifier&lt;br&gt;
&lt;div class="outline-text-5" id="text-org1257c28"&gt;
&lt;p&gt;
This next block of code is also in the &lt;code&gt;Inception.model&lt;/code&gt; definition and is where I'm replacing the last layer with out dog-breed-classification layer.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_transfer.fc = nn.Linear(in_features=CLASSIFIER_INPUTS,
			      out_features=BREEDS,
			      bias=True)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;

&lt;li&gt;&lt;a id="org03951c9"&gt;&lt;/a&gt;The Loss Function and Optimizer&lt;br&gt;
&lt;div class="outline-text-5" id="text-org03951c9"&gt;
&lt;p&gt;
The &lt;code&gt;Inception&lt;/code&gt; class uses the same loss and gradient descent definitions as the naive model did (in the &lt;code&gt;criterion&lt;/code&gt; and &lt;code&gt;optimizer&lt;/code&gt; properties).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;criterion_transfer = nn.CrossEntropyLoss()
optimizer_transfer = optimizer.SGD(model_transfer.parameters(),
				  lr=0.001,
				  momentum=0.9)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgaea16ec" class="outline-4"&gt;
&lt;h4 id="orgaea16ec"&gt;Transfer CLI&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgaea16ec"&gt;
&lt;p&gt;
I made this in order to run the model on paperspace without needing to keep the connection to the server alive (it hadn't occured to me to just save a log file).
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# python&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;functools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;argparse&lt;/span&gt;

&lt;span class="c1"&gt;# pypi&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dotenv&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;PIL&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ImageFile&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torchvision&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datasets&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.optim&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;optimizer&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torchvision.models&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;models&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torchvision.transforms&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;transforms&lt;/span&gt;

&lt;span class="c1"&gt;# this project&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;neurotic.tangles.data_paths&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DataPathTwo&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;neurotic.tangles.timer&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Timer&lt;/span&gt;

&lt;span class="c1"&gt;# the output won't show up if you don't flush it when redirecting it to a file&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;partial&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;flush&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;"__main__"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;argparse&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ArgumentParser&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Test or Train the Inception V3 Dog Classifier"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"--test-only"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"store_true"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			&lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Only run the test"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"--epochs"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			&lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Training epochs (default: &lt;/span&gt;&lt;span class="si"&gt;%(default)s&lt;/span&gt;&lt;span class="s2"&gt;)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	&lt;span class="s2"&gt;"--epoch-offset"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	&lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Offset for the output of epochs (default: &lt;/span&gt;&lt;span class="si"&gt;%(default)s&lt;/span&gt;&lt;span class="s2"&gt;)"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"--restart"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;action&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"store_true"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			&lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"Wipe out old model."&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;arguments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parser&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_args&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;data_sets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataSets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;training_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dog_training_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			 &lt;span class="n"&gt;validation_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dog_validation_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
			 &lt;span class="n"&gt;testing_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dog_testing_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Batches&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datasets&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data_sets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;inception&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Inception&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_sets&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;trainer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Trainer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epochs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;epoch_start&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;epoch_offset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;training_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;validation_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;validation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;testing_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batches&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;testing&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inception&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inception&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inception&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;inception&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;criterion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;model_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;transfer_path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_folder&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;load_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		      &lt;span class="n"&gt;beep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test_only&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="n"&gt;trainer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org70c9b43" class="outline-4"&gt;
&lt;h4 id="org70c9b43"&gt;The Training&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org70c9b43"&gt;
&lt;p&gt;
I re-trained the naive model and trained the inception model on paperspace for 100 epochs each. This took around five hours each so I'm not going to re-run it here, but I'll show how I would train the model and some of the output from the real training. The &lt;code&gt;Tee&lt;/code&gt; class isn't integrated with my &lt;code&gt;trainer&lt;/code&gt; so I can't really show how to train it that way, so I'll show it the orignal function-based way.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;transfer_path = MODEL_PATH.folder.joinpath("model_transfer.pt")
transfer_log = Tee(log_name="transfer_train.log")
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;EPOCHS = 100
inception = Inception()
train(EPOCHS,
      loaders=loaders_transfer,
      model=inception.model,
      optimizer=inception.optimizer,
      criterion=inception.criterion,
      use_cuda=use_cuda,
      save_path=transfer_model_path,
      print_function=transfer_log,
      is_inception=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
And the last lines of the output.
&lt;/p&gt;

&lt;pre class="example"&gt;
Epoch: 98 	Training Loss: 0.973978 	Validation Loss: 0.416819	Elapsed: 0:03:12.167687
Validation loss decreased (0.417785 --&amp;gt; 0.416819). Saving model ...
Epoch: 99 	Training Loss: 0.994163 	Validation Loss: 0.418498	Elapsed: 0:03:17.225706
Epoch: 100 	Training Loss: 0.998819 	Validation Loss: 0.423518	Elapsed: 0:03:18.415953
Training Ended: 2019-01-07 10:55:04.465024
Total Training Time: 5:29:54.161034
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org965ffa5" class="outline-4"&gt;
&lt;h4 id="org965ffa5"&gt;Test It&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org965ffa5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model_transfer.load_state_dict(torch.load(transfer_model_path))
transfer_test_log = Tee("transfer_test.log")
test(loaders_transfer, model_transfer, criterion_transfer, use_cuda, print_function=transfer_test_log)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Test Loss: 0.425383


Test Accuracy: 87% (734/836)
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org2c63c5c" class="outline-2"&gt;
&lt;h2 id="org2c63c5c"&gt;The Dog Breed Classifier&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2c63c5c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org68965a1" class="outline-3"&gt;
&lt;h3 id="org68965a1"&gt;Dog Predictor&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org68965a1"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DogPredictor:
    """Makes dog-breed predictions

    Args:
     model_path: path to the model's state-dict
     device: processor to run the model on
     data_sets: a DataSets object
     inception: an Inception object
    """
    def __init__(self, model_path: str=None,
		 device: torch.device=None,
		 data_sets: DataSets=None,
		 inception: Inception=None) -&amp;gt; None:
	self.model_path = model_path
	self.device = device
	self._data_sets = data_sets
	self._inception = inception
	self._breeds = None
	return

    @property
    def data_sets(self) -&amp;gt; DataSets:
	if self._data_sets is None:
	    self._data_sets = DataSets()
	return self._data_sets

    @property
    def inception(self) -&amp;gt; Inception:
	"""An Inception object"""
	if self._inception is None:
	    self._inception = Inception(
		classes=len(self.data_sets.training.classes),
		model_path=self.model_path,
		device=self.device)
	    self._inception.model.eval()
	return self._inception

    @property
    def breeds(self) -&amp;gt; list:
	"""A list of dog-breeds"""
	if self._breeds is None:
	    self._breeds = [name[4:].replace("_", " ")
			    for name in self.data_sets.training.classes]
	return self._breeds

    def predict_index(self, image_path:str) -&amp;gt; int:
	"""Predicts the index of the breed of the dog in the image

	Args:
	 image_path: path to the image
	Returns:
	 index in the breeds list for the image
	"""
	model = self.inception.model        
	image = Image.open(image_path)
	tensor = self.data_sets.transformer.testing(image)
	# add a batch number
	tensor = tensor.unsqueeze_(0)
	tensor = tensor.to(self.inception.device)
	x = torch.autograd.Variable(tensor)
	output = model(x)
	return output.data.cpu().numpy().argmax()

    def __call__(self, image_path) -&amp;gt; str:
	"""Predicts the breed of the dog in the image

	Args:
	 image_path: path to the image
	Returns:
	 name of the breed
	"""
	return self.breeds[self.predict_index(image_path)]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;predictor = DogPredictor(model_path=transfer_path)
files = list(predictor.data_sets.paths.testing.folder.glob("*/*.jpg"))
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;case = numpy.random.choice(files, 1)[0]
print("Sample: {}".format(case))
predicted = predictor(case)
print("Predicted: {}".format(predicted))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Sample: /home/hades/data/datasets/dog-breed-classification/dogImages/test/109.Norwegian_elkhound/Norwegian_elkhound_07137.jpg
Predicted: Norwegian elkhound

&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for model in MODELS:
    model.cpu()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcca0568" class="outline-3"&gt;
&lt;h3 id="orgcca0568"&gt;The Dog Breed Classifier&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgcca0568"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class DogBreedClassifier:
    """Tries To predict the dog-breed for an image

    Args:
     model_path: path to the inception-model
    """
    def __init__(self, model_path: str) -&amp;gt; None:
	self.model_path = model_path
	self._breed_predictor = None
	self._species_detector = None
	return

    @property
    def breed_predictor(self) -&amp;gt; DogPredictor:
	"""Predictor of dog-breeds"""
	if self._breed_predictor is None:
	    self._breed_predictor = DogPredictor(model_path=self.model_path)
	return self._breed_predictor

    @property
    def species_detector(self) -&amp;gt; SpeciesDetector:
	"""Detector of humans and dogs"""
	if self._species_detector is None:
	    self._species_detector = SpeciesDetector(
		device=self.breed_predictor.inception.device)
	return self._species_detector

    def render(self, image_path: str, species: str, breed: str) -&amp;gt; None:
	"""Renders the image

	Args:
	 image_path: path to the image to render
	 species: identified species
	 breed: identified breed
	"""
	name = " ".join(image_path.name.split(".")[0].split("_")).title()
	figure, axe = pyplot.subplots()
	figure.suptitle("{} ({})".format(species, name), weight="bold")
	axe.set_xlabel("Looks like a {}.".format(breed))
	image = Image.open(image_path)
	axe.tick_params(dict(axis="both",
			     which="both",
			     bottom=False,
			     top=False))
	axe.get_xaxis().set_ticks([])
	axe.get_yaxis().set_ticks([])
	axe_image = axe.imshow(image)
	return

    def __call__(self, image_path:str) -&amp;gt; None:
	"""detects the dog-breed and displays the image

	Args:
	 image_path: path to the image
	"""
	image_path = Path(image_path)
	is_dog = self.species_detector.is_dog(image_path)
	is_human = self.species_detector.is_human(image_path)

	if not is_dog and not is_human:
	    species = "Error: Neither Human nor Dog"
	    breed = "?"
	else:
	    breed = self.breed_predictor(image_path)

	if is_dog and is_human:
	    species = "Human-Dog Hybrid"
	elif is_dog:
	    species = "Dog"
	elif is_human:
	    species = "Human"
	self.render(image_path, species, breed)
	return
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8e6e688" class="outline-2"&gt;
&lt;h2 id="org8e6e688"&gt;Some Sample applications&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org8e6e688"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;classifier = DogBreedClassifier(model_path=transfer_path)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;case = numpy.random.choice(human_files, 1)[0]
classifier(case)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/test_one.png" alt="test_one.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;case = numpy.random.choice(dog_files, 1)[0]
classifier(case)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/test_two.png" alt="test_two.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;case = "rabbit.jpg"
classifier(case)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/test_three.png" alt="test_three.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
Rabbit image from &lt;a href="https://commons.wikimedia.org/wiki/File:Oryctolagus_cuniculus_Tasmania_2.jpg"&gt;Wikimedia&lt;/a&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;case = "hot_dog.jpg"
classifier(case)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/test_four.png" alt="test_four.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The Hot Dog is also from &lt;a href="https://commons.wikimedia.org/wiki/File:NCI_Visuals_Food_Hot_Dog.jpg"&gt;Wikimedia&lt;/a&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;case = human_files_short[34]
classifier(case)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/test_five.png" alt="test_five.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So, somehow my class-based detector got smarter than my function based one and can now tell that this isn't a dogâ¦
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cnn</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-breed-classification/</guid><pubDate>Mon, 26 Nov 2018 21:11:29 GMT</pubDate></item><item><title>Dog Classification Project Overview</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-classification-project-overview/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-classification-project-overview/#orgc208ab0"&gt;Project Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-classification-project-overview/#org931df8a"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-classification-project-overview/#orgf3d7ba7"&gt;Some Rules&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-classification-project-overview/#orge43cee8"&gt;(Optionally) Accelerating the Training Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-classification-project-overview/#org7175b0d"&gt;Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-classification-project-overview/#org70082e5"&gt;Project Submission&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc208ab0" class="outline-2"&gt;
&lt;h2 id="orgc208ab0"&gt;Project Overview&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc208ab0"&gt;
&lt;p&gt;
In this project we will build a pipeline that can be used within a web or mobile app to process real-world, user-supplied images.  Given an image of a dog, our algorithm will identify an estimate of the canineâs breed.  If supplied an image of a human, the code will identify the dog breed that the person most resembles.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org931df8a" class="outline-2"&gt;
&lt;h2 id="org931df8a"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org931df8a"&gt;
&lt;p&gt;
The &lt;a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"&gt;dog dataset&lt;/a&gt; is in a zip-file hosted on Amazon Web Services. The folder should contain three folders (&lt;code&gt;test&lt;/code&gt;, &lt;code&gt;train&lt;/code&gt;, and &lt;code&gt;valid&lt;/code&gt;) and each of these folders should have 133 folders, one for each dog-breed. It looks like the &lt;a href="http://vision.stanford.edu/aditya86/ImageNetDogs/"&gt;Stanford Dogs Dataset&lt;/a&gt;, but the Stanford data set has 120 breeds, so I don't know the actual source.
The &lt;a href="http://vis-www.cs.umass.edu/lfw/lfw.tgz"&gt;human dataset&lt;/a&gt; seems to be the &lt;a href="http://vis-www.cs.umass.edu/lfw/"&gt;Labeled Faces in the Wild&lt;/a&gt; data set which was built to study the problem of facial recognition. It's made up of real photos of people taken from the web. Each photo sits in a sub-folder that was given the name of the person (e.g. &lt;code&gt;Michelle_Yeoh&lt;/code&gt;). The folder hasn't been split into train-test-validiation folders the way the dog dataset was.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf3d7ba7" class="outline-2"&gt;
&lt;h2 id="orgf3d7ba7"&gt;Some Rules&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgf3d7ba7"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;&lt;b&gt;&lt;b&gt;Unless requested, do not modify code that has already been included.&lt;/b&gt;&lt;/b&gt;&lt;/li&gt;

&lt;li&gt;In the notebook, you will need to train CNNs in PyTorch.  If your CNN is taking too long to train, feel free to pursue one of the options under the section &lt;i&gt;Accelerating the Training Process&lt;/i&gt; below.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge43cee8" class="outline-2"&gt;
&lt;h2 id="orge43cee8"&gt;(Optionally) Accelerating the Training Process&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orge43cee8"&gt;
&lt;p&gt;
If your code is taking too long to run, you will need to either reduce the complexity of your chosen CNN architecture or switch to running your code on a GPU.  If you'd like to use a GPU, you can spin up an instance of your own:
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org333d22c" class="outline-3"&gt;
&lt;h3 id="org333d22c"&gt;Amazon Web Services&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org333d22c"&gt;
&lt;p&gt;
You can use Amazon Web Services to launch an EC2 GPU instance. (This costs money, but enrolled students should see a coupon code in their student &lt;code&gt;resources&lt;/code&gt;.)
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org7175b0d" class="outline-2"&gt;
&lt;h2 id="org7175b0d"&gt;Evaluation&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7175b0d"&gt;
&lt;p&gt;
Your project will be reviewed by a Udacity reviewer against the CNN project rubric.  Review this rubric thoroughly and self-evaluate your project before submission.  All criteria found in the rubric must meet specifications for you to pass.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org70082e5" class="outline-2"&gt;
&lt;h2 id="org70082e5"&gt;Project Submission&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org70082e5"&gt;
&lt;p&gt;
Your submission should consist of the github link to your repository.  Your repository should contain:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;The &lt;code&gt;dog_app.ipynb&lt;/code&gt; file with fully functional code, all code cells executed and displaying output, and all questions answered.&lt;/li&gt;
&lt;li&gt;An HTML or PDF export of the project notebook with the name &lt;code&gt;report.html&lt;/code&gt; or &lt;code&gt;report.pdf&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Please do &lt;i&gt;NOT&lt;/i&gt; include any of the project data sets provided in the &lt;code&gt;dogImages/&lt;/code&gt; or &lt;code&gt;lfw/&lt;/code&gt; folders.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>classification</category><category>cnn</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/dog-breed-classifier/dog-classification-project-overview/</guid><pubDate>Mon, 26 Nov 2018 00:33:14 GMT</pubDate></item><item><title>Bike Sharing Project Feedback</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-feedback/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-feedback/#org71ffb0d"&gt;On the Number of Hidden Units&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-feedback/#org7ebbaa6"&gt;On the Learning Rate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org71ffb0d" class="outline-2"&gt;
&lt;h2 id="org71ffb0d"&gt;On the Number of Hidden Units&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org71ffb0d"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Rule of thumb: halfway between number of inputs and outputs&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.quora.com/How-do-I-decide-the-number-of-nodes-in-a-hidden-layer-of-a-neural-network-I-will-be-using-a-three-layer-model"&gt;Quora link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
\[8 \leq \text{number of hidden units} \leq \text{twice the number of inputs}
\]
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7ebbaa6" class="outline-2"&gt;
&lt;h2 id="org7ebbaa6"&gt;On the Learning Rate&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7ebbaa6"&gt;
&lt;p&gt;
\[
0.001 \leq \alpha \leq 0.1
\]
&lt;/p&gt;

&lt;p&gt;
When considering the learning rate calculate \[\frac{\alpha}{\text{number of records}}\] and see if it's too small or too larg.e
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>feedback</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-feedback/</guid><pubDate>Mon, 05 Nov 2018 20:55:10 GMT</pubDate></item><item><title>Bike Sharing Project Answers</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-answers/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-answers/#org773d838"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-answers/#org95d0aed"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-answers/#orgdc6f231"&gt;The Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-answers/#org5978068"&gt;The Hyper Parameters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org773d838" class="outline-2"&gt;
&lt;h2 id="org773d838"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org773d838"&gt;
&lt;p&gt;
The Bike Sharing Project uses a neural network to predict daily ridership for a bike sharing service. The code is split into two parts - a jupyter notebook that you work with and a python file (&lt;code&gt;my_answers.py&lt;/code&gt;) where you put the parts of the code that isn't provided. This creates the &lt;code&gt;my_answer.py&lt;/code&gt; file.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org95d0aed" class="outline-2"&gt;
&lt;h2 id="org95d0aed"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org95d0aed"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgdc6f231" class="outline-2"&gt;
&lt;h2 id="orgdc6f231"&gt;The Neural Network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgdc6f231"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;NeuralNetwork&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Implementation of a neural network with one hidden layer&lt;/span&gt;

&lt;span class="sd"&gt;    Args:&lt;/span&gt;
&lt;span class="sd"&gt;     input_nodes: number of input nodes&lt;/span&gt;
&lt;span class="sd"&gt;     hidden_nodes: number of hidden nodes&lt;/span&gt;
&lt;span class="sd"&gt;     output_nodes: number of output_nodes&lt;/span&gt;
&lt;span class="sd"&gt;     learning_rate: rate at which to update the weights&lt;/span&gt;
&lt;span class="sd"&gt;    """&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_nodes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_nodes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_nodes&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
		 &lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="c1"&gt;# Set number of nodes in input, hidden and output layers.&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;input_nodes&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hidden_nodes&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_nodes&lt;/span&gt;

	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;learning_rate&lt;/span&gt;

	&lt;span class="c1"&gt;# Initialize weights&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_input_to_hidden&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_hidden_to_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;
	&lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd749523" class="outline-3"&gt;
&lt;h3 id="orgd749523"&gt;Input To Hidden Weights&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd749523"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;weights_input_to_hidden&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""Array of weights from input layer to the hidden layer"""&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_input_to_hidden&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_input_to_hidden&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_nodes&lt;/span&gt;&lt;span class="o"&gt;**-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
	    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;input_nodes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_nodes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_input_to_hidden&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The unit-test tries to set the weights so we need a setter.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@weights_input_to_hidden.setter&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;weights_input_to_hidden&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""Sets the weights"""&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_input_to_hidden&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4d41b3c" class="outline-3"&gt;
&lt;h3 id="org4d41b3c"&gt;Hidden To Output Weights&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4d41b3c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@property&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;weights_hidden_to_output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Array of weights for edges from hidden layer to output"""&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_hidden_to_output&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
	&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_hidden_to_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_nodes&lt;/span&gt;&lt;span class="o"&gt;**-&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
	    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hidden_nodes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output_nodes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_hidden_to_output&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Once again, this is for the unit-testing.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@weights_hidden_to_output.setter&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;weights_hidden_to_output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ndarray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="sd"&gt;"""sets the weights for edges from hidden layer to output"""&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_weights_hidden_to_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd027ec2" class="outline-3"&gt;
&lt;h3 id="orgd027ec2"&gt;Activation Function&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd027ec2"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;activation_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""A pass-through to the sigmoid"""&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org68992e6" class="outline-3"&gt;
&lt;h3 id="org68992e6"&gt;Sigmoid&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org68992e6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;"""Calculates the sigmoid of the value"""&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd9c3bfb" class="outline-3"&gt;
&lt;h3 id="orgd9c3bfb"&gt;Train&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd9c3bfb"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;''' Train the network on batch of features and targets. &lt;/span&gt;

&lt;span class="sd"&gt;	Arguments&lt;/span&gt;
&lt;span class="sd"&gt;	---------&lt;/span&gt;

&lt;span class="sd"&gt;	features: 2D array, each row is one data record, each column is a feature&lt;/span&gt;
&lt;span class="sd"&gt;	targets: 1D array of target values&lt;/span&gt;

&lt;span class="sd"&gt;    '''&lt;/span&gt;
    &lt;span class="n"&gt;n_records&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_input_to_hidden&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_hidden_to_output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
   &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;targets&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;            
	&lt;span class="n"&gt;final_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;forward_pass_train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
	&lt;span class="c1"&gt;# Implement the backpropagation function below&lt;/span&gt;
	&lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backpropagation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
	    &lt;span class="n"&gt;final_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
	    &lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_records&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgf345927" class="outline-3"&gt;
&lt;h3 id="orgf345927"&gt;Forward Pass Train&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf345927"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward_pass_train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;''' Implement forward pass here &lt;/span&gt;

&lt;span class="sd"&gt;	Arguments&lt;/span&gt;
&lt;span class="sd"&gt;	---------&lt;/span&gt;
&lt;span class="sd"&gt;	X: features batch&lt;/span&gt;

&lt;span class="sd"&gt;    '''&lt;/span&gt;
    &lt;span class="n"&gt;hidden_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_input_to_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;hidden_outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;activation_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;final_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_hidden_to_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;final_outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;final_inputs&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;final_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_outputs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgce25a6f" class="outline-3"&gt;
&lt;h3 id="orgce25a6f"&gt;Back Propagation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgce25a6f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;backpropagation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;final_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hidden_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;''' Implement backpropagation&lt;/span&gt;

&lt;span class="sd"&gt;	Arguments&lt;/span&gt;
&lt;span class="sd"&gt;	---------&lt;/span&gt;
&lt;span class="sd"&gt;	final_outputs: output from forward pass&lt;/span&gt;
&lt;span class="sd"&gt;	y: target (i.e. label) batch&lt;/span&gt;
&lt;span class="sd"&gt;	delta_weights_i_h: change in weights from input to hidden layers&lt;/span&gt;
&lt;span class="sd"&gt;	delta_weights_h_o: change in weights from hidden to output layers&lt;/span&gt;

&lt;span class="sd"&gt;    '''&lt;/span&gt;
    &lt;span class="n"&gt;error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;final_outputs&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;

    &lt;span class="n"&gt;hidden_error&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_hidden_to_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;output_error_term&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;error&lt;/span&gt;

    &lt;span class="n"&gt;hidden_error_term&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hidden_error&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;hidden_outputs&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;hidden_outputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;hidden_error_term&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;output_error_term&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;hidden_outputs&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4745637" class="outline-3"&gt;
&lt;h3 id="org4745637"&gt;Update Weights&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4745637"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;update_weights&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_records&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;''' Update weights on gradient descent step&lt;/span&gt;

&lt;span class="sd"&gt;	Arguments&lt;/span&gt;
&lt;span class="sd"&gt;	---------&lt;/span&gt;
&lt;span class="sd"&gt;	delta_weights_i_h: change in weights from input to hidden layers&lt;/span&gt;
&lt;span class="sd"&gt;	delta_weights_h_o: change in weights from hidden to output layers&lt;/span&gt;
&lt;span class="sd"&gt;	n_records: number of records&lt;/span&gt;

&lt;span class="sd"&gt;    '''&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_hidden_to_output&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delta_weights_h_o&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;n_records&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_input_to_hidden&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;delta_weights_i_h&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;n_records&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org33af958" class="outline-3"&gt;
&lt;h3 id="org33af958"&gt;Run&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org33af958"&gt;
&lt;p&gt;
&lt;b&gt;Warning:&lt;/b&gt; The MSE function defined in the jupyter notebook won't work if you use &lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html"&gt;numpy.dot&lt;/a&gt; instead of &lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html"&gt;numpy.matmul&lt;/a&gt;. You can make it work by passing in &lt;code&gt;axis=1&lt;/code&gt; to &lt;a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html"&gt;numpy.mean&lt;/a&gt; but I don't think you're allowed to change the things in the jupyter notebook.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;''' Run a forward pass through the network with input features &lt;/span&gt;

&lt;span class="sd"&gt;	Arguments&lt;/span&gt;
&lt;span class="sd"&gt;	---------&lt;/span&gt;
&lt;span class="sd"&gt;	features: 1D array of feature values&lt;/span&gt;
&lt;span class="sd"&gt;    '''&lt;/span&gt;

    &lt;span class="n"&gt;hidden_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_input_to_hidden&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;hidden_outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;activation_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 

    &lt;span class="n"&gt;final_inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hidden_outputs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights_hidden_to_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;final_outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;final_inputs&lt;/span&gt;        
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;final_outputs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5978068" class="outline-2"&gt;
&lt;h2 id="org5978068"&gt;The Hyper Parameters&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org5978068"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;7500&lt;/span&gt;
&lt;span class="n"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.4&lt;/span&gt;
&lt;span class="n"&gt;hidden_nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;
&lt;span class="n"&gt;output_nodes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>neural networks</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/bike-sharing-project-answers/</guid><pubDate>Tue, 30 Oct 2018 22:31:25 GMT</pubDate></item><item><title>The Bike Sharing Project</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org3d6f8cd"&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#orgb769e8b"&gt;Jupyter Setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org4adfb42"&gt;Imports&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org69f369c"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org39af1f1"&gt;The Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org116ad83"&gt;Checking out the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org6d923e5"&gt;Dummy variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org49b43d6"&gt;Scaling target variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org0895995"&gt;Splitting the data into training, testing, and validation sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#orgc2093ee"&gt;Time to build the network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org4fdbf06"&gt;Unit tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#org48c2294"&gt;Training the network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#orgfdef998"&gt;Check out your predictions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/#orga0d946d"&gt;More Variations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3d6f8cd" class="outline-2"&gt;
&lt;h2 id="org3d6f8cd"&gt;Introduction&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org3d6f8cd"&gt;
&lt;p&gt;
This project builds a neural network and uses it to predict daily bike rental ridership.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb769e8b" class="outline-2"&gt;
&lt;h2 id="orgb769e8b"&gt;Jupyter Setup&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb769e8b"&gt;
&lt;p&gt;
This sets some "magic" jupyter values.
&lt;/p&gt;

&lt;p&gt;
Display Matplotlib plots.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('matplotlib', 'inline')
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Reload code from other modules that has changed (otherwise even if you re-run the import the changes won't get picked up).
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
I couldn't find any documentation on this other than people asking how to get it to work. I think it means &lt;a href="https://matplotlib.org/users/prev_whats_new/whats_new_2.0.0.html#support-for-hidpi-retina-displays-in-the-nbagg-and-webagg-backends"&gt;to use a higher resolution if your display supports it&lt;/a&gt;.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4adfb42" class="outline-2"&gt;
&lt;h2 id="org4adfb42"&gt;Imports&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4adfb42"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgf7ffd9f" class="outline-3"&gt;
&lt;h3 id="orgf7ffd9f"&gt;Python Standard Library&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgf7ffd9f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from collections import namedtuple
from functools import partial
from datetime import datetime
import unittest
import sys
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgba9acc9" class="outline-3"&gt;
&lt;h3 id="orgba9acc9"&gt;From PyPi&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgba9acc9"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graphviz import Digraph
from tabulate import tabulate
import numpy
import pandas
import matplotlib.pyplot as pyplot
import seaborn
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org1df60e6" class="outline-3"&gt;
&lt;h3 id="org1df60e6"&gt;This Project&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org1df60e6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from neurotic.tangles.data_paths import DataPath
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgef70d50" class="outline-3"&gt;
&lt;h3 id="orgef70d50"&gt;The Submission&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgef70d50"&gt;
&lt;p&gt;
The submission is set up so that you provide a separate python file where you implement the neural network, so this imports it.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from my_answers import (
    NeuralNetwork,
    iterations,
    learning_rate,
    hidden_nodes,
    output_nodes)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org69f369c" class="outline-2"&gt;
&lt;h2 id="org69f369c"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org69f369c"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6ecfaff" class="outline-3"&gt;
&lt;h3 id="org6ecfaff"&gt;Tables&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6ecfaff"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;table = partial(tabulate, tablefmt="orgtbl", headers="keys", showindex=False)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9d5ecd5" class="outline-3"&gt;
&lt;h3 id="org9d5ecd5"&gt;Plotting&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9d5ecd5"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;seaborn.set_style("whitegrid", rc={"axes.grid": False})
FIGURE_SIZE = (12, 10)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org39af1f1" class="outline-2"&gt;
&lt;h2 id="org39af1f1"&gt;The Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org39af1f1"&gt;
&lt;p&gt;
The data comes from the &lt;a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset"&gt;UCI Machine Learning Repository&lt;/a&gt; (I think). It combines &lt;a href="https://www.capitalbikeshare.com/system-data"&gt;Capital Bikeshare data&lt;/a&gt;, &lt;a href="http://www.freemeteo.com"&gt;Weather Data from i-Weather&lt;/a&gt;, and &lt;a href="https://dchr.dc.gov/page/holiday-schedules"&gt;Washington D.C. holiday information&lt;/a&gt;. The authors note that becaus the bikes are tracked when they are checked out and when they arrive they have become a "virtual sensor network" that tracks how people move through the city (by shared bicycle, at least).
&lt;/p&gt;

&lt;p&gt;
This first bit is the original path that you need for a submission, which is different from where I'm keeping it while working on this. I'm adding an &lt;code&gt;EXPECTED_DATA_PATH&lt;/code&gt; variable because that's being checked in the unit-test for some reason, and I'll need to change it for the submission.
&lt;/p&gt;

&lt;pre class="example"&gt;
data_path = 'Bike-Sharing-Dataset/hour.csv'
EXPECTED_DATA_PATH = data_path.lower()
&lt;/pre&gt;

&lt;p&gt;
This is where it's kept for this post.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;path = DataPath("hour.csv")
data_path = str(path.from_folder)
EXPECTED_DATA_PATH = data_path.lower()
print(path.from_folder)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
../../../data/bike-sharing/hour.csv

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;rides = pandas.read_csv(data_path)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(table(rides.head()))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;instant&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;dteday&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;season&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;yr&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;mnth&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;hr&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;holiday&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;weekday&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;workingday&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;weathersit&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;temp&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;atemp&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;hum&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;windspeed&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;casual&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;registered&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;cnt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;2011-01-01&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.24&lt;/td&gt;
&lt;td class="org-right"&gt;0.2879&lt;/td&gt;
&lt;td class="org-right"&gt;0.81&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;13&lt;/td&gt;
&lt;td class="org-right"&gt;16&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-right"&gt;2011-01-01&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.22&lt;/td&gt;
&lt;td class="org-right"&gt;0.2727&lt;/td&gt;
&lt;td class="org-right"&gt;0.8&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;8&lt;/td&gt;
&lt;td class="org-right"&gt;32&lt;/td&gt;
&lt;td class="org-right"&gt;40&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;2011-01-01&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.22&lt;/td&gt;
&lt;td class="org-right"&gt;0.2727&lt;/td&gt;
&lt;td class="org-right"&gt;0.8&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;5&lt;/td&gt;
&lt;td class="org-right"&gt;27&lt;/td&gt;
&lt;td class="org-right"&gt;32&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-right"&gt;2011-01-01&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.24&lt;/td&gt;
&lt;td class="org-right"&gt;0.2879&lt;/td&gt;
&lt;td class="org-right"&gt;0.75&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;10&lt;/td&gt;
&lt;td class="org-right"&gt;13&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;5&lt;/td&gt;
&lt;td class="org-right"&gt;2011-01-01&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.24&lt;/td&gt;
&lt;td class="org-right"&gt;0.2879&lt;/td&gt;
&lt;td class="org-right"&gt;0.75&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(len(rides.dteday.unique()))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
731

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(table(rides.describe(), showindex=True))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;

&lt;col class="org-right"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Â &lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;instant&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;season&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;yr&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;mnth&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;hr&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;holiday&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;weekday&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;workingday&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;weathersit&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;temp&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;atemp&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;hum&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;windspeed&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;casual&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;registered&lt;/th&gt;
&lt;th scope="col" class="org-right"&gt;cnt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;count&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;mean&lt;/td&gt;
&lt;td class="org-right"&gt;8690&lt;/td&gt;
&lt;td class="org-right"&gt;2.50164&lt;/td&gt;
&lt;td class="org-right"&gt;0.502561&lt;/td&gt;
&lt;td class="org-right"&gt;6.53778&lt;/td&gt;
&lt;td class="org-right"&gt;11.5468&lt;/td&gt;
&lt;td class="org-right"&gt;0.0287704&lt;/td&gt;
&lt;td class="org-right"&gt;3.00368&lt;/td&gt;
&lt;td class="org-right"&gt;0.682721&lt;/td&gt;
&lt;td class="org-right"&gt;1.42528&lt;/td&gt;
&lt;td class="org-right"&gt;0.496987&lt;/td&gt;
&lt;td class="org-right"&gt;0.475775&lt;/td&gt;
&lt;td class="org-right"&gt;0.627229&lt;/td&gt;
&lt;td class="org-right"&gt;0.190098&lt;/td&gt;
&lt;td class="org-right"&gt;35.6762&lt;/td&gt;
&lt;td class="org-right"&gt;153.787&lt;/td&gt;
&lt;td class="org-right"&gt;189.463&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;std&lt;/td&gt;
&lt;td class="org-right"&gt;5017.03&lt;/td&gt;
&lt;td class="org-right"&gt;1.10692&lt;/td&gt;
&lt;td class="org-right"&gt;0.500008&lt;/td&gt;
&lt;td class="org-right"&gt;3.43878&lt;/td&gt;
&lt;td class="org-right"&gt;6.91441&lt;/td&gt;
&lt;td class="org-right"&gt;0.167165&lt;/td&gt;
&lt;td class="org-right"&gt;2.00577&lt;/td&gt;
&lt;td class="org-right"&gt;0.465431&lt;/td&gt;
&lt;td class="org-right"&gt;0.639357&lt;/td&gt;
&lt;td class="org-right"&gt;0.192556&lt;/td&gt;
&lt;td class="org-right"&gt;0.17185&lt;/td&gt;
&lt;td class="org-right"&gt;0.19293&lt;/td&gt;
&lt;td class="org-right"&gt;0.12234&lt;/td&gt;
&lt;td class="org-right"&gt;49.305&lt;/td&gt;
&lt;td class="org-right"&gt;151.357&lt;/td&gt;
&lt;td class="org-right"&gt;181.388&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;min&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.02&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;25%&lt;/td&gt;
&lt;td class="org-right"&gt;4345.5&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.34&lt;/td&gt;
&lt;td class="org-right"&gt;0.3333&lt;/td&gt;
&lt;td class="org-right"&gt;0.48&lt;/td&gt;
&lt;td class="org-right"&gt;0.1045&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-right"&gt;34&lt;/td&gt;
&lt;td class="org-right"&gt;40&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;50%&lt;/td&gt;
&lt;td class="org-right"&gt;8690&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;7&lt;/td&gt;
&lt;td class="org-right"&gt;12&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.5&lt;/td&gt;
&lt;td class="org-right"&gt;0.4848&lt;/td&gt;
&lt;td class="org-right"&gt;0.63&lt;/td&gt;
&lt;td class="org-right"&gt;0.194&lt;/td&gt;
&lt;td class="org-right"&gt;17&lt;/td&gt;
&lt;td class="org-right"&gt;115&lt;/td&gt;
&lt;td class="org-right"&gt;142&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;75%&lt;/td&gt;
&lt;td class="org-right"&gt;13034.5&lt;/td&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;10&lt;/td&gt;
&lt;td class="org-right"&gt;18&lt;/td&gt;
&lt;td class="org-right"&gt;0&lt;/td&gt;
&lt;td class="org-right"&gt;5&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-right"&gt;0.66&lt;/td&gt;
&lt;td class="org-right"&gt;0.6212&lt;/td&gt;
&lt;td class="org-right"&gt;0.78&lt;/td&gt;
&lt;td class="org-right"&gt;0.2537&lt;/td&gt;
&lt;td class="org-right"&gt;48&lt;/td&gt;
&lt;td class="org-right"&gt;220&lt;/td&gt;
&lt;td class="org-right"&gt;281&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;max&lt;/td&gt;
&lt;td class="org-right"&gt;17379&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;12&lt;/td&gt;
&lt;td class="org-right"&gt;23&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;6&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-right"&gt;0.8507&lt;/td&gt;
&lt;td class="org-right"&gt;367&lt;/td&gt;
&lt;td class="org-right"&gt;886&lt;/td&gt;
&lt;td class="org-right"&gt;977&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(len(rides.dteday.unique()) * 24)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
17544

&lt;/pre&gt;

&lt;p&gt;
So there appear to be some hours missing, since there aren't enough rows in the data set.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print("First Hour: {} {}".format(
    rides.dteday.min(),
    rides[rides.dteday == rides.dteday.min()].hr.min()))
print("Last Hour: {} {}".format(
    rides.dteday.max(),
    rides[rides.dteday == rides.dteday.max()].hr.max()))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
First Hour: 2011-01-01 0
Last Hour: 2012-12-31 23

&lt;/pre&gt;

&lt;p&gt;
Well, that's odd. It looks like the span is complete, why are there missing hours?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
counts = rides.groupby(["dteday"]).hr.count()
axe.set_title("Hours Recorded Per Day")
axe.set_xlabel("Day")
axe.set_ylabel("Count")
ax = axe.plot(range(len(counts.index)), counts.values, "o", markerfacecolor='None')
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/date_hours.png" alt="date_hours.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
So it looks like some days they didn't manage to record all the hours.
&lt;/p&gt;

&lt;p&gt;
Assuming this is the UC Irvine dataset, this is the description of the variables.
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Variable&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;instant&lt;/td&gt;
&lt;td class="org-left"&gt;record index&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;dteday&lt;/td&gt;
&lt;td class="org-left"&gt;date&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;season&lt;/td&gt;
&lt;td class="org-left"&gt;season (1:spring, 2:summer, 3:fall, 4:winter)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;yr&lt;/td&gt;
&lt;td class="org-left"&gt;year (0: 2011, 1:2012)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;mnth&lt;/td&gt;
&lt;td class="org-left"&gt;month (1 to 12)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;hr&lt;/td&gt;
&lt;td class="org-left"&gt;hour (0 to 23)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;holiday&lt;/td&gt;
&lt;td class="org-left"&gt;whether day is holiday or not (extracted from  &lt;a href="https://dchr.dc.gov/page/holiday-schedules"&gt;Washington D.C. holiday information&lt;/a&gt;)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;weekday&lt;/td&gt;
&lt;td class="org-left"&gt;day of the week (0 to 6)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;workingday&lt;/td&gt;
&lt;td class="org-left"&gt;if day is neither weekend nor holiday is 1, otherwise is 0.&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;weathersit&lt;/td&gt;
&lt;td class="org-left"&gt;Weather (1, 2, 3, or 4) (see next table)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;temp&lt;/td&gt;
&lt;td class="org-left"&gt;Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;atemp&lt;/td&gt;
&lt;td class="org-left"&gt;Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;hum&lt;/td&gt;
&lt;td class="org-left"&gt;Normalized humidity. The values are divided to 100 (max)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;windspeed&lt;/td&gt;
&lt;td class="org-left"&gt;Normalized wind speed. The values are divided to 67 (max)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;casual&lt;/td&gt;
&lt;td class="org-left"&gt;count of casual users&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;registered&lt;/td&gt;
&lt;td class="org-left"&gt;count of registered users&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;cnt&lt;/td&gt;
&lt;td class="org-left"&gt;count of total rental bikes including both casual and registered&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
&lt;code&gt;weathersit&lt;/code&gt;
&lt;/p&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-right"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-right"&gt;Value&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-right"&gt;1&lt;/td&gt;
&lt;td class="org-left"&gt;Clear, Few clouds, Partly cloudy, Partly cloudy&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;2&lt;/td&gt;
&lt;td class="org-left"&gt;Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;3&lt;/td&gt;
&lt;td class="org-left"&gt;Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-right"&gt;4&lt;/td&gt;
&lt;td class="org-left"&gt;Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org116ad83" class="outline-2"&gt;
&lt;h2 id="org116ad83"&gt;Checking out the data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org116ad83"&gt;
&lt;p&gt;
This dataset has the number of riders for each hour of each day from January 1, 2011 to December 31, 2012. The number of riders is split between casual and registered and summed up in the &lt;code&gt;cnt&lt;/code&gt; column. You can see the first few rows of the data above.
&lt;/p&gt;

&lt;p&gt;
Below is a plot showing the number of bike riders over the first 10 days or so in the data set (some days don't have exactly 24 entries in the data set, so it's not exactly 10 days). You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Rides For the First Ten Days")
first_ten = rides[:24*10]
plot_lines = axe.plot(range(len(first_ten)), first_ten.cnt, label="Count")
lines = axe.plot(range(len(first_ten)), first_ten.cnt,
		 '.',
		 markeredgecolor="r")
axe.set_xlabel("Day")
legend = axe.legend(plot_lines, ["Count"], loc="upper left")
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/riders_first_ten_days.png" alt="riders_first_ten_days.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6d923e5" class="outline-2"&gt;
&lt;h2 id="org6d923e5"&gt;Dummy variables&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org6d923e5"&gt;
&lt;p&gt;
Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to &lt;a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"&gt;get_dummies&lt;/a&gt;.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']
for each in dummy_fields:
    dummies = pandas.get_dummies(rides[each], prefix=each, drop_first=False)
    rides = pandas.concat([rides, dummies], axis=1)

fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', 
		  'weekday', 'atemp', 'mnth', 'workingday', 'hr']
data = rides.drop(fields_to_drop, axis=1)
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data.head())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
   yr  holiday  temp   hum  windspeed  casual  registered  cnt  season_1  \
0   0        0  0.24  0.81        0.0       3          13   16         1   
1   0        0  0.22  0.80        0.0       8          32   40         1   
2   0        0  0.22  0.80        0.0       5          27   32         1   
3   0        0  0.24  0.75        0.0       3          10   13         1   
4   0        0  0.24  0.75        0.0       0           1    1         1   

   season_2    ...      hr_21  hr_22  hr_23  weekday_0  weekday_1  weekday_2  \
0         0    ...          0      0      0          0          0          0   
1         0    ...          0      0      0          0          0          0   
2         0    ...          0      0      0          0          0          0   
3         0    ...          0      0      0          0          0          0   
4         0    ...          0      0      0          0          0          0   

   weekday_3  weekday_4  weekday_5  weekday_6  
0          0          0          0          1  
1          0          0          0          1  
2          0          0          0          1  
3          0          0          0          1  
4          0          0          0          1  

[5 rows x 59 columns]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org49b43d6" class="outline-2"&gt;
&lt;h2 id="org49b43d6"&gt;Scaling target variables&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org49b43d6"&gt;
&lt;p&gt;
To make training the network easier, we'll standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1.
&lt;/p&gt;

&lt;p&gt;
The scaling factors are saved so we can go backwards when we use the network for predictions.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']
# Store scalings in a dictionary so we can convert back later
scaled_features = {}
for each in quant_features:
    mean, std = data[each].mean(), data[each].std()
    scaled_features[each] = [mean, std]
    data.loc[:, each] = (data[each] - mean)/std
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org0895995" class="outline-2"&gt;
&lt;h2 id="org0895995"&gt;Splitting the data into training, testing, and validation sets&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org0895995"&gt;
&lt;p&gt;
We'll save the data for the last approximately 21 days to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual number of riders.
&lt;/p&gt;


&lt;p&gt;
Save data for approximately the last 21 days. 
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LAST_TWENTY_ONE = -21 * 24 
test_data = data[LAST_TWENTY_ONE:]
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Now remove the test data from the data set .
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = data[:LAST_TWENTY_ONE]
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
Separate the data into features and targets.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;target_fields = ['cnt', 'casual', 'registered']
features, targets = data.drop(target_fields, axis=1), data[target_fields]
test_features, test_targets = (test_data.drop(target_fields, axis=1),
			       test_data[target_fields])
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
We'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we'll train on historical data, then try to predict on future data (the validation set).
&lt;/p&gt;

&lt;p&gt;
Hold out the last 60 days or so of the remaining data as a validation set
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;LAST_SIXTY = -60 * 24
train_features, train_targets = features[:LAST_SIXTY], targets[:LAST_SIXTY]
val_features, val_targets = features[LAST_SIXTY:], targets[LAST_SIXTY:]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgc2093ee" class="outline-2"&gt;
&lt;h2 id="orgc2093ee"&gt;Time to build the network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgc2093ee"&gt;
&lt;p&gt;
Below you'll build your network. We've built out the structure. You'll implement both the forward pass and backwards pass through the network. You'll also set the hyperparameters: the learning rate, the number of hidden units, and the number of training passes.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;graph = Digraph(comment="Neural Network", format="png")
graph.attr(rankdir="LR")

with graph.subgraph(name="cluster_input") as cluster:
    cluster.attr(label="Input")
    cluster.node("a", "")
    cluster.node("b", "")
    cluster.node("c", "")

with graph.subgraph(name="cluster_hidden") as cluster:
    cluster.attr(label="Hidden")
    cluster.node("d", "")
    cluster.node("e", "")
    cluster.node("f", "")
    cluster.node("g", "")

with graph.subgraph(name="cluster_output") as cluster:
    cluster.attr(label="Output")
    cluster.node("h", "")


graph.edges(["ad", "ae", "af", "ag",
	     "bd", "be", "bf", "bg",
	     "cd", "ce", "cf", "cg"])

graph.edges(["dh", 'eh', "fh", "gh"])

graph.render("graphs/network.dot")
graph
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/network.dot.png" alt="network.dot.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
The network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is \(f(x)=x\). A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called &lt;b&gt;forward propagation&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;
We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called &lt;b&gt;backpropagation&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;&lt;b&gt;Hint:&lt;/b&gt;&lt;/b&gt; You'll need the derivative of the output activation function (\(f(x) = x\)) for the backpropagation implementation. If you aren't familiar with calculus, this function is equivalent to the equation \(y = x\). What is the slope of that equation? That is the derivative of \(f(x)\).
&lt;/p&gt;

&lt;p&gt;
Below, you have these tasks:
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Implement the sigmoid function to use as the activation function. Set `self.activation_function` in `__init__` to your sigmoid function.&lt;/li&gt;
&lt;li&gt;Implement the forward pass in the `train` method.&lt;/li&gt;
&lt;li&gt;Implement the backpropagation algorithm in the `train` method, including calculating the output error.&lt;/li&gt;
&lt;li&gt;Implement the forward pass in the `run` method.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
In the my_answers.py file, fill out the TODO sections as specified
&lt;/p&gt;

&lt;pre class="example"&gt;
from my_answers import NeuralNetwork
&lt;/pre&gt;
&lt;/div&gt;
&lt;div id="outline-container-org3706f37" class="outline-3"&gt;
&lt;h3 id="org3706f37"&gt;Mean Squared Error&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org3706f37"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def MSE(y, Y):
    return numpy.mean((y-Y)**2)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4fdbf06" class="outline-2"&gt;
&lt;h2 id="org4fdbf06"&gt;Unit tests&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org4fdbf06"&gt;
&lt;p&gt;
Run these unit tests to check the correctness of your network implementation. This will help you be sure your network was implemented correctly befor you starting trying to train it. These tests must all be successful to pass the project.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;inputs = numpy.array([[0.5, -0.2, 0.1]])
targets = numpy.array([[0.4]])

test_w_i_h = numpy.array([[0.1, -0.2],
			  [0.4, 0.5],
			  [-0.3, 0.2]])
test_w_h_o = numpy.array([[0.3],
			  [-0.1]])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org71de600" class="outline-3"&gt;
&lt;h3 id="org71de600"&gt;The TestMethods Class&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org71de600"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class TestMethods(unittest.TestCase):

    ##########
    # Unit tests for data loading
    ##########

    def test_data_path(self):
	# Test that file path to dataset has been unaltered
	self.assertTrue(data_path.lower() == EXPECTED_DATA_PATH)

    def test_data_loaded(self):
	# Test that data frame loaded
	self.assertTrue(isinstance(rides, pandas.DataFrame))

    ##########
    # Unit tests for network functionality
    ##########

    def test_activation(self):
	network = NeuralNetwork(3, 2, 1, 0.5)
	# Test that the activation function is a sigmoid
	self.assertTrue(numpy.all(network.activation_function(0.5) == 1/(1+numpy.exp(-0.5))))

    def test_train(self):
	# Test that weights are updated correctly on training
	network = NeuralNetwork(3, 2, 1, 0.5)
	network.weights_input_to_hidden = test_w_i_h.copy()
	network.weights_hidden_to_output = test_w_h_o.copy()

	network.train(inputs, targets)
	expected = numpy.array([[ 0.37275328], 
				[-0.03172939]])
	actual = network.weights_hidden_to_output
	self.assertTrue(
	    numpy.allclose(expected, actual),
	    "(weights hidden to output) Expected {} Actual: {}".format(
		expected, actual))
	expected = numpy.array([[ 0.10562014, -0.20185996], 
				[0.39775194, 0.50074398], 
				[-0.29887597, 0.19962801]])
	actual = network.weights_input_to_hidden
	self.assertTrue(
	    numpy.allclose(actual,
			   expected), #, 0.1),
	    "(weights input to hidden) Expected: {} Actual: {}".format(
		expected,
		actual))
	return

    def test_run(self):
	# Test correctness of run method
	network = NeuralNetwork(3, 2, 1, 0.5)
	network.weights_input_to_hidden = test_w_i_h.copy()
	network.weights_hidden_to_output = test_w_h_o.copy()

	self.assertTrue(numpy.allclose(network.run(inputs), 0.09998924))

suite = unittest.TestLoader().loadTestsFromModule(TestMethods())
unittest.TextTestRunner().run(suite)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
.....
----------------------------------------------------------------------
Ran 5 tests in 0.006s

OK

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org48c2294" class="outline-2"&gt;
&lt;h2 id="org48c2294"&gt;Training the network&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org48c2294"&gt;
&lt;p&gt;
Here you'll set the hyperparameters for the network. The strategy here is to find hyperparameters such that the error on the training set is low, but you're not overfitting to the data. If you train the network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. That is, the loss on the validation set will start increasing as the training set loss drops.
&lt;/p&gt;

&lt;p&gt;
You'll also be using a method know as Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently. You'll learn more about SGD later.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6b8f336" class="outline-3"&gt;
&lt;h3 id="org6b8f336"&gt;Choose the number of iterations&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6b8f336"&gt;
&lt;p&gt;
This is the number of batches of samples from the training data we'll use to train the network. The more iterations you use, the better the model will fit the data. However, this process can have sharply diminishing returns and can waste computational resources if you use too many iterations.  You want to find a number here where the network has a low training loss, and the validation loss is at a minimum. The ideal number of iterations would be a level that stops shortly after the validation loss is no longer decreasing.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgec94c87" class="outline-3"&gt;
&lt;h3 id="orgec94c87"&gt;Choose the learning rate&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgec94c87"&gt;
&lt;p&gt;
This scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. Normally a good choice to start at is 0.1; however, if you effectively divide the learning rate by n_records, try starting out with a learning rate of 1. In either case, if the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org288a2f6" class="outline-3"&gt;
&lt;h3 id="org288a2f6"&gt;Choose the number of hidden nodes&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org288a2f6"&gt;
&lt;p&gt;
In a model where all the weights are optimized, the more hidden nodes you have, the more accurate the predictions of the model will be.  (A fully optimized model could have weights of zero, after all.) However, the more hidden nodes you have, the harder it will be to optimize the weights of the model, and the more likely it will be that suboptimal weights will lead to overfitting. With overfitting, the model will memorize the training data instead of learning the true pattern, and won't generalize well to unseen data.  
&lt;/p&gt;

&lt;p&gt;
Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden units is too low, then the model won't have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden units you choose.  You'll generally find that the best number of hidden nodes to use ends up being between the number of input and output nodes.
&lt;/p&gt;

&lt;p&gt;
Set the hyperparameters in you myanswers.py file:
&lt;/p&gt;

&lt;pre class="example"&gt;
from my_answers import iterations, learning_rate, hidden_nodes, output_nodes
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;N_i = train_features.shape[1]
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)
losses = {'train':[], 'validation':[]}
print("Inputs: {}, Hidden: {}, Output: {}, Learning Rate: {}".format(
    N_i,
    hidden_nodes,
    output_nodes,
    learning_rate))
print("Starting {} repetitions".format(iterations))
for iteration in range(iterations):
    # Go through a random batch of 128 records from the training data set
    batch = numpy.random.choice(train_features.index, size=128)
    X, y = train_features.loc[batch].values, train_targets.loc[batch]['cnt']

    network.train(X, y)

    # Printing out the training progress
    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)
    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)
    if not iteration % 500:
	sys.stdout.write("\nProgress: {:2.1f}".format(100 * iteration/iterations)
			 + "% ... Training loss: " 
			 + "{:.5f}".format(train_loss)
			 + " ... Validation loss: {:.5f}".format(val_loss))
	sys.stdout.flush()

    losses['train'].append(train_loss)
    losses['validation'].append(val_loss)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4
Starting 7500 repetitions

Progress: 0.0% ... Training loss: 1.09774 ... Validation loss: 1.74283
Progress: 6.7% ... Training loss: 0.27687 ... Validation loss: 0.44356
Progress: 13.3% ... Training loss: 0.24134 ... Validation loss: 0.42289
Progress: 20.0% ... Training loss: 0.20681 ... Validation loss: 0.38749
Progress: 26.7% ... Training loss: 0.16536 ... Validation loss: 0.31655
Progress: 33.3% ... Training loss: 0.13105 ... Validation loss: 0.25414
Progress: 40.0% ... Training loss: 0.10072 ... Validation loss: 0.21108
Progress: 46.7% ... Training loss: 0.08929 ... Validation loss: 0.18401
Progress: 53.3% ... Training loss: 0.07844 ... Validation loss: 0.16669
Progress: 60.0% ... Training loss: 0.07380 ... Validation loss: 0.15336
Progress: 66.7% ... Training loss: 0.07580 ... Validation loss: 0.18654
Progress: 73.3% ... Training loss: 0.06308 ... Validation loss: 0.15848
Progress: 80.0% ... Training loss: 0.06632 ... Validation loss: 0.17960
Progress: 86.7% ... Training loss: 0.05954 ... Validation loss: 0.15988
Progress: 93.3% ... Training loss: 0.05809 ... Validation loss: 0.16016
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time")
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/losses.png" alt="losses.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgfdef998" class="outline-2"&gt;
&lt;h2 id="orgfdef998"&gt;Check out your predictions&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgfdef998"&gt;
&lt;p&gt;
Here, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fig, axe = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
axe.plot(expected, '.', label='Data')
axe.plot(expected, linestyle="--", color="tab:blue", label=None)
axe.plot(predictions,linestyle="--", color="tab:orange", label=None)
axe.plot(predictions, ".", label='Prediction')
axe.set_xlim(right=len(predictions))
legend = axe.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
axe.set_xticks(numpy.arange(len(dates))[12::24])
_ = axe.set_xticklabels(dates[12::24], rotation=45)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/count.png" alt="count.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org05da6a4" class="outline-3"&gt;
&lt;h3 id="org05da6a4"&gt;How well does the model predict the data?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org05da6a4"&gt;
&lt;p&gt;
It looks like it does better initially and then over-predicts the peaks later on.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org01d43c0" class="outline-3"&gt;
&lt;h3 id="org01d43c0"&gt;Where does it fail?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org01d43c0"&gt;
&lt;p&gt;
It doesn't anticipate the drop-off in ridershp as the holidays come around.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org08a5630" class="outline-3"&gt;
&lt;h3 id="org08a5630"&gt;Why does it fail where it does?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org08a5630"&gt;
&lt;p&gt;
Although there might be holidays noted (at least for Christmas), it probably isn't reflecting the extreme change in behavior that the holidays bring about in the United States.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga0d946d" class="outline-2"&gt;
&lt;h2 id="orga0d946d"&gt;More Variations&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orga0d946d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def train_this(hidden_nodes:int, learning_rate:float,
	       output_nodes:int=1, 
	       input_nodes: int=N_i,
	       repetitions: int=100,
	       emit: bool=True):
    """Trains the network using the given values

    Args:
     hidden_nodes: number of nodes in the hidden layer
     learning_rate: amount to change the weights during backpropagation
     output_nodes: number of nodes in the output layer
     input_nodes: number of nodes in the input layer
     repetitions: number of times to train the model
     emit: print information

    Returns:
     test error, losses: MSE against test, dict of losses
    """
    network = NeuralNetwork(input_nodes, hidden_nodes, output_nodes,
			    learning_rate)
    losses = {'train':[], 'validation':[]}
    last_validation_loss = -1
    if emit:        
	print(
	    ("Inputs: {}, Hidden: {}, Output: {}, Learning Rate: {}, "
	     "Repetitions: {}").format(
		 input_nodes,
		 hidden_nodes,
		 output_nodes,
		 learning_rate, 
		 repetitions))
    reported = False
    for iteration in range(repetitions):
	# Go through a random batch of 128 records from the training data set
	batch = numpy.random.choice(train_features.index, size=128)
	X, y = (train_features.iloc[batch].values,
		train_targets.iloc[batch]['cnt'])
	network.train(X, y)

	train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)
	val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)
	losses['train'].append(train_loss)
	losses['validation'].append(val_loss)
	if last_validation_loss == -1:
	    last_validation_loss = val_loss[0] 
	if val_loss[0] &amp;gt; last_validation_loss and not reported:
	    reported = True
	    if emit:
		print("Repetition {} Validation Loss went up by {}".format(
		iteration + 1,
		val_loss[0] - last_validation_loss))
	last_validation_loss = val_loss[0]

    predictions = network.run(test_features)
    expected = (test_targets['cnt']).values
    test_error = MSE(predictions.T, expected)[0]
    if emit:
	print(("Training Error: {:.5f}, "
	       "Validation Error: {:.5f}, "
	       "Test Error: {:.2f}").format(
		   losses["train"][-1][0],
		   losses["validation"][-1][0],
		   test_error))
    return test_error, losses, network
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Parameters = namedtuple(
    "Parameters",
    "hidden_nodes learning_rate trials losses test_error network".split())
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def grid_search(hidden_nodes: list, learning_rates: list, trials: list,
		max_train_error = 0.09, max_validation_error=0.18,
		emit_training:bool=False):
    """does a search for the best parameters

    Args:
     hidden_nodes: list of number of hidden nodes
     learning rates: list of how much to update the weights
     trials: list of number of times to train
     max_train_error: upper ceiling for training error
     max_validation_error: upper ceilining for acceptable validation error
     emit_training: print the statements during training
    """
    best = 1000
    if not type(trials) is list:
	trials = [trials]
    for node_count in hidden_nodes:
	for rate in learning_rates:
	    for trial in trials:
		test_error, losses, network = train_this(node_count, rate,
							 repetitions=trial,
							 emit=emit_training)
		if test_error &amp;lt; best:
		    print("New Best: {:.2f} (Hidden: {}, Learning Rate: {:.2f})".format(
			test_error,
			node_count,
			rate))
		    best = test_error
		    best_parameters = Parameters(hidden_nodes=node_count,
						 learning_rate=rate,
						 trials=trials,
						 losses=losses,
						 test_error=test_error,
						 network=network)
		print()
    return best_parameters
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameters = grid_search(
    hidden_nodes=[14, 28, 42, 56],
    learning_rates=[0.1, 0.01, 0.001],
    trials=200)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
New Best: 0.53 (Hidden: 14, Learning Rate: 0.10)



New Best: 0.47 (Hidden: 28, Learning Rate: 0.10)



New Best: 0.44 (Hidden: 42, Learning Rate: 0.10)



New Best: 0.42 (Hidden: 56, Learning Rate: 0.10)



&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameters = grid_search([42, 56], [0.1, 0.01, 0.001], trials=300)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
New Best: 0.45 (Hidden: 42, Learning Rate: 0.10)



New Best: 0.42 (Hidden: 56, Learning Rate: 0.10)




&lt;/pre&gt;

&lt;p&gt;
So, I wouldn't have guessed it, but the 56 node model does best with a reasonably large learning rate.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameters = grid_search([56, 112], [0.1, 0.2], trials=200)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
New Best: 0.44 (Hidden: 56, Learning Rate: 0.10)

New Best: 0.41 (Hidden: 56, Learning Rate: 0.20)




&lt;/pre&gt;

&lt;p&gt;
Weird.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameters = grid_search([56], [0.2, 0.3], trials=300, emit_training=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 300
Repetition 2 Validation Loss went up by 1.2119413344400733
Training Error: 0.42973, Validation Error: 0.71298, Test Error: 0.36
New Best: 0.36 (Hidden: 56, Learning Rate: 0.20)

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.3, Repetitions: 300
Repetition 2 Validation Loss went up by 47.942730166612094
Training Error: 0.69836, Validation Error: 1.20312, Test Error: 0.63


&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time")
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
losses = parameters.losses
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/better_losses.png" alt="better_losses.png"&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;
It looks like going over 100 doesn't really help the model a lot, or at all, really.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;parameters = grid_search([56], [0.2, 0.3], trials=300, emit_training=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 300
Repetition 2 Validation Loss went up by 0.29155647125413564
Training Error: 0.46915, Validation Error: 0.77756, Test Error: 0.36
New Best: 0.36 (Hidden: 56, Learning Rate: 0.20)

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.3, Repetitions: 300
Repetition 2 Validation Loss went up by 68.10510030432465
Training Error: 0.63604, Validation Error: 1.07500, Test Error: 0.58


&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([56], [0.2], 2000)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
New Best: 0.27 (Hidden: 56, Learning Rate: 0.20)

Elapsed: 0:02:20.654989

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([56], [0.2], 1000)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
New Best: 0.29 (Hidden: 56, Learning Rate: 0.20)

Elapsed: 0:01:51.175404

&lt;/pre&gt;

&lt;p&gt;
I just checked the rubric and you need a training loss below 0.09 and a validation loss below 0.18, regardless of the test loss.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28, 42, 56], [0.01, 0.1, 0.2], 100, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 10 Validation Loss went up by 0.0005887216742490597
Training Error: 0.92157, Validation Error: 1.36966, Test Error: 0.69
New Best: 0.69 (Hidden: 28, Learning Rate: 0.01)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 7 Validation Loss went up by 0.06008857123483735
Training Error: 0.65584, Validation Error: 1.09581, Test Error: 0.52
New Best: 0.52 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 4 Validation Loss went up by 0.010135891697978794
Training Error: 0.61391, Validation Error: 0.99344, Test Error: 0.47
New Best: 0.47 (Hidden: 28, Learning Rate: 0.20)

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 2 Validation Loss went up by 0.0016900520112179684
Training Error: 0.87851, Validation Error: 1.31872, Test Error: 0.66

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 3 Validation Loss went up by 0.08058311852052547
Training Error: 0.66637, Validation Error: 1.09371, Test Error: 0.53

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 3 Validation Loss went up by 0.08181869722819135
Training Error: 0.60174, Validation Error: 0.99664, Test Error: 0.47

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 4 Validation Loss went up by 0.0015646480595301604
Training Error: 0.93732, Validation Error: 1.36061, Test Error: 0.72

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.03097966349747283
Training Error: 0.67087, Validation Error: 1.07306, Test Error: 0.51

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 2 Validation Loss went up by 9.947099886932289
Training Error: 0.65815, Validation Error: 1.17712, Test Error: 0.52

Elapsed: 0:00:47.842652
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.01, 0.1, 0.2], 1000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.01, Repetitions: 1000
Repetition 2 Validation Loss went up by 0.002750823237845479
Training Error: 0.71460, Validation Error: 1.28385, Test Error: 0.59
New Best: 0.59 (Hidden: 28, Learning Rate: 0.01)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 1000
Repetition 5 Validation Loss went up by 0.09885352252565549
Training Error: 0.31086, Validation Error: 0.48591, Test Error: 0.33
New Best: 0.33 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 1000
Repetition 2 Validation Loss went up by 0.09560269140958688
Training Error: 0.28902, Validation Error: 0.44905, Test Error: 0.33

Elapsed: 0:01:59.136160
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.1, 0.2], 2000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 2000
Repetition 2 Validation Loss went up by 0.06973195973646384
Training Error: 0.28625, Validation Error: 0.45083, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 2000
Repetition 3 Validation Loss went up by 0.037295970545350166
Training Error: 0.26864, Validation Error: 0.43831, Test Error: 0.32

Elapsed: 0:02:35.122622
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.05], 4000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 4000
Repetition 4 Validation Loss went up by 0.039021584745929205
Training Error: 0.27045, Validation Error: 0.44738, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.10)

Elapsed: 0:02:36.990482

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.2], 5000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 5000
Repetition 4 Validation Loss went up by 0.32617394730848104
Training Error: 0.18017, Validation Error: 0.32432, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.20)

Elapsed: 0:03:05.664176

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.2, 0.3], 6000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 6000
Repetition 3 Validation Loss went up by 0.18572519005609722
Training Error: 0.22969, Validation Error: 0.38789, Test Error: 0.35
New Best: 0.35 (Hidden: 28, Learning Rate: 0.20)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 6000
Repetition 3 Validation Loss went up by 1.6850265407570482
Training Error: 0.08168, Validation Error: 0.20003, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.30)

Elapsed: 0:07:30.082137
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.3, 0.4], 7000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 7000
Repetition 3 Validation Loss went up by 0.4652683795507646
Training Error: 0.07100, Validation Error: 0.19299, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.30)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 7000
Repetition 2 Validation Loss went up by 8.612689644792866
Training Error: 0.05771, Validation Error: 0.19188, Test Error: 0.22
New Best: 0.22 (Hidden: 28, Learning Rate: 0.40)

Elapsed: 0:09:06.729922
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.4, 0.5], 7500, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 7500
Repetition 3 Validation Loss went up by 3.6207932112624386
Training Error: 0.05942, Validation Error: 0.13644, Test Error: 0.16
New Best: 0.16 (Hidden: 28, Learning Rate: 0.40)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.5, Repetitions: 7500
Repetition 2 Validation Loss went up by 4.101532160572686
Training Error: 0.05710, Validation Error: 0.14214, Test Error: 0.22

Elapsed: 0:09:56.116403
&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.4], 8000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 8000
Repetition 2 Validation Loss went up by 0.6450181997021212
Training Error: 0.05479, Validation Error: 0.14289, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.40)

Elapsed: 0:05:18.546979

&lt;/pre&gt;

&lt;p&gt;
That did worse so it probably overtrains at the 0.4 learning rate. What about 0.3?
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;start = datetime.now()
parameters = grid_search([28], [0.3], 8000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 8000
Repetition 2 Validation Loss went up by 0.3336478297258907
Training Error: 0.06670, Validation Error: 0.16918, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.30)

Elapsed: 0:05:06.761537

&lt;/pre&gt;

&lt;p&gt;
So this did worse than a learning rate of 0.5 at 7500 and much worse than 0.4 at 7500, so maybe that is the optimal (0.4 at 7,500) I'm chasing. It seems king of arbitrary, but it works for the assignment.
&lt;/p&gt;

&lt;p&gt;
The submission is timing out for some reason (it only takes 5 minutes to run but the error says it took more than 7 minutes). I might have to try some compromise runtime.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time (Hidden: {} Learning Rate: {})".format(parameters.hidden_nodes, parameters.learning_rate))
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
losses = parameters.losses
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/found_losses.png" alt="found_losses.png"&gt;
&lt;/p&gt;
&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fig, ax = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = parameters.network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
ax.plot(expected, '.', label='Data')
ax.plot(predictions.values, ".", label='Prediction')
ax.set_xlim(right=len(predictions))
legend = ax.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
ax.set_xticks(numpy.arange(len(dates))[12::24])
_ = ax.set_xticklabels(dates[12::24], rotation=45)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/best_count.png" alt="best_count.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge30bed8" class="outline-3"&gt;
&lt;h3 id="orge30bed8"&gt;How well does the model predict the data?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge30bed8"&gt;
&lt;p&gt;
The model seems to not be able to capture all the variations in the data. I don't think it really did well at all.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4600f83" class="outline-3"&gt;
&lt;h3 id="org4600f83"&gt;Where does it fail?&lt;/h3&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgce896ea" class="outline-3"&gt;
&lt;h3 id="orgce896ea"&gt;Why does it fail where it does?&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgce896ea"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;better_network = train_this(parameters.hidden_nodes, parameters.learning_rate,
			    repetitions=150, emit=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 19, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 43 Validation Loss went up by 0.0008309723799344582
Training Error: 0.92939, Validation Error: 1.44647, Test Error: 0.62

&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;more_parameters = grid_search([10, 20],
				[0.1, 0.01],
				trials=list(range(50, 225, 25)),
				emit_training=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 50
Repetition 10 Validation Loss went up by 0.013827968028145676
Training Error: 0.92764, Validation Error: 1.45833, Test Error: 0.64
New Best: 0.64 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 75
Repetition 11 Validation Loss went up by 0.01268617480459655
Training Error: 0.95884, Validation Error: 1.45576, Test Error: 0.67

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.004302293010363778
Training Error: 0.96904, Validation Error: 1.32713, Test Error: 0.74

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 125
Repetition 2 Validation Loss went up by 0.00011567129175471536
Training Error: 0.96480, Validation Error: 1.38193, Test Error: 0.70

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 150
Repetition 2 Validation Loss went up by 0.00525072377638347
Training Error: 0.95649, Validation Error: 1.29823, Test Error: 0.79

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 175
Repetition 6 Validation Loss went up by 0.0184187066638537
Training Error: 0.94033, Validation Error: 1.48648, Test Error: 0.64
New Best: 0.64 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 200
Repetition 4 Validation Loss went up by 0.006479207709029211
Training Error: 0.96348, Validation Error: 1.38834, Test Error: 0.67

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 50
Repetition 8 Validation Loss went up by 0.00034037805450659597
Training Error: 0.99148, Validation Error: 1.50023, Test Error: 0.71

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 75
Repetition 26 Validation Loss went up by 7.26803968935652e-05
Training Error: 0.94736, Validation Error: 1.42677, Test Error: 0.69

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 55 Validation Loss went up by 0.0005170583384894734
Training Error: 0.92258, Validation Error: 1.52912, Test Error: 0.64

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 125
Repetition 14 Validation Loss went up by 4.7182200476170166e-05
Training Error: 0.96801, Validation Error: 1.43898, Test Error: 0.69

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 25 Validation Loss went up by 0.00025169631184263075
Training Error: 0.94769, Validation Error: 1.28473, Test Error: 0.79

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 175
Repetition 2 Validation Loss went up by 0.0011783405140612935
Training Error: 0.95784, Validation Error: 1.38248, Test Error: 0.75

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 200
Repetition 24 Validation Loss went up by 6.126094364189427e-05
Training Error: 0.95839, Validation Error: 1.29311, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 50
Repetition 4 Validation Loss went up by 0.04115757797958697
Training Error: 0.97326, Validation Error: 1.42168, Test Error: 0.72

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 75
Repetition 3 Validation Loss went up by 0.004544958155855872
Training Error: 0.97079, Validation Error: 1.38326, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.004455074996461583
Training Error: 0.95492, Validation Error: 1.29813, Test Error: 0.80

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 125
Repetition 3 Validation Loss went up by 0.038516428472006314
Training Error: 0.96063, Validation Error: 1.41134, Test Error: 0.68

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 150
Repetition 4 Validation Loss went up by 0.005238556857821708
Training Error: 0.96022, Validation Error: 1.58371, Test Error: 0.67

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 175
Repetition 4 Validation Loss went up by 0.03291822053421889
Training Error: 0.95138, Validation Error: 1.39820, Test Error: 0.67

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 200
Repetition 2 Validation Loss went up by 0.01721971957045554
Training Error: 0.96676, Validation Error: 1.33802, Test Error: 0.72

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 50
Repetition 2 Validation Loss went up by 0.0016568049054144218
Training Error: 0.90793, Validation Error: 1.36444, Test Error: 0.78

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 75
Repetition 24 Validation Loss went up by 1.0669002188823384e-05
Training Error: 0.93910, Validation Error: 1.43277, Test Error: 0.61
New Best: 0.61 (Hidden: 20, Learning Rate: 0.01)

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 13 Validation Loss went up by 0.0006819932351833646
Training Error: 0.95953, Validation Error: 1.30219, Test Error: 0.79

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 125
Repetition 2 Validation Loss went up by 0.001983487952677443
Training Error: 0.96503, Validation Error: 1.34180, Test Error: 0.76

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 2 Validation Loss went up by 0.0014568870668274503
Training Error: 0.95968, Validation Error: 1.34224, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 175
Repetition 22 Validation Loss went up by 0.0009898893960744726
Training Error: 0.94891, Validation Error: 1.25502, Test Error: 0.81

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 200
Repetition 24 Validation Loss went up by 0.0003693045874550993
Training Error: 0.96222, Validation Error: 1.29511, Test Error: 0.75

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;best_parameters = grid_search([10, 20, 30, 40], [0.1, 0.01], 100, True)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.006104362102346439
Training Error: 0.96812, Validation Error: 1.32203, Test Error: 0.76
New Best: 0.76 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 16 Validation Loss went up by 0.00020577471034299855
Training Error: 0.96607, Validation Error: 1.30376, Test Error: 0.79

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.018123806353317784
Training Error: 0.95858, Validation Error: 1.28351, Test Error: 0.76
New Best: 0.76 (Hidden: 20, Learning Rate: 0.10)

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 29 Validation Loss went up by 0.0029168059867459295
Training Error: 0.94895, Validation Error: 1.45710, Test Error: 0.66
New Best: 0.66 (Hidden: 20, Learning Rate: 0.01)

Inputs: 56, Hidden: 30, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.045063972993018675
Training Error: 0.96892, Validation Error: 1.50312, Test Error: 0.69

Inputs: 56, Hidden: 30, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 14 Validation Loss went up by 7.233598065781166e-05
Training Error: 0.96606, Validation Error: 1.32913, Test Error: 0.80

Inputs: 56, Hidden: 40, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 3 Validation Loss went up by 0.05076336638491519
Training Error: 0.95920, Validation Error: 1.45843, Test Error: 0.68

Inputs: 56, Hidden: 40, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 7 Validation Loss went up by 0.0006550737027899434
Training Error: 0.97148, Validation Error: 1.36548, Test Error: 0.79

&lt;/pre&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fig, axe = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = best_parameters.network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
axe.set_title("{} Hidden and Learning Rate: {}".format(best_parameters.hidden_nodes,
						       best_parameters.learning_rate))
axe.plot(expected, '.', label='Data')
axe.plot(predictions.values, ".", label='Prediction')
axe.set_xlim(right=len(predictions))
legend = axe.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
axe.set_xticks(numpy.arange(len(dates))[12::24])
_ = axe.set_xticklabels(dates[12::24], rotation=45)
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="figure"&gt;
&lt;p&gt;&lt;img src="https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/even_bester_count.png" alt="even_bester_count.png"&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>neural networks</category><category>project</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/nano/bike-sharing/the-bike-sharing-project/</guid><pubDate>Tue, 30 Oct 2018 20:34:56 GMT</pubDate></item></channel></rss>