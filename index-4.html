<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>In Too Deep (old posts, page 4) | In Too Deep</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/index-4.html" rel="canonical">
<link href="index-5.html" rel="prev" type="text/html">
<link href="index-3.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/training-neural-networks/">Training Neural Networks</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/training-neural-networks/" rel="bookmark"><time class="published dt-published" datetime="2018-11-19T16:05:52-08:00" itemprop="datePublished" title="2018-11-19 16:05">2018-11-19 16:05</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org368a743">Introduction</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org479adb9">Backpropagation</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org0445067">Losses in PyTorch</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orgdf9c527">Imports</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org0388b1f">The Network</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org5cf3096">The Network</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orgf5a1061">Network 2 (with Log Softmax)</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org9dada11">On To Autograd</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orgaef9d0d">Loss and Autograd together</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#orga5a0ab4">Training the Network</a></li>
<li><a href="/posts/nano/pytorch/training-neural-networks/#org6972f09">Training (For Real This Time)</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org368a743">
<h2 id="org368a743">Introduction</h2>
<div class="outline-text-2" id="text-org368a743">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>The network we built in the previous part isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.</p>
<p>At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.</p>
<p>To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a <b>loss function</b> (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems</p>
<p>\[ \large \ell = \frac{1}{2n}\sum_i^n{\left(y_i - \hat{y}_i\right)^2} \]</p>
<p>where \(n\) is the number of training examples, \(y_i\) are the true labels, and \(\hat{y}_i\) are the predicted labels.</p>
<p>By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called <b>gradient descent</b>. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org479adb9">
<h2 id="org479adb9">Backpropagation</h2>
<div class="outline-text-2" id="text-org479adb9">
<p>For single layer networks, gradient descent is straightforward to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks.</p>
<p>Training multilayer networks is done through <b>backpropagation</b> which is really just an application of the chain rule from calculus. It's easiest to understand if we think of our two layer network as a graph representation.</p>
</div>
<div class="outline-3" id="outline-container-org548540c">
<h3 id="org548540c">The Forward Pass</h3>
<div class="outline-text-3" id="text-org548540c">
<p>In the forward pass through the network, our data and operations go from bottom to top. We pass the input \(x\) through a linear transformation \(L_1\) with weights \(W_1\) and biases \(b_1\). The output then goes through the sigmoid operation \(S\) and another linear transformation \(L_2\). Finally we calculate the loss \(\ell\). We use the loss as a measure of how bad the network's predictions are. The goal then is to adjust the weights and biases to minimize the loss.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org413a96a">
<h3 id="org413a96a">Backwards Pass</h3>
<div class="outline-text-3" id="text-org413a96a">
<p>To train the weights with gradient descent, we propagate the gradient of the loss backwards through the network. Each operation has some gradient between the inputs and outputs. As we send the gradients backwards, we multiply the incoming gradient with the gradient for the operation. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.</p>
<p>\[ \large \frac{\partial \ell}{\partial W_1} = \frac{\partial L_1}{\partial W_1} \frac{\partial S}{\partial L_1} \frac{\partial L_2}{\partial S} \frac{\partial \ell}{\partial L_2} \]</p>
<p>We update our weights using this gradient with some learning rate \(\alpha\).</p>
<p>\[ \large W^\prime_1 = W_1 - \alpha \frac{\partial \ell}{\partial W_1} \]</p>
<p>The learning rate \(\alpha\) is set such that the weight update steps are small enough that the iterative method settles in a minimum.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0445067">
<h2 id="org0445067">Losses in PyTorch</h2>
<div class="outline-text-2" id="text-org0445067">
<p>Let's start by seeing how we calculate the loss with PyTorch. Through the <code>nn</code> module, PyTorch provides losses such as the cross-entropy loss (<a href="https://pytorch.org/docs/stable/nn.html#crossentropyloss"><code>nn.CrossEntropyLoss</code></a>). You'll usually see the loss assigned to <code>criterion</code>. As noted in the last part, with a classification problem such as MNIST, we're using the softmax function to predict class probabilities. With a softmax output, you want to use cross-entropy as the loss. To actually calculate the loss, you first define the criterion then pass in the output of your network and the correct labels.</p>
<p>There is something really important to note here. Looking at <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss">the documentation for <code>nn.CrossEntropyLoss</code></a>:</p>
<blockquote>
<p>This criterion combines <a href="https://pytorch.org/docs/stable/nn.html?highlight=logsoftmax#torch.nn.LogSoftmax"><code>nn.LogSoftmax()</code></a> and <a href="https://pytorch.org/docs/stable/nn.html#nllloss"><code>nn.NLLLoss()</code></a> in one single class.</p>
<p>The input is expected to contain scores for each class.</p>
</blockquote>
<p>This means we need to pass in the raw output of our network into the loss, not the output of the softmax function. This raw output is usually called the <b>logits</b> or <b>scores</b>. We use the logits because softmax gives you probabilities which will often be very close to zero or one but floating-point numbers can't accurately represent values near zero or one (<a href="https://docs.python.org/3/tutorial/floatingpoint.html">read more here</a>). It's usually best to avoid doing calculations with probabilities, typically we use log-probabilities.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgdf9c527">
<h2 id="orgdf9c527">Imports</h2>
<div class="outline-text-2" id="text-orgdf9c527"></div>
<div class="outline-3" id="outline-container-org3d66533">
<h3 id="org3d66533">From Python</h3>
<div class="outline-text-3" id="text-org3d66533">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org6c0b5a0">
<h3 id="org6c0b5a0">From PyPi</h3>
<div class="outline-text-3" id="text-org6c0b5a0">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2a6388a">
<h3 id="org2a6388a">The Udacity Repository</h3>
<div class="outline-text-3" id="text-org2a6388a">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">nano.pytorch</span> <span class="kn">import</span> <span class="n">helper</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9c01039">
<h3 id="org9c01039">Plotting</h3>
<div class="outline-text-3" id="text-org9c01039">
<div class="highlight">
<pre><span></span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0388b1f">
<h2 id="org0388b1f">The Network</h2>
<div class="outline-text-2" id="text-org0388b1f"></div>
<div class="outline-3" id="outline-container-orgae9c4ca">
<h3 id="orgae9c4ca">Define a Transform</h3>
<div class="outline-text-3" id="text-orgae9c4ca">
<p>We are going to create a pipeline to normalize the data. The argument for <code>Normalize</code> are a tuple of means and a tuple of standard-deviations. You use tuples because you need to pass in a value for each of the color channels.</p>
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
                                                     <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)),</span>
                              <span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0f82959">
<h3 id="org0f82959">The Data</h3>
<div class="outline-text-3" id="text-org0f82959">
<p>Once again we're going to use the <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> data-set. It's important to use the same output folder as the last time or you will end up downloading a new copy of the dataset.</p>
<div class="highlight">
<pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">'~/datasets/MNIST/'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">digits</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5cf3096">
<h2 id="org5cf3096">The Network</h2>
<div class="outline-text-2" id="text-org5cf3096">
<p>We're going to build a feed-forward network using the pipeline-style of network definition and then pass in a batch of image to examine the loss.</p>
</div>
<div class="outline-3" id="outline-container-orgeb9169a">
<h3 id="orgeb9169a">Some Constants</h3>
<div class="outline-text-3" id="text-orgeb9169a">
<p>These are the hyperparameters for our model. The number if inputs is the number of pixels in the images. The number of outputs is the number of digits (so 10).</p>
<div class="highlight">
<pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="mi">28</span><span class="o">**</span><span class="mi">2</span>
<span class="n">hidden_nodes_1</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">hidden_nodes_2</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
<p>Since this gets used way further down I'm going to make a namespace for it so (maybe) it'll be easier to remember where the values are from.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">HyperParameters</span><span class="p">:</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="mi">28</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">hidden_nodes_1</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">hidden_nodes_2</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.003</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga50a96d">
<h3 id="orga50a96d">The Model</h3>
<div class="outline-text-3" id="text-orga50a96d">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="n">input_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_nodes_1</span><span class="p">),</span>
        <span class="n">relu_1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_nodes_1</span><span class="p">,</span> <span class="n">hidden_nodes_2</span><span class="p">),</span>
        <span class="n">relu_2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_output</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_nodes_2</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5407587">
<h3 id="org5407587">The Loss</h3>
<div class="outline-text-3" id="text-org5407587">
<p>We're going to use <code>CrossEntropyLoss</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga8ec397">
<h3 id="orga8ec397">The Images</h3>
<div class="outline-text-3" id="text-orga8ec397">
<p>We're going to pull the next (first) batch of images and reshape it.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
torch.Size([64, 1, 28, 28])

</pre>
<p>This will flatten the images.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
torch.Size([64, 784])

</pre>
<p>So, that one isn't so obvious, but when the <code>view</code> method gets passed a <code>-1</code> it interprets it as meaning you want to flatten the tensor. In this case we passed in the number of rows so it just reduces the other dimensions to columns. It kind of seems like you lose a column in there somewhere…</p>
</div>
</div>
<div class="outline-3" id="outline-container-org3f331f4">
<h3 id="org3f331f4">One Pass</h3>
<div class="outline-text-3" id="text-org3f331f4">
<p>We're going to pass our model the images to make a single forward pass and get the <a href="https://en.wikipedia.org/wiki/Logit">logits</a> for them.</p>
<div class="highlight">
<pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</pre></div>
<p>Now we'll calculate our model's loss with the logits and the labels.</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor(2.3135, grad_fn=&lt;NllLossBackward&gt;)

</pre>
<p>According to the original author of this exercise</p>
<blockquote>
<p>…it's more convenient to build the model with a log-softmax output using <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax"><code>nn.LogSoftmax</code></a> or <code>F.log_softmax</code>. Then you can get the actual probabilities by taking the exponential <code>torch.exp(output)</code>. With a log-softmax output, you want to use the negative log likelihood loss, <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss"><code>nn.NLLLoss</code></a>.</p>
</blockquote>
<p>Build a model that returns the log-softmax as the output and calculate the loss using the negative log likelihood loss. Note that for <code>nn.LogSoftmax</code> and <code>F.log_softmax</code> you'll need to set the <code>dim</code> keyword argument appropriately. <code>dim=0</code> calculates softmax across the rows, so each column sums to 1, while <code>dim=1</code> calculates across the columns so each row sums to 1. Think about what you want the output to be and choose <code>dim</code> appropriately.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgf5a1061">
<h2 id="orgf5a1061">Network 2 (with Log Softmax)</h2>
<div class="outline-text-2" id="text-orgf5a1061">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="n">input_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_nodes_1</span><span class="p">),</span>
        <span class="n">relu_1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_nodes_1</span><span class="p">,</span> <span class="n">hidden_nodes_2</span><span class="p">),</span>
        <span class="n">relu_2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_output</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_nodes_2</span><span class="p">,</span> <span class="n">outputs</span><span class="p">),</span>
        <span class="n">log_softmax</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
<p>And now our loss.</p>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span> 
</pre></div>
<p>Now we get the next batch of images.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
</pre></div>
<p>And once again we flatten them.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>A forward pass on the batch.</p>
<div class="highlight">
<pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</pre></div>
<p>Calculate the loss with the logits and the labels</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor(2.3208, grad_fn=&lt;NllLossBackward&gt;)

</pre>
<p>So that's interesting, but what does it mean?</p>
</div>
</div>
<div class="outline-2" id="outline-container-org9dada11">
<h2 id="org9dada11">On To Autograd</h2>
<div class="outline-text-2" id="text-org9dada11">
<p>Now that we know how to calculate a loss, how do we use it to perform backpropagation? Torch provides a module, <code>autograd</code>, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set <code>requires_grad = True</code> on a tensor. You can do this at creation with the <code>requires_grad</code> keyword, or at any time with <code>x.requires_grad_(True)</code>.</p>
<p>You can turn off gradients for a block of code with the <code>torch.no_grad()</code> content:</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="o">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
<span class="bp">False</span>
</pre></div>
<p>Also, you can turn on or off gradients altogether with <code>torch.set_grad_enabled(True|False)</code>.</p>
<p>The gradients are computed with respect to some variable <code>z</code> with <code>z.backward()</code>. This does a backward pass through the operations that created <code>z</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([[-0.7567, -0.2352],
        [-0.9346,  0.3097]], requires_grad=True)

</pre>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([[0.5726, 0.0553],
        [0.8735, 0.0959]], grad_fn=&lt;PowBackward0&gt;)

</pre>
<p>We can see the operation that created <code>y</code>, a power operation <code>PowBackward0</code>.</p>
<p><code>grad_fn</code> shows the function that generated this variable</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</pre></div>
<pre class="example">
&lt;PowBackward0 object at 0x7f591c505c50&gt;

</pre>
<p>The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor <code>y</code> to a scalar value, the mean.</p>
<div class="highlight">
<pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor(0.3993, grad_fn=&lt;MeanBackward1&gt;)

</pre>
<p>You can check the gradients for <code>x</code> and <code>y</code> but they are empty currently.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
<pre class="example">
None

</pre>
<p>To calculate the gradients, you need to run the <code>.backward</code> method on a Variable, <code>z</code> for example. This will calculate the gradient for <code>z</code> with respect to <code>x</code></p>
<p>\[ \frac{\partial z}{\partial x} = \frac{\partial}{\partial x}\left[\frac{1}{n}\sum_i^n x_i^2\right] = \frac{x}{2} \]</p>
<div class="highlight">
<pre><span></span><span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([[-0.3783, -0.1176],
        [-0.4673,  0.1548]])
tensor([[-0.3783, -0.1176],
        [-0.4673,  0.1548]], grad_fn=&lt;DivBackward0&gt;)

</pre>
<p>These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. Once we have the gradients we can make a gradient descent step.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgaef9d0d">
<h2 id="orgaef9d0d">Loss and Autograd together</h2>
<div class="outline-text-2" id="text-orgaef9d0d">
<p>When we create a network with PyTorch, all of the parameters are initialized with <code>requires_grad = True</code>. This means that when we calculate the loss and call <code>loss.backward()</code>, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. Below you can see an example of calculating the gradients using a backwards pass.</p>
<p>Get the next batch.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>Now get the logits and loss for the batch.</p>
<div class="highlight">
<pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
<p>This is what the weights from the input layer to the first hidden layer look like before and after the backward-pass.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Before backward pass: </span><span class="se">\n</span><span class="s1">{}</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">))</span>

<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s1">'After backward pass: </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
<pre class="example">
Before backward pass: 
None

After backward pass: 
 tensor([[ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],
        [ 0.0011,  0.0011,  0.0011,  ...,  0.0011,  0.0011,  0.0011],
        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004],
        ...,
        [ 0.0001,  0.0001,  0.0001,  ...,  0.0001,  0.0001,  0.0001],
        [ 0.0003,  0.0003,  0.0003,  ...,  0.0003,  0.0003,  0.0003],
        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005]])
</pre></div>
</div>
<div class="outline-2" id="outline-container-orga5a0ab4">
<h2 id="orga5a0ab4">Training the Network</h2>
<div class="outline-text-2" id="text-orga5a0ab4">
<p>There's one last piece we need to start training, an optimizer that we'll use to update the weights with the gradients. We get these from PyTorch's <a href="https://pytorch.org/docs/stable/optim.html"><code>optim</code> package</a>(). For example we can use stochastic gradient descent with <code>optim.SGD</code>. You can see how to define an optimizer below.</p>
<p>Optimizers require the parameters to optimize and a learning rate.</p>
<div class="highlight">
<pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
<p>Now we know how to use all the individual parts so it's time to see how they work together. Let's consider just one learning step before looping through all the data. The general process with PyTorch:</p>
<ol class="org-ol">
<li>Make a forward pass through the network</li>
<li>Use the network output to calculate the loss</li>
<li>Perform a backward pass through the network with <code>loss.backward()</code> to calculate the gradients</li>
<li>Take a step with the optimizer to update the weights</li>
</ol>
<p>Below I'll go through one training step and print out the weights and gradients so you can see how it changes. Note the line of code: <code>optimizer.zero_grad()</code>. When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches.</p>
<p>Here's the weights for the first set of edges in the network before we start:</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Initial weights - '</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
<pre class="example">
Initial weights -  Parameter containing:
tensor([[ 0.0170,  0.0055, -0.0258,  ..., -0.0295, -0.0028,  0.0312],
        [ 0.0246,  0.0314,  0.0259,  ..., -0.0091, -0.0276, -0.0238],
        [ 0.0336, -0.0133,  0.0045,  ..., -0.0284,  0.0278,  0.0029],
        ...,
        [-0.0085, -0.0300,  0.0222,  ...,  0.0066, -0.0162,  0.0062],
        [-0.0303, -0.0324, -0.0237,  ..., -0.0230,  0.0137, -0.0268],
        [-0.0327,  0.0012,  0.0174,  ...,  0.0311,  0.0058,  0.0034]],
       requires_grad=True)

</pre>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
<span class="n">images</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
</pre></div>
<p>Clear the gradients.</p>
<div class="highlight">
<pre><span></span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
<p>Make a forward pass, then a backward pass, then update the weights and check the gradient.</p>
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Gradient -'</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
<pre class="example">
Gradient - tensor([[-0.0076, -0.0076, -0.0076,  ..., -0.0076, -0.0076, -0.0076],
        [-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006],
        [-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],
        ...,
        [-0.0028, -0.0028, -0.0028,  ..., -0.0028, -0.0028, -0.0028],
        [-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],
        [ 0.0027,  0.0027,  0.0027,  ...,  0.0027,  0.0027,  0.0027]])

</pre>
<p>Now take an update step and check out the new weights.</p>
<div class="highlight">
<pre><span></span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Updated weights - '</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
<pre class="example">
Updated weights -  Parameter containing:
tensor([[ 0.0171,  0.0056, -0.0257,  ..., -0.0294, -0.0027,  0.0313],
        [ 0.0246,  0.0314,  0.0259,  ..., -0.0091, -0.0276, -0.0238],
        [ 0.0336, -0.0133,  0.0045,  ..., -0.0284,  0.0278,  0.0029],
        ...,
        [-0.0084, -0.0300,  0.0223,  ...,  0.0066, -0.0161,  0.0062],
        [-0.0303, -0.0324, -0.0237,  ..., -0.0229,  0.0137, -0.0268],
        [-0.0327,  0.0011,  0.0173,  ...,  0.0310,  0.0058,  0.0034]],
       requires_grad=True)

</pre>
<p>If you compare it to the first weights you'll notice that the first cell is the same, but many of the others have very small changes made to them. The first steps in the descent.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org6972f09">
<h2 id="org6972f09">Training (For Real This Time)</h2>
<div class="outline-text-2" id="text-org6972f09">
<p>Now we'll put this algorithm into a loop so we can go through all the images. First some nomenclature - one pass through the entire dataset is called an <b>epoch</b>. So we're going to loop through <code>data_loader</code> to get our training batches. For each batch, we'll do a training pass where we calculate the loss, do a backwards pass, and update the weights. Then we'll start all over again with the batches until we're out of epochs.</p>
</div>
<div class="outline-4" id="outline-container-org1d78c28">
<h4 id="org1d78c28">Set It Up</h4>
<div class="outline-text-4" id="text-org1d78c28">
<p>Since we took a couple of passes with the old model already I'll re-define it (I don't know if there's a reset function).</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">OrderedDict</span><span class="p">(</span>
        <span class="n">input_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span>
                                  <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_nodes_1</span><span class="p">),</span>
        <span class="n">relu_1</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_hidden</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_nodes_1</span><span class="p">,</span>
                                   <span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_nodes_2</span><span class="p">),</span>
        <span class="n">relu_2</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">hidden_to_output</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">hidden_nodes_2</span><span class="p">,</span>
                                   <span class="n">HyperParameters</span><span class="o">.</span><span class="n">outputs</span><span class="p">),</span>
        <span class="n">log_softmax</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">HyperParameters</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org95c970c">
<h4 id="org95c970c">Train It</h4>
<div class="outline-text-4" id="text-org95c970c">
<div class="highlight">
<pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="c1"># Flatten MNIST images</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Training loss: {running_loss/len(data_loader)}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Training loss: 1.961392556680545
Training loss: 0.9206915147014773
Training loss: 0.5431230474414348
Training loss: 0.4353313447792393
Training loss: 0.38809780185537807
Training loss: 0.3599447336580072
Training loss: 0.3397818624115448
Training loss: 0.323730937088095
Training loss: 0.3114365364696934
Training loss: 0.3002190677198901
</pre>
<p>So there's a little bit of voodoo going on there - we never pass the model to the loss function or the optimizer, but somehow calling them updates the model. It feels a little like matplotlib's state-machine form. It's neat, but I'm not sure I like it as much as I do object-oriented programming.</p>
<p>With the network trained, we can check out it's predictions.</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="c1"># Turn off gradients to speed up this part</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="c1"># Output of the network are logits, need to take softmax for probabilities</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">helper</span><span class="o">.</span><span class="n">view_classify</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">probabilities</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="probabilities.png" src="/posts/nano/pytorch/training-neural-networks/probabilities.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">probabilities</span><span class="o">.</span><span class="n">argmax</span><span class="p">())</span>
</pre></div>
<pre class="example">
tensor(6)

</pre>
<p>Amazingly, it did really well. One thing to note is that I originally made the epoch count higher but didn't remember to make a new network, optimizer, and loss, and the network ended up doing poorly. I don't know what messed it up, maybe I reset the network but not the optimizers, or some such, but anyway, here it is.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/">Backpropagation Implementation (Again)</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/" rel="bookmark"><time class="published dt-published" datetime="2018-11-18T13:41:28-08:00" itemprop="datePublished" title="2018-11-18 13:41">2018-11-18 13:41</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/#org9387ede">Set Up</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/#org72a11cd">The Data</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/#orgb2e8d46">Pre-Processing the Data</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/#org6606f39">The Algorithm</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/#org6bf35ca">Hyperparameters</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/#orgc130932">Train It</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/#org23a05a6">More Backpropagation Reading</a></li>
</ul>
</div>
</div>
<p>This is an example of implementing back-propagation using the UCLA Student Admissions data that we used earlier for training with gradient descent.</p>
<div class="outline-2" id="outline-container-org9387ede">
<h2 id="org9387ede">Set Up</h2>
<div class="outline-text-2" id="text-org9387ede"></div>
<div class="outline-3" id="outline-container-orgd9e180d">
<h3 id="orgd9e180d">Imports</h3>
<div class="outline-text-3" id="text-orgd9e180d"></div>
<div class="outline-4" id="outline-container-orgf6eaa15">
<h4 id="orgf6eaa15">Python</h4>
<div class="outline-text-4" id="text-orgf6eaa15">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgad750be">
<h4 id="orgad750be">PyPi</h4>
<div class="outline-text-4" id="text-orgad750be">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Graph</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb88c89c">
<h4 id="orgb88c89c">This Project</h4>
<div class="outline-text-4" id="text-orgb88c89c">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
<span class="kn">from</span> <span class="nn">neurotic.tangles.helpers</span> <span class="kn">import</span> <span class="n">org_table</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org0799ce6">
<h3 id="org0799ce6">Set the Random Seed</h3>
<div class="outline-text-3" id="text-org0799ce6">
<div class="highlight">
<pre><span></span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">21</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgbe86323">
<h3 id="orgbe86323">Helper Functions</h3>
<div class="outline-text-3" id="text-orgbe86323">
<p>Once again, the sigmoid.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate sigmoid</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org72a11cd">
<h2 id="org72a11cd">The Data</h2>
<div class="outline-text-2" id="text-org72a11cd">
<p>We are using data originally take from the <a href="https://stats.idre.ucla.edu/">UCLA Institute for Digital Research and Education</a> representing a group of students who applied for grad school at UCLA.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"student_data.csv"</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">org_table</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">380</td>
<td class="org-right">3.61</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">660</td>
<td class="org-right">3.67</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">800</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">640</td>
<td class="org-right">3.19</td>
<td class="org-right">4</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">520</td>
<td class="org-right">2.93</td>
<td class="org-right">4</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-2" id="outline-container-orgb2e8d46">
<h2 id="orgb2e8d46">Pre-Processing the Data</h2>
<div class="outline-text-2" id="text-orgb2e8d46"></div>
<div class="outline-3" id="outline-container-org14dfa72">
<h3 id="org14dfa72">Dummy Variables</h3>
<div class="outline-text-3" id="text-org14dfa72">
<p>Since the <code>rank</code> values are ordinal, not numeric, we need to create some one-hot-encoded columns for it using <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html">get_dummies</a>.</p>
<div class="highlight">
<pre><span></span><span class="n">rank_counts</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">"rank"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">rank_counts</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">==</span> <span class="n">data</span><span class="p">[</span><span class="s2">"rank_{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">)]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">org_table</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">380</td>
<td class="org-right">3.61</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">660</td>
<td class="org-right">3.67</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">800</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">640</td>
<td class="org-right">3.19</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">520</td>
<td class="org-right">2.93</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-3" id="outline-container-org81eb2f6">
<h3 id="org81eb2f6">Standardization</h3>
<div class="outline-text-3" id="text-org81eb2f6">
<p>Now I'll convert the <code>gre</code> and <code>gpa</code> to have a mean of 0 and a variance of 1 using sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale">scale</a> function.</p>
<div class="highlight">
<pre><span></span><span class="n">data</span><span class="p">[</span><span class="s2">"gre"</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">gre</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">"float64"</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s2">"gpa"</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">org_table</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">showindex</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">&nbsp;</th>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">72</td>
<td class="org-right">0</td>
<td class="org-right">-0.933502</td>
<td class="org-right">0.000263095</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">358</td>
<td class="org-right">1</td>
<td class="org-right">-0.240093</td>
<td class="org-right">0.789548</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">187</td>
<td class="org-right">0</td>
<td class="org-right">-0.0667406</td>
<td class="org-right">-1.34152</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">93</td>
<td class="org-right">0</td>
<td class="org-right">-0.0667406</td>
<td class="org-right">-1.20997</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">380</td>
<td class="org-right">0</td>
<td class="org-right">0.973373</td>
<td class="org-right">0.68431</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="n">data</span><span class="o">.</span><span class="n">gre</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
<span class="k">assert</span> <span class="n">data</span><span class="o">.</span><span class="n">gre</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="n">data</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
<span class="k">assert</span> <span class="n">data</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgafdd9ae">
<h3 id="orgafdd9ae">Setting up the training and testing data</h3>
<div class="outline-text-3" id="text-orgafdd9ae">
<p><code>features_all</code> is the input (<i>x</i>) data and <code>targets_all</code> is the target (<i>y</i>) data.</p>
<div class="highlight">
<pre><span></span><span class="n">features_all</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">"admit"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">"columns"</span><span class="p">)</span>
<span class="n">targets_all</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">admit</span>
</pre></div>
<p>Now we'll split it into training and testing sets.</p>
<div class="highlight">
<pre><span></span><span class="n">features</span><span class="p">,</span> <span class="n">features_test</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">targets_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">features_all</span><span class="p">,</span> <span class="n">targets_all</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org6606f39">
<h2 id="org6606f39">The Algorithm</h2>
<div class="outline-text-2" id="text-org6606f39">
<p>These are the basic steps to train the network with backpropagation.</p>
<ul class="org-ul">
<li>Set the weights for each layer to 0
<ul class="org-ul">
<li>Input to hidden weights: \(\Delta w_{ij} = 0\)</li>
<li>Hidden to output weights: \(\Delta W_j=0\)</li>
</ul>
</li>
<li>For each entry in the training data:
<ul class="org-ul">
<li>make a forward pass to get the output: \(\hat{y}\)</li>
<li>Calculate the error gradient for the output: \(\delta^o=(y - \hat{y})f'(\sum_j W_j a_j)\)</li>
<li>Propagate the errors to the hidden layer: \(\delta_j^h = \delta^o W_j f'(h_j)\)</li>
<li>Update the weight steps:
<ul class="org-ul">
<li>\(\Delta W_j = \Delta W_j + \delta^o a_j\)</li>
<li>\(\Delta w_{ij} = \Delta w_{ij} + \delta_j^h a_i\)</li>
</ul>
</li>
</ul>
</li>
<li>Update the weights (\(\eta\) is the learning rate and <i>m</i> is the number of records)
<ul class="org-ul">
<li>\(W_j = W_j + \eta \Delta W_j/m\)</li>
<li>\(w_{ij} = w_{ij} + \eta \Delta w_{ij}/m\)</li>
</ul>
</li>
<li>Repeat for \(\epsilon\) epochs</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org6bf35ca">
<h2 id="org6bf35ca">Hyperparameters</h2>
<div class="outline-text-2" id="text-org6bf35ca">
<p>These are the <i>hyperparameters</i> that we set to define the training. We're going to use 2 hidden units.</p>
<div class="highlight">
<pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="n">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span>

<span class="c1"># the input layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"a"</span><span class="p">,</span> <span class="s2">"GRE"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"b"</span><span class="p">,</span> <span class="s2">"GPA"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"c"</span><span class="p">,</span> <span class="s2">"Rank 1"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"d"</span><span class="p">,</span> <span class="s2">"Rank 2"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"e"</span><span class="p">,</span> <span class="s2">"Rank 3"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"f"</span><span class="p">,</span> <span class="s2">"Rank 4"</span><span class="p">)</span>

<span class="c1"># the hidden layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="s2">"h1"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"h"</span><span class="p">,</span> <span class="s2">"h2"</span><span class="p">)</span>

<span class="c1"># the output layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="s2">"abcdef"</span>
<span class="n">hidden</span> <span class="o">=</span> <span class="s2">"gh"</span>

<span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)])</span>
<span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="n">h</span> <span class="o">+</span> <span class="s2">"i"</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hidden</span><span class="p">])</span>

<span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">"graphs/network.dot"</span><span class="p">)</span>
<span class="n">graph</span>
</pre></div>
<div class="figure">
<p><img alt="network.dot.png" src="/posts/nano/introduction-to-neural-networks/backpropagation-implementation-again/network.dot.png"></p>
</div>
<p>Well train it for 2,000 epochs with a learning rate of 0.005.</p>
<div class="highlight">
<pre><span></span><span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span>
</pre></div>
<p>We'll be using the <code>n_records</code>, and <code>n_features</code> to set up the weights matrices. <code>n_records</code> is also used to average out the amount of change we make to the weights (otherwise each weight would get the sum of all the corrections). <code>last_loss</code> is used for reporting epochs that do worse than the previous epoch.</p>
<div class="highlight">
<pre><span></span><span class="n">n_records</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span>
<span class="n">last_loss</span> <span class="o">=</span> <span class="bp">None</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgecbeb84">
<h3 id="orgecbeb84">Initialize the Weights</h3>
<div class="outline-text-3" id="text-orgecbeb84">
<p>We're going to use a normally distributed set of random weights to start with. The <code>scale</code> is the spread of the distribution we're sampling from. A rule-of-thumb for the spread is to use \(\frac{1}{\sqrt{n}}\) where <i>n</i> is the numeber of input units. This keeps the input to the sigmoid low, even as the number of inputs goes up.</p>
<div class="highlight">
<pre><span></span><span class="n">weights_input_to_hidden</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_features</span> <span class="o">**</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span>
                                           <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
<span class="n">weights_hidden_to_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_features</span> <span class="o">**</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span>
                                            <span class="n">size</span><span class="o">=</span><span class="n">n_hidden</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc130932">
<h2 id="orgc130932">Train It</h2>
<div class="outline-text-2" id="text-orgc130932">
<p>Now, we'll train the network using backpropagation.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">delta_weights_input_to_hidden</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">weights_input_to_hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">delta_weights_hidden_to_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">weights_hidden_to_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">hidden_input</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_input_to_hidden</span><span class="p">)</span>
        <span class="n">hidden_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_output</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_hidden_to_output</span><span class="p">))</span>

        <span class="c1">## Backward pass ##</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">output</span>
        <span class="n">output_error_term</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>

        <span class="n">hidden_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights_hidden_to_output</span><span class="o">.</span><span class="n">T</span>
                        <span class="o">*</span> <span class="n">output_error_term</span><span class="p">)</span>
        <span class="n">hidden_error_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_error</span>
                             <span class="o">*</span>  <span class="n">hidden_output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hidden_output</span><span class="p">))</span>

        <span class="n">delta_weights_hidden_to_output</span> <span class="o">+=</span> <span class="n">output_error_term</span> <span class="o">*</span> <span class="n">hidden_output</span>
        <span class="n">delta_weights_input_to_hidden</span> <span class="o">+=</span> <span class="n">hidden_error_term</span> <span class="o">*</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

    <span class="n">weights_input_to_hidden</span> <span class="o">+=</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">delta_weights_input_to_hidden</span><span class="p">)</span><span class="o">/</span><span class="n">n_records</span>
    <span class="n">weights_hidden_to_output</span> <span class="o">+=</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">delta_weights_hidden_to_output</span><span class="p">)</span><span class="o">/</span><span class="n">n_records</span>

    <span class="c1"># Printing out the mean square error on the training set</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="p">(</span><span class="n">epochs</span> <span class="o">/</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">hidden_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights_input_to_hidden</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden_output</span><span class="p">,</span>
                             <span class="n">weights_hidden_to_output</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">last_loss</span> <span class="ow">and</span> <span class="n">last_loss</span> <span class="o">&lt;</span> <span class="n">loss</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"Train loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">"  WARNING - Loss Increasing"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"Train loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span>
</pre></div>
<pre class="example">
Train loss:  0.2508914323518061
Train loss:  0.24921862835632544
Train loss:  0.24764092608110996
Train loss:  0.24615251717689884
Train loss:  0.24474791403688867
Train loss:  0.24342194353528698
Train loss:  0.24216973842045766
Train loss:  0.24098672692610631
Train loss:  0.23986862108158177
Train loss:  0.2388114041271259
</pre>
<p>Now we'll calculate the accuracy of the model.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features_test</span><span class="p">,</span> <span class="n">weights_input_to_hidden</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">weights_hidden_to_output</span><span class="p">))</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">out</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">targets_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Prediction accuracy: {:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
<pre class="example">
Prediction accuracy: 0.750

</pre></div>
</div>
<div class="outline-2" id="outline-container-org23a05a6">
<h2 id="org23a05a6">More Backpropagation Reading</h2>
<div class="outline-text-2" id="text-org23a05a6">
<ul class="org-ul">
<li><a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand backprop</a>: Backpropagation has failure points that you have to know or you might get bitten by it.</li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/introduction-to-neural-networks/backpropagation/">Backpropagation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/introduction-to-neural-networks/backpropagation/" rel="bookmark"><time class="published dt-published" datetime="2018-11-17T20:24:16-08:00" itemprop="datePublished" title="2018-11-17 20:24">2018-11-17 20:24</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation/#org194082a">Background</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation/#org73cfdc6">Imports</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation/#org834eec1">The Sigmoid</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation/#org7f9c747">Initial Values</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation/#org473a81e">Forward pass</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/backpropagation/#org4d493c9">Backwards pass</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org194082a">
<h2 id="org194082a">Background</h2>
<div class="outline-text-2" id="text-org194082a">
<p>We're going to extend the backpropagation from a single layer to multiple hidden layers. The amount of change at each layer (the delta) that you make uses the same equation no matter how many layers you have.</p>
<p>\[ \Delta w_{pq} = \eta \delta_{output} X_{in} \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-org73cfdc6">
<h2 id="org73cfdc6">Imports</h2>
<div class="outline-text-2" id="text-org73cfdc6">
<p>We'll be sticking with our numpy-based implementation of a neural network.</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org834eec1">
<h2 id="org834eec1">The Sigmoid</h2>
<div class="outline-text-2" id="text-org834eec1">
<p>This is our familiar activation function.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate sigmoid</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org7f9c747">
<h2 id="org7f9c747">Initial Values</h2>
<div class="outline-text-2" id="text-org7f9c747">
<p>We're going to do a single forward pass followed by backpropagation, so I'll make the values random since we're not really going to validate them..</p>
<div class="highlight">
<pre><span></span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>

<span class="n">weights_input_to_hidden</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">weights_hidden_to_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Variable</th>
<th class="org-left" scope="col">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">x</td>
<td class="org-left">[ 0.08 2.19 -0.13]</td>
</tr>
<tr>
<td class="org-left">y</td>
<td class="org-left">0.85</td>
</tr>
<tr>
<td class="org-left">eta</td>
<td class="org-left">0.75</td>
</tr>
<tr>
<td class="org-left">Input Weights</td>
<td class="org-left">[0.67 0.99]</td>
</tr>
<tr>
<td class="org-left">&nbsp;</td>
<td class="org-left">[0.26 0.03]</td>
</tr>
<tr>
<td class="org-left">&nbsp;</td>
<td class="org-left">[0.64 0.85]</td>
</tr>
<tr>
<td class="org-left">Hidden To Output Weights</td>
<td class="org-left">[0.74]</td>
</tr>
<tr>
<td class="org-left">&nbsp;</td>
<td class="org-left">[0.02]</td>
</tr>
</tbody>
</table>
<p>The input has 3 nodes and the hidden layer has 2, so our weights from the input layer to the hidden layer has shape 3 rows and 2 columns. The output has one node so the weights from the hidden to output layer has 2 rows (to match the hidden layer) and 1 column to match the output layer. In the lecture they use a vector with 2 entries instead. As far as I can tell it works the same either way.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org473a81e">
<h2 id="org473a81e">Forward pass</h2>
<div class="outline-text-2" id="text-org473a81e">
<div class="highlight">
<pre><span></span><span class="n">hidden_layer_input</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_input_to_hidden</span><span class="p">)</span>
<span class="n">hidden_layer_output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_layer_input</span><span class="p">)</span>

<span class="n">output_layer_in</span> <span class="o">=</span> <span class="n">hidden_layer_output</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_hidden_to_output</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">output_layer_in</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org4d493c9">
<h2 id="org4d493c9">Backwards pass</h2>
<div class="outline-text-2" id="text-org4d493c9"></div>
<div class="outline-3" id="outline-container-org6aa9ccd">
<h3 id="org6aa9ccd">The Output Error</h3>
<div class="outline-text-3" id="text-org6aa9ccd">
<p>Our error is \(y - \hat{y}\).</p>
<div class="highlight">
<pre><span></span><span class="n">error</span> <span class="o">=</span> <span class="n">target</span> <span class="o">-</span> <span class="n">output</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org6d2b7e6">
<h3 id="org6d2b7e6">Output Error Term</h3>
<div class="outline-text-3" id="text-org6d2b7e6">
<p>Our output error term:</p>
\begin{align} \textit{output error term} &amp;= (y - \hat{y}) \times (\hat{y} \times \sigma'(x))\\ &amp;= error \times \hat{y} \times (1 - \hat{y}) \end{align}
<div class="highlight">
<pre><span></span><span class="n">output_error_term</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7ce1fb5">
<h3 id="org7ce1fb5">The Hidden Layer Error Term</h3>
<div class="outline-text-3" id="text-org7ce1fb5">
<p>The hidden layer error term is the output error term scaled by the weight between them times the derivative of the activation function.</p>
<p>\[ \delta^h = W\delta^o f'(h)\\ \]</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_error_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights_hidden_to_output</span><span class="o">.</span><span class="n">T</span>
                     <span class="o">*</span> <span class="n">output_error_term</span>
                     <span class="o">*</span> <span class="n">hidden_layer_output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hidden_layer_output</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org85b74f7">
<h3 id="org85b74f7">The Hidden To Output Weight Update</h3>
<div class="outline-text-3" id="text-org85b74f7">
<p>\[ \Delta W = \eta \delta^o a \]</p>
<p>Where <i>a</i> is the output of the hidden layer.</p>
<div class="highlight">
<pre><span></span><span class="n">delta_w_h_o</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">output_error_term</span> <span class="o">*</span> <span class="n">hidden_layer_output</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgda4f382">
<h3 id="orgda4f382">The Input To Hidden Weight Update</h3>
<div class="outline-text-3" id="text-orgda4f382">
<p>\[ \Delta w_i = \eta \delta^h x_i \]</p>
<p>The update is the learning rate times the hidden unit error times the input values.</p>
<div class="highlight">
<pre><span></span><span class="n">delta_w_i_h</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">hidden_error_term</span> <span class="o">*</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Change in weights for hidden layer to output layer:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">delta_w_h_o</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Change in weights for input layer to hidden layer:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">delta_w_i_h</span><span class="p">)</span>
</pre></div>
<pre class="example">
Change in weights for hidden layer to output layer:
[0.02634231 0.02119776]
Change in weights for input layer to hidden layer:
[[ 5.70726224e-04  1.72873580e-05]
 [ 1.57375099e-02  4.76690849e-04]
 [-9.69255871e-04 -2.93588634e-05]]

</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/">Multi-Layer Perceptrons</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/" rel="bookmark"><time class="published dt-published" datetime="2018-11-17T18:51:40-08:00" itemprop="datePublished" title="2018-11-17 18:51">2018-11-17 18:51</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/#orgef2a6bd">Set Up</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/#orgfb3f8e5">The Activation Function</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/#org8c36b25">Defining Our Network</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/#orga2018f5">Forward Pass</a></li>
</ul>
</div>
</div>
<p>This is basically like the previous gradient-descent post but with more layers.</p>
<div class="outline-2" id="outline-container-orgef2a6bd">
<h2 id="orgef2a6bd">Set Up</h2>
<div class="outline-text-2" id="text-orgef2a6bd"></div>
<div class="outline-3" id="outline-container-org684a3b5">
<h3 id="org684a3b5">Imports</h3>
<div class="outline-text-3" id="text-org684a3b5"></div>
<div class="outline-4" id="outline-container-org6803101">
<h4 id="org6803101">From PyPi</h4>
<div class="outline-text-4" id="text-org6803101">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Graph</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgfb3f8e5">
<h2 id="orgfb3f8e5">The Activation Function</h2>
<div class="outline-text-2" id="text-orgfb3f8e5">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate sigmoid</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org8c36b25">
<h2 id="org8c36b25">Defining Our Network</h2>
<div class="outline-text-2" id="text-org8c36b25">
<p>These variables will define our network size.</p>
<div class="highlight">
<pre><span></span><span class="n">N_input</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">N_hidden</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">N_output</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
<p>Which produces a network like this.</p>
<div class="highlight">
<pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="n">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span>

<span class="c1"># input layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"a"</span><span class="p">,</span> <span class="s2">"x1"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"b"</span><span class="p">,</span> <span class="s2">"x2"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"c"</span><span class="p">,</span> <span class="s2">"x3"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"d"</span><span class="p">,</span> <span class="s2">"x4"</span><span class="p">)</span>

<span class="c1"># hidden layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"e"</span><span class="p">,</span> <span class="s2">"h1"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"f"</span><span class="p">,</span> <span class="s2">"h2"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="s2">"h3"</span><span class="p">)</span>

<span class="c1"># output layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"h"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>

<span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="s2">"ae"</span><span class="p">,</span> <span class="s2">"af"</span><span class="p">,</span> <span class="s2">"ag"</span><span class="p">,</span> <span class="s2">"be"</span><span class="p">,</span> <span class="s2">"bf"</span><span class="p">,</span> <span class="s2">"bg"</span><span class="p">,</span> <span class="s2">"ce"</span><span class="p">,</span> <span class="s2">"cf"</span><span class="p">,</span> <span class="s2">"cg"</span><span class="p">,</span> <span class="s2">"de"</span><span class="p">,</span> <span class="s2">"df"</span><span class="p">,</span> <span class="s2">"dg"</span><span class="p">])</span>
<span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="s2">"eh"</span><span class="p">,</span> <span class="s2">"ei"</span><span class="p">,</span> <span class="s2">"fh"</span><span class="p">,</span> <span class="s2">"fi"</span><span class="p">,</span> <span class="s2">"gh"</span><span class="p">,</span> <span class="s2">"gi"</span><span class="p">])</span>

<span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">"graphs/network.dot"</span><span class="p">)</span>
<span class="n">graph</span>
</pre></div>
<div class="figure">
<p><img alt="network.dot.png" src="/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/network.dot.png"></p>
</div>
<p>Next, set the random seed.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
<p>Some fake data to train on.</p>
<div class="highlight">
<pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(4,)

</pre>
<p>Now initialize our weights.</p>
<div class="highlight">
<pre><span></span><span class="n">weights_input_to_hidden</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N_input</span><span class="p">,</span> <span class="n">N_hidden</span><span class="p">))</span>
<span class="n">weights_hidden_to_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N_hidden</span><span class="p">,</span> <span class="n">N_output</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">weights_input_to_hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">weights_hidden_to_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(4, 3)
(3, 2)

</pre></div>
</div>
<div class="outline-2" id="outline-container-orga2018f5">
<h2 id="orga2018f5">Forward Pass</h2>
<div class="outline-text-2" id="text-orga2018f5">
<p>This is one forward pass through our network.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer_in</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_input_to_hidden</span><span class="p">)</span>
<span class="n">hidden_layer_out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_layer_in</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Hidden-layer Output:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_layer_out</span><span class="p">)</span>
</pre></div>
<pre class="example">
Hidden-layer Output:
[0.5609517  0.4810582  0.44218495]

</pre>
<p>Now our output.</p>
<div class="highlight">
<pre><span></span><span class="n">output_layer_in</span> <span class="o">=</span> <span class="n">hidden_layer_out</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_hidden_to_output</span><span class="p">)</span>
<span class="n">output_layer_out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">output_layer_in</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Output-layer Output:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output_layer_out</span><span class="p">)</span>
</pre></div>
<pre class="example">
Output-layer Output:
[0.49936449 0.46156347]

</pre></div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/">Training with Gradient Descent</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/" rel="bookmark"><time class="published dt-published" datetime="2018-11-17T13:42:37-08:00" itemprop="datePublished" title="2018-11-17 13:42">2018-11-17 13:42</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/#org3f698af">Set Up</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/#org6409453">The Data</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/#org81edfbc">Pre-Processing the Data</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/#org2a54343">The Error</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/#orgb7a4a91">The General Training Algorithm</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/#org208294c">The Numpy Implementation</a></li>
</ul>
</div>
</div>
<p>This is an example of implementing gradient descent to update the weights in a neural network.</p>
<div class="outline-2" id="outline-container-org3f698af">
<h2 id="org3f698af">Set Up</h2>
<div class="outline-text-2" id="text-org3f698af"></div>
<div class="outline-3" id="outline-container-orgf8f9079">
<h3 id="orgf8f9079">Imports</h3>
<div class="outline-text-3" id="text-orgf8f9079"></div>
<div class="outline-4" id="outline-container-org327e3a0">
<h4 id="org327e3a0">From PyPi</h4>
<div class="outline-text-4" id="text-org327e3a0">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org37b1418">
<h4 id="org37b1418">This Project</h4>
<div class="outline-text-4" id="text-org37b1418">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
<span class="kn">from</span> <span class="nn">neurotic.tangles.helpers</span> <span class="kn">import</span> <span class="n">org_table</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgecf2fe4">
<h3 id="orgecf2fe4">Plotting</h3>
<div class="outline-text-3" id="text-orgecf2fe4">
<div class="highlight">
<pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">FIGURE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org6409453">
<h2 id="org6409453">The Data</h2>
<div class="outline-text-2" id="text-org6409453">
<p>We will use data originally take from the <a href="https://stats.idre.ucla.edu/">UCLA Institute for Digital Research and Education</a> (I couldn't find the dataset when I went to look for it). It has three features:</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Feature</th>
<th class="org-left" scope="col">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><code>gre</code></td>
<td class="org-left">Graduate Record Exam score</td>
</tr>
<tr>
<td class="org-left"><code>gpa</code></td>
<td class="org-left">Grade Point Average</td>
</tr>
<tr>
<td class="org-left"><code>rank</code></td>
<td class="org-left">Rank of the undergraduate school</td>
</tr>
</tbody>
</table>
<p>The <code>rank</code> is a scale from 1 to 4, with 1 being the most prestigious school and 4 being the least.</p>
<p>It also has one output value - <code>admit</code> which indicates whether the student was admitted or not.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"student_data.csv"</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">org_table</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">380</td>
<td class="org-right">3.61</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">660</td>
<td class="org-right">3.67</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">800</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">640</td>
<td class="org-right">3.19</td>
<td class="org-right">4</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">520</td>
<td class="org-right">2.93</td>
<td class="org-right">4</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(400, 7)

</pre>
<p>So there are 400 applications - not a huge data set.</p>
<div class="highlight">
<pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"gpa"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"gre"</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">"admit"</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">"rank"</span><span class="p">,</span>
                       <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">col_wrap</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">title</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"UCLA Student Admissions"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="admissions.png" src="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/admissions.png"></p>
</div>
<p>It does look like the rank of the school matters, perhaps even more than scores.</p>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">"PuBuGn_d"</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">catplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"rank"</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">"count"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                           <span class="n">height</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">12</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"UCLA Student Submissions By Rank"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="rank_distribution.png" src="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/rank_distribution.png"></p>
</div>
<p>So most of the applicants came from second and third-ranked schools.</p>
<div class="highlight">
<pre><span></span><span class="n">admitted</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">admit</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
<span class="k">with</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">"PuBuGn_d"</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">catplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"rank"</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">"count"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">admitted</span><span class="p">,</span>
                           <span class="n">height</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">12</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"UCLA Student Admissions By Rank"</span><span class="p">,</span>
                              <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
</pre></div>
<p><img alt="admitted_ranks.png" src="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/admitted_ranks.png"> And it looks like most of the admitted were from first and second-ranked schools, with most coming from the second-ranked schools.</p>
<div class="highlight">
<pre><span></span><span class="n">admission_rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">admitted</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">sort</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
                  <span class="o">/</span><span class="n">data</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">sort</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="n">admission_rate</span> <span class="o">=</span> <span class="p">(</span><span class="n">admission_rate</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">admission_rate</span> <span class="o">=</span> <span class="n">admission_rate</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">admission_rate</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Rank"</span><span class="p">,</span> <span class="s2">"Percent Admitted"</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">org_table</span><span class="p">(</span><span class="n">admission_rate</span><span class="p">))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Rank</th>
<th class="org-right" scope="col">Percent Admitted</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">54.1</td>
</tr>
<tr>
<td class="org-right">2</td>
<td class="org-right">35.76</td>
</tr>
<tr>
<td class="org-right">3</td>
<td class="org-right">23.14</td>
</tr>
<tr>
<td class="org-right">4</td>
<td class="org-right">17.91</td>
</tr>
</tbody>
</table>
<p>So, even though the second-tier schools had the most admitted, the top-tier school was admitted at a higher rate.</p>
</div>
<div class="outline-3" id="outline-container-orgb215e5e">
<h3 id="orgb215e5e">Did GRE Matter?</h3>
<div class="outline-text-3" id="text-orgb215e5e">
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">"hls"</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">catplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"rank"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"gre"</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">"admit"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                           <span class="n">height</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">12</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Admissions by School Rank"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="gre_rank_admissions.png" src="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/gre_rank_admissions.png"></p>
</div>
<p>This one's a little tough to say, it looks like it's better to have a higher GRE, but once you get below 700 it isn't as clear, at least not to me.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org34aee81">
<h3 id="org34aee81">What about GPA?</h3>
<div class="outline-text-3" id="text-org34aee81">
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">"hls"</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">catplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"rank"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"gpa"</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">"admit"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                           <span class="n">height</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="mi">12</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Admissions by School Rank"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="gpa_rank_admissions.png" src="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/gpa_rank_admissions.png"></p>
</div>
<p>This one seems even less demonstrative than GRE does.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org81edfbc">
<h2 id="org81edfbc">Pre-Processing the Data</h2>
<div class="outline-text-2" id="text-org81edfbc"></div>
<div class="outline-3" id="outline-container-org2cba93b">
<h3 id="org2cba93b">Dummy Variables</h3>
<div class="outline-text-3" id="text-org2cba93b">
<p>Since the <code>rank</code> values are ordinal, not numeric, we need to create some one-hot-encoded columns for it using <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html">get_dummies</a>.</p>
<p>First I'll get some counts so I can double-check my work. Note to future self: <code>rank</code> is a pandas <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rank.html">DataFrame method</a>, so naming a column 'rank' is probably not such a great idea.</p>
<div class="highlight">
<pre><span></span><span class="n">rank_counts</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">],</span> <span class="n">prefix</span><span class="o">=</span><span class="s2">"rank"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">rank_counts</span><span class="p">[</span><span class="n">rank</span><span class="p">]</span> <span class="o">==</span> <span class="n">data</span><span class="p">[</span><span class="s2">"rank_{}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rank</span><span class="p">)]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">org_table</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">380</td>
<td class="org-right">3.61</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">660</td>
<td class="org-right">3.67</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">800</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">640</td>
<td class="org-right">3.19</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">520</td>
<td class="org-right">2.93</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-3" id="outline-container-orgfac2949">
<h3 id="orgfac2949">Standardization</h3>
<div class="outline-text-3" id="text-orgfac2949">
<p>Now I'll convert the <code>gre</code> and <code>gpa</code> to have a mean of 0 and a variance of 1 using sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn.preprocessing.scale">scale</a> function.</p>
<div class="highlight">
<pre><span></span><span class="n">data</span><span class="p">[</span><span class="s2">"gre"</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">gre</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">"float64"</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s2">"gpa"</span><span class="p">]</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">org_table</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">-0.240093</td>
<td class="org-right">-0.394379</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.973373</td>
<td class="org-right">1.60514</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">-0.413445</td>
<td class="org-right">-0.0260464</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.106612</td>
<td class="org-right">-0.631165</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">-0.760149</td>
<td class="org-right">-1.52569</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">gre</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">gre</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">gpa</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">())</span>
</pre></div>
<pre class="example">
-0.0
1.0
0.0
1.0

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org2a54343">
<h2 id="org2a54343">The Error</h2>
<div class="outline-text-2" id="text-org2a54343">
<p>For this we're going to use the Mean Square Error.</p>
<p>\[ E = \frac{1}{2m}\sum_{\mu} (y^{\mu} - \hat{y}^{\mu})^2 \]</p>
<p>This doesn't actually change our training, it just acts as an estimate of the error as we train so we can see that the model is getting better (hopefully).</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgb7a4a91">
<h2 id="orgb7a4a91">The General Training Algorithm</h2>
<div class="outline-text-2" id="text-orgb7a4a91">
<ul class="org-ul">
<li>Set the weight delta to 0 (\(\Delta w_i = 0\))</li>
<li>For each record in the training data:
<ul class="org-ul">
<li>Make a forward pass to get the output: \(\hat{y} = f\left(\sum_{i} w_i x_i \right)\)</li>
<li>Calculate the error: \(\delta = (y - \hat{y}) f'\left(\sum_i w_i x_i\right)\)</li>
<li>Update the weight delta: \(\Delta w_i = \Delta w_i + \delta x_i\)</li>
</ul>
</li>
<li>Update the weights : \(w_i = w_i + \eta \frac{\Delta w_i}{m}\)</li>
<li>Repeart for \(e\) epochs</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org208294c">
<h2 id="org208294c">The Numpy Implementation</h2>
<div class="outline-text-2" id="text-org208294c">
<p>I'm going to implement the previous algorithm using numpy.</p>
</div>
<div class="outline-3" id="outline-container-orgd8db8d0">
<h3 id="orgd8db8d0">Setting up the Data</h3>
<div class="outline-text-3" id="text-orgd8db8d0">
<p>We need to set up the training and testing data. The lecture uses numpy exclusively, but as with the standardization I'll cheat a little and use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">sklearn</a>. The lecture uses a slightly different naming scheme from the one you normally see in the python machine learning community (e.g. <code>X_train</code>, <code>y_train</code>) which I'll stick with it so that I don't get errors just from using the wrong names. Truthfully, I kind of like these names better, although the use of the suffix <code>_test</code> without the use of the suffix <code>_train</code> seems confusing.</p>
<div class="highlight">
<pre><span></span><span class="n">features_all</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">"admit"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">"columns"</span><span class="p">)</span>
<span class="n">targets_all</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">admit</span>
</pre></div>
<p>The example given uses 10 % of the data for testing and 90% for training.</p>
<div class="highlight">
<pre><span></span><span class="n">features</span><span class="p">,</span> <span class="n">features_test</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">targets_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features_all</span><span class="p">,</span> <span class="n">targets_all</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">features_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">targets_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(360, 6)
(360,)
(40, 6)
(40,)

</pre></div>
</div>
<div class="outline-3" id="outline-container-org15b2f06">
<h3 id="org15b2f06">The Sigmoid</h3>
<div class="outline-text-3" id="text-org15b2f06">
<p>This is our activation function.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate sigmoid</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">limit</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">limit</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">*</span><span class="n">limit</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"$\sigma(x)$"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="sigmoid.png" src="/posts/nano/introduction-to-neural-networks/training-with-gradient-descent/sigmoid.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgeb6bfac">
<h3 id="orgeb6bfac">Some Setup</h3>
<div class="outline-text-3" id="text-orgeb6bfac">
<p>To make the outcome reproducible I'll set the random seed.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">17</span><span class="p">)</span>
</pre></div>
<p>Now some variables need to be set up for the print output.</p>
<div class="highlight">
<pre><span></span><span class="n">n_records</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span>
<span class="n">last_loss</span> <span class="o">=</span> <span class="bp">None</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8a3d49b">
<h3 id="org8a3d49b">Initialize weights</h3>
<div class="outline-text-3" id="text-org8a3d49b">
<p>We're going to use a normally distributed set of random weights to start with. The <code>scale</code> is the spread of the distribution we're sampling from. A rule-of-thumb for the spread is to use \(\frac{1}{\sqrt{n}}\) where <i>n</i> is the numeber of input units. This keeps the input to the sigmoid low, even as the number of inputs goes up.</p>
<div class="highlight">
<pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">n_features</span><span class="o">**.</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd870e60">
<h3 id="orgd870e60">Set Up The Learning</h3>
<div class="outline-text-3" id="text-orgd870e60">
<p>Now some neural network hyperparameters - how long do we train and how fast do we learn at each pass?</p>
<div class="highlight">
<pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">learnrate</span> <span class="o">=</span> <span class="mf">0.5</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org418962d">
<h3 id="org418962d">The Training Loop</h3>
<div class="outline-text-3" id="text-org418962d">
<p>This is where we do the actual training (gradient descent).</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">delta_weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>

        <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">output</span>

        <span class="n">error_term</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="p">(</span><span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">))</span>

        <span class="n">delta_weights</span> <span class="o">+=</span> <span class="n">error_term</span> <span class="o">*</span> <span class="n">x</span>

    <span class="n">weights</span> <span class="o">+=</span> <span class="p">(</span><span class="n">learnrate</span> <span class="o">*</span> <span class="n">delta_weights</span><span class="p">)</span><span class="o">/</span><span class="n">n_records</span>

    <span class="c1"># Printing out the mean square error on the training set</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="p">(</span><span class="n">epochs</span> <span class="o">/</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_loss</span> <span class="ow">and</span> <span class="n">last_loss</span> <span class="o">&lt;</span> <span class="n">loss</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"Train loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">"  WARNING - Loss Increasing"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"Train loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span>
</pre></div>
<pre class="example">
Train loss:  0.31403028569037034
Train loss:  0.20839043233748233
Train loss:  0.19937544110681996
Train loss:  0.19697280538767817
Train loss:  0.19607622516320752
Train loss:  0.19567788493090374
Train loss:  0.19548034981121246
Train loss:  0.19537454797678722
Train loss:  0.19531455174429538
Train loss:  0.19527902197312702
</pre></div>
</div>
<div class="outline-3" id="outline-container-org5e54273">
<h3 id="org5e54273">Testing</h3>
<div class="outline-text-3" id="text-org5e54273">
<p>Calculate accuracy on test data</p>
<div class="highlight">
<pre><span></span><span class="n">test_out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features_test</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">test_out</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">targets_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Prediction accuracy: {:.3f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
<pre class="example">
Prediction accuracy: 0.750

</pre>
<p>Not great, but then again, we had a fairly small data set to start with.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/introduction-to-neural-networks/gradient-descent-again/">Gradient Descent (Again)</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/introduction-to-neural-networks/gradient-descent-again/" rel="bookmark"><time class="published dt-published" datetime="2018-11-17T13:03:35-08:00" itemprop="datePublished" title="2018-11-17 13:03">2018-11-17 13:03</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/introduction-to-neural-networks/gradient-descent-again/#org1f340bf">Some Math</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/gradient-descent-again/#orga57742c">An Implementation</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org1f340bf">
<h2 id="org1f340bf">Some Math</h2>
<div class="outline-text-2" id="text-org1f340bf">
<p>One weight update for gradient descent is calculated as:</p>
<p>\[ \Delta w_i = \eta \delta x_i \]</p>
<p>And the error term \(\delta\) is calculated as:</p>
\begin{align} \delta &amp;= (y - \hat{y}) f'(h)\\ &amp;= (y - \hat{y})f'\left(\sum w_i x_i\right) \end{align}
<p>If we are using the <i>sigmoid</i> activation function as \(f(x)\):</p>
<p>\[ \sigma(x) = \frac{1}{1 - e^{-x}} \]</p>
<p>Then its derivative \(f'(x)\) is:</p>
<p>\[ \sigma(x) (1 - \sigma(x)) \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-orga57742c">
<h2 id="orga57742c">An Implementation</h2>
<div class="outline-text-2" id="text-orga57742c"></div>
<div class="outline-3" id="outline-container-orge231db4">
<h3 id="orge231db4">Imports</h3>
<div class="outline-text-3" id="text-orge231db4">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org6dcac25">
<h3 id="org6dcac25">The Sigmoid</h3>
<div class="outline-text-3" id="text-org6dcac25">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Our activation function</span>

<span class="sd">    Args:</span>
<span class="sd">     x: the input array</span>

<span class="sd">    Returns:</span>
<span class="sd">     the sigmoid of x</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9d8c1f3">
<h3 id="org9d8c1f3">The Sigmoid Derivative</h3>
<div class="outline-text-3" id="text-org9d8c1f3">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    The derivative of the sigmoid</span>

<span class="sd">    Args:</span>
<span class="sd">     x: the input</span>

<span class="sd">    Returns:</span>
<span class="sd">     the sigmoid derivative of x</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2e55d52">
<h3 id="org2e55d52">Setup The Network</h3>
<div class="outline-text-3" id="text-org2e55d52">
<div class="highlight">
<pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># Initial weights</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org3853a59">
<h3 id="org3853a59">The Network</h3>
<div class="outline-text-3" id="text-org3853a59">
<p>This will calculate a single gradient descent step.</p>
</div>
<div class="outline-4" id="outline-container-orgf2a9601">
<h4 id="orgf2a9601">The Fordward pass</h4>
<div class="outline-text-4" id="text-orgf2a9601">
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org806d226">
<h4 id="org806d226">Backwards Propagation</h4>
<div class="outline-text-4" id="text-org806d226">
<div class="highlight">
<pre><span></span><span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span>

<span class="n">error_term</span> <span class="o">=</span> <span class="n">error</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">hidden_layer</span><span class="p">)</span>

<span class="n">delta_w</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">error_term</span> <span class="o">*</span> <span class="n">x</span>

<span class="k">print</span><span class="p">(</span><span class="s1">'Neural Network output:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Amount of Error:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Change in Weights:'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">delta_w</span><span class="p">)</span>
</pre></div>
<pre class="example">
Neural Network output:
0.6899744811276125
Amount of Error:
-0.1899744811276125
Change in Weights:
[-0.02031869 -0.04063738 -0.06095608 -0.08127477]

</pre></div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/">Part 2 - Neural Networks in Pytorch</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/" rel="bookmark"><time class="published dt-published" datetime="2018-11-16T13:20:22-08:00" itemprop="datePublished" title="2018-11-16 13:20">2018-11-16 13:20</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/#org78deb64">Introduction</a></li>
<li><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/#org591f50b">Set Up</a></li>
<li><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/#orgfe119e2">The First Network</a></li>
<li><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/#org1eddf30">Building networks with PyTorch</a></li>
<li><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/#org881cbff">Activation functions</a></li>
<li><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/#org179c27e">Let's Build a Network</a></li>
<li><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/#orge29ca5d">Forward pass</a></li>
<li><a href="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/#orgc6844ec">Using <code>nn.Sequential</code></a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org78deb64">
<h2 id="org78deb64">Introduction</h2>
<div class="outline-text-2" id="text-org78deb64">
<p>Deep learning networks tend to be massive with dozens or hundreds of layers, that's where the term "deep" comes from. You can build one of these deep networks using only weight matrices as we did in the previous notebook, but in general it's very cumbersome and difficult to implement. PyTorch has a nice module <a href="https://pytorch.org/docs/stable/nn.html">nn</a> that provides a nice way to efficiently build large neural networks.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org591f50b">
<h2 id="org591f50b">Set Up</h2>
<div class="outline-text-2" id="text-org591f50b"></div>
<div class="outline-3" id="outline-container-org7cb503a">
<h3 id="org7cb503a">Python</h3>
<div class="outline-text-3" id="text-org7cb503a">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org60012be">
<h3 id="org60012be">Imports</h3>
<div class="outline-text-3" id="text-org60012be"></div>
<div class="outline-4" id="outline-container-orgbd4eb81">
<h4 id="orgbd4eb81">PyPi</h4>
<div class="outline-text-4" id="text-orgbd4eb81">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc1a6fb1">
<h4 id="orgc1a6fb1">From the Nano-Degree Repository</h4>
<div class="outline-text-4" id="text-orgc1a6fb1">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">nano.pytorch</span> <span class="kn">import</span> <span class="n">helper</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org85d9032">
<h3 id="org85d9032">Plotting</h3>
<div class="outline-text-3" id="text-org85d9032">
<div class="highlight">
<pre><span></span><span class="n">get_python</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_python</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgfe119e2">
<h2 id="orgfe119e2">The First Network</h2>
<div class="outline-text-2" id="text-orgfe119e2">
<p>Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we'll use the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels. Our goal is to build a neural network that can take one of these images and predict the digit in the image.</p>
<p>First up, we need to get our dataset. This is provided through the <a href="https://pytorch.org/docs/stable/torchvision/index.html">torchvision</a> package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later. (see <a href="https://pytorch.org/docs/stable/torchvision/datasets.html">torchvision.dataset</a> and <a href="https://pytorch.org/docs/stable/torchvision/transforms.html">torchvision.transforms</a>).</p>
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
</pre></div>
<p>Transformers:</p>
<ul class="org-ul">
<li><code>transforms.compose</code> lets you set up multiple transforms in a pipeline.</li>
<li><code>transforms.Normalize</code> normalizes images using the mean and standard deviation.</li>
<li><code>transforms.ToTensor</code> converts images to Tensors.</li>
</ul>
<p>Define a transform to normalize the data.</p>
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                              <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)),</span>
                              <span class="p">])</span>
</pre></div>
<p>Download and load the training data</p>
<p><a href="https://pytorch.org/docs/stable/data.html"><code>torch.utils.data.DataLoader</code></a> builds an iterator over the data.</p>
<div class="highlight">
<pre><span></span><span class="n">trainset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">'~/datasets/MNIST/'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>The data-set isn't actually part of pytorch, it just downloads it from the web (unless you already downloaded it).</p>
<pre class="example">
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Processing...
Done!
</pre>
<p>We have the training data loaded into <code>trainloader</code> and we make that an iterator with <code>iter(trainloader)</code>. Later, we'll use this to loop through the dataset for training, something like:</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">trainloader</span><span class="p">:</span>
    <span class="n">do</span> <span class="n">things</span> <span class="k">with</span> <span class="n">images</span> <span class="ow">and</span> <span class="n">labels</span>
</pre></div>
<p>The <code>trainloader</code> has a batch size of 64, and <code>shuffle=True</code>. The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a <b>batch</b>. And <code>shuffle=True</code> tells it to shuffle the dataset every time we start going through the data loader again. But here I'm just grabbing the first batch so we can check out the data. We can see below that <code>images</code> is just a tensor with size <i>(64, 1, 28, 28)</i>. So, 64 images per batch, 1 color channel, and 28x28 images.</p>
<div class="highlight">
<pre><span></span><span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">images</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
&lt;class 'torch.Tensor'&gt;
torch.Size([64, 1, 28, 28])
torch.Size([64])

</pre>
<p>Here's what one of the images looks like.</p>
<div class="highlight">
<pre><span></span><span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'Greys_r'</span><span class="p">);</span>
</pre></div>
<div class="figure">
<p><img alt="image.png" src="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/image.png"></p>
</div>
<p>Can you tell what that is? I couldn't, so I looked it up.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
tensor(6)

</pre>
<p>First, let's try to build a simple network for this dataset using weight matrices and matrix multiplications. Then, we'll see how to do it using PyTorch's <code>nn</code> module which provides a much more convenient and powerful method for defining network architectures.</p>
</div>
<div class="outline-3" id="outline-container-org0b34c6a">
<h3 id="org0b34c6a">Flattening The Input</h3>
<div class="outline-text-3" id="text-org0b34c6a">
<p>The networks you've seen so far are called <b>fully-connected</b> or <b>dense</b> networks. Each unit in one layer is connected to each unit in the next layer. In fully-connected networks, the input to each layer must be a one-dimensional vector (which can be stacked into a 2D tensor as a batch of multiple examples). However, our images are 28x28 2D tensors, so we need to convert them into 1D vectors. Thinking about sizes, we need to convert the batch of images with shape `(64, 1, 28, 28)` to a have a shape of `(64, 784)`, 784 is 28 times 28. This is typically called <b>flattening</b>, we flattened the 2D images into 1D vectors.</p>
<p>Previously you built a network with one output unit. Here we need 10 output units, one for each digit. We want our network to predict the digit shown in an image, so what we'll do is calculate probabilities that the image is of any one digit or class. This ends up being a discrete probability distribution over the classes (digits) that tells us the most likely class for the image. That means we need 10 output units for the 10 classes (digits). We'll see how to convert the network output into a probability distribution next.</p>
<p>Now we're going to flatten the batch of images <code>images</code> then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. It will use a sigmoid activation for the hidden layer and no activation function for the output layer.</p>
<div class="highlight">
<pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
<p>Now we have 10 outputs for our network. We want to pass in an image to our network and get out a probability distribution over the classes that tells us the likely class(es) the image belongs to.</p>
<p>For an untrained network that hasn't seen any data yet the output probability distribution will be a uniform distribution with equal probabilities for each class.</p>
<p>To calculate this probability distribution, we often use the <a href="https://en.wikipedia.org/wiki/Softmax_function"><b>softmax</b> function</a>. Mathematically this looks like</p>
<p>\[ \Large \sigma(x_i) = \cfrac{e^{x_i}}{\sum_k^K{e^{x_k}}} \]</p>
<p>What this does is squish each input \(x_i\) between 0 and 1 and normalizes the values to give you a proper probability distribution where the probabilites sum up to one.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org60f04e4">
<h3 id="org60f04e4">Softmax Implementation</h3>
<div class="outline-text-3" id="text-org60f04e4">
<p>Implement a function <code>softmax</code> that performs the softmax calculation and returns probability distributions for each example in the batch. Note that you'll need to pay attention to the shapes when doing this. If you have a tensor <code>a</code> with shape <code>(64, 10)</code> and a tensor <code>b</code> with shape <code>(64,)</code>, doing <code>a/b</code> will give you an error because PyTorch will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. The way to think about this is for each of the 64 examples, you only want to divide by one value, the sum in the denominator. So you need <code>b</code> to have a shape of <code>(64, 1)</code>. This way PyTorch will divide the 10 values in each row of <code>a</code> by the one value in each row of <code>b</code>. Pay attention to how you take the sum as well. You'll need to define the <code>dim</code> keyword in <code>torch.sum</code>. Setting <code>dim=0</code> takes the sum across the rows while <code>dim=1</code> takes the sum across the columns.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the softmax"""</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">numerator</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">numerator</span><span class="o">/</span><span class="n">denominator</span>
</pre></div>
<p>Here, out should be the output of the network in the previous excercise with shape (64,10)</p>
<div class="highlight">
<pre><span></span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
<p>Does it have the right shape? Should be (64, 10)</p>
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span><span class="p">(</span><span class="n">probabilities</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
torch.Size([64, 10])

</pre>
<p>Does it sum to 1?</p>
<div class="highlight">
<pre><span></span><span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000])

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org1eddf30">
<h2 id="org1eddf30">Building networks with PyTorch</h2>
<div class="outline-text-2" id="text-org1eddf30">
<p>PyTorch provides a module <a href="https://pytorch.org/docs/stable/nn.html">nn</a> that makes building networks much simpler. Here I'll show you how to build the same one as above with 784 inputs, 256 hidden units, 10 output units and a softmax output.</p>
</div>
<div class="outline-3" id="outline-container-orgdc34447">
<h3 id="orgdc34447">The Class Definition</h3>
<div class="outline-text-3" id="text-orgdc34447">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Inputs to hidden layer linear transformation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="c1"># Output layer, 10 units - one for each digit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

        <span class="c1"># Define sigmoid activation and softmax output </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Pass the input tensor through each of our operations</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
<p>Let's go through this bit by bit.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org38b8d11">
<h3 id="org38b8d11">Inherit from nn.Module</h3>
<div class="outline-text-3" id="text-org38b8d11">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</pre></div>
<p>Here we're inheriting from <a href="https://pytorch.org/docs/stable/nn.html#module"><code>nn.Module</code></a>. Combined with <code>super().__init__()</code> this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from <code>nn.Module</code> when you're creating a class for your network. The name of the class itself can be anything.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgc90da68">
<h3 id="orgc90da68">The Hidden Layer</h3>
<div class="outline-text-3" id="text-orgc90da68">
<div class="highlight">
<pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
</pre></div>
<p>This line creates a module for a <a href="https://pytorch.org/docs/stable/nn.html#linear-layers">linear transformation</a>, \(x\mathbf{W} + b\), with 784 inputs and 256 outputs and assigns it to <code>self.hidden</code>. The module automatically creates the weight and bias tensors which we'll use in the <code>forward</code> method. You can access the weight and bias tensors once the network once it's create at <code>net.hidden.weight</code> and <code>net.hidden.bias</code>.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgc102ac2">
<h3 id="orgc102ac2">The Output Layer</h3>
<div class="outline-text-3" id="text-orgc102ac2">
<div class="highlight">
<pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
<p>Similarly, this creates another linear transformation with 256 inputs and 10 outputs.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org3d392b0">
<h3 id="org3d392b0">The Activation Layers</h3>
<div class="outline-text-3" id="text-org3d392b0">
<div class="highlight">
<pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>Next we set up our <a href="https://pytorch.org/docs/stable/nn.html#sigmoid">sigmoid</a> activation method and <a href="https://pytorch.org/docs/stable/nn.html#softmax">softmax</a> output method. The argument <code>dim</code> tells it which axis to use. Setting it to 1 tells it to sum the columns, so you will end up with 1 entry for every row.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgb385514">
<h3 id="orgb385514">The Forward-Pass Method</h3>
<div class="outline-text-3" id="text-orgb385514">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</pre></div>
<p>PyTorch networks created with <code>nn.Module</code> must have a <code>forward</code> method defined. It takes in a tensor <code>x</code> and passes it through the operations you defined in the <code>__init__</code> method.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
<p>Here the input tensor <code>x</code> is passed through each operation and reassigned to <code>x</code>. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the <code>__init__</code> method doesn't matter, but you'll need to sequence the operations correctly in the <code>forward</code> method.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgdbab6fe">
<h3 id="orgdbab6fe">Instantiating the Model</h3>
<div class="outline-text-3" id="text-orgdbab6fe">
<p>Now we can create a <code>Network</code> object.</p>
<p>Here's what the text representation for an instance looks like.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example">
Network(
  (hidden): Linear(in_features=784, out_features=256, bias=True)
  (output): Linear(in_features=256, out_features=10, bias=True)
  (sigmoid): Sigmoid()
  (softmax): Softmax()
)

</pre>
<p>You can define the network somewhat more concisely and clearly using the <a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional"><code>torch.nn.functional</code></a> module. This is the most common way you'll see networks defined as many operations are simple element-wise functions. We normally import this module as <code>F</code>, <code>import torch.nn.functional as F</code>.</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>

<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># Inputs to hidden layer linear transformation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="c1"># Output layer, 10 units - one for each digit</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Hidden layer with sigmoid activation</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># Output layer with softmax activation</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>        
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org881cbff">
<h2 id="org881cbff">Activation functions</h2>
<div class="outline-text-2" id="text-org881cbff">
<p>So far we've only been looking at the softmax activation, but in general any function can be used as an activation function. The only requirement is that for a network to approximate a non-linear function, the activation functions must be non-linear. Here are a few more examples of common activation functions: Tanh (hyperbolic tangent), and ReLU (rectified linear unit).</p>
<p>In practice, the ReLU function is used almost exclusively as the activation function for hidden layers.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org179c27e">
<h2 id="org179c27e">Let's Build a Network</h2>
<div class="outline-text-2" id="text-org179c27e">
<p>We're going to create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the <a href="https://pytorch.org/docs/stable/nn.html#relu"><code>nn.ReLU</code></a> module or <code>F.relu</code> function.</p>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">element</span><span class="p">)</span> <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Rectified Linear Unit (ReLU)"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="relu.png" src="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/relu.png"></p>
</div>
<p>The ReLU is a function with the form of \(y = max(0, x)\).</p>
<p>We're going to create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> activation, and finally an output layer with a softmax activation as shown above. You can use a ReLU activation with the <a href="https://pytorch.org/docs/stable/nn.html#relu"><code>nn.ReLU</code></a> module or <code>F.relu</code> function.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">ReluNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""Creates a network with two hidden layers</span>

<span class="sd">    Each hidden layer will use ReLU activation</span>
<span class="sd">    The output will use softmax activation</span>

<span class="sd">    Args:</span>
<span class="sd">     inputs: number of input nodes</span>
<span class="sd">     hidden_one: number of nodes in the first hidden layer</span>
<span class="sd">     hidden_two: number of nodes in the second layer</span>
<span class="sd">     outputs: number of nodes in the output layer</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span>
                 <span class="n">hidden_one</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                 <span class="n">hidden_two</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                 <span class="n">outputs</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_to_hidden_one</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_one</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_one_to_hidden_two</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_one</span><span class="p">,</span> <span class="n">hidden_two</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_two_to_output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_two</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Does the forward-pass through the network"""</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_to_hidden_one</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_one_to_hidden_two</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_two_to_output</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ReluNet</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example">
ReluNet(
  (input_to_hidden_one): Linear(in_features=784, out_features=128, bias=True)
  (hidden_one_to_hidden_two): Linear(in_features=128, out_features=64, bias=True)
  (hidden_two_to_output): Linear(in_features=64, out_features=10, bias=True)
  (relu): ReLU()
  (softmax): Softmax()
)

</pre></div>
<div class="outline-3" id="outline-container-orga9695f3">
<h3 id="orga9695f3">Initializing weights and biases</h3>
<div class="outline-text-3" id="text-orga9695f3">
<p>The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with <code>model.fc1.weight</code> for instance.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_one_to_hidden_two</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_one_to_hidden_two</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</pre></div>
<pre class="example">
Parameter containing:
tensor([[-0.0489, -0.0440, -0.0060,  ..., -0.0246, -0.0269,  0.0096],
        [ 0.0739,  0.0338, -0.0180,  ..., -0.0785, -0.0467, -0.0290],
        [-0.0117, -0.0637,  0.0105,  ...,  0.0158,  0.0126, -0.0255],
        ...,
        [ 0.0077, -0.0302,  0.0320,  ..., -0.0089, -0.0645, -0.0595],
        [-0.0269, -0.0370, -0.0317,  ...,  0.0258,  0.0334,  0.0240],
        [ 0.0227,  0.0195,  0.0731,  ...,  0.0510,  0.0119, -0.0791]],
       requires_grad=True)
Parameter containing:
tensor([-0.0820, -0.0675,  0.0483, -0.0245,  0.0227,  0.0306, -0.0397,  0.0602,
         0.0737, -0.0517, -0.0539,  0.0142,  0.0129, -0.0251,  0.0813,  0.0114,
         0.0445, -0.0508,  0.0709, -0.0684, -0.0822,  0.0084, -0.0751,  0.0594,
        -0.0248,  0.0041,  0.0369, -0.0762, -0.0170,  0.0306, -0.0295, -0.0396,
        -0.0442, -0.0408,  0.0189, -0.0410,  0.0593, -0.0696, -0.0551, -0.0633,
         0.0681,  0.0720,  0.0678,  0.0486,  0.0795, -0.0340,  0.0176,  0.0837,
        -0.0152,  0.0514, -0.0676,  0.0065,  0.0309, -0.0441, -0.0364, -0.0513,
        -0.0145, -0.0328,  0.0282,  0.0612, -0.0549, -0.0411,  0.0456,  0.0129],
       requires_grad=True)
</pre>
<p>For custom initialization, we want to modify these tensors in place. These are actually <a href="https://pytorch.org/docs/0.3.1/autograd.html">autograd Variables</a>, which perform automatic differentiation for us, so we need to get back the actual tensors with <code>model.hidden_one_to_hidden_two.weight.data</code>. Once we have the tensors, we can fill them with zeros (for biases) or random normal values.</p>
<p>Set biases to all zeros:</p>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden_one</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
<p>Sample from random normal with standard dev = 0.01</p>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden_one</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orge29ca5d">
<h2 id="orge29ca5d">Forward pass</h2>
<div class="outline-text-2" id="text-orge29ca5d">
<p>Now that we have a network, let's see what happens when we pass in an image.</p>
</div>
<div class="outline-3" id="outline-container-org17c2e1e">
<h3 id="org17c2e1e">Grab some data</h3>
<div class="outline-text-3" id="text-org17c2e1e">
<p>This next block grabs one batch of image data.</p>
<div class="highlight">
<pre><span></span><span class="n">batch</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">trainloader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
</pre></div>
<p>Now we need to resize the images into a 1D vector. The new shape is (batch size, color channels, image pixels).</p>
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org00414f2">
<h3 id="org00414f2">Forward pass through the network</h3>
<div class="outline-text-3" id="text-org00414f2">
<div class="highlight">
<pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">highest</span> <span class="o">=</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">highest</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[:,</span> <span class="n">highest</span><span class="p">])</span>
</pre></div>
<pre class="example">
tensor(2)
tensor([0.1173], grad_fn=&lt;SelectBackward&gt;)

</pre>
<p>It looks like we're predicting a 2.</p>
<div class="highlight">
<pre><span></span><span class="n">helper</span><span class="o">.</span><span class="n">view_classify</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">probabilities</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="image_2.png" src="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/image_2.png"></p>
</div>
<p>As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet so all the weights are random.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc6844ec">
<h2 id="orgc6844ec">Using <code>nn.Sequential</code></h2>
<div class="outline-text-2" id="text-orgc6844ec">
<p>PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, <code>nn.Sequential</code> (<a href="https://pytorch.org/docs/master/nn.html#torch.nn.Sequential">documentation</a>). This is how you use <code>Sequential</code> to build the equivalent network.</p>
</div>
<div class="outline-3" id="outline-container-orgaa61de1">
<h3 id="orgaa61de1">Hyperparameters For Our Network</h3>
<div class="outline-text-3" id="text-orgaa61de1">
<div class="highlight">
<pre><span></span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">hidden_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org076ac75">
<h3 id="org076ac75">Build a Feed-Forward Network</h3>
<div class="outline-text-3" id="text-org076ac75">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_size</span><span class="p">),</span>
                      <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example">
Sequential(
  (0): Linear(in_features=784, out_features=128, bias=True)
  (1): ReLU()
  (2): Linear(in_features=128, out_features=64, bias=True)
  (3): ReLU()
  (4): Linear(in_features=64, out_features=10, bias=True)
  (5): Softmax()
)

</pre></div>
</div>
<div class="outline-3" id="outline-container-org59da8d6">
<h3 id="org59da8d6">Forward Pass</h3>
<div class="outline-text-3" id="text-org59da8d6">
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">trainloader</span><span class="p">))</span>
<span class="n">images</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">helper</span><span class="o">.</span><span class="n">view_classify</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">probabilities</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="image_3.png" src="/posts/nano/pytorch/part-2-neural-networks-in-pytorch/image_3.png"></p>
</div>
<p>The operations are availble by passing in the appropriate index. For example, if you want to get the first Linear operation and look at the weights, you'd use <code>model[0]</code>.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
<pre class="example">
Linear(in_features=784, out_features=128, bias=True)
Parameter containing:
tensor([[-0.0229,  0.0106,  0.0077,  ...,  0.0079, -0.0073, -0.0182],
        [-0.0066,  0.0245,  0.0241,  ...,  0.0344,  0.0281,  0.0034],
        [-0.0349,  0.0127,  0.0119,  ..., -0.0351,  0.0160,  0.0235],
        ...,
        [-0.0328,  0.0114,  0.0204,  ...,  0.0265, -0.0114,  0.0215],
        [-0.0214, -0.0027, -0.0279,  ..., -0.0297, -0.0112, -0.0189],
        [ 0.0217,  0.0208, -0.0328,  ...,  0.0341,  0.0270, -0.0198]],
       requires_grad=True)
</pre>
<p>You can also pass in an <code>OrderedDict</code> to name the individual layers and operations, instead of using incremental integers. Note that dictionary keys must be unique, so <span class="underline">each operation must have a different name</span>.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([</span>
                      <span class="p">(</span><span class="s1">'input_to_hidden'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span>
                      <span class="p">(</span><span class="s1">'relu_1'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                      <span class="p">(</span><span class="s1">'hidden_to_hidden'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
                      <span class="p">(</span><span class="s1">'relu_2'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()),</span>
                      <span class="p">(</span><span class="s1">'output'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">output_size</span><span class="p">)),</span>
                      <span class="p">(</span><span class="s1">'softmax'</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))]))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example">
Sequential(
  (input_to_hidden): Linear(in_features=784, out_features=128, bias=True)
  (relu_1): ReLU()
  (hidden_to_hidden): Linear(in_features=128, out_features=64, bias=True)
  (relu_2): ReLU()
  (output): Linear(in_features=64, out_features=10, bias=True)
  (softmax): Softmax()
)

</pre>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">model</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="n">model</span><span class="o">.</span><span class="n">input_to_hidden</span>
</pre></div>
<pre class="example">
Linear(in_features=784, out_features=128, bias=True)
Linear(in_features=784, out_features=128, bias=True)

</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/sentiment_analysis/inspecting-the-weights/">Inspecting the Weights</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/sentiment_analysis/inspecting-the-weights/" rel="bookmark"><time class="published dt-published" datetime="2018-11-13T22:29:20-08:00" itemprop="datePublished" title="2018-11-13 22:29">2018-11-13 22:29</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/sentiment_analysis/inspecting-the-weights/#orgdaa9655">Set Up</a></li>
<li><a href="/posts/nano/sentiment_analysis/inspecting-the-weights/#org7ba52ad">What's Going on in the Weights?</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgdaa9655">
<h2 id="orgdaa9655">Set Up</h2>
<div class="outline-text-2" id="text-orgdaa9655"></div>
<div class="outline-3" id="outline-container-org4018fcc">
<h3 id="org4018fcc">Imports</h3>
<div class="outline-text-3" id="text-org4018fcc"></div>
<div class="outline-4" id="outline-container-orgb5f8fa0">
<h4 id="orgb5f8fa0">Python</h4>
<div class="outline-text-4" id="text-orgb5f8fa0">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org58b067a">
<h4 id="org58b067a">PyPi</h4>
<div class="outline-text-4" id="text-org58b067a">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">bokeh.embed</span> <span class="kn">import</span> <span class="n">autoload_static</span>
<span class="kn">from</span> <span class="nn">bokeh.plotting</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">figure</span><span class="p">,</span>
    <span class="n">ColumnDataSource</span><span class="p">,</span>
    <span class="p">)</span>
<span class="kn">from</span> <span class="nn">bokeh.models</span> <span class="kn">import</span> <span class="n">LabelSet</span>
<span class="kn">import</span> <span class="nn">bokeh.resources</span>
<span class="kn">import</span> <span class="nn">matplotlib.colors</span> <span class="kn">as</span> <span class="nn">colors</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org40b7874">
<h4 id="org40b7874">This Project</h4>
<div class="outline-text-4" id="text-org40b7874">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">sentiment_noise_reduction</span> <span class="kn">import</span> <span class="n">SentimentNoiseReduction</span>
<span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7ba52ad">
<h2 id="org7ba52ad">What's Going on in the Weights?</h2>
<div class="outline-text-2" id="text-org7ba52ad">
<p>Let's start with a model that doesn't have any noise cancellation.</p>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"x_train.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"y_train.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">mlp_full</span> <span class="o">=</span> <span class="n">SentimentNoiseReduction</span><span class="p">(</span><span class="n">lower_bound</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                   <span class="n">polarity_cutoff</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                   <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                   <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">mlp_full</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00 % Speed(reviews/sec): 0.00 Error: [-0.5] #Correct: 1 #Trained: 1 Training Accuracy: 100.00 %
Progress: 4.17 % Speed(reviews/sec): 125.00 Error: [-0.38320156] #Correct: 740 #Trained: 1001 Training Accuracy: 73.93 %
Progress: 8.33 % Speed(reviews/sec): 222.22 Error: [-0.26004622] #Correct: 1529 #Trained: 2001 Training Accuracy: 76.41 %
Progress: 12.50 % Speed(reviews/sec): 300.00 Error: [-0.40350302] #Correct: 2376 #Trained: 3001 Training Accuracy: 79.17 %
Progress: 16.67 % Speed(reviews/sec): 363.64 Error: [-0.23990249] #Correct: 3187 #Trained: 4001 Training Accuracy: 79.66 %
Progress: 20.83 % Speed(reviews/sec): 416.67 Error: [-0.14119144] #Correct: 4002 #Trained: 5001 Training Accuracy: 80.02 %
Progress: 25.00 % Speed(reviews/sec): 461.54 Error: [-0.06442389] #Correct: 4829 #Trained: 6001 Training Accuracy: 80.47 %
Progress: 29.17 % Speed(reviews/sec): 500.00 Error: [-0.03508728] #Correct: 5690 #Trained: 7001 Training Accuracy: 81.27 %
Progress: 33.33 % Speed(reviews/sec): 533.33 Error: [-0.05110633] #Correct: 6548 #Trained: 8001 Training Accuracy: 81.84 %
Progress: 37.50 % Speed(reviews/sec): 562.50 Error: [-0.07432703] #Correct: 7404 #Trained: 9001 Training Accuracy: 82.26 %
Progress: 41.67 % Speed(reviews/sec): 588.24 Error: [-0.26512013] #Correct: 8272 #Trained: 10001 Training Accuracy: 82.71 %
Progress: 45.83 % Speed(reviews/sec): 578.95 Error: [-0.14067275] #Correct: 9129 #Trained: 11001 Training Accuracy: 82.98 %
Progress: 50.00 % Speed(reviews/sec): 600.00 Error: [-0.01215903] #Correct: 9994 #Trained: 12001 Training Accuracy: 83.28 %
Progress: 54.17 % Speed(reviews/sec): 619.05 Error: [-0.33825111] #Correct: 10864 #Trained: 13001 Training Accuracy: 83.56 %
Progress: 58.33 % Speed(reviews/sec): 636.36 Error: [-0.00522004] #Correct: 11721 #Trained: 14001 Training Accuracy: 83.72 %
Progress: 62.50 % Speed(reviews/sec): 652.17 Error: [-0.49523538] #Correct: 12553 #Trained: 15001 Training Accuracy: 83.68 %
Progress: 66.67 % Speed(reviews/sec): 666.67 Error: [-0.20026672] #Correct: 13390 #Trained: 16001 Training Accuracy: 83.68 %
Progress: 70.83 % Speed(reviews/sec): 680.00 Error: [-0.20786817] #Correct: 14243 #Trained: 17001 Training Accuracy: 83.78 %
Progress: 75.00 % Speed(reviews/sec): 692.31 Error: [-0.03469862] #Correct: 15108 #Trained: 18001 Training Accuracy: 83.93 %
Progress: 79.17 % Speed(reviews/sec): 703.70 Error: [-0.99460657] #Correct: 15982 #Trained: 19001 Training Accuracy: 84.11 %
Progress: 83.33 % Speed(reviews/sec): 689.66 Error: [-0.0523489] #Correct: 16867 #Trained: 20001 Training Accuracy: 84.33 %
Progress: 87.50 % Speed(reviews/sec): 700.00 Error: [-0.28370015] #Correct: 17734 #Trained: 21001 Training Accuracy: 84.44 %
Progress: 91.67 % Speed(reviews/sec): 709.68 Error: [-0.33222958] #Correct: 18616 #Trained: 22001 Training Accuracy: 84.61 %
Progress: 95.83 % Speed(reviews/sec): 718.75 Error: [-0.17177784] #Correct: 19475 #Trained: 23001 Training Accuracy: 84.67 %
Training Time: 0:00:33.579950
</pre>
<p>Now here's a function to find the similarity of words in the vocabulary to a word, based on the dot product of the weights from the input layer to the hidden layer.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_most_similar_words</span><span class="p">(</span><span class="n">focus</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"horrible"</span><span class="p">,</span> <span class="n">count</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""Returns a list of similar words based on weights"""</span>
    <span class="n">most_similar</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">mlp_full</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">:</span>
        <span class="n">most_similar</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
            <span class="n">mlp_full</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">[</span><span class="n">mlp_full</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]],</span>
            <span class="n">mlp_full</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">[</span><span class="n">mlp_full</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">focus</span><span class="p">]])</span>    
    <span class="k">return</span> <span class="n">most_similar</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">similar</span> <span class="o">=</span> <span class="n">get_most_similar_words</span><span class="p">(</span><span class="s2">"excellent"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"|Token| Similarity|"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"|-+-|"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">similarity</span> <span class="ow">in</span> <span class="n">similar</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|{}|{:.2f}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">similarity</span><span class="p">))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Token</th>
<th class="org-right" scope="col">Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">excellent</td>
<td class="org-right">0.15</td>
</tr>
<tr>
<td class="org-left">perfect</td>
<td class="org-right">0.13</td>
</tr>
<tr>
<td class="org-left">great</td>
<td class="org-right">0.11</td>
</tr>
<tr>
<td class="org-left">amazing</td>
<td class="org-right">0.10</td>
</tr>
<tr>
<td class="org-left">wonderful</td>
<td class="org-right">0.10</td>
</tr>
<tr>
<td class="org-left">best</td>
<td class="org-right">0.10</td>
</tr>
<tr>
<td class="org-left">today</td>
<td class="org-right">0.09</td>
</tr>
<tr>
<td class="org-left">fun</td>
<td class="org-right">0.09</td>
</tr>
<tr>
<td class="org-left">loved</td>
<td class="org-right">0.08</td>
</tr>
<tr>
<td class="org-left">definitely</td>
<td class="org-right">0.08</td>
</tr>
</tbody>
</table>
<p><i>excellent</i> was, ouf course, most similar to itself, but we can see that the network's weights are most similar to each other when the words are most similar to each other - the network has 'learned' what words are similar to <i>excellent</i> using the training set.</p>
<p>Now a negative example.</p>
<div class="highlight">
<pre><span></span><span class="n">similar</span> <span class="o">=</span> <span class="n">get_most_similar_words</span><span class="p">(</span><span class="s2">"terrible"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"|Token|Similarity|"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"|-+-|"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">similarity</span> <span class="ow">in</span> <span class="n">similar</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|{}|{:.2f}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">similarity</span><span class="p">))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Token</th>
<th class="org-right" scope="col">Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">worst</td>
<td class="org-right">0.18</td>
</tr>
<tr>
<td class="org-left">awful</td>
<td class="org-right">0.13</td>
</tr>
<tr>
<td class="org-left">waste</td>
<td class="org-right">0.12</td>
</tr>
<tr>
<td class="org-left">poor</td>
<td class="org-right">0.10</td>
</tr>
<tr>
<td class="org-left">boring</td>
<td class="org-right">0.10</td>
</tr>
<tr>
<td class="org-left">terrible</td>
<td class="org-right">0.10</td>
</tr>
<tr>
<td class="org-left">bad</td>
<td class="org-right">0.08</td>
</tr>
<tr>
<td class="org-left">dull</td>
<td class="org-right">0.08</td>
</tr>
<tr>
<td class="org-left">worse</td>
<td class="org-right">0.08</td>
</tr>
<tr>
<td class="org-left">poorly</td>
<td class="org-right">0.07</td>
</tr>
</tbody>
</table>
<p>Once again, the more similar words were in sentiment, the closer the weights leading from their inputs became.</p>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"pos_neg_log_ratios.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">pos_neg_ratios</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">words_to_visualize</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">ratio</span> <span class="ow">in</span> <span class="n">pos_neg_ratios</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="k">if</span><span class="p">(</span><span class="n">word</span> <span class="ow">in</span> <span class="n">mlp_full</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">):</span>
        <span class="n">words_to_visualize</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">ratio</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">pos_neg_ratios</span><span class="o">.</span><span class="n">most_common</span><span class="p">()))[</span><span class="mi">0</span><span class="p">:</span><span class="mi">500</span><span class="p">]:</span>
    <span class="k">if</span><span class="p">(</span><span class="n">word</span> <span class="ow">in</span> <span class="n">mlp_full</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">):</span>
        <span class="n">words_to_visualize</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">neg</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">colors_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">vectors_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_to_visualize</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">pos_neg_ratios</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">vectors_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mlp_full</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">[</span><span class="n">mlp_full</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]])</span>
        <span class="k">if</span><span class="p">(</span><span class="n">pos_neg_ratios</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">pos</span><span class="o">+=</span><span class="mi">1</span>
            <span class="n">colors_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"#00ff00"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">neg</span><span class="o">+=</span><span class="mi">1</span>
            <span class="n">colors_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">"#000000"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">words_top_ted_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">vectors_list</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">plot</span> <span class="o">=</span> <span class="n">figure</span><span class="p">(</span><span class="n">tools</span><span class="o">=</span><span class="s2">"pan,wheel_zoom,reset,save"</span><span class="p">,</span>
              <span class="n">toolbar_location</span><span class="o">=</span><span class="s2">"above"</span><span class="p">,</span>
              <span class="n">plot_width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
              <span class="n">plot_height</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
              <span class="n">title</span><span class="o">=</span><span class="s2">"vector T-SNE for most polarized words"</span><span class="p">)</span>

<span class="n">source</span> <span class="o">=</span> <span class="n">ColumnDataSource</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">x1</span><span class="o">=</span><span class="n">words_top_ted_tsne</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
                                    <span class="n">x2</span><span class="o">=</span><span class="n">words_top_ted_tsne</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
                                    <span class="n">names</span><span class="o">=</span><span class="n">words_to_visualize</span><span class="p">,</span>
                                    <span class="n">color</span><span class="o">=</span><span class="n">colors_list</span><span class="p">))</span>

<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x1"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"x2"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">,</span> <span class="n">fill_color</span><span class="o">=</span><span class="s2">"color"</span><span class="p">)</span>

<span class="n">word_labels</span> <span class="o">=</span> <span class="n">LabelSet</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x1"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"x2"</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">"names"</span><span class="p">,</span> <span class="n">y_offset</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
                  <span class="n">text_font_size</span><span class="o">=</span><span class="s2">"8pt"</span><span class="p">,</span> <span class="n">text_color</span><span class="o">=</span><span class="s2">"#555555"</span><span class="p">,</span>
                  <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">,</span> <span class="n">text_align</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">add_layout</span><span class="p">(</span><span class="n">word_labels</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">FOLDER_PATH</span> <span class="o">=</span> <span class="s2">"../../../files/posts/nano/sentiment_analysis/inspecting-the-weights/"</span>
<span class="n">FILE_NAME</span> <span class="o">=</span> <span class="s2">"tsne.js"</span>
<span class="n">bokeh_cdn</span> <span class="o">=</span> <span class="n">bokeh</span><span class="o">.</span><span class="n">resources</span><span class="o">.</span><span class="n">CDN</span>
<span class="n">javascript</span><span class="p">,</span> <span class="n">source</span> <span class="o">=</span> <span class="n">autoload_static</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">bokeh_cdn</span><span class="p">,</span> <span class="n">FILE_NAME</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">FOLDER_PATH</span> <span class="o">+</span> <span class="n">FILE_NAME</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
    <span class="n">writer</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">javascript</span><span class="p">)</span>
</pre></div>
<script id="f6d6808a-e555-43f2-ad3a-a1ace816aa10" src="/posts/nano/sentiment_analysis/inspecting-the-weights/tsne.js"></script>
<p>Green indicates positive words, black indicates negative words, but it looks like none of the 500 most common words are negative.</p>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/sentiment_analysis/further-noise-reduction/">Further Noise Reduction</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/sentiment_analysis/further-noise-reduction/" rel="bookmark"><time class="published dt-published" datetime="2018-11-13T21:15:00-08:00" itemprop="datePublished" title="2018-11-13 21:15">2018-11-13 21:15</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/sentiment_analysis/further-noise-reduction/#orgeacc8d2">Set Up</a></li>
<li><a href="posts/nano/sentiment_analysis/further-noise-reduction/#orgf408023">Further Noise Reduction</a></li>
<li><a href="posts/nano/sentiment_analysis/further-noise-reduction/#orgde0e4cb">Reducing Noise by Strategically Reducing the Vocabulary</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgeacc8d2">
<h2 id="orgeacc8d2">Set Up</h2>
<div class="outline-text-2" id="text-orgeacc8d2"></div>
<div class="outline-3" id="outline-container-org8c5bd30">
<h3 id="org8c5bd30">Debug</h3>
<div class="outline-text-3" id="text-org8c5bd30">
<div class="highlight">
<pre><span></span><span class="o">%</span><span class="n">load_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org866ce0a">
<h3 id="org866ce0a">Imports</h3>
<div class="outline-text-3" id="text-org866ce0a"></div>
<div class="outline-4" id="outline-container-org81b27d7">
<h4 id="org81b27d7">Python Standard Library</h4>
<div class="outline-text-4" id="text-org81b27d7">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org63963ab">
<h4 id="org63963ab">PyPi</h4>
<div class="outline-text-4" id="text-org63963ab">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org14e60f3">
<h4 id="org14e60f3">This Project</h4>
<div class="outline-text-4" id="text-org14e60f3">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org1e82ada">
<h3 id="org1e82ada">Tables</h3>
<div class="outline-text-3" id="text-org1e82ada">
<div class="highlight">
<pre><span></span><span class="n">table</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">tabulate</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">"orgtbl"</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">"keys"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb2f4fa1">
<h3 id="orgb2f4fa1">Plotting</h3>
<div class="outline-text-3" id="text-orgb2f4fa1">
<div class="highlight">
<pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">"whitegrid"</span><span class="p">)</span>
<span class="n">FIGURE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0477ef0">
<h3 id="org0477ef0">Helpers</h3>
<div class="outline-text-3" id="text-org0477ef0">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">print_most_common</span><span class="p">(</span><span class="n">counter</span><span class="p">:</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">count</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""Prints most common tokens as an org-tabel"""</span>
    <span class="n">tokens</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">reverse</span><span class="o">=</span><span class="n">bottom</span><span class="p">)[:</span><span class="n">count</span><span class="p">]:</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">counts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">Token</span><span class="o">=</span><span class="n">tokens</span><span class="p">,</span> <span class="n">Count</span><span class="o">=</span><span class="n">counts</span><span class="p">)))</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgf408023">
<h2 id="orgf408023">Further Noise Reduction</h2>
<div class="outline-text-2" id="text-orgf408023">
<p>Speeding up our network by only using relevant nodes was a useful thing insofar as it lets us train bigger datasets without having to wait infeasible amounts of time, but it doesn't directly address the problem we saw earlier, which is that many of our nodes don't actually contribute to the classification.</p>
<p>Here's the words that are most commonly positive.</p>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"pos_neg_ratios.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
    <span class="n">pos_neg_ratios</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">writer</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">print_most_common</span><span class="p">(</span><span class="n">pos_neg_ratios</span><span class="p">)</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Token</th>
<th class="org-right" scope="col">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">&nbsp;</td>
<td class="org-right">0.976102</td>
</tr>
<tr>
<td class="org-left">.</td>
<td class="org-right">0.952936</td>
</tr>
<tr>
<td class="org-left">a</td>
<td class="org-right">1.05504</td>
</tr>
<tr>
<td class="org-left">aa</td>
<td class="org-right">0.5</td>
</tr>
<tr>
<td class="org-left">aaa</td>
<td class="org-right">0.428571</td>
</tr>
<tr>
<td class="org-left">aaaaaaah</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">aaaaah</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">aaaaatch</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">aaaahhhhhhh</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">aaaand</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
<p>It's difficult to imagine that these are really telling us how to discern a positive review, since they are mostly names, not descriptive adjectives, nouns, or the like.</p>
<p>Here's the most common negative words.</p>
<div class="highlight">
<pre><span></span><span class="n">print_most_common</span><span class="p">(</span><span class="n">pos_neg_ratios</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Token</th>
<th class="org-right" scope="col">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zzzzzzzzzzzzz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zzzzzzzzzzzz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zzzzzzzz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zzzzz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zzzz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zz</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">zyuranger</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">frequency_frequency</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>

<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">cnt</span> <span class="ow">in</span> <span class="n">total_counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">():</span>
    <span class="n">frequency_frequency</span><span class="p">[</span><span class="n">cnt</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">frequency_frequency</span><span class="o">.</span><span class="n">most_common</span><span class="p">())))</span>
</pre></div>
<div class="figure">
<p><img alt="frequencies.png" src="posts/nano/sentiment_analysis/further-noise-reduction/frequencies.png"></p>
</div>
<p>As we can see from the plot, there are a small number of terms that make up a significant amount of the tokens, and a significant amount of the terms that don't really contribute to the outcome.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgde0e4cb">
<h2 id="orgde0e4cb">Reducing Noise by Strategically Reducing the Vocabulary</h2>
<div class="outline-text-2" id="text-orgde0e4cb">
<p>We're going to try and improve the network by not including tokens that are too rare or don't contribute enough to the sentiments.</p>
<div class="highlight">
<pre><span></span><span class="c1"># python standard library</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># from pypi</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">sentimental_network</span> <span class="kn">import</span> <span class="n">SentiMental</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgc6c69cb">
<h3 id="orgc6c69cb">The Sentiment Noise Reduction Network</h3>
<div class="outline-text-3" id="text-orgc6c69cb">
<p>This is going to be kind of another overhaul of our network. We're going to build off of our previous Sentiment Network that only did calculations on tokens per review, not on the entire vocabulary.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">SentimentNoiseReduction</span><span class="p">(</span><span class="n">SentiMental</span><span class="p">):</span>
    <span class="sd">"""reduces noise</span>

<span class="sd">    ... uml::</span>

<span class="sd">       SentimentNoiseReduction --|&gt; SentiMental</span>

<span class="sd">    Args:</span>
<span class="sd">     lower_bound: threshold to add token to network</span>
<span class="sd">     polarity_cutoff: threshold for positive-negative ratio for words</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">polarity_cutoff</span><span class="p">,</span> <span class="n">lower_bound</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lower_bound</span> <span class="o">=</span> <span class="n">lower_bound</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">polarity_cutoff</span> <span class="o">=</span> <span class="n">polarity_cutoff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positive_counts</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_negative_counts</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_total_counts</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positive_negative_ratios</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org95e42d7">
<h3 id="org95e42d7">The Review Vocabulary</h3>
<div class="outline-text-3" id="text-org95e42d7">
<p>Our first change is that we'll only add words to the vocabulary that meet certain thresholds. Unfortunately the way the attributes are currently set up, this needs the counts to be set up so it has the side effect of calling the <code>count_tokens</code> method.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">review_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sd">"""list of tokens in the reviews"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># this needs to be called before total counts is used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count_tokens</span><span class="p">()</span>
        <span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reviews</span><span class="p">:</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">review</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">))</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">(</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span>
                      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_counts</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">lower_bound</span><span class="p">)</span>
            <span class="n">tokens</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span>
                <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positive_negative_ratios</span><span class="p">[</span><span class="n">token</span><span class="p">])</span>
                       <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">polarity_cutoff</span><span class="p">)</span>
            <span class="n">vocabulary</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org37cef9c">
<h3 id="org37cef9c">Positive Counts</h3>
<div class="outline-text-3" id="text-org37cef9c">
<p>This is actually a building-block for our positive-to-negative ratios. It just holds the counts of the tokens in the positive reviews.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">positive_counts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="sd">"""Token counts for positive reviews"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_counts</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positive_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_counts</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc8cdb9c">
<h3 id="orgc8cdb9c">Negative Counts</h3>
<div class="outline-text-3" id="text-orgc8cdb9c">
<p>Like the negative counts, this is the count of tokens in the negative reviews.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">negative_counts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="sd">"""Token counts for negative reviews"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative_counts</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_negative_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative_counts</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7077730">
<h3 id="org7077730">Total Counts</h3>
<div class="outline-text-3" id="text-org7077730">
<p>Once again related to the outher counts, this holds the counts for all tokens, regardless of their sentiment.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">total_counts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="sd">"""Token counts for total reviews"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_counts</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_total_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_counts</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org646d373">
<h3 id="org646d373">Positive to Negative Ratios</h3>
<div class="outline-text-3" id="text-org646d373">
<p>This holds the logarithms of the ratios of positive to negative sentiments for a given token.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">positive_negative_ratios</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="sd">"""log-ratio of positive to negative reviews"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_negative_ratios</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">positive_negative_ratios</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
        <span class="n">positive_negative_ratios</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span><span class="n">token</span><span class="p">:</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">positive_counts</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
             <span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">negative_counts</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
             <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_counts</span><span class="p">})</span>
        <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">ratio</span> <span class="ow">in</span> <span class="n">positive_negative_ratios</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">ratio</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">positive_negative_ratios</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">ratio</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">positive_negative_ratios</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">ratio</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positive_negative_ratios</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positive_negative_ratios</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">positive_negative_ratios</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_negative_ratios</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org09d4a95">
<h3 id="org09d4a95">Count Tokens</h3>
<div class="outline-text-3" id="text-org09d4a95">
<p>This is a method to populate the token counters.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">count_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Populate the count-tokens"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reset_counters</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">review</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reviews</span><span class="p">):</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">review</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_counts</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="s2">"POSITIVE"</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">positive_counts</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>        
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">negative_counts</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd24652f">
<h3 id="orgd24652f">Reset Counters</h3>
<div class="outline-text-3" id="text-orgd24652f">
<p>This sets all the counters back to none. It is called by the <code>count_tokens</code> method, but in practice shouldn't really be needed.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">reset_counters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Set the counters back to none"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_positive_counts</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_negative_counts</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_total_counts</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_positive_negative_ratios</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9499d85">
<h3 id="org9499d85">Train and Test The Network</h3>
<div class="outline-text-3" id="text-org9499d85">
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"x_train.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"y_train.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">sentiment_noise_reduction</span> <span class="kn">import</span> <span class="n">SentimentNoiseReduction</span>
<span class="n">sentimental</span> <span class="o">=</span> <span class="n">SentimentNoiseReduction</span><span class="p">(</span><span class="n">lower_bound</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                                      <span class="n">polarity_cutoff</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                                      <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                      <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sentimental</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00 % Speed(reviews/sec): 0.00 Error: [-0.5] #Correct: 1 #Trained: 1 Training Accuracy: 100.00 %
Progress: 4.17 % Speed(reviews/sec): 111.11 Error: [-0.36634265] #Correct: 748 #Trained: 1001 Training Accuracy: 74.73 %
Progress: 8.33 % Speed(reviews/sec): 200.00 Error: [-0.2621193] #Correct: 1549 #Trained: 2001 Training Accuracy: 77.41 %
Progress: 12.50 % Speed(reviews/sec): 272.73 Error: [-0.39176697] #Correct: 2396 #Trained: 3001 Training Accuracy: 79.84 %
Progress: 16.67 % Speed(reviews/sec): 333.33 Error: [-0.24778501] #Correct: 3211 #Trained: 4001 Training Accuracy: 80.25 %
Progress: 20.83 % Speed(reviews/sec): 384.62 Error: [-0.16868621] #Correct: 4031 #Trained: 5001 Training Accuracy: 80.60 %
Progress: 25.00 % Speed(reviews/sec): 428.57 Error: [-0.05009294] #Correct: 4857 #Trained: 6001 Training Accuracy: 80.94 %
Progress: 29.17 % Speed(reviews/sec): 466.67 Error: [-0.04235332] #Correct: 5726 #Trained: 7001 Training Accuracy: 81.79 %
Progress: 33.33 % Speed(reviews/sec): 500.00 Error: [-0.05128397] #Correct: 6583 #Trained: 8001 Training Accuracy: 82.28 %
Progress: 37.50 % Speed(reviews/sec): 529.41 Error: [-0.09180182] #Correct: 7434 #Trained: 9001 Training Accuracy: 82.59 %
Progress: 41.67 % Speed(reviews/sec): 555.56 Error: [-0.3652018] #Correct: 8307 #Trained: 10001 Training Accuracy: 83.06 %
Progress: 45.83 % Speed(reviews/sec): 578.95 Error: [-0.21013078] #Correct: 9162 #Trained: 11001 Training Accuracy: 83.28 %
Progress: 50.00 % Speed(reviews/sec): 600.00 Error: [-0.01534277] #Correct: 10021 #Trained: 12001 Training Accuracy: 83.50 %
Progress: 54.17 % Speed(reviews/sec): 619.05 Error: [-0.25971145] #Correct: 10893 #Trained: 13001 Training Accuracy: 83.79 %
Progress: 58.33 % Speed(reviews/sec): 636.36 Error: [-0.0084308] #Correct: 11754 #Trained: 14001 Training Accuracy: 83.95 %
Progress: 62.50 % Speed(reviews/sec): 652.17 Error: [-0.46920695] #Correct: 12591 #Trained: 15001 Training Accuracy: 83.93 %
Progress: 66.67 % Speed(reviews/sec): 666.67 Error: [-0.19061036] #Correct: 13441 #Trained: 16001 Training Accuracy: 84.00 %
Progress: 70.83 % Speed(reviews/sec): 680.00 Error: [-0.22740865] #Correct: 14295 #Trained: 17001 Training Accuracy: 84.08 %
Progress: 75.00 % Speed(reviews/sec): 692.31 Error: [-0.0372273] #Correct: 15171 #Trained: 18001 Training Accuracy: 84.28 %
Progress: 79.17 % Speed(reviews/sec): 703.70 Error: [-0.99387849] #Correct: 16045 #Trained: 19001 Training Accuracy: 84.44 %
Progress: 83.33 % Speed(reviews/sec): 714.29 Error: [-0.05559484] #Correct: 16930 #Trained: 20001 Training Accuracy: 84.65 %
Progress: 87.50 % Speed(reviews/sec): 724.14 Error: [-0.35082069] #Correct: 17805 #Trained: 21001 Training Accuracy: 84.78 %
Progress: 91.67 % Speed(reviews/sec): 733.33 Error: [-0.43847381] #Correct: 18693 #Trained: 22001 Training Accuracy: 84.96 %
Progress: 95.83 % Speed(reviews/sec): 741.94 Error: [-0.1589986] #Correct: 19546 #Trained: 23001 Training Accuracy: 84.98 %
Training Time: 0:00:32.760293
</pre>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"x_test.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"y_test.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sentimental</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00% Speed(reviews/sec): 0.00 #Correct: 1 #Tested: 1 Testing Accuracy: 100.00 %
Progress: 10.00% Speed(reviews/sec): 0.00 #Correct: 92 #Tested: 101 Testing Accuracy: 91.09 %
Progress: 20.00% Speed(reviews/sec): 0.00 #Correct: 176 #Tested: 201 Testing Accuracy: 87.56 %
Progress: 30.00% Speed(reviews/sec): 0.00 #Correct: 266 #Tested: 301 Testing Accuracy: 88.37 %
Progress: 40.00% Speed(reviews/sec): 0.00 #Correct: 353 #Tested: 401 Testing Accuracy: 88.03 %
Progress: 50.00% Speed(reviews/sec): 0.00 #Correct: 443 #Tested: 501 Testing Accuracy: 88.42 %
Progress: 60.00% Speed(reviews/sec): 0.00 #Correct: 531 #Tested: 601 Testing Accuracy: 88.35 %
Progress: 70.00% Speed(reviews/sec): 0.00 #Correct: 605 #Tested: 701 Testing Accuracy: 86.31 %
Progress: 80.00% Speed(reviews/sec): 0.00 #Correct: 683 #Tested: 801 Testing Accuracy: 85.27 %
Progress: 90.00% Speed(reviews/sec): 0.00 #Correct: 770 #Tested: 901 Testing Accuracy: 85.46 %
</pre>
<p>Strangely it deson't seem to have sped up the time or improved the testing accuracy. Now a network with a higher polarity cutoff.</p>
<div class="highlight">
<pre><span></span><span class="n">sentimental</span> <span class="o">=</span> <span class="n">SentimentNoiseReduction</span><span class="p">(</span><span class="n">lower_bound</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                                      <span class="n">polarity_cutoff</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                                      <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                      <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">sentimental</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00 % Speed(reviews/sec): 0.00 Error: [-0.5] #Correct: 1 #Trained: 1 Training Accuracy: 100.00 %
Progress: 4.17 % Speed(reviews/sec): 125.00 Error: [-0.39461068] #Correct: 840 #Trained: 1001 Training Accuracy: 83.92 %
Progress: 8.33 % Speed(reviews/sec): 250.00 Error: [-0.51977448] #Correct: 1659 #Trained: 2001 Training Accuracy: 82.91 %
Progress: 12.50 % Speed(reviews/sec): 333.33 Error: [-0.58021736] #Correct: 2490 #Trained: 3001 Training Accuracy: 82.97 %
Progress: 16.67 % Speed(reviews/sec): 444.44 Error: [-0.48964892] #Correct: 3300 #Trained: 4001 Training Accuracy: 82.48 %
Progress: 20.83 % Speed(reviews/sec): 555.56 Error: [-0.41779146] #Correct: 4112 #Trained: 5001 Training Accuracy: 82.22 %
Progress: 25.00 % Speed(reviews/sec): 666.67 Error: [-0.118178] #Correct: 4925 #Trained: 6001 Training Accuracy: 82.07 %
Progress: 29.17 % Speed(reviews/sec): 777.78 Error: [-0.260138] #Correct: 5758 #Trained: 7001 Training Accuracy: 82.25 %
Progress: 33.33 % Speed(reviews/sec): 888.89 Error: [-0.20240952] #Correct: 6590 #Trained: 8001 Training Accuracy: 82.36 %
Progress: 37.50 % Speed(reviews/sec): 900.00 Error: [-0.33177588] #Correct: 7428 #Trained: 9001 Training Accuracy: 82.52 %
Progress: 41.67 % Speed(reviews/sec): 1000.00 Error: [-0.38912057] #Correct: 8276 #Trained: 10001 Training Accuracy: 82.75 %
Progress: 45.83 % Speed(reviews/sec): 1100.00 Error: [-0.26656737] #Correct: 9113 #Trained: 11001 Training Accuracy: 82.84 %
Progress: 50.00 % Speed(reviews/sec): 1200.00 Error: [-0.24639801] #Correct: 9953 #Trained: 12001 Training Accuracy: 82.93 %
Progress: 54.17 % Speed(reviews/sec): 1300.00 Error: [-0.25407967] #Correct: 10813 #Trained: 13001 Training Accuracy: 83.17 %
Progress: 58.33 % Speed(reviews/sec): 1272.73 Error: [-0.09205417] #Correct: 11658 #Trained: 14001 Training Accuracy: 83.27 %
Progress: 62.50 % Speed(reviews/sec): 1363.64 Error: [-0.33561732] #Correct: 12484 #Trained: 15001 Training Accuracy: 83.22 %
Progress: 66.67 % Speed(reviews/sec): 1454.55 Error: [-0.25248647] #Correct: 13309 #Trained: 16001 Training Accuracy: 83.18 %
Progress: 70.83 % Speed(reviews/sec): 1545.45 Error: [-0.17532308] #Correct: 14150 #Trained: 17001 Training Accuracy: 83.23 %
Progress: 75.00 % Speed(reviews/sec): 1636.36 Error: [-0.06026015] #Correct: 15002 #Trained: 18001 Training Accuracy: 83.34 %
Progress: 79.17 % Speed(reviews/sec): 1583.33 Error: [-0.96510939] #Correct: 15874 #Trained: 19001 Training Accuracy: 83.54 %
Progress: 83.33 % Speed(reviews/sec): 1666.67 Error: [-0.12708723] #Correct: 16732 #Trained: 20001 Training Accuracy: 83.66 %
Progress: 87.50 % Speed(reviews/sec): 1750.00 Error: [-0.11112597] #Correct: 17603 #Trained: 21001 Training Accuracy: 83.82 %
Progress: 91.67 % Speed(reviews/sec): 1833.33 Error: [-0.26326772] #Correct: 18456 #Trained: 22001 Training Accuracy: 83.89 %
Progress: 95.83 % Speed(reviews/sec): 1916.67 Error: [-0.33464499] #Correct: 19311 #Trained: 23001 Training Accuracy: 83.96 %
Training Time: 0:00:13.196065
</pre>
<div class="highlight">
<pre><span></span><span class="n">sentimental</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00% Speed(reviews/sec): 0.00 #Correct: 0 #Tested: 1 Testing Accuracy: 0.00 %
Progress: 10.00% Speed(reviews/sec): 0.00 #Correct: 85 #Tested: 101 Testing Accuracy: 84.16 %
Progress: 20.00% Speed(reviews/sec): 0.00 #Correct: 172 #Tested: 201 Testing Accuracy: 85.57 %
Progress: 30.00% Speed(reviews/sec): 0.00 #Correct: 263 #Tested: 301 Testing Accuracy: 87.38 %
Progress: 40.00% Speed(reviews/sec): 0.00 #Correct: 341 #Tested: 401 Testing Accuracy: 85.04 %
Progress: 50.00% Speed(reviews/sec): 0.00 #Correct: 431 #Tested: 501 Testing Accuracy: 86.03 %
Progress: 60.00% Speed(reviews/sec): 0.00 #Correct: 515 #Tested: 601 Testing Accuracy: 85.69 %
Progress: 70.00% Speed(reviews/sec): 0.00 #Correct: 589 #Tested: 701 Testing Accuracy: 84.02 %
Progress: 80.00% Speed(reviews/sec): 0.00 #Correct: 660 #Tested: 801 Testing Accuracy: 82.40 %
Progress: 90.00% Speed(reviews/sec): 0.00 #Correct: 745 #Tested: 901 Testing Accuracy: 82.69 %
</pre>
<p>This speeds it up quite a bit (at least the training), although the trade-off in accuracy might be something to watch out for. But in some cases the speed-up will help either to run the model or to use bigger data-sets. In fact, if we had a larger data set it's entirely possible that the trade-off would be worth it.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/sentiment_analysis/making-the-network-more-efficient/">Making the Network More Efficient</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/sentiment_analysis/making-the-network-more-efficient/" rel="bookmark"><time class="published dt-published" datetime="2018-11-13T14:45:07-08:00" itemprop="datePublished" title="2018-11-13 14:45">2018-11-13 14:45</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/sentiment_analysis/making-the-network-more-efficient/#org87d0176">Set Up</a></li>
<li><a href="posts/nano/sentiment_analysis/making-the-network-more-efficient/#orgf151e5b">Loading the Network</a></li>
<li><a href="posts/nano/sentiment_analysis/making-the-network-more-efficient/#org11b1749">Analyzing Inefficiencies in our Network</a></li>
<li><a href="posts/nano/sentiment_analysis/making-the-network-more-efficient/#orgb05526f">Making our Network More Efficient</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org87d0176">
<h2 id="org87d0176">Set Up</h2>
<div class="outline-text-2" id="text-org87d0176"></div>
<div class="outline-3" id="outline-container-orgeee4e37">
<h3 id="orgeee4e37">Imports</h3>
<div class="outline-text-3" id="text-orgeee4e37"></div>
<div class="outline-4" id="outline-container-org05c65ba">
<h4 id="org05c65ba">Python</h4>
<div class="outline-text-4" id="text-org05c65ba">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgdf16722">
<h4 id="orgdf16722">PyPy</h4>
<div class="outline-text-4" id="text-orgdf16722">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org94d4934">
<h4 id="org94d4934">This Project</h4>
<div class="outline-text-4" id="text-org94d4934">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">network_helpers</span> <span class="kn">import</span> <span class="n">update_input_layer</span>
<span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
<span class="kn">from</span> <span class="nn">sentiment_renetwork</span> <span class="kn">import</span> <span class="n">SentimentRenetwork</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgf151e5b">
<h2 id="orgf151e5b">Loading the Network</h2>
<div class="outline-text-2" id="text-orgf151e5b">
<p>I pickled our last network where we converted it from counting all the tokens in a review to just noting if the word was in the review.</p>
<div class="highlight">
<pre><span></span><span class="n">sentimental</span> <span class="o">=</span> <span class="n">SentimentRenetwork</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"x_train.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">x_train</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"y_train.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s1">'x_test.pkl'</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">x_test</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"y_test.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"sentimental_renetwork.pkl"</span><span class="p">)</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">sentimental</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org11b1749">
<h2 id="org11b1749">Analyzing Inefficiencies in our Network</h2>
<div class="outline-text-2" id="text-org11b1749">
<p>One of the problems with the way we're doing this is that the input layer is fairly large.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">sentimental</span><span class="o">.</span><span class="n">input_layer</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(1, 72810)

</pre>
<p>It has almost 73,000 inputs, and most of the reviews are going to only match a small subset of the nodes, so when we do our calculations to pass values on to the hidden layers, most of the arithmetic isn't doing anything because the 0 input is being multiplied by the weight, which sets it to 0 before then being added to the other inputs. Numpy is fast, but maybe getting rid of the extra computations will make it better.</p>
<p>Let's look at a toy example, we'll start with an empty input layer.</p>
<div class="highlight">
<pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]

</pre>
<p>Now, we'll say that our review has two token in it that match our vocabulary.</p>
<div class="highlight">
<pre><span></span><span class="n">input_layer</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">input_layer</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]

</pre>
<p>Okay, so that's the input layer, now we'll make a set of weights.</p>
<div class="highlight">
<pre><span></span><span class="n">weights_input_to_hidden</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
<p>And now we'll take the dot-product to see what the input to the hidden layer will be.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_output</span> <span class="o">=</span> <span class="n">input_layer</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights_input_to_hidden</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
</pre></div>
<pre class="example">
[-2.94776967 -1.0695755   1.30840025  1.1845772  -1.73688691]

</pre>
<p>But what happens if we only update the nodes that have a value?</p>
<div class="highlight">
<pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
    <span class="n">hidden_layer</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">*</span> <span class="n">weights_input_to_hidden</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_layer</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">hidden_layer</span><span class="p">,</span> <span class="n">hidden_output</span><span class="p">)</span>
</pre></div>
<pre class="example">
[-2.94776967 -1.0695755   1.30840025  1.1845772  -1.73688691]

</pre>
<p>We get the same outcome but this time we did fewer computations.</p>
<p>But now, you might be wondering - <i>Why are we multiplying the weights by 1?</i>. And that's a good question, the answer is that is a translation of what the neural network is doing - every node that matches a token in the review gets a one which is multiplied by the weights - but looking at it, it doesn't make sense, does it?</p>
</div>
<div class="outline-3" id="outline-container-org988d591">
<h3 id="org988d591">Take Two</h3>
<div class="outline-text-3" id="text-org988d591">
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">:</span>
    <span class="n">hidden_layer</span> <span class="o">+=</span> <span class="p">(</span><span class="n">weights_input_to_hidden</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">hidden_output</span><span class="p">,</span> <span class="n">hidden_layer</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_layer</span><span class="p">)</span>
</pre></div>
<pre class="example">
[-2.94776967 -1.0695755   1.30840025  1.1845772  -1.73688691]

</pre>
<p>So now we've reduced our calculation to two additions. Of course, there's the question of the efficiency of a for loop in python versus vector multiplication in numpy. But maybe it helps.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb05526f">
<h2 id="orgb05526f">Making our Network More Efficient</h2>
<div class="outline-text-2" id="text-orgb05526f">
<p>We're going to make the <code>SentimentNetwork</code> more efficient by eliminating unnecessary multiplications and additions that occur during forward and backward propagation. Unfortunately this is going to require more work than with the previous example.</p>
</div>
<div class="outline-3" id="outline-container-org19ad71a">
<h3 id="org19ad71a">Imports</h3>
<div class="outline-text-3" id="text-org19ad71a">
<p>We're going to eliminate the input layer entirely here so I'm going to use the pre-noise-reduction network.</p>
<div class="highlight">
<pre><span></span><span class="c1"># python standard library</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="c1"># from pypi</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">sentiment_network</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Classification</span><span class="p">,</span>
    <span class="n">SentimentNetwork</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgacafb7d">
<h3 id="orgacafb7d">The Sentimental Constructor</h3>
<div class="outline-text-3" id="text-orgacafb7d">
<p>We're adding a hidden layer to the network.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">SentiMental</span><span class="p">(</span><span class="n">SentimentNetwork</span><span class="p">):</span>
    <span class="sd">"""Implements a slightly optimized version"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layer</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_for_label</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">return</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">hidden_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""The hidden layer nodes"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layer</span>

    <span class="nd">@hidden_layer.setter</span>
    <span class="k">def</span> <span class="nf">hidden_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nodes</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sd">"""Set the hidden nodes"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_layer</span> <span class="o">=</span> <span class="n">nodes</span>
        <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc03d5ab">
<h3 id="orgc03d5ab">Target for the Label</h3>
<div class="outline-text-3" id="text-orgc03d5ab">
<p>Although we have a method to get the target I'm going to add a dictionary version as well</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">target_for_label</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""target to label map"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_for_label</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_target_for_label</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">POSITIVE</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">NEGATIVE</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_for_label</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org3cbdcaa">
<h3 id="org3cbdcaa">The Train Method</h3>
<div class="outline-text-3" id="text-org3cbdcaa">
<p>Because we're eliminating the input layer and adding a hidden layer we have to re-do the training method from scratch.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reviews</span><span class="p">:</span><span class="nb">list</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span><span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""Trains the model</span>

<span class="sd">    Args:</span>
<span class="sd">     reviews: list of reviews</span>
<span class="sd">     labels: list of labels for each review</span>
<span class="sd">    """</span>
    <span class="c1"># there are side-effects that require self.reviews and self.labels</span>
    <span class="c1"># maybe I should re-factor.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reviews</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">reviews</span><span class="p">,</span> <span class="n">labels</span>

    <span class="c1"># make sure out we have a matching number of reviews and labels</span>
    <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">reviews</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
        <span class="n">correct_so_far</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># loop through all the given reviews and run a forward and backward pass,</span>
    <span class="c1"># updating weights for every item</span>
    <span class="n">reviews_labels</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">reviews</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="n">n_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">reviews</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">review</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reviews_labels</span><span class="p">):</span>
        <span class="c1"># feed-forward</span>
        <span class="c1"># Note: I keep thining I can just call run, but our error correction needs</span>
        <span class="c1"># the input layer so we have to do all the calculations</span>
        <span class="c1"># input layer is a list of indices for unique words in the review</span>
        <span class="c1"># that are in our vocabulary</span>

        <span class="n">input_layer</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
                       <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">review</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">))</span>
                       <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span> <span class="o">*=</span> <span class="mi">0</span>

        <span class="c1"># here there's no multiplcation, just an implicit multiplication of 1</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">input_layer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>

        <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_outputs</span><span class="p">)</span>

        <span class="c1"># Backpropagation</span>
        <span class="c1"># we need to calculate the output_error separately to update our correct count</span>
        <span class="n">output_error</span> <span class="o">=</span> <span class="n">output</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_for_label</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>

        <span class="c1"># we applied a sigmoid to the output so we need to apply the derivative</span>
        <span class="n">hidden_to_output_delta</span> <span class="o">=</span> <span class="n">output_error</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid_output_to_derivative</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="n">input_to_hidden_error</span> <span class="o">=</span> <span class="n">hidden_to_output_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="c1"># we didn't apply a function to the inputs to the hidden layer</span>
        <span class="c1"># so we don't need a derivative</span>
        <span class="n">input_to_hidden_delta</span> <span class="o">=</span> <span class="n">input_to_hidden_error</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
            <span class="n">hidden_to_output_delta</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">input_layer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>
                <span class="o">*</span> <span class="n">input_to_hidden_delta</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">output</span> <span class="o">&lt;</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">label</span><span class="o">==</span><span class="s2">"NEGATIVE"</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">output</span> <span class="o">&gt;=</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">label</span><span class="o">==</span><span class="s2">"POSITIVE"</span><span class="p">):</span>
                <span class="n">correct_so_far</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">index</span> <span class="o">%</span> <span class="mi">1000</span><span class="p">:</span>
                <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
                <span class="n">reviews_per_second</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span><span class="o">/</span><span class="n">elapsed_time</span><span class="o">.</span><span class="n">seconds</span>
                                      <span class="k">if</span> <span class="n">elapsed_time</span><span class="o">.</span><span class="n">seconds</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span>
                    <span class="s2">"Progress: {:.2f} %"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">index</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">reviews</span><span class="p">))</span>
                    <span class="o">+</span> <span class="s2">" Speed(reviews/sec): {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reviews_per_second</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">" Error: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_error</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="o">+</span> <span class="s2">" #Correct: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">correct_so_far</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">" #Trained: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">" Training Accuracy: {:.2f} %"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">correct_so_far</span> <span class="o">*</span> <span class="mi">100</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">"Training Time: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org524c6ac">
<h3 id="org524c6ac">The Run Method</h3>
<div class="outline-text-3" id="text-org524c6ac">
<p>As with training, the method is different enought that we have to re-do it.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">review</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">translate</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Classification</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Returns a POSITIVE or NEGATIVE prediction for the given review.</span>

<span class="sd">    Args:</span>
<span class="sd">     review: the review to classify</span>
<span class="sd">     translate: convert output to a string</span>

<span class="sd">    Returns:</span>
<span class="sd">     classification for the review</span>
<span class="sd">    """</span>
    <span class="n">nodes</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">token</span><span class="p">]</span>
             <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">review</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">))</span>
             <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span> <span class="o">*=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>

    <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_layer</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_outputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">translate</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="s2">"POSITIVE"</span> <span class="k">if</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="s2">"NEGATIVE"</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">sentimental_network</span> <span class="kn">import</span> <span class="n">SentiMental</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sentimental</span> <span class="o">=</span> <span class="n">SentiMental</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">sentimental</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00 % Speed(reviews/sec): 0.00 Error: [-0.5] #Correct: 1 #Trained: 1 Training Accuracy: 100.00 %
Progress: 4.17 % Speed(reviews/sec): 500.00 Error: [-0.12803969] #Correct: 745 #Trained: 1001 Training Accuracy: 74.43 %
Progress: 8.33 % Speed(reviews/sec): 666.67 Error: [-0.05466563] #Correct: 1542 #Trained: 2001 Training Accuracy: 77.06 %
Progress: 12.50 % Speed(reviews/sec): 750.00 Error: [-0.76659525] #Correct: 2378 #Trained: 3001 Training Accuracy: 79.24 %
Progress: 16.67 % Speed(reviews/sec): 666.67 Error: [-0.13244093] #Correct: 3185 #Trained: 4001 Training Accuracy: 79.61 %
Progress: 20.83 % Speed(reviews/sec): 714.29 Error: [-0.03716464] #Correct: 3997 #Trained: 5001 Training Accuracy: 79.92 %
Progress: 25.00 % Speed(reviews/sec): 750.00 Error: [-0.00921009] #Correct: 4835 #Trained: 6001 Training Accuracy: 80.57 %
Progress: 29.17 % Speed(reviews/sec): 777.78 Error: [-0.00274399] #Correct: 5703 #Trained: 7001 Training Accuracy: 81.46 %
Progress: 33.33 % Speed(reviews/sec): 727.27 Error: [-0.0040905] #Correct: 6555 #Trained: 8001 Training Accuracy: 81.93 %
Progress: 37.50 % Speed(reviews/sec): 750.00 Error: [-0.02414385] #Correct: 7412 #Trained: 9001 Training Accuracy: 82.35 %
Progress: 41.67 % Speed(reviews/sec): 769.23 Error: [-0.11133286] #Correct: 8282 #Trained: 10001 Training Accuracy: 82.81 %
Progress: 45.83 % Speed(reviews/sec): 785.71 Error: [-0.05147756] #Correct: 9143 #Trained: 11001 Training Accuracy: 83.11 %
Progress: 50.00 % Speed(reviews/sec): 750.00 Error: [-0.00178148] #Correct: 10006 #Trained: 12001 Training Accuracy: 83.38 %
Progress: 54.17 % Speed(reviews/sec): 764.71 Error: [-0.3016099] #Correct: 10874 #Trained: 13001 Training Accuracy: 83.64 %
Progress: 58.33 % Speed(reviews/sec): 777.78 Error: [-0.00105685] #Correct: 11741 #Trained: 14001 Training Accuracy: 83.86 %
Progress: 62.50 % Speed(reviews/sec): 750.00 Error: [-0.49072786] #Correct: 12584 #Trained: 15001 Training Accuracy: 83.89 %
Progress: 66.67 % Speed(reviews/sec): 761.90 Error: [-0.18036635] #Correct: 13414 #Trained: 16001 Training Accuracy: 83.83 %
Progress: 70.83 % Speed(reviews/sec): 772.73 Error: [-0.17892538] #Correct: 14265 #Trained: 17001 Training Accuracy: 83.91 %
Progress: 75.00 % Speed(reviews/sec): 782.61 Error: [-0.00702446] #Correct: 15127 #Trained: 18001 Training Accuracy: 84.03 %
Progress: 79.17 % Speed(reviews/sec): 760.00 Error: [-0.99885025] #Correct: 16000 #Trained: 19001 Training Accuracy: 84.21 %
Progress: 83.33 % Speed(reviews/sec): 769.23 Error: [-0.02833534] #Correct: 16873 #Trained: 20001 Training Accuracy: 84.36 %
Progress: 87.50 % Speed(reviews/sec): 777.78 Error: [-0.22776195] #Correct: 17746 #Trained: 21001 Training Accuracy: 84.50 %
Progress: 91.67 % Speed(reviews/sec): 785.71 Error: [-0.22165232] #Correct: 18630 #Trained: 22001 Training Accuracy: 84.68 %
Progress: 95.83 % Speed(reviews/sec): 766.67 Error: [-0.13901935] #Correct: 19489 #Trained: 23001 Training Accuracy: 84.73 %
Training Time: 0:00:31.545636
</pre>
<p>That trained much faster than the earlier models.</p>
<div class="highlight">
<pre><span></span><span class="n">sentimental</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00% Speed(reviews/sec): 0.00 #Correct: 1 #Tested: 1 Testing Accuracy: 100.00 %
Progress: 10.00% Speed(reviews/sec): 0.00 #Correct: 92 #Tested: 101 Testing Accuracy: 91.09 %
Progress: 20.00% Speed(reviews/sec): 0.00 #Correct: 178 #Tested: 201 Testing Accuracy: 88.56 %
Progress: 30.00% Speed(reviews/sec): 0.00 #Correct: 268 #Tested: 301 Testing Accuracy: 89.04 %
Progress: 40.00% Speed(reviews/sec): 0.00 #Correct: 351 #Tested: 401 Testing Accuracy: 87.53 %
Progress: 50.00% Speed(reviews/sec): 0.00 #Correct: 442 #Tested: 501 Testing Accuracy: 88.22 %
Progress: 60.00% Speed(reviews/sec): 0.00 #Correct: 533 #Tested: 601 Testing Accuracy: 88.69 %
Progress: 70.00% Speed(reviews/sec): 0.00 #Correct: 610 #Tested: 701 Testing Accuracy: 87.02 %
Progress: 80.00% Speed(reviews/sec): 0.00 #Correct: 689 #Tested: 801 Testing Accuracy: 86.02 %
Progress: 90.00% Speed(reviews/sec): 0.00 #Correct: 777 #Tested: 901 Testing Accuracy: 86.24 %
</pre>
<p>I still can't figure out why the test-set does better than the training set.</p>
</div>
</div>
</div>
</div>
</article>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="index-5.html" rel="prev">Newer posts</a></li>
<li class="next"><a href="index-3.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer">Contents © 2020 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> </div>
</div>
</div>
</article>
</div>
</div>
</div>
</body>
</html>
