<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neurotic Networking (old posts, page 2) | Neurotic Networking</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/index-2.html" rel="canonical">
<link href="index-3.html" rel="prev" type="text/html">
<link href="index-1.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/Neurotic-Networking/"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/bike-sharing/the-bike-sharing-project/">The Bike Sharing Project</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/bike-sharing/the-bike-sharing-project/" rel="bookmark"><time class="published dt-published" datetime="2018-10-30T13:34:56-07:00" itemprop="datePublished" title="2018-10-30 13:34">2018-10-30 13:34</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#org3d26931">Introduction</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#orge329f35">Jupyter Setup</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#org1d09d17">Imports</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#org0e143f1">Set Up</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#org1831cbf">The Data</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#org2c6528d">Checking out the data</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#orga7e67d1">Dummy variables</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#orgdb3cd53">Scaling target variables</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#orga4d9052">Splitting the data into training, testing, and validation sets</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#org7ef85c0">Time to build the network</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#orgd0b7ac8">Unit tests</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#orgfef54b3">Training the network</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#org382d997">Check out your predictions</a></li>
<li><a href="posts/nano/bike-sharing/the-bike-sharing-project/#orgb79ccf7">More Variations</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org3d26931">
<h2 id="org3d26931">Introduction</h2>
<div class="outline-text-2" id="text-org3d26931">
<p>This project builds a neural network and uses it to predict daily bike rental ridership.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orge329f35">
<h2 id="orge329f35">Jupyter Setup</h2>
<div class="outline-text-2" id="text-orge329f35">
<p>This sets some "magic" jupyter values.</p>
<p>Display Matplotlib plots.</p>
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
</pre></div>
<p>Reload code from other modules that has changed (otherwise even if you re-run the import the changes won't get picked up).</p>
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')
</pre></div>
<p>I couldn't find any documentation on this other than people asking how to get it to work. I think it means <a href="https://matplotlib.org/users/prev_whats_new/whats_new_2.0.0.html#support-for-hidpi-retina-displays-in-the-nbagg-and-webagg-backends">to use a higher resolution if your display supports it</a>.</p>
<div class="highlight">
<pre><span></span># get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org1d09d17">
<h2 id="org1d09d17">Imports</h2>
<div class="outline-text-2" id="text-org1d09d17"></div>
<div class="outline-3" id="outline-container-org0dc2359">
<h3 id="org0dc2359">Python Standard Library</h3>
<div class="outline-text-3" id="text-org0dc2359">
<div class="highlight">
<pre><span></span>from collections import namedtuple
from functools import partial
from datetime import datetime
import unittest
import sys
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge32ed1e">
<h3 id="orge32ed1e">From PyPi</h3>
<div class="outline-text-3" id="text-orge32ed1e">
<div class="highlight">
<pre><span></span>from graphviz import Digraph
from tabulate import tabulate
import numpy
import pandas
import matplotlib.pyplot as pyplot
import seaborn
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org55cfb2f">
<h3 id="org55cfb2f">This Project</h3>
<div class="outline-text-3" id="text-org55cfb2f">
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPath
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgde47863">
<h3 id="orgde47863">The Submission</h3>
<div class="outline-text-3" id="text-orgde47863">
<p>The submission is set up so that you provide a separate python file where you implement the neural network, so this imports it.</p>
<div class="highlight">
<pre><span></span>from my_answers import (
    NeuralNetwork,
    iterations,
    learning_rate,
    hidden_nodes,
    output_nodes)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0e143f1">
<h2 id="org0e143f1">Set Up</h2>
<div class="outline-text-2" id="text-org0e143f1"></div>
<div class="outline-3" id="outline-container-orgc1c4fdd">
<h3 id="orgc1c4fdd">Tables</h3>
<div class="outline-text-3" id="text-orgc1c4fdd">
<div class="highlight">
<pre><span></span>table = partial(tabulate, tablefmt="orgtbl", headers="keys", showindex=False)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org10aa81b">
<h3 id="org10aa81b">Plotting</h3>
<div class="outline-text-3" id="text-org10aa81b">
<div class="highlight">
<pre><span></span>seaborn.set_style("whitegrid", rc={"axes.grid": False})
FIGURE_SIZE = (12, 10)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1831cbf">
<h2 id="org1831cbf">The Data</h2>
<div class="outline-text-2" id="text-org1831cbf">
<p>The data comes from the <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset">UCI Machine Learning Repository</a> (I think). It combines <a href="https://www.capitalbikeshare.com/system-data">Capital Bikeshare data</a>, <a href="http://www.freemeteo.com">Weather Data from i-Weather</a>, and <a href="https://dchr.dc.gov/page/holiday-schedules">Washington D.C. holiday information</a>. The authors note that becaus the bikes are tracked when they are checked out and when they arrive they have become a "virtual sensor network" that tracks how people move through the city (by shared bicycle, at least).</p>
<p>This first bit is the original path that you need for a submission, which is different from where I'm keeping it while working on this. I'm adding an <code>EXPECTED_DATA_PATH</code> variable because that's being checked in the unit-test for some reason, and I'll need to change it for the submission.</p>
<pre class="example">
data_path = 'Bike-Sharing-Dataset/hour.csv'
EXPECTED_DATA_PATH = data_path.lower()
</pre>
<p>This is where it's kept for this post.</p>
<div class="highlight">
<pre><span></span>path = DataPath("hour.csv")
data_path = str(path.from_folder)
EXPECTED_DATA_PATH = data_path.lower()
print(path.from_folder)
</pre></div>
<pre class="example">
../../../data/bike-sharing/hour.csv
</pre>
<div class="highlight">
<pre><span></span>rides = pandas.read_csv(data_path)
</pre></div>
<div class="highlight">
<pre><span></span>print(table(rides.head()))
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">instant</th>
<th class="org-right" scope="col">dteday</th>
<th class="org-right" scope="col">season</th>
<th class="org-right" scope="col">yr</th>
<th class="org-right" scope="col">mnth</th>
<th class="org-right" scope="col">hr</th>
<th class="org-right" scope="col">holiday</th>
<th class="org-right" scope="col">weekday</th>
<th class="org-right" scope="col">workingday</th>
<th class="org-right" scope="col">weathersit</th>
<th class="org-right" scope="col">temp</th>
<th class="org-right" scope="col">atemp</th>
<th class="org-right" scope="col">hum</th>
<th class="org-right" scope="col">windspeed</th>
<th class="org-right" scope="col">casual</th>
<th class="org-right" scope="col">registered</th>
<th class="org-right" scope="col">cnt</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.24</td>
<td class="org-right">0.2879</td>
<td class="org-right">0.81</td>
<td class="org-right">0</td>
<td class="org-right">3</td>
<td class="org-right">13</td>
<td class="org-right">16</td>
</tr>
<tr>
<td class="org-right">2</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.22</td>
<td class="org-right">0.2727</td>
<td class="org-right">0.8</td>
<td class="org-right">0</td>
<td class="org-right">8</td>
<td class="org-right">32</td>
<td class="org-right">40</td>
</tr>
<tr>
<td class="org-right">3</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.22</td>
<td class="org-right">0.2727</td>
<td class="org-right">0.8</td>
<td class="org-right">0</td>
<td class="org-right">5</td>
<td class="org-right">27</td>
<td class="org-right">32</td>
</tr>
<tr>
<td class="org-right">4</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">3</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.24</td>
<td class="org-right">0.2879</td>
<td class="org-right">0.75</td>
<td class="org-right">0</td>
<td class="org-right">3</td>
<td class="org-right">10</td>
<td class="org-right">13</td>
</tr>
<tr>
<td class="org-right">5</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">4</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.24</td>
<td class="org-right">0.2879</td>
<td class="org-right">0.75</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span>print(len(rides.dteday.unique()))
</pre></div>
<pre class="example">
731
</pre>
<div class="highlight">
<pre><span></span>print(table(rides.describe(), showindex=True))
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">&nbsp;</th>
<th class="org-right" scope="col">instant</th>
<th class="org-right" scope="col">season</th>
<th class="org-right" scope="col">yr</th>
<th class="org-right" scope="col">mnth</th>
<th class="org-right" scope="col">hr</th>
<th class="org-right" scope="col">holiday</th>
<th class="org-right" scope="col">weekday</th>
<th class="org-right" scope="col">workingday</th>
<th class="org-right" scope="col">weathersit</th>
<th class="org-right" scope="col">temp</th>
<th class="org-right" scope="col">atemp</th>
<th class="org-right" scope="col">hum</th>
<th class="org-right" scope="col">windspeed</th>
<th class="org-right" scope="col">casual</th>
<th class="org-right" scope="col">registered</th>
<th class="org-right" scope="col">cnt</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">count</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
</tr>
<tr>
<td class="org-left">mean</td>
<td class="org-right">8690</td>
<td class="org-right">2.50164</td>
<td class="org-right">0.502561</td>
<td class="org-right">6.53778</td>
<td class="org-right">11.5468</td>
<td class="org-right">0.0287704</td>
<td class="org-right">3.00368</td>
<td class="org-right">0.682721</td>
<td class="org-right">1.42528</td>
<td class="org-right">0.496987</td>
<td class="org-right">0.475775</td>
<td class="org-right">0.627229</td>
<td class="org-right">0.190098</td>
<td class="org-right">35.6762</td>
<td class="org-right">153.787</td>
<td class="org-right">189.463</td>
</tr>
<tr>
<td class="org-left">std</td>
<td class="org-right">5017.03</td>
<td class="org-right">1.10692</td>
<td class="org-right">0.500008</td>
<td class="org-right">3.43878</td>
<td class="org-right">6.91441</td>
<td class="org-right">0.167165</td>
<td class="org-right">2.00577</td>
<td class="org-right">0.465431</td>
<td class="org-right">0.639357</td>
<td class="org-right">0.192556</td>
<td class="org-right">0.17185</td>
<td class="org-right">0.19293</td>
<td class="org-right">0.12234</td>
<td class="org-right">49.305</td>
<td class="org-right">151.357</td>
<td class="org-right">181.388</td>
</tr>
<tr>
<td class="org-left">min</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.02</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">25%</td>
<td class="org-right">4345.5</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">4</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.34</td>
<td class="org-right">0.3333</td>
<td class="org-right">0.48</td>
<td class="org-right">0.1045</td>
<td class="org-right">4</td>
<td class="org-right">34</td>
<td class="org-right">40</td>
</tr>
<tr>
<td class="org-left">50%</td>
<td class="org-right">8690</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">7</td>
<td class="org-right">12</td>
<td class="org-right">0</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0.5</td>
<td class="org-right">0.4848</td>
<td class="org-right">0.63</td>
<td class="org-right">0.194</td>
<td class="org-right">17</td>
<td class="org-right">115</td>
<td class="org-right">142</td>
</tr>
<tr>
<td class="org-left">75%</td>
<td class="org-right">13034.5</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">10</td>
<td class="org-right">18</td>
<td class="org-right">0</td>
<td class="org-right">5</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">0.66</td>
<td class="org-right">0.6212</td>
<td class="org-right">0.78</td>
<td class="org-right">0.2537</td>
<td class="org-right">48</td>
<td class="org-right">220</td>
<td class="org-right">281</td>
</tr>
<tr>
<td class="org-left">max</td>
<td class="org-right">17379</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">12</td>
<td class="org-right">23</td>
<td class="org-right">1</td>
<td class="org-right">6</td>
<td class="org-right">1</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0.8507</td>
<td class="org-right">367</td>
<td class="org-right">886</td>
<td class="org-right">977</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span>print(len(rides.dteday.unique()) * 24)
</pre></div>
<pre class="example">
17544
</pre>
<p>So there appear to be some hours missing, since there aren't enough rows in the data set.</p>
<div class="highlight">
<pre><span></span>print("First Hour: {} {}".format(
    rides.dteday.min(),
    rides[rides.dteday == rides.dteday.min()].hr.min()))
print("Last Hour: {} {}".format(
    rides.dteday.max(),
    rides[rides.dteday == rides.dteday.max()].hr.max()))
</pre></div>
<pre class="example">
First Hour: 2011-01-01 0
Last Hour: 2012-12-31 23
</pre>
<p>Well, that's odd. It looks like the span is complete, why are there missing hours?</p>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
counts = rides.groupby(["dteday"]).hr.count()
axe.set_title("Hours Recorded Per Day")
axe.set_xlabel("Day")
axe.set_ylabel("Count")
ax = axe.plot(range(len(counts.index)), counts.values, "o", markerfacecolor='None')
</pre></div>
<div class="figure">
<p><img alt="date_hours.png" src="posts/nano/bike-sharing/the-bike-sharing-project/date_hours.png"></p>
</div>
<p>So it looks like some days they didn't manage to record all the hours.</p>
<p>Assuming this is the UC Irvine dataset, this is the description of the variables.</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Variable</th>
<th class="org-left" scope="col">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">instant</td>
<td class="org-left">record index</td>
</tr>
<tr>
<td class="org-left">dteday</td>
<td class="org-left">date</td>
</tr>
<tr>
<td class="org-left">season</td>
<td class="org-left">season (1:spring, 2:summer, 3:fall, 4:winter)</td>
</tr>
<tr>
<td class="org-left">yr</td>
<td class="org-left">year (0: 2011, 1:2012)</td>
</tr>
<tr>
<td class="org-left">mnth</td>
<td class="org-left">month (1 to 12)</td>
</tr>
<tr>
<td class="org-left">hr</td>
<td class="org-left">hour (0 to 23)</td>
</tr>
<tr>
<td class="org-left">holiday</td>
<td class="org-left">whether day is holiday or not (extracted from <a href="https://dchr.dc.gov/page/holiday-schedules">Washington D.C. holiday information</a>)</td>
</tr>
<tr>
<td class="org-left">weekday</td>
<td class="org-left">day of the week (0 to 6)</td>
</tr>
<tr>
<td class="org-left">workingday</td>
<td class="org-left">if day is neither weekend nor holiday is 1, otherwise is 0.</td>
</tr>
<tr>
<td class="org-left">weathersit</td>
<td class="org-left">Weather (1, 2, 3, or 4) (see next table)</td>
</tr>
<tr>
<td class="org-left">temp</td>
<td class="org-left">Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)</td>
</tr>
<tr>
<td class="org-left">atemp</td>
<td class="org-left">Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)</td>
</tr>
<tr>
<td class="org-left">hum</td>
<td class="org-left">Normalized humidity. The values are divided to 100 (max)</td>
</tr>
<tr>
<td class="org-left">windspeed</td>
<td class="org-left">Normalized wind speed. The values are divided to 67 (max)</td>
</tr>
<tr>
<td class="org-left">casual</td>
<td class="org-left">count of casual users</td>
</tr>
<tr>
<td class="org-left">registered</td>
<td class="org-left">count of registered users</td>
</tr>
<tr>
<td class="org-left">cnt</td>
<td class="org-left">count of total rental bikes including both casual and registered</td>
</tr>
</tbody>
</table>
<p><code>weathersit</code></p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Value</th>
<th class="org-left" scope="col">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">Clear, Few clouds, Partly cloudy, Partly cloudy</td>
</tr>
<tr>
<td class="org-right">2</td>
<td class="org-left">Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist</td>
</tr>
<tr>
<td class="org-right">3</td>
<td class="org-left">Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds</td>
</tr>
<tr>
<td class="org-right">4</td>
<td class="org-left">Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-2" id="outline-container-org2c6528d">
<h2 id="org2c6528d">Checking out the data</h2>
<div class="outline-text-2" id="text-org2c6528d">
<p>This dataset has the number of riders for each hour of each day from January 1, 2011 to December 31, 2012. The number of riders is split between casual and registered and summed up in the <code>cnt</code> column. You can see the first few rows of the data above.</p>
<p>Below is a plot showing the number of bike riders over the first 10 days or so in the data set (some days don't have exactly 24 entries in the data set, so it's not exactly 10 days). You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model.</p>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Rides For the First Ten Days")
first_ten = rides[:24*10]
plot_lines = axe.plot(range(len(first_ten)), first_ten.cnt, label="Count")
lines = axe.plot(range(len(first_ten)), first_ten.cnt,
                 '.',
                 markeredgecolor="r")
axe.set_xlabel("Day")
legend = axe.legend(plot_lines, ["Count"], loc="upper left")
</pre></div>
<div class="figure">
<p><img alt="riders_first_ten_days.png" src="posts/nano/bike-sharing/the-bike-sharing-project/riders_first_ten_days.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga7e67d1">
<h2 id="orga7e67d1">Dummy variables</h2>
<div class="outline-text-2" id="text-orga7e67d1">
<p>Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html">get_dummies</a>.</p>
<div class="highlight">
<pre><span></span>dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']
for each in dummy_fields:
    dummies = pandas.get_dummies(rides[each], prefix=each, drop_first=False)
    rides = pandas.concat([rides, dummies], axis=1)

fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', 
                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']
data = rides.drop(fields_to_drop, axis=1)
</pre></div>
<div class="highlight">
<pre><span></span>print(data.head())
</pre></div>
<pre class="example">
   yr  holiday  temp   hum  windspeed  casual  registered  cnt  season_1  \
0   0        0  0.24  0.81        0.0       3          13   16         1   
1   0        0  0.22  0.80        0.0       8          32   40         1   
2   0        0  0.22  0.80        0.0       5          27   32         1   
3   0        0  0.24  0.75        0.0       3          10   13         1   
4   0        0  0.24  0.75        0.0       0           1    1         1   

   season_2    ...      hr_21  hr_22  hr_23  weekday_0  weekday_1  weekday_2  \
0         0    ...          0      0      0          0          0          0   
1         0    ...          0      0      0          0          0          0   
2         0    ...          0      0      0          0          0          0   
3         0    ...          0      0      0          0          0          0   
4         0    ...          0      0      0          0          0          0   

   weekday_3  weekday_4  weekday_5  weekday_6  
0          0          0          0          1  
1          0          0          0          1  
2          0          0          0          1  
3          0          0          0          1  
4          0          0          0          1  

[5 rows x 59 columns]
</pre></div>
</div>
<div class="outline-2" id="outline-container-orgdb3cd53">
<h2 id="orgdb3cd53">Scaling target variables</h2>
<div class="outline-text-2" id="text-orgdb3cd53">
<p>To make training the network easier, we'll standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1.</p>
<p>The scaling factors are saved so we can go backwards when we use the network for predictions.</p>
<div class="highlight">
<pre><span></span>quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']
# Store scalings in a dictionary so we can convert back later
scaled_features = {}
for each in quant_features:
    mean, std = data[each].mean(), data[each].std()
    scaled_features[each] = [mean, std]
    data.loc[:, each] = (data[each] - mean)/std
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orga4d9052">
<h2 id="orga4d9052">Splitting the data into training, testing, and validation sets</h2>
<div class="outline-text-2" id="text-orga4d9052">
<p>We'll save the data for the last approximately 21 days to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual number of riders.</p>
<p>Save data for approximately the last 21 days.</p>
<div class="highlight">
<pre><span></span>LAST_TWENTY_ONE = -21 * 24 
test_data = data[LAST_TWENTY_ONE:]
</pre></div>
<p>Now remove the test data from the data set .</p>
<div class="highlight">
<pre><span></span>data = data[:LAST_TWENTY_ONE]
</pre></div>
<p>Separate the data into features and targets.</p>
<div class="highlight">
<pre><span></span>target_fields = ['cnt', 'casual', 'registered']
features, targets = data.drop(target_fields, axis=1), data[target_fields]
test_features, test_targets = (test_data.drop(target_fields, axis=1),
                               test_data[target_fields])
</pre></div>
<p>We'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we'll train on historical data, then try to predict on future data (the validation set).</p>
<p>Hold out the last 60 days or so of the remaining data as a validation set</p>
<div class="highlight">
<pre><span></span>LAST_SIXTY = -60 * 24
train_features, train_targets = features[:LAST_SIXTY], targets[:LAST_SIXTY]
val_features, val_targets = features[LAST_SIXTY:], targets[LAST_SIXTY:]
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org7ef85c0">
<h2 id="org7ef85c0">Time to build the network</h2>
<div class="outline-text-2" id="text-org7ef85c0">
<p>Below you'll build your network. We've built out the structure. You'll implement both the forward pass and backwards pass through the network. You'll also set the hyperparameters: the learning rate, the number of hidden units, and the number of training passes.</p>
<div class="highlight">
<pre><span></span>graph = Digraph(comment="Neural Network", format="png")
graph.attr(rankdir="LR")

with graph.subgraph(name="cluster_input") as cluster:
    cluster.attr(label="Input")
    cluster.node("a", "")
    cluster.node("b", "")
    cluster.node("c", "")

with graph.subgraph(name="cluster_hidden") as cluster:
    cluster.attr(label="Hidden")
    cluster.node("d", "")
    cluster.node("e", "")
    cluster.node("f", "")
    cluster.node("g", "")

with graph.subgraph(name="cluster_output") as cluster:
    cluster.attr(label="Output")
    cluster.node("h", "")


graph.edges(["ad", "ae", "af", "ag",
             "bd", "be", "bf", "bg",
             "cd", "ce", "cf", "cg"])

graph.edges(["dh", 'eh', "fh", "gh"])

graph.render("graphs/network.dot")
graph
</pre></div>
<div class="figure">
<p><img alt="network.dot.png" src="posts/nano/bike-sharing/the-bike-sharing-project/network.dot.png"></p>
</div>
<p>The network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is \(f(x)=x\). A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called <b>forward propagation</b>.</p>
<p>We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called <b>backpropagation</b>.</p>
<p><b>Hint:</b> You'll need the derivative of the output activation function (\(f(x) = x\)) for the backpropagation implementation. If you aren't familiar with calculus, this function is equivalent to the equation \(y = x\). What is the slope of that equation? That is the derivative of \(f(x)\).</p>
<p>Below, you have these tasks:</p>
<ol class="org-ol">
<li>Implement the sigmoid function to use as the activation function. Set `self.activation_function` in `__init__` to your sigmoid function.</li>
<li>Implement the forward pass in the `train` method.</li>
<li>Implement the backpropagation algorithm in the `train` method, including calculating the output error.</li>
<li>Implement the forward pass in the `run` method.</li>
</ol>
<p>In the my_answers.py file, fill out the TODO sections as specified</p>
<pre class="example">
from my_answers import NeuralNetwork
</pre></div>
<div class="outline-3" id="outline-container-orgb7c2857">
<h3 id="orgb7c2857">Mean Squared Error</h3>
<div class="outline-text-3" id="text-orgb7c2857">
<div class="highlight">
<pre><span></span>def MSE(y, Y):
    return numpy.mean((y-Y)**2)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd0b7ac8">
<h2 id="orgd0b7ac8">Unit tests</h2>
<div class="outline-text-2" id="text-orgd0b7ac8">
<p>Run these unit tests to check the correctness of your network implementation. This will help you be sure your network was implemented correctly befor you starting trying to train it. These tests must all be successful to pass the project.</p>
<div class="highlight">
<pre><span></span>inputs = numpy.array([[0.5, -0.2, 0.1]])
targets = numpy.array([[0.4]])

test_w_i_h = numpy.array([[0.1, -0.2],
                          [0.4, 0.5],
                          [-0.3, 0.2]])
test_w_h_o = numpy.array([[0.3],
                          [-0.1]])
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgff165bb">
<h3 id="orgff165bb">The TestMethods Class</h3>
<div class="outline-text-3" id="text-orgff165bb">
<div class="highlight">
<pre><span></span>class TestMethods(unittest.TestCase):

    ##########
    # Unit tests for data loading
    ##########

    def test_data_path(self):
        # Test that file path to dataset has been unaltered
        self.assertTrue(data_path.lower() == EXPECTED_DATA_PATH)

    def test_data_loaded(self):
        # Test that data frame loaded
        self.assertTrue(isinstance(rides, pandas.DataFrame))

    ##########
    # Unit tests for network functionality
    ##########

    def test_activation(self):
        network = NeuralNetwork(3, 2, 1, 0.5)
        # Test that the activation function is a sigmoid
        self.assertTrue(numpy.all(network.activation_function(0.5) == 1/(1+numpy.exp(-0.5))))

    def test_train(self):
        # Test that weights are updated correctly on training
        network = NeuralNetwork(3, 2, 1, 0.5)
        network.weights_input_to_hidden = test_w_i_h.copy()
        network.weights_hidden_to_output = test_w_h_o.copy()

        network.train(inputs, targets)
        expected = numpy.array([[ 0.37275328], 
                                [-0.03172939]])
        actual = network.weights_hidden_to_output
        self.assertTrue(
            numpy.allclose(expected, actual),
            "(weights hidden to output) Expected {} Actual: {}".format(
                expected, actual))
        expected = numpy.array([[ 0.10562014, -0.20185996], 
                                [0.39775194, 0.50074398], 
                                [-0.29887597, 0.19962801]])
        actual = network.weights_input_to_hidden
        self.assertTrue(
            numpy.allclose(actual,
                           expected), #, 0.1),
            "(weights input to hidden) Expected: {} Actual: {}".format(
                expected,
                actual))
        return

    def test_run(self):
        # Test correctness of run method
        network = NeuralNetwork(3, 2, 1, 0.5)
        network.weights_input_to_hidden = test_w_i_h.copy()
        network.weights_hidden_to_output = test_w_h_o.copy()

        self.assertTrue(numpy.allclose(network.run(inputs), 0.09998924))

suite = unittest.TestLoader().loadTestsFromModule(TestMethods())
unittest.TextTestRunner().run(suite)
</pre></div>
<pre class="example">
.....
----------------------------------------------------------------------
Ran 5 tests in 0.006s

OK
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgfef54b3">
<h2 id="orgfef54b3">Training the network</h2>
<div class="outline-text-2" id="text-orgfef54b3">
<p>Here you'll set the hyperparameters for the network. The strategy here is to find hyperparameters such that the error on the training set is low, but you're not overfitting to the data. If you train the network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. That is, the loss on the validation set will start increasing as the training set loss drops.</p>
<p>You'll also be using a method know as Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently. You'll learn more about SGD later.</p>
</div>
<div class="outline-3" id="outline-container-org2c56149">
<h3 id="org2c56149">Choose the number of iterations</h3>
<div class="outline-text-3" id="text-org2c56149">
<p>This is the number of batches of samples from the training data we'll use to train the network. The more iterations you use, the better the model will fit the data. However, this process can have sharply diminishing returns and can waste computational resources if you use too many iterations. You want to find a number here where the network has a low training loss, and the validation loss is at a minimum. The ideal number of iterations would be a level that stops shortly after the validation loss is no longer decreasing.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org653a5e3">
<h3 id="org653a5e3">Choose the learning rate</h3>
<div class="outline-text-3" id="text-org653a5e3">
<p>This scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. Normally a good choice to start at is 0.1; however, if you effectively divide the learning rate by n_records, try starting out with a learning rate of 1. In either case, if the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org0b2cb45">
<h3 id="org0b2cb45">Choose the number of hidden nodes</h3>
<div class="outline-text-3" id="text-org0b2cb45">
<p>In a model where all the weights are optimized, the more hidden nodes you have, the more accurate the predictions of the model will be. (A fully optimized model could have weights of zero, after all.) However, the more hidden nodes you have, the harder it will be to optimize the weights of the model, and the more likely it will be that suboptimal weights will lead to overfitting. With overfitting, the model will memorize the training data instead of learning the true pattern, and won't generalize well to unseen data.</p>
<p>Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden units is too low, then the model won't have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden units you choose. You'll generally find that the best number of hidden nodes to use ends up being between the number of input and output nodes.</p>
<p>Set the hyperparameters in you myanswers.py file:</p>
<pre class="example">
from my_answers import iterations, learning_rate, hidden_nodes, output_nodes
</pre>
<div class="highlight">
<pre><span></span>N_i = train_features.shape[1]
</pre></div>
<div class="highlight">
<pre><span></span>network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)
losses = {'train':[], 'validation':[]}
print("Inputs: {}, Hidden: {}, Output: {}, Learning Rate: {}".format(
    N_i,
    hidden_nodes,
    output_nodes,
    learning_rate))
print("Starting {} repetitions".format(iterations))
for iteration in range(iterations):
    # Go through a random batch of 128 records from the training data set
    batch = numpy.random.choice(train_features.index, size=128)
    X, y = train_features.loc[batch].values, train_targets.loc[batch]['cnt']

    network.train(X, y)

    # Printing out the training progress
    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)
    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)
    if not iteration % 500:
        sys.stdout.write("\nProgress: {:2.1f}".format(100 * iteration/iterations)
                         + "% ... Training loss: " 
                         + "{:.5f}".format(train_loss)
                         + " ... Validation loss: {:.5f}".format(val_loss))
        sys.stdout.flush()

    losses['train'].append(train_loss)
    losses['validation'].append(val_loss)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4
Starting 7500 repetitions

Progress: 0.0% ... Training loss: 1.09774 ... Validation loss: 1.74283
Progress: 6.7% ... Training loss: 0.27687 ... Validation loss: 0.44356
Progress: 13.3% ... Training loss: 0.24134 ... Validation loss: 0.42289
Progress: 20.0% ... Training loss: 0.20681 ... Validation loss: 0.38749
Progress: 26.7% ... Training loss: 0.16536 ... Validation loss: 0.31655
Progress: 33.3% ... Training loss: 0.13105 ... Validation loss: 0.25414
Progress: 40.0% ... Training loss: 0.10072 ... Validation loss: 0.21108
Progress: 46.7% ... Training loss: 0.08929 ... Validation loss: 0.18401
Progress: 53.3% ... Training loss: 0.07844 ... Validation loss: 0.16669
Progress: 60.0% ... Training loss: 0.07380 ... Validation loss: 0.15336
Progress: 66.7% ... Training loss: 0.07580 ... Validation loss: 0.18654
Progress: 73.3% ... Training loss: 0.06308 ... Validation loss: 0.15848
Progress: 80.0% ... Training loss: 0.06632 ... Validation loss: 0.17960
Progress: 86.7% ... Training loss: 0.05954 ... Validation loss: 0.15988
Progress: 93.3% ... Training loss: 0.05809 ... Validation loss: 0.16016
</pre>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time")
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="losses.png" src="posts/nano/bike-sharing/the-bike-sharing-project/losses.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org382d997">
<h2 id="org382d997">Check out your predictions</h2>
<div class="outline-text-2" id="text-org382d997">
<p>Here, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly.</p>
<div class="highlight">
<pre><span></span>fig, axe = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
axe.plot(expected, '.', label='Data')
axe.plot(expected, linestyle="--", color="tab:blue", label=None)
axe.plot(predictions,linestyle="--", color="tab:orange", label=None)
axe.plot(predictions, ".", label='Prediction')
axe.set_xlim(right=len(predictions))
legend = axe.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
axe.set_xticks(numpy.arange(len(dates))[12::24])
_ = axe.set_xticklabels(dates[12::24], rotation=45)
</pre></div>
<div class="figure">
<p><img alt="count.png" src="posts/nano/bike-sharing/the-bike-sharing-project/count.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-org1f20dce">
<h3 id="org1f20dce">How well does the model predict the data?</h3>
<div class="outline-text-3" id="text-org1f20dce">
<p>It looks like it does better initially and then over-predicts the peaks later on.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org2d8b810">
<h3 id="org2d8b810">Where does it fail?</h3>
<div class="outline-text-3" id="text-org2d8b810">
<p>It doesn't anticipate the drop-off in ridershp as the holidays come around.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgc3bd50a">
<h3 id="orgc3bd50a">Why does it fail where it does?</h3>
<div class="outline-text-3" id="text-orgc3bd50a">
<p>Although there might be holidays noted (at least for Christmas), it probably isn't reflecting the extreme change in behavior that the holidays bring about in the United States.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb79ccf7">
<h2 id="orgb79ccf7">More Variations</h2>
<div class="outline-text-2" id="text-orgb79ccf7">
<div class="highlight">
<pre><span></span>def train_this(hidden_nodes:int, learning_rate:float,
               output_nodes:int=1, 
               input_nodes: int=N_i,
               repetitions: int=100,
               emit: bool=True):
    """Trains the network using the given values

    Args:
     hidden_nodes: number of nodes in the hidden layer
     learning_rate: amount to change the weights during backpropagation
     output_nodes: number of nodes in the output layer
     input_nodes: number of nodes in the input layer
     repetitions: number of times to train the model
     emit: print information

    Returns:
     test error, losses: MSE against test, dict of losses
    """
    network = NeuralNetwork(input_nodes, hidden_nodes, output_nodes,
                            learning_rate)
    losses = {'train':[], 'validation':[]}
    last_validation_loss = -1
    if emit:        
        print(
            ("Inputs: {}, Hidden: {}, Output: {}, Learning Rate: {}, "
             "Repetitions: {}").format(
                 input_nodes,
                 hidden_nodes,
                 output_nodes,
                 learning_rate, 
                 repetitions))
    reported = False
    for iteration in range(repetitions):
        # Go through a random batch of 128 records from the training data set
        batch = numpy.random.choice(train_features.index, size=128)
        X, y = (train_features.iloc[batch].values,
                train_targets.iloc[batch]['cnt'])
        network.train(X, y)

        train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)
        val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)
        losses['train'].append(train_loss)
        losses['validation'].append(val_loss)
        if last_validation_loss == -1:
            last_validation_loss = val_loss[0] 
        if val_loss[0] &gt; last_validation_loss and not reported:
            reported = True
            if emit:
                print("Repetition {} Validation Loss went up by {}".format(
                iteration + 1,
                val_loss[0] - last_validation_loss))
        last_validation_loss = val_loss[0]

    predictions = network.run(test_features)
    expected = (test_targets['cnt']).values
    test_error = MSE(predictions.T, expected)[0]
    if emit:
        print(("Training Error: {:.5f}, "
               "Validation Error: {:.5f}, "
               "Test Error: {:.2f}").format(
                   losses["train"][-1][0],
                   losses["validation"][-1][0],
                   test_error))
    return test_error, losses, network
</pre></div>
<div class="highlight">
<pre><span></span>Parameters = namedtuple(
    "Parameters",
    "hidden_nodes learning_rate trials losses test_error network".split())
</pre></div>
<div class="highlight">
<pre><span></span>def grid_search(hidden_nodes: list, learning_rates: list, trials: list,
                max_train_error = 0.09, max_validation_error=0.18,
                emit_training:bool=False):
    """does a search for the best parameters

    Args:
     hidden_nodes: list of number of hidden nodes
     learning rates: list of how much to update the weights
     trials: list of number of times to train
     max_train_error: upper ceiling for training error
     max_validation_error: upper ceilining for acceptable validation error
     emit_training: print the statements during training
    """
    best = 1000
    if not type(trials) is list:
        trials = [trials]
    for node_count in hidden_nodes:
        for rate in learning_rates:
            for trial in trials:
                test_error, losses, network = train_this(node_count, rate,
                                                         repetitions=trial,
                                                         emit=emit_training)
                if test_error &lt; best:
                    print("New Best: {:.2f} (Hidden: {}, Learning Rate: {:.2f})".format(
                        test_error,
                        node_count,
                        rate))
                    best = test_error
                    best_parameters = Parameters(hidden_nodes=node_count,
                                                 learning_rate=rate,
                                                 trials=trials,
                                                 losses=losses,
                                                 test_error=test_error,
                                                 network=network)
                print()
    return best_parameters
</pre></div>
<div class="highlight">
<pre><span></span>parameters = grid_search(
    hidden_nodes=[14, 28, 42, 56],
    learning_rates=[0.1, 0.01, 0.001],
    trials=200)
</pre></div>
<pre class="example">
New Best: 0.53 (Hidden: 14, Learning Rate: 0.10)



New Best: 0.47 (Hidden: 28, Learning Rate: 0.10)



New Best: 0.44 (Hidden: 42, Learning Rate: 0.10)



New Best: 0.42 (Hidden: 56, Learning Rate: 0.10)



</pre>
<div class="highlight">
<pre><span></span>parameters = grid_search([42, 56], [0.1, 0.01, 0.001], trials=300)
</pre></div>
<pre class="example">
New Best: 0.45 (Hidden: 42, Learning Rate: 0.10)



New Best: 0.42 (Hidden: 56, Learning Rate: 0.10)



</pre>
<p>So, I wouldn't have guessed it, but the 56 node model does best with a reasonably large learning rate.</p>
<div class="highlight">
<pre><span></span>parameters = grid_search([56, 112], [0.1, 0.2], trials=200)
</pre></div>
<pre class="example">
New Best: 0.44 (Hidden: 56, Learning Rate: 0.10)

New Best: 0.41 (Hidden: 56, Learning Rate: 0.20)



</pre>
<p>Weird.</p>
<div class="highlight">
<pre><span></span>parameters = grid_search([56], [0.2, 0.3], trials=300, emit_training=True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 300
Repetition 2 Validation Loss went up by 1.2119413344400733
Training Error: 0.42973, Validation Error: 0.71298, Test Error: 0.36
New Best: 0.36 (Hidden: 56, Learning Rate: 0.20)

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.3, Repetitions: 300
Repetition 2 Validation Loss went up by 47.942730166612094
Training Error: 0.69836, Validation Error: 1.20312, Test Error: 0.63

</pre>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time")
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
losses = parameters.losses
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="better_losses.png" src="posts/nano/bike-sharing/the-bike-sharing-project/better_losses.png"></p>
</div>
<p>It looks like going over 100 doesn't really help the model a lot, or at all, really.</p>
<div class="highlight">
<pre><span></span>parameters = grid_search([56], [0.2, 0.3], trials=300, emit_training=True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 300
Repetition 2 Validation Loss went up by 0.29155647125413564
Training Error: 0.46915, Validation Error: 0.77756, Test Error: 0.36
New Best: 0.36 (Hidden: 56, Learning Rate: 0.20)

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.3, Repetitions: 300
Repetition 2 Validation Loss went up by 68.10510030432465
Training Error: 0.63604, Validation Error: 1.07500, Test Error: 0.58

</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([56], [0.2], 2000)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
New Best: 0.27 (Hidden: 56, Learning Rate: 0.20)

Elapsed: 0:02:20.654989
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([56], [0.2], 1000)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
New Best: 0.29 (Hidden: 56, Learning Rate: 0.20)

Elapsed: 0:01:51.175404
</pre>
<p>I just checked the rubric and you need a training loss below 0.09 and a validation loss below 0.18, regardless of the test loss.</p>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28, 42, 56], [0.01, 0.1, 0.2], 100, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 10 Validation Loss went up by 0.0005887216742490597
Training Error: 0.92157, Validation Error: 1.36966, Test Error: 0.69
New Best: 0.69 (Hidden: 28, Learning Rate: 0.01)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 7 Validation Loss went up by 0.06008857123483735
Training Error: 0.65584, Validation Error: 1.09581, Test Error: 0.52
New Best: 0.52 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 4 Validation Loss went up by 0.010135891697978794
Training Error: 0.61391, Validation Error: 0.99344, Test Error: 0.47
New Best: 0.47 (Hidden: 28, Learning Rate: 0.20)

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 2 Validation Loss went up by 0.0016900520112179684
Training Error: 0.87851, Validation Error: 1.31872, Test Error: 0.66

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 3 Validation Loss went up by 0.08058311852052547
Training Error: 0.66637, Validation Error: 1.09371, Test Error: 0.53

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 3 Validation Loss went up by 0.08181869722819135
Training Error: 0.60174, Validation Error: 0.99664, Test Error: 0.47

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 4 Validation Loss went up by 0.0015646480595301604
Training Error: 0.93732, Validation Error: 1.36061, Test Error: 0.72

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.03097966349747283
Training Error: 0.67087, Validation Error: 1.07306, Test Error: 0.51

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 2 Validation Loss went up by 9.947099886932289
Training Error: 0.65815, Validation Error: 1.17712, Test Error: 0.52

Elapsed: 0:00:47.842652
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.01, 0.1, 0.2], 1000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.01, Repetitions: 1000
Repetition 2 Validation Loss went up by 0.002750823237845479
Training Error: 0.71460, Validation Error: 1.28385, Test Error: 0.59
New Best: 0.59 (Hidden: 28, Learning Rate: 0.01)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 1000
Repetition 5 Validation Loss went up by 0.09885352252565549
Training Error: 0.31086, Validation Error: 0.48591, Test Error: 0.33
New Best: 0.33 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 1000
Repetition 2 Validation Loss went up by 0.09560269140958688
Training Error: 0.28902, Validation Error: 0.44905, Test Error: 0.33

Elapsed: 0:01:59.136160
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.1, 0.2], 2000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 2000
Repetition 2 Validation Loss went up by 0.06973195973646384
Training Error: 0.28625, Validation Error: 0.45083, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 2000
Repetition 3 Validation Loss went up by 0.037295970545350166
Training Error: 0.26864, Validation Error: 0.43831, Test Error: 0.32

Elapsed: 0:02:35.122622
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.05], 4000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 4000
Repetition 4 Validation Loss went up by 0.039021584745929205
Training Error: 0.27045, Validation Error: 0.44738, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.10)

Elapsed: 0:02:36.990482
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.2], 5000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 5000
Repetition 4 Validation Loss went up by 0.32617394730848104
Training Error: 0.18017, Validation Error: 0.32432, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.20)

Elapsed: 0:03:05.664176
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.2, 0.3], 6000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 6000
Repetition 3 Validation Loss went up by 0.18572519005609722
Training Error: 0.22969, Validation Error: 0.38789, Test Error: 0.35
New Best: 0.35 (Hidden: 28, Learning Rate: 0.20)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 6000
Repetition 3 Validation Loss went up by 1.6850265407570482
Training Error: 0.08168, Validation Error: 0.20003, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.30)

Elapsed: 0:07:30.082137
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.3, 0.4], 7000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 7000
Repetition 3 Validation Loss went up by 0.4652683795507646
Training Error: 0.07100, Validation Error: 0.19299, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.30)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 7000
Repetition 2 Validation Loss went up by 8.612689644792866
Training Error: 0.05771, Validation Error: 0.19188, Test Error: 0.22
New Best: 0.22 (Hidden: 28, Learning Rate: 0.40)

Elapsed: 0:09:06.729922
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.4, 0.5], 7500, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 7500
Repetition 3 Validation Loss went up by 3.6207932112624386
Training Error: 0.05942, Validation Error: 0.13644, Test Error: 0.16
New Best: 0.16 (Hidden: 28, Learning Rate: 0.40)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.5, Repetitions: 7500
Repetition 2 Validation Loss went up by 4.101532160572686
Training Error: 0.05710, Validation Error: 0.14214, Test Error: 0.22

Elapsed: 0:09:56.116403
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.4], 8000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 8000
Repetition 2 Validation Loss went up by 0.6450181997021212
Training Error: 0.05479, Validation Error: 0.14289, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.40)

Elapsed: 0:05:18.546979
</pre>
<p>That did worse so it probably overtrains at the 0.4 learning rate. What about 0.3?</p>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.3], 8000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 8000
Repetition 2 Validation Loss went up by 0.3336478297258907
Training Error: 0.06670, Validation Error: 0.16918, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.30)

Elapsed: 0:05:06.761537
</pre>
<p>So this did worse than a learning rate of 0.5 at 7500 and much worse than 0.4 at 7500, so maybe that is the optimal (0.4 at 7,500) I'm chasing. It seems king of arbitrary, but it works for the assignment.</p>
<p>The submission is timing out for some reason (it only takes 5 minutes to run but the error says it took more than 7 minutes). I might have to try some compromise runtime.</p>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time (Hidden: {} Learning Rate: {})".format(parameters.hidden_nodes, parameters.learning_rate))
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
losses = parameters.losses
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="found_losses.png" src="posts/nano/bike-sharing/the-bike-sharing-project/found_losses.png"></p>
</div>
<div class="highlight">
<pre><span></span>fig, ax = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = parameters.network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
ax.plot(expected, '.', label='Data')
ax.plot(predictions.values, ".", label='Prediction')
ax.set_xlim(right=len(predictions))
legend = ax.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
ax.set_xticks(numpy.arange(len(dates))[12::24])
_ = ax.set_xticklabels(dates[12::24], rotation=45)
</pre></div>
<div class="figure">
<p><img alt="best_count.png" src="posts/nano/bike-sharing/the-bike-sharing-project/best_count.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-org3224f91">
<h3 id="org3224f91">How well does the model predict the data?</h3>
<div class="outline-text-3" id="text-org3224f91">
<p>The model seems to not be able to capture all the variations in the data. I don't think it really did well at all.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgb42a960">
<h3 id="orgb42a960">Where does it fail?</h3>
</div>
<div class="outline-3" id="outline-container-org8cc9342">
<h3 id="org8cc9342">Why does it fail where it does?</h3>
<div class="outline-text-3" id="text-org8cc9342">
<div class="highlight">
<pre><span></span>better_network = train_this(parameters.hidden_nodes, parameters.learning_rate,
                            repetitions=150, emit=True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 19, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 43 Validation Loss went up by 0.0008309723799344582
Training Error: 0.92939, Validation Error: 1.44647, Test Error: 0.62
</pre>
<div class="highlight">
<pre><span></span>more_parameters = grid_search([10, 20],
                                [0.1, 0.01],
                                trials=list(range(50, 225, 25)),
                                emit_training=True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 50
Repetition 10 Validation Loss went up by 0.013827968028145676
Training Error: 0.92764, Validation Error: 1.45833, Test Error: 0.64
New Best: 0.64 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 75
Repetition 11 Validation Loss went up by 0.01268617480459655
Training Error: 0.95884, Validation Error: 1.45576, Test Error: 0.67

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.004302293010363778
Training Error: 0.96904, Validation Error: 1.32713, Test Error: 0.74

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 125
Repetition 2 Validation Loss went up by 0.00011567129175471536
Training Error: 0.96480, Validation Error: 1.38193, Test Error: 0.70

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 150
Repetition 2 Validation Loss went up by 0.00525072377638347
Training Error: 0.95649, Validation Error: 1.29823, Test Error: 0.79

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 175
Repetition 6 Validation Loss went up by 0.0184187066638537
Training Error: 0.94033, Validation Error: 1.48648, Test Error: 0.64
New Best: 0.64 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 200
Repetition 4 Validation Loss went up by 0.006479207709029211
Training Error: 0.96348, Validation Error: 1.38834, Test Error: 0.67

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 50
Repetition 8 Validation Loss went up by 0.00034037805450659597
Training Error: 0.99148, Validation Error: 1.50023, Test Error: 0.71

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 75
Repetition 26 Validation Loss went up by 7.26803968935652e-05
Training Error: 0.94736, Validation Error: 1.42677, Test Error: 0.69

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 55 Validation Loss went up by 0.0005170583384894734
Training Error: 0.92258, Validation Error: 1.52912, Test Error: 0.64

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 125
Repetition 14 Validation Loss went up by 4.7182200476170166e-05
Training Error: 0.96801, Validation Error: 1.43898, Test Error: 0.69

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 25 Validation Loss went up by 0.00025169631184263075
Training Error: 0.94769, Validation Error: 1.28473, Test Error: 0.79

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 175
Repetition 2 Validation Loss went up by 0.0011783405140612935
Training Error: 0.95784, Validation Error: 1.38248, Test Error: 0.75

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 200
Repetition 24 Validation Loss went up by 6.126094364189427e-05
Training Error: 0.95839, Validation Error: 1.29311, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 50
Repetition 4 Validation Loss went up by 0.04115757797958697
Training Error: 0.97326, Validation Error: 1.42168, Test Error: 0.72

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 75
Repetition 3 Validation Loss went up by 0.004544958155855872
Training Error: 0.97079, Validation Error: 1.38326, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.004455074996461583
Training Error: 0.95492, Validation Error: 1.29813, Test Error: 0.80

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 125
Repetition 3 Validation Loss went up by 0.038516428472006314
Training Error: 0.96063, Validation Error: 1.41134, Test Error: 0.68

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 150
Repetition 4 Validation Loss went up by 0.005238556857821708
Training Error: 0.96022, Validation Error: 1.58371, Test Error: 0.67

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 175
Repetition 4 Validation Loss went up by 0.03291822053421889
Training Error: 0.95138, Validation Error: 1.39820, Test Error: 0.67

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 200
Repetition 2 Validation Loss went up by 0.01721971957045554
Training Error: 0.96676, Validation Error: 1.33802, Test Error: 0.72

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 50
Repetition 2 Validation Loss went up by 0.0016568049054144218
Training Error: 0.90793, Validation Error: 1.36444, Test Error: 0.78

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 75
Repetition 24 Validation Loss went up by 1.0669002188823384e-05
Training Error: 0.93910, Validation Error: 1.43277, Test Error: 0.61
New Best: 0.61 (Hidden: 20, Learning Rate: 0.01)

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 13 Validation Loss went up by 0.0006819932351833646
Training Error: 0.95953, Validation Error: 1.30219, Test Error: 0.79

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 125
Repetition 2 Validation Loss went up by 0.001983487952677443
Training Error: 0.96503, Validation Error: 1.34180, Test Error: 0.76

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 2 Validation Loss went up by 0.0014568870668274503
Training Error: 0.95968, Validation Error: 1.34224, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 175
Repetition 22 Validation Loss went up by 0.0009898893960744726
Training Error: 0.94891, Validation Error: 1.25502, Test Error: 0.81

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 200
Repetition 24 Validation Loss went up by 0.0003693045874550993
Training Error: 0.96222, Validation Error: 1.29511, Test Error: 0.75

</pre>
<div class="highlight">
<pre><span></span>best_parameters = grid_search([10, 20, 30, 40], [0.1, 0.01], 100, True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.006104362102346439
Training Error: 0.96812, Validation Error: 1.32203, Test Error: 0.76
New Best: 0.76 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 16 Validation Loss went up by 0.00020577471034299855
Training Error: 0.96607, Validation Error: 1.30376, Test Error: 0.79

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.018123806353317784
Training Error: 0.95858, Validation Error: 1.28351, Test Error: 0.76
New Best: 0.76 (Hidden: 20, Learning Rate: 0.10)

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 29 Validation Loss went up by 0.0029168059867459295
Training Error: 0.94895, Validation Error: 1.45710, Test Error: 0.66
New Best: 0.66 (Hidden: 20, Learning Rate: 0.01)

Inputs: 56, Hidden: 30, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.045063972993018675
Training Error: 0.96892, Validation Error: 1.50312, Test Error: 0.69

Inputs: 56, Hidden: 30, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 14 Validation Loss went up by 7.233598065781166e-05
Training Error: 0.96606, Validation Error: 1.32913, Test Error: 0.80

Inputs: 56, Hidden: 40, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 3 Validation Loss went up by 0.05076336638491519
Training Error: 0.95920, Validation Error: 1.45843, Test Error: 0.68

Inputs: 56, Hidden: 40, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 7 Validation Loss went up by 0.0006550737027899434
Training Error: 0.97148, Validation Error: 1.36548, Test Error: 0.79

</pre>
<div class="highlight">
<pre><span></span>fig, axe = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = best_parameters.network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
axe.set_title("{} Hidden and Learning Rate: {}".format(best_parameters.hidden_nodes,
                                                       best_parameters.learning_rate))
axe.plot(expected, '.', label='Data')
axe.plot(predictions.values, ".", label='Prediction')
axe.set_xlim(right=len(predictions))
legend = axe.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
axe.set_xticks(numpy.arange(len(dates))[12::24])
_ = axe.set_xticklabels(dates[12::24], rotation=45)
</pre></div>
<div class="figure">
<p><img alt="even_bester_count.png" src="posts/nano/bike-sharing/the-bike-sharing-project/even_bester_count.png"></p>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/introduction-to-neural-networks/student-admissions/">Student Admissions</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/introduction-to-neural-networks/student-admissions/" rel="bookmark"><time class="published dt-published" datetime="2018-10-29T12:21:25-07:00" itemprop="datePublished" title="2018-10-29 12:21">2018-10-29 12:21</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#org6a0c107">Introduction</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#org223fa51">Imports</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#orgab54c10">Some Set Up</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#orgdb03c25">Loading the data</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#orgb804af5">Plotting the data</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#org1fc09c0">One-Hot Encoding the Rank</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#orga1d7dad">Scaling the data</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#org3b2fe8b">Splitting the data into Training and Testing</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#org5f8cd32">Splitting the data into features and targets (labels)</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#orgbef7def">Training the 2-layer Neural Network</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#orgec5263a">Backpropagate the error</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/student-admissions/#org62c116c">Calculating the Accuracy on the Test Data</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org6a0c107">
<h2 id="org6a0c107">Introduction</h2>
<div class="outline-text-2" id="text-org6a0c107">
<p>In this notebook, I'll student admissions to graduate school at UCLA based on three pieces of data:</p>
<ul class="org-ul">
<li>GRE Scores (Test)</li>
<li>GPA Scores (Grades)</li>
<li>Class rank (1-4)</li>
</ul>
<p>The dataset originally came from here: <a href="http://www.ats.ucla.edu/">http://www.ats.ucla.edu/</a> (although I couldn't find it).</p>
</div>
</div>
<div class="outline-2" id="outline-container-org223fa51">
<h2 id="org223fa51">Imports</h2>
<div class="outline-text-2" id="text-org223fa51"></div>
<div class="outline-3" id="outline-container-org880276c">
<h3 id="org880276c">From python</h3>
<div class="outline-text-3" id="text-org880276c">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd55171e">
<h3 id="orgd55171e">From PyPi</h3>
<div class="outline-text-3" id="text-orgd55171e">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf1c8911">
<h3 id="orgf1c8911">This Project</h3>
<div class="outline-text-3" id="text-orgf1c8911">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgab54c10">
<h2 id="orgab54c10">Some Set Up</h2>
<div class="outline-text-2" id="text-orgab54c10"></div>
<div class="outline-3" id="outline-container-org5c34c5b">
<h3 id="org5c34c5b">Tables</h3>
<div class="outline-text-3" id="text-org5c34c5b">
<div class="highlight">
<pre><span></span><span class="n">table</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">tabulate</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s1">'orgtbl'</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">"keys"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgdc9b092">
<h3 id="orgdc9b092">Plotting</h3>
<div class="outline-text-3" id="text-orgdc9b092">
<div class="highlight">
<pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">)</span>
<span class="n">FIGURE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgdb03c25">
<h2 id="orgdb03c25">Loading the data</h2>
<div class="outline-text-2" id="text-orgdb03c25">
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"student_data.csv"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="p">)</span>
</pre></div>
<pre class="example">
../../../data/introduction-to-neural-networks/student_data.csv
</pre>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">380</td>
<td class="org-right">3.61</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">660</td>
<td class="org-right">3.67</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">800</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">640</td>
<td class="org-right">3.19</td>
<td class="org-right">4</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">520</td>
<td class="org-right">2.93</td>
<td class="org-right">4</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">describe</span><span class="p">(),</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">&nbsp;</th>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">count</td>
<td class="org-right">400</td>
<td class="org-right">400</td>
<td class="org-right">400</td>
<td class="org-right">400</td>
</tr>
<tr>
<td class="org-left">mean</td>
<td class="org-right">0.3175</td>
<td class="org-right">587.7</td>
<td class="org-right">3.3899</td>
<td class="org-right">2.485</td>
</tr>
<tr>
<td class="org-left">std</td>
<td class="org-right">0.466087</td>
<td class="org-right">115.517</td>
<td class="org-right">0.380567</td>
<td class="org-right">0.94446</td>
</tr>
<tr>
<td class="org-left">min</td>
<td class="org-right">0</td>
<td class="org-right">220</td>
<td class="org-right">2.26</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">25%</td>
<td class="org-right">0</td>
<td class="org-right">520</td>
<td class="org-right">3.13</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">50%</td>
<td class="org-right">0</td>
<td class="org-right">580</td>
<td class="org-right">3.395</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">75%</td>
<td class="org-right">1</td>
<td class="org-right">660</td>
<td class="org-right">3.67</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">max</td>
<td class="org-right">1</td>
<td class="org-right">800</td>
<td class="org-right">4</td>
<td class="org-right">4</td>
</tr>
</tbody>
</table>
<p>So we have 400 applicants with about 32% of them being admitted. I don't know how to interpret the rank, maybe that's the quarter the student was in.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgb804af5">
<h2 id="orgb804af5">Plotting the data</h2>
<div class="outline-text-2" id="text-orgb804af5">
<p>First let's make a plot of our data to see how it looks. In order to have a 2D plot, let's ingore the rank.</p>
</div>
<div class="outline-3" id="outline-container-orgd754992">
<h3 id="orgd754992">Plot Points</h3>
<div class="outline-text-3" id="text-orgd754992">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">plot_points</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">identifier</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"All"</span><span class="p">):</span>
    <span class="sd">"""Plots the GRE vs GPA</span>

<span class="sd">    Args:</span>
<span class="sd">     data: frame with the admission, GRE, and GPA data</span>
<span class="sd">     identifier: something to identify the data set</span>
<span class="sd">    """</span>
    <span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"GRE vs GPA and Admissions to UCLA Graduate School (</span><span class="si">{}</span><span class="s2">)"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">identifier</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="s2">"gre"</span><span class="p">,</span><span class="s2">"gpa"</span><span class="p">]])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">"admit"</span><span class="p">])</span>
    <span class="n">admitted</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">rejected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">)]</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">rejected</span><span class="p">],</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">rejected</span><span class="p">],</span>
                <span class="n">s</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'red'</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'k'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Rejected"</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">admitted</span><span class="p">],</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">admitted</span><span class="p">],</span>
                <span class="n">s</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">'cyan'</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">'k'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Admitted"</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Test (GRE)'</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Grades (GPA)'</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org6cc43d8">
<h3 id="org6cc43d8">GRE Vs GPA</h3>
<div class="outline-text-3" id="text-org6cc43d8">
<div class="highlight">
<pre><span></span><span class="n">plot_points</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="gre_vs_gpa.png" src="posts/nano/introduction-to-neural-networks/student-admissions/gre_vs_gpa.png"></p>
</div>
<p>Roughly, it looks like the students with high scores in the grades and test passed, while the ones with low scores didn't, but the data is not as nicely separable as we hoped it would be (to say the least). Maybe it would help to take the rank into account? Let's make 4 plots, each one for each rank.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org3e3f6f8">
<h3 id="org3e3f6f8">By Rank</h3>
<div class="outline-text-3" id="text-org3e3f6f8"></div>
<div class="outline-4" id="outline-container-org1331857">
<h4 id="org1331857">Separating the ranks</h4>
<div class="outline-text-4" id="text-org1331857">
<div class="highlight">
<pre><span></span><span class="n">data_rank_1</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span>
<span class="n">data_rank_2</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">]</span><span class="o">==</span><span class="mi">2</span><span class="p">]</span>
<span class="n">data_rank_3</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">]</span><span class="o">==</span><span class="mi">3</span><span class="p">]</span>
<span class="n">data_rank_4</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">]</span><span class="o">==</span><span class="mi">4</span><span class="p">]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">plot_points</span><span class="p">(</span><span class="n">data_rank_1</span><span class="p">,</span> <span class="s2">"Rank 1"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="rank_1.png" src="posts/nano/introduction-to-neural-networks/student-admissions/rank_1.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">plot_points</span><span class="p">(</span><span class="n">data_rank_2</span><span class="p">,</span> <span class="s2">"Rank 2"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="rank_2.png" src="posts/nano/introduction-to-neural-networks/student-admissions/rank_2.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">plot_points</span><span class="p">(</span><span class="n">data_rank_3</span><span class="p">,</span> <span class="s2">"Rank 3"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="rank_3.png" src="posts/nano/introduction-to-neural-networks/student-admissions/rank_3.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">plot_points</span><span class="p">(</span><span class="n">data_rank_4</span><span class="p">,</span> <span class="s2">"Rank 4"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="rank_4.png" src="posts/nano/introduction-to-neural-networks/student-admissions/rank_4.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">ranked</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">"rank"</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">fraction</span> <span class="o">=</span> <span class="p">(</span><span class="n">ranked</span><span class="o">/</span><span class="n">data</span><span class="o">.</span><span class="n">admit</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">fraction</span><span class="p">[[</span><span class="s2">"rank"</span><span class="p">,</span> <span class="s2">"admit"</span><span class="p">]]))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">rank</th>
<th class="org-right" scope="col">admit</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.259843</td>
</tr>
<tr>
<td class="org-right">2</td>
<td class="org-right">0.425197</td>
</tr>
<tr>
<td class="org-right">3</td>
<td class="org-right">0.220472</td>
</tr>
<tr>
<td class="org-right">4</td>
<td class="org-right">0.0944882</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Fraction Admitted By Rank"</span><span class="p">)</span>
<span class="n">axe</span> <span class="o">=</span> <span class="n">fraction</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"rank"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"admit"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axe</span><span class="p">,</span> <span class="n">rot</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="rank_bar.png" src="posts/nano/introduction-to-neural-networks/student-admissions/rank_bar.png"></p>
</div>
<p>This looks more promising, as it seems that the lower the rank, the higher the acceptance rate (with rank 2 being the dominant rank among the admitted). Let's use the rank as one of our inputs. In order to do this, we should one-hot encode it.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1fc09c0">
<h2 id="org1fc09c0">One-Hot Encoding the Rank</h2>
<div class="outline-text-2" id="text-org1fc09c0">
<p>We'll do the one-hot-encoding using pandas' <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html">get_dummies</a> function.</p>
<div class="highlight">
<pre><span></span><span class="n">one_hot_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"rank"</span><span class="p">])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">one_hot_data</span><span class="o">.</span><span class="n">head</span><span class="p">()))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">380</td>
<td class="org-right">3.61</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">660</td>
<td class="org-right">3.67</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">800</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">640</td>
<td class="org-right">3.19</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">520</td>
<td class="org-right">2.93</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-2" id="outline-container-orga1d7dad">
<h2 id="orga1d7dad">Scaling the data</h2>
<div class="outline-text-2" id="text-orga1d7dad">
<p>The next step is to scale the data. We notice that the range for grades is 1.0-4.0, whereas the range for test scores is roughly 200-800, which is much larger. This means our data is skewed, and that makes it hard for a neural network to handle. Let's fit our two features into a range of 0-1, by dividing the grades by 4.0, and the test score by 800.</p>
</div>
<div class="outline-3" id="outline-container-org43cbac0">
<h3 id="org43cbac0">Making a copy of our data</h3>
<div class="outline-text-3" id="text-org43cbac0">
<div class="highlight">
<pre><span></span><span class="n">processed_data</span> <span class="o">=</span> <span class="n">one_hot_data</span><span class="p">[:]</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5511dbb">
<h3 id="org5511dbb">Scale the columns</h3>
<div class="outline-text-3" id="text-org5511dbb">
<div class="highlight">
<pre><span></span><span class="n">processed_data</span><span class="p">[</span><span class="s2">"gpa"</span><span class="p">]</span> <span class="o">=</span> <span class="n">one_hot_data</span><span class="p">[</span><span class="s2">"gpa"</span><span class="p">]</span><span class="o">/</span><span class="mi">4</span>
<span class="n">processed_data</span><span class="p">[</span><span class="s2">"gre"</span><span class="p">]</span> <span class="o">=</span> <span class="n">one_hot_data</span><span class="p">[</span><span class="s2">"gre"</span><span class="p">]</span><span class="o">/</span><span class="mi">800</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">processed_data</span><span class="o">.</span><span class="n">head</span><span class="p">()))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.475</td>
<td class="org-right">0.9025</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.825</td>
<td class="org-right">0.9175</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.8</td>
<td class="org-right">0.7975</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.65</td>
<td class="org-right">0.7325</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org3b2fe8b">
<h2 id="org3b2fe8b">Splitting the data into Training and Testing</h2>
<div class="outline-text-2" id="text-org3b2fe8b">
<p>In order to test our algorithm, we'll split the data into a Training and a Testing set by sampling the data's index (using <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html">numpy.random.choice</a>) to find the training set and dropping the sample (<a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html">pandas.DataFrame.drop</a>) from the data to create the test set. The size of the testing set will be 10% of the total data.</p>
<div class="highlight">
<pre><span></span><span class="n">training_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">processed_data</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">processed_data</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
                             <span class="n">size</span><span class="o">=</span><span class="n">training_size</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">processed_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">sample</span><span class="p">],</span> <span class="n">processed_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Number of training samples is"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Number of testing samples is"</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">))</span>
</pre></div>
<pre class="example">
Number of training samples is 360
Number of testing samples is 40
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">train_data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.85</td>
<td class="org-right">0.77</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.7</td>
<td class="org-right">0.745</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.775</td>
<td class="org-right">0.7625</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.825</td>
<td class="org-right">0.8975</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.75</td>
<td class="org-right">0.85</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.65</td>
<td class="org-right">0.975</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.775</td>
<td class="org-right">0.8325</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.875</td>
<td class="org-right">0.8175</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.475</td>
<td class="org-right">0.835</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.725</td>
<td class="org-right">0.84</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">test_data</span><span class="p">[:</span><span class="mi">10</span><span class="p">]))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.5</td>
<td class="org-right">0.77</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.875</td>
<td class="org-right">0.77</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.875</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.65</td>
<td class="org-right">0.8225</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.45</td>
<td class="org-right">0.785</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.75</td>
<td class="org-right">0.7875</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.725</td>
<td class="org-right">0.865</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.775</td>
<td class="org-right">0.795</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-right">0.725</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0.55</td>
<td class="org-right">0.8625</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-2" id="outline-container-org5f8cd32">
<h2 id="org5f8cd32">Splitting the data into features and targets (labels)</h2>
<div class="outline-text-2" id="text-org5f8cd32">
<p>Now, as a final step before the training, we'll split the data into features (X) and targets (y).</p>
<div class="highlight">
<pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'admit'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">"columns"</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s1">'admit'</span><span class="p">]</span>
<span class="n">features_test</span> <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">'admit'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s2">"columns"</span><span class="p">)</span>
<span class="n">targets_test</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="s1">'admit'</span><span class="p">]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="n">features</span><span class="p">[:</span><span class="mi">10</span><span class="p">]))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">gre</th>
<th class="org-right" scope="col">gpa</th>
<th class="org-right" scope="col">rank_1</th>
<th class="org-right" scope="col">rank_2</th>
<th class="org-right" scope="col">rank_3</th>
<th class="org-right" scope="col">rank_4</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0.85</td>
<td class="org-right">0.77</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">0.7</td>
<td class="org-right">0.745</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0.775</td>
<td class="org-right">0.7625</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0.825</td>
<td class="org-right">0.8975</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0.75</td>
<td class="org-right">0.85</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0.65</td>
<td class="org-right">0.975</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0.775</td>
<td class="org-right">0.8325</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0.875</td>
<td class="org-right">0.8175</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0.475</td>
<td class="org-right">0.835</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0.725</td>
<td class="org-right">0.84</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">admit</span><span class="o">=</span><span class="n">targets</span><span class="p">[:</span><span class="mi">10</span><span class="p">])))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">admit</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-2" id="outline-container-orgbef7def">
<h2 id="orgbef7def">Training the 2-layer Neural Network</h2>
<div class="outline-text-2" id="text-orgbef7def">
<p>The following function trains the 2-layer neural network. First, we'll write some helper functions.</p>
</div>
<div class="outline-3" id="outline-container-org73e9d64">
<h3 id="org73e9d64">Helper Functions</h3>
<div class="outline-text-3" id="text-org73e9d64">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<p>and the derivative of the <code>sigmoid</code>.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">error_formula</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">y</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgec5263a">
<h2 id="orgec5263a">Backpropagate the error</h2>
<div class="outline-text-2" id="text-orgec5263a">
<p>Now it's your turn to shine. Write the error term. Remember that this is given by the equation \[ -(y-\hat{y}) \sigma'(x) \]</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">error_term_formula</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span> <span class="o">*</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-org4f22956">
<h3 id="org4f22956">Training</h3>
<div class="outline-text-3" id="text-org4f22956">
<div class="highlight">
<pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">learn_rate</span> <span class="o">=</span> <span class="mf">0.5</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org73ade14">
<h4 id="org73ade14">Training function</h4>
<div class="outline-text-4" id="text-org73ade14">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train_nn</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learnrate</span><span class="p">):</span>

    <span class="c1"># Use to same seed to make debugging easier</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="n">n_records</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">last_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Initialize weights</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_features</span><span class="o">**.</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_features</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">del_w</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
            <span class="c1"># Loop through all records, x is the input, y is the target</span>

            <span class="c1"># Activation of the output unit</span>
            <span class="c1">#   Notice we multiply the inputs and the weights here </span>
            <span class="c1">#   rather than storing h as a separate variable </span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>

            <span class="c1"># The error, the target minus the network output</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">error_formula</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

            <span class="c1"># The error term</span>
            <span class="c1">#   Notice we calulate f'(h) here instead of defining a separate</span>
            <span class="c1">#   sigmoid_prime function. This just makes it faster because we</span>
            <span class="c1">#   can re-use the result of the sigmoid function stored in</span>
            <span class="c1">#   the output variable</span>
            <span class="n">error_term</span> <span class="o">=</span> <span class="n">error_term_formula</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>

            <span class="c1"># The gradient descent step, the error times the gradient times the inputs</span>
            <span class="n">del_w</span> <span class="o">+=</span> <span class="n">error_term</span> <span class="o">*</span> <span class="n">x</span>

        <span class="c1"># Update the weights here. The learning rate times the </span>
        <span class="c1"># change in weights, divided by the number of records to average</span>
        <span class="n">weights</span> <span class="o">+=</span> <span class="n">learnrate</span> <span class="o">*</span> <span class="n">del_w</span> <span class="o">/</span> <span class="n">n_records</span>

        <span class="c1"># Printing out the mean square error on the training set</span>
        <span class="k">if</span> <span class="n">e</span> <span class="o">%</span> <span class="p">(</span><span class="n">epochs</span> <span class="o">/</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Epoch:"</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">last_loss</span> <span class="ow">and</span> <span class="n">last_loss</span> <span class="o">&lt;</span> <span class="n">loss</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"Train loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">"  WARNING - Loss Increasing"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"Train loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
            <span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"========="</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Finished training!"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">train_nn</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">)</span>
</pre></div>
<pre class="example">
Epoch: 0
Train loss:  0.27247853979302755
=========
Epoch: 100
Train loss:  0.20397593223991445
=========
Epoch: 200
Train loss:  0.2014297690420066
=========
Epoch: 300
Train loss:  0.2003513187214578
=========
Epoch: 400
Train loss:  0.19984320017443669
=========
Epoch: 500
Train loss:  0.19956325048732546
=========
Epoch: 600
Train loss:  0.19938027609704898
=========
Epoch: 700
Train loss:  0.1992416788675009
=========
Epoch: 800
Train loss:  0.19912513146497982
=========
Epoch: 900
Train loss:  0.19902058341953008
=========
Finished training!
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org62c116c">
<h2 id="org62c116c">Calculating the Accuracy on the Test Data</h2>
<div class="outline-text-2" id="text-org62c116c">
<div class="highlight">
<pre><span></span><span class="n">test_out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features_test</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">test_out</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">targets_test</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Prediction accuracy: </span><span class="si">{:.3f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
<pre class="example">
Prediction accuracy: 0.575
</pre>
<p>Not horrible, considering the test-set, but not great either.</p>
</div>
<div class="outline-3" id="outline-container-orgae629d8">
<h3 id="orgae629d8">Try More Epochs</h3>
<div class="outline-text-3" id="text-orgae629d8">
<div class="highlight">
<pre><span></span><span class="n">weights_2</span> <span class="o">=</span> <span class="n">train_nn</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">learn_rate</span><span class="p">)</span>
</pre></div>
<pre class="example">
Epoch: 0
Train loss:  0.27247853979302755
=========
Epoch: 200
Train loss:  0.2014297690420066
=========
Epoch: 400
Train loss:  0.19984320017443669
=========
Epoch: 600
Train loss:  0.19938027609704898
=========
Epoch: 800
Train loss:  0.19912513146497982
=========
Epoch: 1000
Train loss:  0.19892324129363695
=========
Epoch: 1200
Train loss:  0.19874162735565162
=========
Epoch: 1400
Train loss:  0.19857138905455757
=========
Epoch: 1600
Train loss:  0.1984095079666442
=========
Epoch: 1800
Train loss:  0.1982546851201456
=========
Finished training!
</pre>
<div class="highlight">
<pre><span></span><span class="n">test_out</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features_test</span><span class="p">,</span> <span class="n">weights_2</span><span class="p">))</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">test_out</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">targets_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Prediction accuracy: </span><span class="si">{:.3f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
<pre class="example">
Prediction accuracy: 0.575
</pre>
<p>It doesn't make a noticeable difference. Maybe this is the best it can do with only these features.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/introduction-to-neural-networks/gradient-descent-practice/">Gradient Descent Practice</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/introduction-to-neural-networks/gradient-descent-practice/" rel="bookmark"><time class="published dt-published" datetime="2018-10-26T19:32:58-07:00" itemprop="datePublished" title="2018-10-26 19:32">2018-10-26 19:32</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent-practice/#org118b9ae">Imports</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent-practice/#org33d76f1">Helpers</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent-practice/#org8f20861">The Data</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent-practice/#org22a7480">The Basic Functions</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent-practice/#org84c3904">Training function</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent-practice/#org2943f9f">Simpler Training</a></li>
</ul>
</div>
</div>
<p>This will implement the basic functions of the Gradient Descent algorithm to find the boundary in a small dataset.</p>
<div class="outline-2" id="outline-container-org118b9ae">
<h2 id="org118b9ae">Imports</h2>
<div class="outline-text-2" id="text-org118b9ae"></div>
<div class="outline-3" id="outline-container-org3f70c20">
<h3 id="org3f70c20">From Pypi</h3>
<div class="outline-text-3" id="text-org3f70c20">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org898c934">
<h3 id="org898c934">This Project</h3>
<div class="outline-text-3" id="text-org898c934">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org33d76f1">
<h2 id="org33d76f1">Helpers</h2>
<div class="outline-text-2" id="text-org33d76f1"></div>
<div class="outline-3" id="outline-container-org5d7d41d">
<h3 id="org5d7d41d">For Plotting</h3>
<div class="outline-text-3" id="text-org5d7d41d"></div>
<div class="outline-4" id="outline-container-orgd018cc4">
<h4 id="orgd018cc4">Plot Points</h4>
<div class="outline-text-4" id="text-orgd018cc4">
<p>This first function is used to plot the data points as a scatter plot.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">plot_points</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">axe</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">"""Makes a scatter plot</span>

<span class="sd">    Args:</span>
<span class="sd">     X: array of inputs</span>
<span class="sd">     y: array of labels (1's and 0's)</span>
<span class="sd">     axe: matplotlib axis object</span>

<span class="sd">    Return:</span>
<span class="sd">     axe: matplotlib axis</span>
<span class="sd">    """</span>
    <span class="n">admitted</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">rejected</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">axe</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">rejected</span><span class="p">],</span>
                <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">rejected</span><span class="p">],</span>
                <span class="n">s</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"blue"</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="s2">"Rejects"</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">admitted</span><span class="p">],</span>
                   <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">admitted</span><span class="p">],</span>
                   <span class="n">s</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'red'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'k'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Accepted"</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">axe</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8217401">
<h4 id="org8217401">Display</h4>
<div class="outline-text-4" id="text-org8217401">
<p>The somewhat obscurely named <code>display</code> function is used to plot the separation lines of our model.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">display</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'g--'</span><span class="p">,</span> <span class="n">axe</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">"""Makes a line plot</span>

<span class="sd">    Args:</span>
<span class="sd">     m: slope for the line</span>
<span class="sd">     b: intercept for the line</span>
<span class="sd">     color: color and line type for plot</span>
<span class="sd">     axe: matplotlib axis</span>

<span class="sd">    Return:</span>
<span class="sd">     axe: matplotlib axis</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">axe</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="n">b</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">axe</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">)</span>
<span class="n">FIGURE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org8f20861">
<h2 id="org8f20861">The Data</h2>
<div class="outline-text-2" id="text-org8f20861">
<p>We'll load the data from <code>data.csv</code> which has three columns - the first two are the inputs and the third is the label that we are trying to predict for the inputs.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"data.csv"</span><span class="p">)</span>
</pre></div>
<p>My setup is a little different than the Udacity setup so I'm going to have to get into the habit of setting the file before submission.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="p">)</span>
<span class="n">DATA_FILE</span> <span class="o">=</span> <span class="n">path</span><span class="o">.</span><span class="n">from_folder</span>
</pre></div>
<pre class="example">
../../../data/introduction_to_neural_networks/data.csv
</pre>
<p>I'm not sure exactly why, but the data is loaded as a pandas DataFrame (with <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html">read_csv</a>) and then converted into two <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html">arrays</a>.</p>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_FILE</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Data</th>
<th class="org-right" scope="col">Rows</th>
<th class="org-left" scope="col">Columns</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">X</td>
<td class="org-right">100</td>
<td class="org-left">2</td>
</tr>
<tr>
<td class="org-left">y</td>
<td class="org-right">100</td>
<td class="org-left">N/A</td>
</tr>
</tbody>
</table>
<p>Here's what our data looks like. I don't know what the inputs represent so I didn't label the axes, but it is a plot of the first input variable vs the second input variable, with the colors determined by the labels (y-values).</p>
<div class="highlight">
<pre><span></span><span class="n">axe</span> <span class="o">=</span> <span class="n">plot_points</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="data_scatter.png" src="posts/nano/introduction-to-neural-networks/gradient-descent-practice/data_scatter.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org22a7480">
<h2 id="org22a7480">The Basic Functions</h2>
<div class="outline-text-2" id="text-org22a7480"></div>
<div class="outline-3" id="outline-container-orgaedd12c">
<h3 id="orgaedd12c">The Sigmoid activation function</h3>
<div class="outline-text-3" id="text-orgaedd12c">
<p>This is the function that pushes the probabilities that are produced to be close to 1 or 0 so we can classify the inputs.</p>
<p>\[\sigma(x) = \frac{1}{1+e^{-x}}\]</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the sigmoid of x</span>

<span class="sd">    Args:</span>
<span class="sd">     x: input to classify</span>

<span class="sd">    Returns:</span>
<span class="sd">     sigmoid of x</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="sigmoid.png" src="posts/nano/introduction-to-neural-networks/gradient-descent-practice/sigmoid.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org42ec457">
<h3 id="org42ec457">Output (prediction) formula</h3>
<div class="outline-text-3" id="text-org42ec457">
<p>This function takes the dot product of the weights and inputs and adds the bias before returning the sigmoid of the calculation.</p>
<p>\[\hat{y} = \sigma(w_1 x_1 + w_2 x_2 + b)\]</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">output_formula</span><span class="p">(</span><span class="n">features</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                   <span class="n">weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                   <span class="n">bias</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Predicts the outcomes for the inputs</span>

<span class="sd">    Args:</span>
<span class="sd">     features: inputs variables</span>
<span class="sd">     weights: array of weights for the variables</span>
<span class="sd">     bias: array of constants to adjust the output</span>

<span class="sd">    Returns:</span>
<span class="sd">     an array of predicted labels for the inputs</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org56b84b1">
<h3 id="org56b84b1">Error function (log-loss)</h3>
<div class="outline-text-3" id="text-org56b84b1">
<p>This is used for reporting, since the actual updating of the weights uses the gradient.</p>
<p>\[Error(y, \hat{y}) = - y \log(\hat{y}) - (1-y) \log(1-\hat{y})\]</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">error_formula</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the amount of error</span>

<span class="sd">    Args:</span>
<span class="sd">     y: the true labels</span>
<span class="sd">     output: the predicted labels</span>

<span class="sd">    Returns:</span>
<span class="sd">     amount of error in the output</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org92ff502">
<h3 id="org92ff502">The function that updates the weights (the gradient descent step)</h3>
<div class="outline-text-3" id="text-org92ff502">
<p>This makes a prediction of the labels based on the inputs (using <code>output_formula</code>) and then updates the weights and bias based on the amount of error it had in the predictions.</p>
<p>\[ w_i \longrightarrow w_i + \alpha (y - \hat{y}) x_i\\ b \longrightarrow b + \alpha (y - \hat{y})\\ \]</p>
<p>Where \(\alpha\) is our learning rate and \(\hat{y}\) is our prediction for <i>y</i>.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Updates the weights based on the amount of error</span>

<span class="sd">    Args:</span>
<span class="sd">     x: inputs</span>
<span class="sd">     y: actual labels</span>
<span class="sd">     weights: amount to weight each input</span>
<span class="sd">     bias: constant to adjust the output</span>
<span class="sd">     learning_rate: how much to adjust the weights</span>

<span class="sd">    Return:</span>
<span class="sd">     w, b: the updated weights</span>
<span class="sd">    """</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">output_formula</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">+=</span>  <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">bias</span> <span class="o">+=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org84c3904">
<h2 id="org84c3904">Training function</h2>
<div class="outline-text-2" id="text-org84c3904">
<p>This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">44</span><span class="p">)</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">graph_lines</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Trains a model using gradient descent</span>

<span class="sd">    Args:</span>
<span class="sd">     features: matrix of inputs</span>
<span class="sd">     targets: array of labels for the inputs</span>
<span class="sd">     epochs: number of times to train the model</span>
<span class="sd">     learning_rate: how much to adjust the weights per epoch</span>

<span class="sd">    Returns:</span>
<span class="sd">     weights, bias, errors, plot_x, plot_y: What we learned and how we improved</span>
<span class="sd">    """</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_records</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">last_loss</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n_features</span><span class="o">**.</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_features</span><span class="p">)</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">plot_x</span><span class="p">,</span> <span class="n">plot_y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># train on each row in the training data</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">output_formula</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">error_formula</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
            <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

        <span class="c1"># Printing out the log-loss error on the training set</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">output_formula</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">error_formula</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">out</span><span class="p">))</span>
        <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="p">(</span><span class="n">epochs</span> <span class="o">/</span> <span class="mi">10</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">========== Epoch </span><span class="si">{}</span><span class="s2"> =========="</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">last_loss</span> <span class="ow">and</span> <span class="n">last_loss</span> <span class="o">&lt;</span> <span class="n">loss</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"Training loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s2">"  WARNING - Loss Increasing"</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"Training loss: "</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
            <span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">out</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
            <span class="n">accuracy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">targets</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy: "</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">graph_lines</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">%</span> <span class="p">(</span><span class="n">epochs</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plot_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">plot_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">bias</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="n">plot_x</span><span class="p">,</span> <span class="n">plot_y</span>
</pre></div>
<p>Time to train the algorithm.</p>
<p>When we run the function, we'll obtain the following:</p>
<ul class="org-ul">
<li>10 updates with the current training loss and accuracy</li>
<li>A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs.</li>
<li>A plot of the error function. Notice how it decreases as we go through more epochs.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="n">plot_x</span><span class="p">,</span> <span class="n">plot_y</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
<pre class="example">

========== Epoch 0 ==========
Training loss:  0.7135845195381634
Accuracy:  0.4

========== Epoch 10 ==========
Training loss:  0.6225835210454962
Accuracy:  0.59

========== Epoch 20 ==========
Training loss:  0.5548744083669508
Accuracy:  0.74

========== Epoch 30 ==========
Training loss:  0.501606141872473
Accuracy:  0.84

========== Epoch 40 ==========
Training loss:  0.4593334641861401
Accuracy:  0.86

========== Epoch 50 ==========
Training loss:  0.42525543433469976
Accuracy:  0.93

========== Epoch 60 ==========
Training loss:  0.3973461571671399
Accuracy:  0.93

========== Epoch 70 ==========
Training loss:  0.3741469765239074
Accuracy:  0.93

========== Epoch 80 ==========
Training loss:  0.35459973368161973
Accuracy:  0.94

========== Epoch 90 ==========
Training loss:  0.3379273658879921
Accuracy:  0.94
</pre>
<p>As you can see from the output the accuracy is getting better while the training loss (the mean of the error) is going down.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Plotting the solution boundary</span>
<span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Solution boundary"</span><span class="p">)</span>
<span class="n">learning</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">plot_x</span><span class="p">,</span> <span class="n">plot_y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">learn_x</span><span class="p">,</span> <span class="n">learn_y</span> <span class="ow">in</span> <span class="n">learning</span><span class="p">:</span>
    <span class="n">display</span><span class="p">(</span><span class="n">learn_x</span><span class="p">,</span> <span class="n">learn_y</span><span class="p">,</span> <span class="n">axe</span><span class="o">=</span><span class="n">axe</span><span class="p">)</span>
<span class="n">axe</span> <span class="o">=</span> <span class="n">display</span><span class="p">(</span><span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">bias</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">axe</span><span class="p">)</span>

<span class="c1"># Plotting the data</span>
<span class="n">axe</span> <span class="o">=</span> <span class="n">plot_points</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">axe</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="training.png" src="posts/nano/introduction-to-neural-networks/gradient-descent-practice/training.png"></p>
</div>
<p>The green lines are the boundary as the model is trained, the black line is the final separator. While pretty, the green lines kind of obscure how well the sepration did. Here's just the final line with the input data.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Plotting the solution boundary</span>
<span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"The Final Model"</span><span class="p">)</span>
<span class="n">axe</span> <span class="o">=</span> <span class="n">display</span><span class="p">(</span><span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">bias</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">axe</span><span class="p">)</span>

<span class="c1"># Plotting the data</span>
<span class="n">axe</span> <span class="o">=</span> <span class="n">plot_points</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">axe</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="model.png" src="posts/nano/introduction-to-neural-networks/gradient-descent-practice/model.png"></p>
</div>
<p>Finally, this is the amount of error as the model is trained.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Plotting the error</span>
<span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Error Plot"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Number of epochs'</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Error'</span><span class="p">)</span>
<span class="n">axe</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="error.png" src="posts/nano/introduction-to-neural-networks/gradient-descent-practice/error.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org2943f9f">
<h2 id="org2943f9f">Simpler Training</h2>
<div class="outline-text-2" id="text-org2943f9f">
<p>If you squint at the <code>train</code> function you might notice that a considerable amount of it is used for reporting, making it a little harder to read than necessary. This is the same function without the extra reporting.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">only_train</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Trains a model using gradient descent</span>

<span class="sd">    Args:</span>
<span class="sd">     features: matrix of inputs</span>
<span class="sd">     targets: array of labels for the inputs</span>
<span class="sd">     epochs: number of times to train the model</span>
<span class="sd">     learning_rate: how much to adjust the weights per epoch</span>

<span class="sd">    Returns:</span>
<span class="sd">     weights, bias: Our final model</span>
<span class="sd">    """</span>
    <span class="n">number_of_records</span><span class="p">,</span> <span class="n">number_of_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">number_of_records</span><span class="o">**.</span><span class="mi">5</span><span class="p">,</span>
                                  <span class="n">size</span><span class="o">=</span><span class="n">number_of_features</span><span class="p">)</span>
    <span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
            <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">only_train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1"># Plotting the solution boundary</span>
<span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"The Final Model"</span><span class="p">)</span>
<span class="n">axe</span> <span class="o">=</span> <span class="n">display</span><span class="p">(</span><span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">bias</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">'black'</span><span class="p">,</span> <span class="n">axe</span><span class="p">)</span>

<span class="c1"># Plotting the data</span>
<span class="n">axe</span> <span class="o">=</span> <span class="n">plot_points</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">axe</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="model_2.png" src="posts/nano/introduction-to-neural-networks/gradient-descent-practice/model_2.png"></p>
</div>
<p>And the model for our linear classifier:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">((</span><span class="s2">"</span><span class="se">\\</span><span class="s2">[</span><span class="se">\n</span><span class="s2">w_0 x_0 + w_1 x_1 + b "</span>
       <span class="s2">"= </span><span class="si">{:.2f}</span><span class="s2">x_0 + </span><span class="si">{:.2f}</span><span class="s2">x_1 + </span><span class="si">{:.2f}</span><span class="se">\n\\</span><span class="s2">]"</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                                       <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                       <span class="n">bias</span><span class="p">))</span>
</pre></div>
<p>\[ w_0 x_0 + w_1 x_1 + b = -3.13x_0 + -3.62x_1 + 3.31 \]</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/introduction-to-neural-networks/gradient-descent/">Gradient Descent</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/introduction-to-neural-networks/gradient-descent/" rel="bookmark"><time class="published dt-published" datetime="2018-10-26T18:30:34-07:00" itemprop="datePublished" title="2018-10-26 18:30">2018-10-26 18:30</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent/#orga93a704">What is this about?</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent/#orge8de096">How does Gradient Descent Work?</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent/#orga241b74">Okay, but what again?</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/gradient-descent/#org1b63e79">So, how do you put it all together?</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orga93a704">
<h2 id="orga93a704">What is this about?</h2>
<div class="outline-text-2" id="text-orga93a704">
<p>We have an initial network and we make a prediction using the inputs. USing the output we can calculate the error. Now that we have the error we need to update our weights - how do we do this? With <a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a>, a method that tries to pursue a downward trajectory using the slope of our errors.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orge8de096">
<h2 id="orge8de096">How does Gradient Descent Work?</h2>
<div class="outline-text-2" id="text-orge8de096">
<p>We start by making an initial prediction.</p>
<p>\[ \hat{y} = \sigma(Wx+b) \]</p>
<p>which it turns out is not accurate. We then subtract the gradient of the error (a partial derivative \(\frac{\delta E}{\delta W}\)) multiplied by some learning rate \(\alpha\) that governs how much we are willing to change at each step down the hill.</p>
<p>\[ w'_i \gets w_i - \alpha \frac{\delta E}{\delta W_i}\\ b' \gets b -\alpha \frac{\delta E}{\delta b}\\ \hat{y'} = \sigma(W'x t b') \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-orga241b74">
<h2 id="orga241b74">Okay, but what again?</h2>
<div class="outline-text-2" id="text-orga241b74">
<p>To find our gradient we need to take some derivatives. Let's start with the derivative of our sigmoid.</p>
<p>\[ \sigma' = \sigma(x) (1 - \sigma(x)) \] The lecturer shows the derivation, but take my word for it, this is what it is.</p>
<p>Our error is:</p>
<p>\[ E = -y \ln(\hat{y}) - (1 - y)\ln(1 - \hat{y}) \]</p>
<p>and the derivative of this error:</p>
<p>\[ \frac{\delta}{\delta_{wj}}\hat{y} = \hat{y}(1 - \hat{y}) \dot x_j \]</p>
<p>Trust me, this is the derivation. And the derivation of our error becomes:</p>
<p>\[ \frac{\delta}{\delta w_j} = -(y - \hat{y})x_j \]</p>
<p>and for the bias term we get:</p>
<p>\[ \frac{\delta}{\delta b} = -(y - \hat{y}) \]</p>
<p>And our overall gradient can be written as:</p>
<p>\[ \Delta E = -(y - \hat{y})(x_1, \ldots, x_n, 1) \]</p>
<p>So our gradient is the coordinates of the points times the error. This means that the closer our prediction is to the true value, the smaller the gradient will be, and vice-versa, much like the <i>Perceptron Trick</i> we learned earlier.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org1b63e79">
<h2 id="org1b63e79">So, how do you put it all together?</h2>
<div class="outline-text-2" id="text-org1b63e79">
<p>Okay, this is how you update your weights. First, scale \(\alpha\) to match your data set (divide by the number of rows). \[ \alpha = \frac{1}{m}\alpha \]</p>
<p>Now calculate your new weights.</p>
<p>\[ w_i' \gets w_i + \alpha(y - \hat{y})x_i \]</p>
<p>And the new bias.</p>
<p>\[ b' \gets b \alpha(y - \hat{y}) \]</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/grokking/04_gradient_descent/compare-and-learn/">Compare and Learn</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/grokking/04_gradient_descent/compare-and-learn/" rel="bookmark"><time class="published dt-published" datetime="2018-10-26T10:54:02-07:00" itemprop="datePublished" title="2018-10-26 10:54">2018-10-26 10:54</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<p>i#t+BEGIN_COMMENT .. title: Compare and Learn .. slug: compare-and-learn .. date: 2018-10-26 10:54:02 UTC-07:00 .. tags: grokking,gradient descent .. category: Grokking .. link: .. description: Introduction to Gradient Descent. .. type: text #+END_COMMENT</p>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#orgba50fce">Imports</a></li>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#org48b19fb">What is this about?</a></li>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#orgbca33d4">What's the next step after Predict?</a></li>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#orgce36bcb">So how do we learn?</a></li>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#orgdcd04a7">Training the Model</a></li>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#orgacbe4da">Is there a better way to update the weights?</a></li>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#orgdd8a650">A Discursion On Derivatives</a></li>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#orgf9c447c">When does this work?</a></li>
<li><a href="posts/grokking/04_gradient_descent/compare-and-learn/#org7994874">Fixing the Big Input Problem</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgba50fce">
<h2 id="orgba50fce">Imports</h2>
<div class="outline-text-2" id="text-orgba50fce"></div>
<div class="outline-3" id="outline-container-org937c8c1">
<h3 id="org937c8c1">From PyPi</h3>
<div class="outline-text-3" id="text-org937c8c1">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Digraph</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8dcd1de">
<h3 id="org8dcd1de">Setup the plotting</h3>
<div class="outline-text-3" id="text-org8dcd1de">
<div class="highlight">
<pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="o">.</span><span class="n">figure_format</span><span class="o">=</span><span class="s1">'retina'</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">)</span>
<span class="n">FIGURE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org48b19fb">
<h2 id="org48b19fb">What is this about?</h2>
<div class="outline-text-2" id="text-org48b19fb">
<p>The three main steps in training a model are:</p>
<ol class="org-ol">
<li>Predict</li>
<li>Compare</li>
<li>Learn</li>
</ol>
<p><i>Forward Propagation</i> was about the <i>Predict</i> step - we fed some inputs to a network and it output its predictions. Now we're going to look an steps 2 and 3 - <i>Compare</i> and <i>Learn</i>, the steps where we figure out how to improve the weights in our network.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgbca33d4">
<h2 id="orgbca33d4">What's the next step after Predict?</h2>
<div class="outline-text-2" id="text-orgbca33d4">
<p>As noted above, step 2 is <i>Compare</i> meaning compare our predictions with what we know to be the real answers (so this is <a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning</a>) and see how well (or bad) we did.</p>
</div>
<div class="outline-3" id="outline-container-org3a3b0a2">
<h3 id="org3a3b0a2">Okay, but then what?</h3>
<div class="outline-text-3" id="text-org3a3b0a2">
<p>After <i>Compare</i> we move on to the <i>Learn</i> step where we adjust the weights based on the errors we found in <i>Compare</i>. In this case we'll use <a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a> to find new weights for the network.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgdc477a2">
<h3 id="orgdc477a2">So, how do we find the error again?</h3>
<div class="outline-text-3" id="text-orgdc477a2">
<p>There are many different ways to measure error, each with different positive and negative attributes, but in this case we're going to use <a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean Squared Error</a>. Here's an example with one measurement.</p>
<div class="highlight">
<pre><span></span><span class="n">weight</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">input_value</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">input_value</span>
<span class="c1"># you implicitly divide by 1 to get the mean of the square</span>
<span class="n">actual_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">expected</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="n">expected_error</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">expected_error</span> <span class="o">-</span> <span class="n">actual_error</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.01</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Error: </span><span class="si">{:.2f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">actual_error</span><span class="p">))</span>
</pre></div>
<pre class="example">
Error: 0.30
</pre>
<p>Two things to note about the consequences of squaring the error - one is that it's always positive which is useful because you might have both positive and negative errors which would tend to cancel each other out when you take the mean (the <code>actual_error</code> above is a mean with an implied count of 1), even though both positive and negative errors are wrong, the other consequence is that the greater the error, the larger it grows (it follows a parabola instead of a line).</p>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Squared vs Unsquared Errors"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Squared Errors"</span> <span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Errors"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="squared_error.png" src="posts/grokking/04_gradient_descent/compare-and-learn/squared_error.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgce36bcb">
<h2 id="orgce36bcb">So how do we learn?</h2>
<div class="outline-text-2" id="text-orgce36bcb">
<p>One method is <i>Hot-and-Cold Learning</i>. With this method you move your weights a little and pick the one that improves the error-rate. This first example will go back to the one feature network that tries to predict if a team will win using the average number of toes they have.</p>
<div class="highlight">
<pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="n">comment</span><span class="o">=</span><span class="s2">"Toes"</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">attr</span><span class="p">(</span><span class="n">rankdir</span><span class="o">=</span><span class="s2">"LR"</span><span class="p">)</span>
<span class="c1"># input layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"a"</span><span class="p">,</span> <span class="s2">"Toes"</span><span class="p">)</span>

<span class="c1"># output layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"b"</span><span class="p">,</span> <span class="s2">"Win"</span><span class="p">)</span>

<span class="c1"># edge</span>
<span class="n">graph</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s2">"a"</span><span class="p">,</span> <span class="s2">"b"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Weight"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">"graphs/toes_model.dot"</span><span class="p">)</span>
<span class="n">graph</span>
</pre></div>
<div class="figure">
<p><img alt="toes_model.dot.png" src="posts/grokking/04_gradient_descent/compare-and-learn/toes_model.dot.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">toes_model</span><span class="p">(</span><span class="n">toes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Predicts if the team will win based on the number of toes</span>

<span class="sd">    Return:</span>
<span class="sd">     predction: probability of winning</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">toes</span> <span class="o">*</span> <span class="n">weight</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">print_error</span><span class="p">(</span><span class="n">predicted</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">actual</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"Toes Model"</span><span class="p">,</span>
                <span class="n">separator</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Prints the (mean) squared error</span>

<span class="sd">    Args:</span>

<span class="sd">     predicted: what the model predicted</span>
<span class="sd">     actual: whether the team won or not</span>
<span class="sd">     label: something to identify the model</span>
<span class="sd">     separator: How to separate the output</span>
<span class="sd">    Returns:</span>
<span class="sd">     mse: the error</span>
<span class="sd">    """</span>
    <span class="n">error</span> <span class="o">=</span> <span class="p">(</span><span class="n">actual</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">separator</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">label</span><span class="p">,</span>
                          <span class="s2">"Predicted: </span><span class="si">{:.2f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted</span><span class="p">),</span>
                          <span class="s2">"Actual: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">actual</span><span class="p">),</span>
                          <span class="s2">"MSE: </span><span class="si">{:.4f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">error</span><span class="p">)]))</span>
    <span class="k">return</span> <span class="n">error</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">toes</span> <span class="o">=</span> <span class="mf">8.5</span>
<span class="n">actual</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">toes</span><span class="p">)</span>
<span class="n">error_original</span> <span class="o">=</span> <span class="n">print_error</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="n">actual</span><span class="p">)</span>
</pre></div>
<pre class="example">
Toes Model
Predicted: 0.85
Actual: 1
MSE: 0.0225
</pre>
<p>So our model has a Mean Squared Error of around 0.02, how do we make it better with the Hot and Cold Method? By trying a larger and smaller weight and using the one that makes the error smaller.</p>
<div class="highlight">
<pre><span></span><span class="n">weight_change</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">weight</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">knob_turned_up</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">toes</span><span class="p">,</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">weight_change</span><span class="p">)</span>
<span class="n">knob_turned_down</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">toes</span><span class="p">,</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">weight_change</span><span class="p">)</span>

<span class="n">error_up</span> <span class="o">=</span> <span class="n">print_error</span><span class="p">(</span><span class="n">knob_turned_up</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="s2">"Turned Up"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="n">error_down</span> <span class="o">=</span> <span class="n">print_error</span><span class="p">(</span><span class="n">knob_turned_down</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="s2">"Turned Down"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Turned Up
Predicted: 0.94
Actual: 1
MSE: 0.0042

Turned Down
Predicted: 0.77
Actual: 1
MSE: 0.0552
</pre>
<p>Looking at the error, it looks like making the weight higher improved the score, so we should adjust our weight upwards.</p>
<div class="highlight">
<pre><span></span><span class="k">if</span> <span class="n">error_original</span> <span class="o">&gt;</span> <span class="n">error_up</span> <span class="ow">or</span> <span class="n">error_original</span> <span class="o">&gt;</span> <span class="n">error_down</span><span class="p">:</span>
    <span class="n">change_direction</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">error_down</span> <span class="o">&lt;</span> <span class="n">error_original</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">weight_updated</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">change_direction</span> <span class="o">*</span> <span class="n">weight_change</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">toes</span><span class="p">,</span> <span class="n">weight_updated</span><span class="p">)</span>
    <span class="n">error_update</span> <span class="o">=</span> <span class="n">print_error</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="s2">"Updated Model"</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">error_update</span> <span class="o">&lt;</span> <span class="n">error_original</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Model didn't improve."</span><span class="p">)</span>
</pre></div>
<pre class="example">
Updated Model
Predicted: 0.94
Actual: 1
MSE: 0.0042
</pre>
<p>So, this is what machine learning is really about, finding the parameters that give the best prediction. This is why it is often called a <b>search</b> problem - each of your parameters can have a variety of weights (infinite, actually) so what you are doing when you train your model is searching the space of weights to find the set that gives the best outcome for your metric. In this case we are looking to minimize our Mean Squared Error.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgdcd04a7">
<h2 id="orgdcd04a7">Training the Model</h2>
<div class="outline-text-2" id="text-orgdcd04a7">
<p>Rather than than trying to do the checks one at a time, we can run the Hot and Cold Learning in a loop to tune our model.</p>
<div class="highlight">
<pre><span></span><span class="n">weight</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">input_value</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">actual</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">weight_change</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">input_value</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="n">print_error</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="s2">"Step    1 Weight: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weight</span><span class="p">),</span> <span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">)</span>
<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimal_step</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">**</span><span class="mi">8</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2001</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">input_value</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">error</span> <span class="o">=</span> <span class="p">(</span><span class="n">print_error</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span>
                         <span class="s2">"Step </span><span class="si">{:4}</span><span class="s2"> Weight: </span><span class="si">{:.2f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">weight</span><span class="p">),</span> <span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">)</span>
             <span class="k">if</span> <span class="ow">not</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="k">else</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">optimal_step</span> <span class="ow">and</span> <span class="n">error</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
        <span class="n">optimal_step</span> <span class="o">=</span> <span class="n">step</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">up_prediction</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">input_value</span><span class="p">,</span> <span class="n">weight</span> <span class="o">+</span> <span class="n">weight_change</span><span class="p">)</span>
    <span class="n">down_prediction</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">input_value</span><span class="p">,</span> <span class="n">weight</span> <span class="o">-</span> <span class="n">weight_change</span><span class="p">)</span>
    <span class="n">up_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">up_prediction</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">down_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">down_prediction</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">direction</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">down_error</span> <span class="o">&lt;</span> <span class="n">up_error</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="n">weight</span> <span class="o">+=</span> <span class="n">direction</span> <span class="o">*</span> <span class="n">weight_change</span>
    <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Optimum Reached at Step </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimal_step</span><span class="p">))</span>
</pre></div>
<pre class="example">
Step    1 Weight: 0.5   Predicted: 0.25 Actual: 0.8     MSE: 0.3025
Step  100 Weight: 0.60  Predicted: 0.30 Actual: 0.8     MSE: 0.2505
Step  200 Weight: 0.70  Predicted: 0.35 Actual: 0.8     MSE: 0.2030
Step  300 Weight: 0.80  Predicted: 0.40 Actual: 0.8     MSE: 0.1604
Step  400 Weight: 0.90  Predicted: 0.45 Actual: 0.8     MSE: 0.1229
Step  500 Weight: 1.00  Predicted: 0.50 Actual: 0.8     MSE: 0.0903
Step  600 Weight: 1.10  Predicted: 0.55 Actual: 0.8     MSE: 0.0628
Step  700 Weight: 1.20  Predicted: 0.60 Actual: 0.8     MSE: 0.0402
Step  800 Weight: 1.30  Predicted: 0.65 Actual: 0.8     MSE: 0.0227
Step  900 Weight: 1.40  Predicted: 0.70 Actual: 0.8     MSE: 0.0101
Step 1000 Weight: 1.50  Predicted: 0.75 Actual: 0.8     MSE: 0.0026
Step 1100 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 1200 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 1300 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 1400 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 1500 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 1600 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 1700 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 1800 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 1900 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step 2000 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Optimum Reached at Step 1100
</pre>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Hot and Cold Mean Squared Error"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Training Repetition"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)),</span> <span class="n">errors</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="hot_and_cold_error.png" src="posts/grokking/04_gradient_descent/compare-and-learn/hot_and_cold_error.png"></p>
</div>
<p>Looking at the output you can see that it reached an error of (nearly) zero at the 1,100th repetition. Based on the plot it looks like it kind of slowed down at the end, which is odd since we're using addition and subtraction, but I guess as the weight gets bigger the proportion of change you add becomes less.</p>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Hot and Cold Mean Squared Error vs Weights"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Weight"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="s2">"."</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="hot_and_cold_weights_vs_error.png" src="posts/grokking/04_gradient_descent/compare-and-learn/hot_and_cold_weights_vs_error.png"></p>
</div>
<p>Our optimal weight appears to be 1.6. Given the simplicity of our model we can check by solving the equation.</p>
<p>\[ prediction = weight \times input\\ weight = \frac{prediction}{input} \]</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">prediction</span><span class="o">/</span><span class="n">input_value</span><span class="p">)</span>
</pre></div>
<pre class="example">
1.6009999999999343
</pre>
<p>So, that looks about right.</p>
</div>
<div class="outline-3" id="outline-container-org1c2578f">
<h3 id="org1c2578f">Pros and Cons of Hot and Cold Learning</h3>
<div class="outline-text-3" id="text-org1c2578f">
<p>The main thing that Hot and Cold Learning has going for it is that it is simple to understand and implement. There are a couple of problems with it though:</p>
<ul class="org-ul">
<li>You have to make multiple predictions per knob to make a decision on the change to make.</li>
<li>The amount you change the weight at each step can make it impossible to get the right weight, and in most cases you won't have just one input value so it's hard to know what to pick</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgacbe4da">
<h2 id="orgacbe4da">Is there a better way to update the weights?</h2>
<div class="outline-text-2" id="text-orgacbe4da">
<p>With <i>Hot and Cold Learning</i> we make multiple predictions to decide which direction to add a set amount to the weight. But we can instead use the error to change our weight, and in doing so we will change both direction and scale based on the error. In this case <i>error</i> means "pure error", or just the difference between the prediction and the actual value.</p>
<p>\[ error = prediction - actual \]</p>
<p>Since we don't square it the error will be positive if our prediction is too high and negative if it is too low. We don't want to just use the difference, though, because we are adjusting a weight that gets multiplied by the input, so we need to scale the amount of change by the input.</p>
<p>Note that the ordering is now important - you have to subtract the error in the version above and add it if the terms are switched.</p>
<p>\[ adjustment = error * input\\ weights' = weights - adjustment \]</p>
<p>This is the method of <a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a>. This is how it looks run on our previous problem.</p>
<div class="highlight">
<pre><span></span><span class="n">weight</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">input_value</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">actual</span> <span class="o">=</span> <span class="mf">0.8</span>

<span class="n">prediction</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">input_value</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="n">print_error</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="s2">"Step    1 Weight: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weight</span><span class="p">),</span> <span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">)</span>
<span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimal_step</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">**</span><span class="mi">8</span>
<span class="n">optimal_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">print_every</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">stop_after</span> <span class="o">=</span> <span class="mi">30</span>
<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2001</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">toes_model</span><span class="p">(</span><span class="n">input_value</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_value</span>
    <span class="n">weight</span> <span class="o">-=</span> <span class="n">difference</span>
    <span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>    
    <span class="n">error</span> <span class="o">=</span> <span class="p">(</span><span class="n">print_error</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span>
                         <span class="s2">"Step </span><span class="si">{:4}</span><span class="s2"> Weight: </span><span class="si">{:.2f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">weight</span><span class="p">),</span> <span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">)</span>
             <span class="k">if</span> <span class="ow">not</span> <span class="n">step</span> <span class="o">%</span> <span class="n">print_every</span> <span class="k">else</span> <span class="p">(</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">optimal_step</span> <span class="ow">and</span> <span class="n">error</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
        <span class="n">optimal_step</span> <span class="o">=</span> <span class="n">step</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">error</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
        <span class="n">optimal_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">optimal_count</span> <span class="o">&gt;=</span> <span class="n">stop_after</span><span class="p">:</span>
        <span class="k">break</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Optimum Reached at Step </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimal_step</span><span class="p">))</span>
</pre></div>
<pre class="example">
Step    1 Weight: 0.5   Predicted: 0.25 Actual: 0.8     MSE: 0.3025
Step    5 Weight: 1.34  Predicted: 0.63 Actual: 0.8     MSE: 0.0303
Step   10 Weight: 1.54  Predicted: 0.76 Actual: 0.8     MSE: 0.0017
Step   15 Weight: 1.59  Predicted: 0.79 Actual: 0.8     MSE: 0.0001
Step   20 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step   25 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step   30 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step   35 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step   40 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step   45 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step   50 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step   55 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Step   60 Weight: 1.60  Predicted: 0.80 Actual: 0.8     MSE: 0.0000
Optimum Reached at Step 30
</pre>
<p>So it now hits the optimal solution at the 30th step instead of the 1,100th step (although it really seems to reach it at step 20, I think the difference is a rounding problem).</p>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Gradient Descent Mean Squared Error"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Training Repetition"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)),</span> <span class="n">errors</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="gradient_descent_error.png" src="posts/grokking/04_gradient_descent/compare-and-learn/gradient_descent_error.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Gradient Descent Mean Squared Error vs Weights"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Weight"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="s2">"o"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="gradient_descent_weights_vs_error.png" src="posts/grokking/04_gradient_descent/compare-and-learn/gradient_descent_weights_vs_error.png"></p>
</div>
<p>The plots show what we already saw in the output, that Gradient Descent converges on a solution much faster than Hot and Cold Learning does.</p>
</div>
<div class="outline-3" id="outline-container-org9e5dba0">
<h3 id="org9e5dba0">Why multiply the error by the input?</h3>
<div class="outline-text-3" id="text-org9e5dba0">
<p>This has three main effects called <i>stopping, negative reversal</i>, and <i>scaling</i>.</p>
</div>
<div class="outline-4" id="outline-container-org3ccb6a0">
<h4 id="org3ccb6a0">What is Stopping?</h4>
<div class="outline-text-4" id="text-org3ccb6a0">
<p>Stopping refers to the case where the input is 0. If that's the case then we don't want to adjust the weight so multiplying the error by the input nullifies it.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgf8b8bf6">
<h4 id="orgf8b8bf6">What is Negative Reversal?</h4>
<div class="outline-text-4" id="text-orgf8b8bf6">
<p>The sign of the input changes which direction we want the weight to change, so multiplying it by the input keeps the change moving in the right direction even when the sign of the input changes.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org67f500b">
<h4 id="org67f500b">What is Scaling?</h4>
<div class="outline-text-4" id="text-org67f500b">
<p>The larger the input, the greater the amount of change it will add. This can be a bad thing, since the inputs can now have an outsized (negative) effect.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgdd8a650">
<h2 id="orgdd8a650">A Discursion On Derivatives</h2>
<div class="outline-text-2" id="text-orgdd8a650">
<p>What we're doing when we train our model to minimize our error. In the Mean Squared Error equation:</p>
<p>\[ MSE = \frac{1}{n} \sum_{i=1}^n ((input \times weight) - actual)^2 \]</p>
<p>The only thing we can change is the <code>weight</code>, the <code>input</code> and <code>actual output</code> is set by the data. So what we're interested in is how the error changes as we change the weight. The relationship between how the output changes in relationship to how the input changes is the <a href="https://en.wikipedia.org/wiki/Derivative">derivative</a>. One way to think of the derivative is the slope at a point on a line. If you have a straight line the slope will be the same everywhere on it, but if it is curved then different points will have different slopes.</p>
<p>In our case our input is the <code>weight</code> and the output is the <code>error</code>. If you think about slope as \[\frac{rise}{run}\] you'll notice that the bigger the rise, the bigger the slope (since we're taking it at a point the <i>run</i> is infinitesimal), and it's positive going up and negative going down, so if you think of the plot of the MSE earlier, the further you go away from the center (where the error is zero), the steeper the slope, and moving away from the center is always moving up, so the slope is always positive, and moving toward the center where the error is zero is always moving down, so the slope is negative.</p>
<p>What we want, then, is to move our weights in the opposite direction of the slope. There's more math involved to explain this than I can handle right now, but when we calculate our weight adjustment, we are calculating the derivative, and since we want to move in the opposite direction of the derivative, we negate it. And the further away we are from the true value (where our error is zero), the greater the difference is, as we would expect from the slope of our line.</p>
<p>\[ \Delta = prediction - actual\\ \Delta_{weighted} = \Delta \times input\\ weight' = weight - \Delta_{weighted} \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgf9c447c">
<h2 id="orgf9c447c">When does this work?</h2>
<div class="outline-text-2" id="text-orgf9c447c">
<p>Well, it's easier to look at when it doesn't work than when it does.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">OneNode</span><span class="p">:</span>
    <span class="sd">"""Implements a single-node network</span>

<span class="sd">    Args:</span>
<span class="sd">     weight: the starting weight for the edge from the input to the output</span>
<span class="sd">     input_value: the input to the node</span>
<span class="sd">     actual: the actual output we are trying to predict</span>

<span class="sd">     training_steps: how many times to train the model</span>
<span class="sd">     tolerance: how close to zero we need our error to be</span>
<span class="sd">     print_every: how often to print training status</span>
<span class="sd">     stop_after: how many times to keep going after the optimal was found</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">input_value</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">actual</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                 <span class="n">training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                 <span class="n">tolerance</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="o">**</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">print_every</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stop_after</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">original_weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_value</span> <span class="o">=</span> <span class="n">input_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actual</span> <span class="o">=</span> <span class="n">actual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_steps</span> <span class="o">=</span> <span class="n">training_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tolerance</span> <span class="o">=</span> <span class="n">tolerance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print_every</span> <span class="o">=</span> <span class="n">print_every</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop_after</span> <span class="o">=</span> <span class="n">stop_after</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_errors</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">errors</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
        <span class="sd">"""list of MSE values"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_errors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_errors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_errors</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
        <span class="sd">"""List of weights built during training"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
        <span class="sd">"""List of predictions made"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">prediction</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">"""The current model's prediction"""</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_value</span>

    <span class="k">def</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">"""The mean squared error for the prediction"""</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prediction</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prediction</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">actual</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">except</span> <span class="ne">OverflowError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"prediction: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prediction</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"actual: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actual</span><span class="p">))</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">print_error</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                    <span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">separator</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"</span><span class="se">\t</span><span class="s2">"</span><span class="p">,</span>
                    <span class="n">force_print</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">store_error</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">"""Prints the (mean) squared error</span>

<span class="sd">       Args:</span>
<span class="sd">        step: what step this is</span>
<span class="sd">        separator: How to separate the output</span>
<span class="sd">        force_print: ignore the step count and print anyway</span>
<span class="sd">        store_error: whether to add to the errors</span>
<span class="sd">       Returns:</span>
<span class="sd">        mse: the error</span>
<span class="sd">       """</span>
        <span class="n">error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">store_error</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">force_print</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_every</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"Input: </span><span class="si">{}</span><span class="s2"> Actual Output: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_value</span><span class="p">,</span>
                                                       <span class="bp">self</span><span class="o">.</span><span class="n">actual</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">separator</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s2">"Step: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">),</span>
                                  <span class="s2">"Weight: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span>
                                  <span class="s2">"Predicted: </span><span class="si">{:.2f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prediction</span><span class="p">),</span>
                                  <span class="s2">"MSE: </span><span class="si">{:.4f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">error</span><span class="p">)]))</span>
        <span class="k">return</span> <span class="n">error</span>

    <span class="k">def</span> <span class="nf">adjust_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""Takes the gradient descent step"""</span>
        <span class="n">scaled_derivative</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prediction</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">actual</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">-=</span> <span class="n">scaled_derivative</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Trains our model on the values</span>

<span class="sd">       """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_value</span>
        <span class="n">optimal_step</span> <span class="o">=</span> <span class="n">optimal_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_error</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">force_print</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">store_error</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">error</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">print_error</span><span class="p">(</span><span class="n">step</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">error</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">adjust_weight</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">optimal_step</span> <span class="ow">and</span> <span class="n">error</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">tolerance</span><span class="p">:</span>
                <span class="n">optimal_step</span> <span class="o">=</span> <span class="n">step</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">error</span> <span class="o">&lt;</span> <span class="n">tolerance</span><span class="p">:</span>
                <span class="n">optimal_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">optimal_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_after</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Optimum Reached at Step </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimal_step</span><span class="p">))</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">plot_errors_over_time</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="s2">"linear"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""Plots the error as it is trained</span>

<span class="sd">       Args:</span>
<span class="sd">        scale: y-axis scale</span>
<span class="sd">       """</span>
        <span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
        <span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Gradient Descent Mean Squared Error"</span><span class="p">)</span>
        <span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Training Repetition"</span><span class="p">)</span>
        <span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
        <span class="n">axe</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">plot_errors_vs_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"o"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""Plots errors given weights"""</span>
        <span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
        <span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Gradient Descent Mean Squared Error vs Weights"</span><span class="p">)</span>
        <span class="n">axe</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Weight"</span><span class="p">)</span>
        <span class="n">axe</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"MSE"</span><span class="p">)</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">errors</span><span class="p">,</span> <span class="n">style</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">plot_weight_distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""Plots the distribution of the weights"""</span>
        <span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">FIGURE_SIZE</span><span class="p">)</span>
        <span class="n">axe</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Distribution Of Weights"</span><span class="p">)</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="n">seaborn</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
        <span class="k">return</span>


    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Resets the properties"""</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_errors</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">original_weight</span>
        <span class="k">return</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-org5212db5">
<h3 id="org5212db5">Really Big Inputs</h3>
<div class="outline-text-3" id="text-org5212db5">
<div class="highlight">
<pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">OneNode</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">input_value</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
<pre class="example">
Input: 5 Actual Output: 0.1
Step: 1 Weight: 0.1     Predicted: 0.50 MSE: 0.1600
Input: 5 Actual Output: 0.1
Step: 30        Weight: -8.496029205125367e+38  Predicted: -4248014602562683567255766167412625375232.00 MSE: 18045628063585794511112671022597693816764790617265863683813015097617735965736960.0000
Input: 5 Actual Output: 0.1
Step: 60        Weight: -2.1654753676302967e+80 Predicted: -1082737683815148355196656106977575165473067380893761893846901009426287310965571584.00       MSE: 1172320891953392254658250331342439625964504567088377053437708818301948653571886896236931555371967400274585017142698424782066192956435576184566862647806273773371392.0000
Input: 5 Actual Output: 0.1
Step: 90        Weight: -5.51938258990998e+121  Predicted: -275969129495499010312802960568845701886516344963538842611688052470072992651229004282112996195046030793242534509005731004416.00      MSE: 76158960434503501277477316231095001035615545032300898123025508833954593912013767638712660721529293958181614329406854402338675069139074333788043867020532267345247209268079568592806683870329999393916410491701860464641755145654976460424275531137024.0000
(34, 'Numerical result out of range')
prediction: 1.5336245888098994e+154
actual: 0.1
</pre>
<p>So, when the input is too big, the prediction explodes.</p>
<div class="highlight">
<pre><span></span><span class="n">network</span><span class="o">.</span><span class="n">plot_errors_over_time</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="too_big_errors.png" src="posts/grokking/04_gradient_descent/compare-and-learn/too_big_errors.png"></p>
</div>
<p>It looks at first like there was no error and then all of a sudden it got huge - but if you look at the scale of the y-axis it maxes out at over \(4\times10^{305}\) and then the overflow error causes it to quit. So when the input is too large, the network goes out of control.</p>
<div class="highlight">
<pre><span></span><span class="n">network</span><span class="o">.</span><span class="n">plot_errors_vs_weights</span><span class="p">()</span>
</pre></div>
<p><img alt="too_big_errors_vs_weights.png" src="posts/grokking/04_gradient_descent/compare-and-learn/too_big_errors_vs_weights.png"> So now it looks like there aren't really many weights, but if you look at both the X and Y scales you can see that they're really huge, so most of the points are probably centered around 0 (relative to the overall scale) and then all of a sudden they go crazy on the last two points.</p>
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span>
<span class="n">network</span><span class="o">.</span><span class="n">plot_weight_distribution</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="too_big_distribution_of_weights.png" src="posts/grokking/04_gradient_descent/compare-and-learn/too_big_distribution_of_weights.png"></p>
</div>
<p>So we have a small number of weights that are very large. Or a lot of very large weights with a small number of very-very-large weights.</p>
<div class="highlight">
<pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</pre></div>
<pre class="example">
count     1.130000e+02
mean     2.605805e+151
std      2.888965e+152
min     -1.278020e+152
25%      -6.526920e+74
50%      -1.900000e+00
75%       1.566461e+76
max      3.067249e+153
dtype: float64
</pre>
<p>So the median is -1.9 and the mean is \(0.6 \times 10^{151}\). Looks like there are some outliers.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7994874">
<h2 id="org7994874">Fixing the Big Input Problem</h2>
<div class="outline-text-2" id="text-org7994874">
<p>The problem with big inputs is that they cause the gradient descent to explode. Remember our error function:</p>
<p>\[ error = input \times (predicted - actual)\\ = input \times ((input \times weights) - actual) \]</p>
<p>If the input is big the error will be big, and since our correction to the weights is based on the error:</p>
<p>\[ weights' = weights - error \]</p>
<p>our weights can start to swing wildly back and forth with the error growing larger and larger and swinginig between positive and negative numbers. The larger the input, the larger the error, the larger the derivative will be in the opposite direction.</p>
<p>The fix is to only update using a fraction of the correction. We find some value (\(\alpha\)) and multiply it by the change to reduce the influence any one change has. How do we find \(\alpha\)? Well, that turns out to be done by trial and error. If your error goes up as you train, then you probably have to make it smaller.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">AlphaNode</span><span class="p">(</span><span class="n">OneNode</span><span class="p">):</span>
    <span class="sd">"""Incorporates weight update reduction</span>

<span class="sd">    Args:</span>
<span class="sd">     alpha: amount to weight the update</span>

<span class="sd">     weight: the starting weight for the edge from the input to the output</span>
<span class="sd">     input_value: the input to the node</span>
<span class="sd">     actual: the actual output we are trying to predict</span>

<span class="sd">     training_steps: how many times to train the model</span>
<span class="sd">     tolerance: how close to zero we need our error to be</span>
<span class="sd">     print_every: how often to print training status</span>
<span class="sd">     stop_after: how many times to keep going after the optimal was found</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="vm">__class__</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">adjust_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">"""Takes the gradient descent step with alpha weight"""</span>
        <span class="n">scaled_derivative</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prediction</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">actual</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">scaled_derivative</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span>
</pre></div>
<p>Why does this help? Our problem is that the large inputs cause the gradient descent to overshoot past the value that would give zero error and by reducing the amount any one change can have we reduce the likelihood that this will happen.</p>
<div class="highlight">
<pre><span></span><span class="n">alpha_network</span> <span class="o">=</span> <span class="n">AlphaNode</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">input_value</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                          <span class="n">actual</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">print_every</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">alpha_network</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
<pre class="example">
Input: 5 Actual Output: 0.8
Step: 1 Weight: 0.5     Predicted: 2.50 MSE: 2.8900
Input: 5 Actual Output: 0.8
Step: 30        Weight: -43463.41342612052      Predicted: -217317.07   MSE: 47227055374.1942
Input: 5 Actual Output: 0.8
Step: 60        Weight: -8334186242.344855      Predicted: -41670931211.72      MSE: 1736466508118929965056.0000
Input: 5 Actual Output: 0.8
Step: 90        Weight: -1598089039844441.2     Predicted: -7990445199222206.00 MSE: 63847214481773219178788224499712.0000
Input: 5 Actual Output: 0.8
Step: 120       Weight: -3.064352661386353e+20  Predicted: -1532176330693176459264.00   MSE: 2347564308336405932287023519199639310958592.0000
Input: 5 Actual Output: 0.8
Step: 150       Weight: -5.875928686839428e+25  Predicted: -293796434341971398081118208.00      MSE: 86316344832056315635271476663737243674922770633850880.0000
Input: 5 Actual Output: 0.8
Step: 180       Weight: -1.1267155496783526e+31 Predicted: -56335777483917627928853446918144.00 MSE: 3173719824717480263078840848567916525595895306706307529098395648.0000
Optimum Reached at Step 0
</pre>
<p>Okay, so that still didn't work, maybe if \(\alpha\) was smaller?</p>
<div class="highlight">
<pre><span></span><span class="n">alpha_network</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">alpha_network</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
<pre class="example">
Input: 5 Actual Output: 0.8
Step: 1 Weight: 0.5     Predicted: 2.50 MSE: 2.8900
Input: 5 Actual Output: 0.8
Step: 30        Weight: 0.1600809572142104      Predicted: 0.80 MSE: 0.0000
Input: 5 Actual Output: 0.8
Step: 60        Weight: 0.16000001445750855     Predicted: 0.80 MSE: 0.0000
Optimum Reached at Step 34
</pre>
<p>So now our model is able to reach the correct value again. Can you figure out what the best \(\alpha\) is ahead of time? The magic eight ball says no. At this point in time the best way to find the hyperparameters for machine learning is to try them until you find the ones that perform the best.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/introduction-to-neural-networks/logistic-regression/">Logistic Regression</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/introduction-to-neural-networks/logistic-regression/" rel="bookmark"><time class="published dt-published" datetime="2018-10-26T07:44:43-07:00" itemprop="datePublished" title="2018-10-26 07:44">2018-10-26 07:44</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/introduction-to-neural-networks/logistic-regression/#org1c7e8e8">What is Logistic Regression?</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/logistic-regression/#org5ac05ac">The Error Function</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org1c7e8e8">
<h2 id="org1c7e8e8">What is Logistic Regression?</h2>
<div class="outline-text-2" id="text-org1c7e8e8">
<p>Logistic Regression is a classification algorithm that estimates the probability of a classification given an input. It is one of the foundations of deep learning.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org5ac05ac">
<h2 id="org5ac05ac">The Error Function</h2>
<div class="outline-text-2" id="text-org5ac05ac">
<p>\[ error = -\frac{1}{m} \sum^m_{i=1} (1-y)\ln(1-\hat{y}) + y \ln \hat{y} \]</p>
<p>To build our model we add weights (W) and a bias term (b) to the error function \(E(W,b)\).</p>
<p>\[ E(W,b) = -\frac{1}{m} \sum^m_{i=1} (1-y_i)\ln(1-\sigma(Wx^{(i)}) + b) + y_i \ln(\sigma(Wx^{(i)} + b)) \]</p>
<p>This is the function for the binary case, but you can generalize it to more cases using this function.</p>
<p>\[ E(W, b) = -\frac{1}{m} \sum^m_{i=1} \sum^n_{j=1} y_{ij} \ln(\hat{y_{ij}}) \]</p>
<p>The goal is to minimize this function to get the best model.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/introduction-to-neural-networks/multi-class-cross-entropy/">Multi-Class Cross Entropy</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/introduction-to-neural-networks/multi-class-cross-entropy/" rel="bookmark"><time class="published dt-published" datetime="2018-10-25T21:24:59-07:00" itemprop="datePublished" title="2018-10-25 21:24">2018-10-25 21:24</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/introduction-to-neural-networks/multi-class-cross-entropy/#org4c28af8">Our Probabilities</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/multi-class-cross-entropy/#org3a5c292">So, what does this mean?</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org4c28af8">
<h2 id="org4c28af8">Our Probabilities</h2>
<div class="outline-text-2" id="text-org4c28af8">
<p>Weh have three doors behind which could be one of three animals. These are the probabilities that if you open a door, you will find a particular animal behind it.</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left">
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Animal</th>
<th class="org-left" scope="col">Door 1</th>
<th class="org-left" scope="col">Door 2</th>
<th class="org-left" scope="col">Door 3</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Duck</td>
<td class="org-left">\(P_{11}\)</td>
<td class="org-left">\(P_{12}\)</td>
<td class="org-left">\(P_{13}\)</td>
</tr>
<tr>
<td class="org-left">Beaver</td>
<td class="org-left">\(P_{21}\)</td>
<td class="org-left">\(P_{22}\)</td>
<td class="org-left">\(P_{23}\)</td>
</tr>
<tr>
<td class="org-left">Walrus</td>
<td class="org-left">\(P_{31}\)</td>
<td class="org-left">\(P_{32}\)</td>
<td class="org-left">\(P_{33}\)</td>
</tr>
</tbody>
</table>
<p>\[ \textit{Cross Entropy} = - \sum^n_{i=1} \sum^m_{j=1} y_{ij} \ln (p_{ij}) \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-org3a5c292">
<h2 id="org3a5c292">So, what does this mean?</h2>
<div class="outline-text-2" id="text-org3a5c292">
<p>Cross Entropy is inversely proportional to the the total probability of an outcome - so the higher the cross entropy you calculate, the less likely it is that the outcome you are looking at will happen.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/">Okay, but what about this deep-learning stuff?</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/" rel="bookmark"><time class="published dt-published" datetime="2018-10-24T13:26:12-07:00" itemprop="datePublished" title="2018-10-24 13:26">2018-10-24 13:26</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/#orgb9651f2">Imports</a></li>
<li><a href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/#org4cad1d5">Typing</a></li>
<li><a href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/#orgcf0582f">What is this about?</a></li>
<li><a href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/#org1646983">Okay, so how do you implement that?</a></li>
<li><a href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/#orgefb4e37">Let's try it out</a></li>
<li><a href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/#org034427b">Okay, but can we do that with numpy?</a></li>
<li><a href="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/#org1f8b40a">Okay, so what was this about again?</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgb9651f2">
<h2 id="orgb9651f2">Imports</h2>
<div class="outline-text-2" id="text-orgb9651f2"></div>
<div class="outline-3" id="outline-container-org12f14b6">
<h3 id="org12f14b6">From Python</h3>
<div class="outline-text-3" id="text-org12f14b6">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7670748">
<h3 id="org7670748">From Pypi</h3>
<div class="outline-text-3" id="text-org7670748">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Digraph</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4cad1d5">
<h2 id="org4cad1d5">Typing</h2>
<div class="outline-text-2" id="text-org4cad1d5">
<p>This is to develop some type hinting.</p>
<div class="highlight">
<pre><span></span><span class="n">Vector</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
<span class="n">Matrix</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="n">Vector</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgcf0582f">
<h2 id="orgcf0582f">What is this about?</h2>
<div class="outline-text-2" id="text-orgcf0582f">
<p>I previously looked at a model with multiple inputs and outputs to predict whether a team would win or lose and how the fans would feel in response to the outcome. Now I'm going to stack the network on top of another one to 'deepen' the network.</p>
<div class="highlight">
<pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="n">comment</span><span class="o">=</span><span class="s2">"Hidden Layers"</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span>
<span class="c1"># input layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"A"</span><span class="p">,</span> <span class="s2">"Toes"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"B"</span><span class="p">,</span> <span class="s2">"Wins"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"Fans"</span><span class="p">)</span>

<span class="c1"># Hidden Layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"D"</span><span class="p">,</span> <span class="s2">"H1"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"E"</span><span class="p">,</span> <span class="s2">"H2"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"F"</span><span class="p">,</span> <span class="s2">"H3"</span><span class="p">)</span>

<span class="c1"># Output Layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"G"</span><span class="p">,</span> <span class="s2">"Hurt"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"H"</span><span class="p">,</span> <span class="s2">"Win"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"I"</span><span class="p">,</span> <span class="s2">"Sad"</span><span class="p">)</span>

<span class="c1"># Input to hidden edges</span>
<span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="s2">"AD"</span><span class="p">,</span> <span class="s2">"AE"</span><span class="p">,</span> <span class="s2">"AF"</span><span class="p">,</span>
             <span class="s2">"BD"</span><span class="p">,</span> <span class="s2">"BE"</span><span class="p">,</span> <span class="s2">"BF"</span><span class="p">,</span>
             <span class="s2">"CD"</span><span class="p">,</span> <span class="s2">"CE"</span><span class="p">,</span> <span class="s2">"CF"</span><span class="p">,</span>
<span class="p">])</span>

<span class="c1"># Hidden to output egdes</span>
<span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="s2">"DG"</span><span class="p">,</span> <span class="s2">"DH"</span><span class="p">,</span> <span class="s2">"DI"</span><span class="p">,</span>
             <span class="s2">"EG"</span><span class="p">,</span> <span class="s2">"EH"</span><span class="p">,</span> <span class="s2">"EI"</span><span class="p">,</span>
             <span class="s2">"FG"</span><span class="p">,</span> <span class="s2">"FH"</span><span class="p">,</span> <span class="s2">"FI"</span><span class="p">,</span>
<span class="p">])</span>
<span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">"graphs/hidden_layer.dot"</span><span class="p">)</span>
<span class="n">graph</span>
</pre></div>
<div class="figure">
<p><img alt="hidden_layer.dot.png" src="posts/grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/hidden_layer.dot.png"></p>
</div>
<p>These networks between the input and output layers are called <i>hidden layers</i>.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org1646983">
<h2 id="org1646983">Okay, so how do you implement that?</h2>
<div class="outline-text-2" id="text-org1646983">
<p>It works like our previous model except that you insert an extra vector-matrix-multiplication call between the inputs and outputs. For this example I'm going to do it as a class so that I can check the hidden layer's values more easily, but otherwise you would do it using matrices and vectors.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">HiddenLayer</span><span class="p">:</span>
    <span class="sd">"""Implements a neural network with one hidden layer</span>

<span class="sd">    Args:</span>
<span class="sd">     inputs: vector of input values</span>
<span class="sd">     input_to_hidden_weights: vector of weights for the first layer</span>
<span class="sd">     hidden_to_output_weights: vector of weights of the second layer</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Vector</span><span class="p">,</span> <span class="n">input_to_hidden_weights</span><span class="p">:</span> <span class="n">Vector</span><span class="p">,</span>
                 <span class="n">hidden_to_output_weights</span><span class="p">:</span> <span class="n">Vector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_to_hidden_weights</span> <span class="o">=</span> <span class="n">input_to_hidden_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_to_output_weights</span> <span class="o">=</span> <span class="n">hidden_to_output_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">hidden_output</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Vector</span><span class="p">:</span>
        <span class="sd">"""the output of the hidden layer"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_output</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_matrix_multiplication</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_to_hidden_weights</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_output</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">predictions</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Vector</span><span class="p">:</span>
        <span class="sd">"""Predictions for the inputs"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_matrix_multiplication</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_output</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hidden_to_output_weights</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predictions</span>

    <span class="k">def</span> <span class="nf">vector_matrix_multiplication</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vector</span><span class="p">:</span> <span class="n">Vector</span><span class="p">,</span>
                                     <span class="n">matrix</span><span class="p">:</span> <span class="n">Matrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Vector</span><span class="p">:</span>
        <span class="sd">"""calculates the dot-product for each row of the matrix</span>

<span class="sd">       Args:</span>
<span class="sd">        vector: input with one cell for each row in the matrix</span>
<span class="sd">        matrix: input with rows of the same length as the vector</span>

<span class="sd">       Returns:</span>
<span class="sd">        vector: dot-products for the vector and matrix rows</span>
<span class="sd">       """</span>
        <span class="n">vector_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">vector_length</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
        <span class="n">rows</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">matrix</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">dot_product</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="n">matrix</span><span class="p">[</span><span class="n">row</span><span class="p">])</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">dot_product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vector_1</span><span class="p">:</span><span class="n">Vector</span><span class="p">,</span> <span class="n">vector_2</span><span class="p">:</span><span class="n">Vector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">"""Calculate the dot-product of the two vectors</span>

<span class="sd">       Returns:</span>
<span class="sd">        dot-product: the dot product of the two vectors</span>
<span class="sd">       """</span>
        <span class="n">vector_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vector_1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">vector_length</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">vector_2</span><span class="p">)</span>
        <span class="n">entries</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">vector_length</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">((</span><span class="n">vector_1</span><span class="p">[</span><span class="n">entry</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector_2</span><span class="p">[</span><span class="n">entry</span><span class="p">]</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">entries</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgefb4e37">
<h2 id="orgefb4e37">Let's try it out</h2>
<div class="outline-text-2" id="text-orgefb4e37"></div>
<div class="outline-3" id="outline-container-orgc6b5998">
<h3 id="orgc6b5998">The Input Layer To Hidden Layer Weights</h3>
<div class="outline-text-3" id="text-orgc6b5998">
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Toes</th>
<th class="org-right" scope="col">Wins</th>
<th class="org-right" scope="col">Fans</th>
<th class="org-left" scope="col">&nbsp;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">-0.1</td>
<td class="org-left">h<sub>0</sub></td>
</tr>
<tr>
<td class="org-right">-0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.9</td>
<td class="org-left">h<sub>1</sub></td>
</tr>
<tr>
<td class="org-right">0.1</td>
<td class="org-right">0.4</td>
<td class="org-right">0.1</td>
<td class="org-left">h<sub>2</sub></td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">input_to_hidden_weights</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">],</span>
                           <span class="p">[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                           <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org405448e">
<h3 id="org405448e">The Weights From the Hidden Layer to the Outputs</h3>
<div class="outline-text-3" id="text-org405448e">
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">h0</th>
<th class="org-right" scope="col">h1</th>
<th class="org-right" scope="col">h2</th>
<th class="org-left" scope="col">&nbsp;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0.3</td>
<td class="org-right">1.1</td>
<td class="org-right">-0.3</td>
<td class="org-left">hurt</td>
</tr>
<tr>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.0</td>
<td class="org-left">won</td>
</tr>
<tr>
<td class="org-right">0.0</td>
<td class="org-right">1.3</td>
<td class="org-right">0.1</td>
<td class="org-left">sad</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer_to_output_weights</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">],</span>
                                  <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
                                  <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]]</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org4b524c2">
<h3 id="org4b524c2">Testing it Out</h3>
<div class="outline-text-3" id="text-org4b524c2">
<div class="highlight">
<pre><span></span><span class="n">toes</span> <span class="o">=</span> <span class="p">[</span><span class="mf">8.5</span><span class="p">,</span> <span class="mf">9.5</span><span class="p">,</span> <span class="mf">9.9</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]</span>
<span class="n">wins</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.65</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="n">fans</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">expected_hiddens</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.295</span><span class="p">,</span> <span class="mf">1.23</span><span class="p">]</span>
<span class="n">expected_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.214</span><span class="p">,</span> <span class="mf">0.145</span><span class="p">,</span> <span class="mf">0.507</span><span class="p">]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">first_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">toes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">wins</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fans</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">network</span> <span class="o">=</span> <span class="n">HiddenLayer</span><span class="p">(</span><span class="n">first_input</span><span class="p">,</span>
                       <span class="n">input_to_hidden_weights</span><span class="p">,</span>
                       <span class="n">hidden_layer_to_output_weights</span><span class="p">)</span>
<span class="n">hidden_outputs</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">hidden_output</span>
<span class="n">expected_actual</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">expected_hiddens</span><span class="p">,</span> <span class="n">hidden_outputs</span><span class="p">)</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">**</span><span class="mi">5</span>
<span class="n">labels</span> <span class="o">=</span> <span class="s2">"h0 h1 h2"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"|Node | Expected | Actual"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"|-+-+-|"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">expected_actual</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"|</span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2">| </span><span class="si">{:.2f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">expected</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Node</th>
<th class="org-right" scope="col">Expected</th>
<th class="org-right" scope="col">Actual</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">h0</td>
<td class="org-right">0.86</td>
<td class="org-right">0.86</td>
</tr>
<tr>
<td class="org-left">h1</td>
<td class="org-right">0.295</td>
<td class="org-right">0.29</td>
</tr>
<tr>
<td class="org-left">h2</td>
<td class="org-right">1.23</td>
<td class="org-right">1.23</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">predictions</span>
<span class="n">expected_actual</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">expected_outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">**</span><span class="mi">3</span>
<span class="n">labels</span> <span class="o">=</span> <span class="s2">"Hurt Won Sad"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"|Node | Expected | Actual"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"|-+-+-|"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">expected_actual</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"|</span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2">| </span><span class="si">{:.3f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">expected</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Node</th>
<th class="org-right" scope="col">Expected</th>
<th class="org-right" scope="col">Actual</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Hurt</td>
<td class="org-right">0.214</td>
<td class="org-right">0.214</td>
</tr>
<tr>
<td class="org-left">Won</td>
<td class="org-right">0.145</td>
<td class="org-right">0.145</td>
</tr>
<tr>
<td class="org-left">Sad</td>
<td class="org-right">0.507</td>
<td class="org-right">0.506</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org034427b">
<h2 id="org034427b">Okay, but can we do that with numpy?</h2>
<div class="outline-text-2" id="text-org034427b">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">one_hidden_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Vector</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Matrix</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Converts arguments to numpy and calculates predictions</span>

<span class="sd">    Args:</span>
<span class="sd">     inputs: array of inputs</span>
<span class="sd">     weights: matrix with two rows of weights</span>

<span class="sd">    Returns:</span>
<span class="sd">     predictions: predicted values for each output node</span>
<span class="sd">    """</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hidden</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
<p>One thing to watch out for here is that the dot product won't raise an error if you don't transpose the weights, but you will get the wrong values.</p>
<div class="highlight">
<pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_to_hidden_weights</span><span class="p">,</span>
           <span class="n">hidden_layer_to_output_weights</span><span class="p">]</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">one_hidden_layer</span><span class="p">(</span><span class="n">first_input</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="n">expected_actual</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">expected_outputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">tolerance</span> <span class="o">=</span> <span class="mf">0.1</span><span class="o">**</span><span class="mi">3</span>
<span class="n">labels</span> <span class="o">=</span> <span class="s2">"Hurt Won Sad"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"|Node | Expected | Actual"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"|-+-+-|"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">expected_actual</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"|</span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2">| </span><span class="si">{:.3f}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">expected</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tolerance</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Node</th>
<th class="org-right" scope="col">Expected</th>
<th class="org-right" scope="col">Actual</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Hurt</td>
<td class="org-right">0.214</td>
<td class="org-right">0.214</td>
</tr>
<tr>
<td class="org-left">Won</td>
<td class="org-right">0.145</td>
<td class="org-right">0.145</td>
</tr>
<tr>
<td class="org-left">Sad</td>
<td class="org-right">0.507</td>
<td class="org-right">0.506</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-2" id="outline-container-org1f8b40a">
<h2 id="org1f8b40a">Okay, so what was this about again?</h2>
<div class="outline-text-2" id="text-org1f8b40a">
<p>This showed that you can stack networks up and have the outputs of one layer feed into the next until you reach the output layer. This is called <i>Forward Propagation</i>. Although I mentioned deep-learning in the title this really isn't an example yet, it's more like a multilayer perceptron, but it's deeper than two-layers, anyway.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/introduction-to-neural-networks/maximum-likelihood/">Maximum Likelihood</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/introduction-to-neural-networks/maximum-likelihood/" rel="bookmark"><time class="published dt-published" datetime="2018-10-23T21:29:52-07:00" itemprop="datePublished" title="2018-10-23 21:29">2018-10-23 21:29</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/introduction-to-neural-networks/maximum-likelihood/#orgfa87ac3">What is this about?</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/maximum-likelihood/#orgfc0b8e0">Yeah, okay, but how do you do this?</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/maximum-likelihood/#org9b4fec4">The Problem With Products</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/maximum-likelihood/#orge8a063e">Cross Entropy</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/maximum-likelihood/#org217dbc7">Okay, but how do we implement this?</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgfa87ac3">
<h2 id="orgfa87ac3">What is this about?</h2>
<div class="outline-text-2" id="text-orgfa87ac3">
<p>We want a way to train our neural network based on the data we have - how do we do this? One way is to use <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood</a> where we give weights based on the past occurrences for each score.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgfc0b8e0">
<h2 id="orgfc0b8e0">Yeah, okay, but how do you do this?</h2>
<div class="outline-text-2" id="text-orgfc0b8e0">
<p>First, remember that our probability for any point being 0 or 1 is based on the sigmoid.</p>
<p>\[ \hat{y} = \sigma(Wx+b) \]</p>
<p>Where \(\hat{y}\) is the probability that a point is non-negative.</p>
<p>So we can take the product of the sigmoid of all the points in our data set and find the probability that any point is a 1. If we were to find a model that maximized this probability, we would have a model that separated our categories - this is the Maximum Likelihood Model.</p>
<p>To be more specific, we calculate \(\hat{y}\) for all of our training set points and multiply them to get the total probability (multiplication is an AND operation - \(p(a) \land p(b) \land p(c) = p(a) \times p(b) \time p(c)\)) then we adjust our moder to maximize this probability.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org9b4fec4">
<h2 id="org9b4fec4">The Problem With Products</h2>
<div class="outline-text-2" id="text-org9b4fec4">
<p>Each of our probablilities is less than 1, so the more of them you have, the smaller their product will become. What we want to do is use addition - which is where the logarithm comes in.</p>
<p>\[ p(a) * p(b) * p(c) = \log(a) + \log p(b) + \log p(4) \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-orge8a063e">
<h2 id="orge8a063e">Cross Entropy</h2>
<div class="outline-text-2" id="text-orge8a063e">
<p>Our logarithms give us the values that we want to maximize, but another way to look at is as "we want to minimize the error". We can do this by using the negatives of the logarithms to find the error and trying to minimize their sums.</p>
<p>\[ \textit{cross entropy} = -\log p(a) - \log p(b) - \log p(4) \]</p>
<p>More generally:</p>
<p>\[ \textit{Cross Entropy} = -\sum_{i=1}^m y_i \ln(p_i) + (1 - y_i)\ln(1-p_i) \]</p>
<p>Where <i>y</i> is vector of 1's and 0's. When <i>y</i> is 0, the left term is 0 and when <i>y</i> is 1 the right term is 0 so it works as sort of a conditional to choose which term to use.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org217dbc7">
<h2 id="org217dbc7">Okay, but how do we implement this?</h2>
<div class="outline-text-2" id="text-org217dbc7">
<p>Write a function that takes as input two lists Y, P, and returns the float corresponding to their cross-entropy.</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">P</span><span class="p">):</span>
    <span class="sd">"""calculates the cross entropy of two lists</span>

<span class="sd">    Args:</span>
<span class="sd">     Y: lists of 1s and 0s</span>
<span class="sd">     P: lists of probabilities that Y is 1</span>
<span class="sd">    Returns:</span>
<span class="sd">     cross-entropy: the cross entropy of the two lists</span>
<span class="sd">    """</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">not_Y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span>    
    <span class="n">P</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
    <span class="n">not_P</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">P</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">Y</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">)</span> <span class="o">+</span> <span class="n">not_Y</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">not_P</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">Y</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> 
<span class="n">P</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="mf">4.8283137373</span>
<span class="n">entropy</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">entropy</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">entropy</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="o">**</span><span class="mi">5</span>
</pre></div>
<pre class="example">
4.828313737302301
</pre></div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/introduction-to-neural-networks/one-hot-encoding/">One-Hot Encoding</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/introduction-to-neural-networks/one-hot-encoding/" rel="bookmark"><time class="published dt-published" datetime="2018-10-23T21:16:36-07:00" itemprop="datePublished" title="2018-10-23 21:16">2018-10-23 21:16</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/introduction-to-neural-networks/one-hot-encoding/#orgb2fece0">The Problem</a></li>
<li><a href="posts/nano/introduction-to-neural-networks/one-hot-encoding/#orgebf026c">One Solution</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgb2fece0">
<h2 id="orgb2fece0">The Problem</h2>
<div class="outline-text-2" id="text-orgb2fece0">
<p>We are dealing with categories - Duck, Beaver, and Walrus - but our classifier works with numbers, what do we do?</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgebf026c">
<h2 id="orgebf026c">One Solution</h2>
<div class="outline-text-2" id="text-orgebf026c">
<p><a href="https://en.wikipedia.org/wiki/One-hot">one-hot encoding</a>, in this context, means taking each of our classifications and creating a column for it and putting a 1 in the row if it matches the column and a 0 otherwise.</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Sighting</th>
<th class="org-right" scope="col">Duck</th>
<th class="org-right" scope="col">Beaver</th>
<th class="org-right" scope="col">Walrus</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">2</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-right">3</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-right">4</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="index-3.html" rel="prev">Newer posts</a></li>
<li class="next"><a href="index-1.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
