<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>In Too Deep (old posts, page 3) | In Too Deep</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/index-3.html" rel="canonical">
<link href="index-4.html" rel="prev" type="text/html">
<link href="index-2.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/pytorch/tensors-in-pytorch/">Tensors In PyTorch</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/pytorch/tensors-in-pytorch/" rel="bookmark"><time class="published dt-published" datetime="2018-11-11T16:02:32-08:00" itemprop="datePublished" title="2018-11-11 16:02">2018-11-11 16:02</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#org1da7c06">Introduction to Deep Learning with PyTorch</a></li>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#org2341cb7">Neural Networks</a></li>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#org0669c3a">Tensors</a></li>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#org1c6fdbc">Imports</a></li>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#org3887f83">The Activation Function</a></li>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#orgcc77ca9">Generate some data</a></li>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#org945132a">Stack them up!</a></li>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#orgaf4553f">Generate some data</a></li>
<li><a href="/posts/nano/pytorch/tensors-in-pytorch/#org4084e46">Numpy to Torch and back</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org1da7c06">
<h2 id="org1da7c06">Introduction to Deep Learning with PyTorch</h2>
<div class="outline-text-2" id="text-org1da7c06">
<p>In this notebook, you'll get introduced to <a href="http://pytorch.org/">PyTorch</a>, a framework for building and training neural networks. PyTorch, in a lot of ways, behaves like the arrays you love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org2341cb7">
<h2 id="org2341cb7">Neural Networks</h2>
<div class="outline-text-2" id="text-org2341cb7">
<p>Deep Learning is based on artificial neural networks which have been around in some form since the late 1950s. The networks are built from individual parts approximating neurons, typically called units or simply "neurons." Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.</p>
<p>Mathematically this looks like:</p>
<p>\[ y = f(w_1 x_1 + w_2 x_2 + b) \\ y = f\left(\sum_i w_i x_i +b \right) \]</p>
<p>With vectors this is the dot/inner product of two vectors:</p>
<p>$$ h = x_1 , x_2 ⋅ x_n<br>
⋅</p>
\begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_n \end{bmatrix}
<p>$$</p>
</div>
</div>
<div class="outline-2" id="outline-container-org0669c3a">
<h2 id="org0669c3a">Tensors</h2>
<div class="outline-text-2" id="text-org0669c3a">
<p>It turns out neural network computations are just a bunch of linear algebra operations on <b>tensors</b>, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.</p>
<p>Now that we have the basics covered, it's time to explore how we can use PyTorch to build a simple neural network.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org1c6fdbc">
<h2 id="org1c6fdbc">Imports</h2>
<div class="outline-text-2" id="text-org1c6fdbc"></div>
<div class="outline-3" id="outline-container-org8f6be9c">
<h3 id="org8f6be9c">From PyPi</h3>
<div class="outline-text-3" id="text-org8f6be9c">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org3887f83">
<h2 id="org3887f83">The Activation Function</h2>
<div class="outline-text-2" id="text-org3887f83">
<p>Using <a href="https://pytorch.org/docs/stable/torch.html?highlight=exp#torch.exp">pytorch's exp</a> function looks a lot like it did with numpy.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">activation</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">""" Sigmoid activation function </span>

<span class="sd">       Arguments</span>
<span class="sd">       ---------</span>
<span class="sd">       x: torch.Tensor</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgcc77ca9">
<h2 id="orgcc77ca9">Generate some data</h2>
<div class="outline-text-2" id="text-orgcc77ca9">
<p><a href="https://pytorch.org/docs/stable/torch.html?highlight=exp#torch.exp">Set the random seed</a> so things are predictable.</p>
<div class="highlight">
<pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgf171606">
<h3 id="orgf171606">Features</h3>
<div class="outline-text-3" id="text-orgf171606">
<p>Our features will be a tensor of 3 random normal variables created with <a href="https://pytorch.org/docs/stable/torch.html?highlight=randn#torch.randn">torch.randn</a>.</p>
<div class="highlight">
<pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc925962">
<h3 id="orgc925962">True weights for our data, random normal variables again</h3>
<div class="outline-text-3" id="text-orgc925962">
<p><a href="https://pytorch.org/docs/stable/torch.html?highlight=randn_like#torch.randn_like">randn_like</a> creates a tensor of random numbers that is the same size as the tensor it is given.</p>
<div class="highlight">
<pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7ac1e36">
<h3 id="org7ac1e36">And a true bias term.</h3>
<div class="outline-text-3" id="text-org7ac1e36">
<div class="highlight">
<pre><span></span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
<p>Above I generated data we can use to get the output of our simple network. This is all just random for now, going forward we'll start using normal data. Going through each relevant line:</p>
<p>`features = torch.randn((1, 5))` creates a tensor with shape `(1, 5)`, one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one.</p>
<p>`weights = torch.randn_like(features)` creates another tensor with the same shape as `features`, again containing values from a normal distribution.</p>
<p>Finally, `bias = torch.randn((1, 1))` creates a single value from a normal distribution.</p>
<p>PyTorch tensors can be added, multiplied, subtracted, etc, just like Numpy arrays. In general, you'll use PyTorch tensors pretty much the same way you'd use Numpy arrays. They come with some nice benefits though such as GPU acceleration which we'll get to later. For now, use the generated data to calculate the output of this simple single layer network.</p>
<p><b>Exercise</b>: Calculate the output of the network with input features `features`, weights `weights`, and bias `bias`. Similar to Numpy, PyTorch has a <a href="https://pytorch.org/docs/stable/torch.html#torch.sum"><code>torch.sum()</code></a> function, as well as a `.sum()` method on tensors, for taking sums. Use the function `activation` defined above as the activation function.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org3f62a64">
<h3 id="org3f62a64">Calculate the output of this network using the weights and bias tensors</h3>
<div class="outline-text-3" id="text-org3f62a64">
<p>You can do the multiplication and sum in the same operation using a matrix multiplication. In general, you'll want to use matrix multiplications since they are more efficient and accelerated using modern libraries and high-performance computing on GPUs.</p>
<p>Here, we want to do a matrix multiplication of the features and the weights. For this we can use [`torch.mm()`] or <a href="https://pytorch.org/docs/stable/torch.html#torch.mm"><code>torch.matmul()</code></a> which is somewhat more complicated and supports broadcasting. If we try to do it with `features` and `weights` as they are, we'll get an error:</p>
<pre class="example">
torch.mm(features, weights);
 
 ---------------------------------------------------------------------------
 RuntimeError                              Traceback (most recent call last)
 &lt;python-input-13-15d592eb5279&gt; in &lt;module&gt;()
 ----&gt; 1 torch.mm(features, weights)
 
 RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033
</pre>
<p>As you're building neural networks in any framework, you'll see this often. Really often. What's happening here is our tensors aren't the correct shapes to perform a matrix multiplication. Remember that for matrix multiplications, the number of columns in the first tensor must equal to the number of rows in the second column. Both `features` and `weights` have the same shape, `(1, 5)`. This means we need to change the shape of `weights` to get the matrix multiplication to work.</p>
<p><b>Note:</b> To see the shape of a tensor called `tensor`, use `tensor.shape`. If you're building neural networks, you'll be using this method often.</p>
<p>There are a few options here: <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape"><code>weights.reshape()</code></a>, <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_"><code>weights.resize_()</code></a>, and <a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view"><code>weights.view()</code></a>.</p>
<p>-`weights.reshape(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.</p>
<ul class="org-ul">
<li>`weights.resize_(a, b)` returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed <b>in-place</b>. Here is a great forum thread to <a href="https://discuss.pytorch.org/t/what-is-in-place-operation/16244">read more about in-place operations</a> in PyTorch.</li>
<li>`weights.view(a, b)` will return a new tensor with the same data as `weights` with size `(a, b)`.</li>
</ul>
<p>I usually use `.view()`, but any of the three methods will work for this. So, now we can reshape `weights` to have five rows and one column with something like `weights.view(5, 1)`.</p>
<p><b>Exercise</b>: Calculate the output of our little network using matrix multiplication.</p>
<div class="highlight">
<pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">product</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="n">total</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">activation</span><span class="p">(</span><span class="n">total</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
</pre></div>
<pre class="example">
tensor(0.1595)

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org945132a">
<h2 id="org945132a">Stack them up!</h2>
<div class="outline-text-2" id="text-org945132a">
<p>That's how you can calculate the output for a single neuron. The real power of this algorithm happens when you start stacking these individual units into layers and stacks of layers, into a network of neurons. The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix.</p>
<p>The first layer shown on the bottom here are the inputs, understandably called the <b>input layer</b>. The middle layer is called the <b>hidden layer</b>, and the final layer (on the right) is the <b>output layer</b>. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer (\(h_1\) and \(h_2\) here) can be calculated</p>
<p>\[ \vec{h} = [h_1 \, h_2] =</p>
\begin{bmatrix} x_1 \, x_2 \cdots \, x_n \end{bmatrix}
<p>⋅</p>
\begin{bmatrix} w_{11} & w_{12} \\ w_{21} &amp;w_{22} \\ \vdots &amp;\vdots \\ w_{n1} &amp;w_{n2} \end{bmatrix}
<p>\]</p>
<p>The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply</p>
<p>\[ y = f_2 \! \left(\, f_1 \! \left(\vec{x} \, \mathbf{W_1}\right) \mathbf{W_2} \right) \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgaf4553f">
<h2 id="orgaf4553f">Generate some data</h2>
<div class="outline-text-2" id="text-orgaf4553f">
<p>Set the random seed so things are predictable</p>
<div class="highlight">
<pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
<p>The features are 3 random normal variables that will be your input.</p>
<div class="highlight">
<pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
<p>Define the size of each layer in our network</p>
<div class="highlight">
<pre><span></span><span class="n">n_input</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>     <span class="c1"># Number of input units, must match number of input features</span>
<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">2</span>                    <span class="c1"># Number of hidden units </span>
<span class="n">n_output</span> <span class="o">=</span> <span class="mi">1</span>                    <span class="c1"># Number of output units</span>
</pre></div>
<p>Weights for inputs to hidden layer</p>
<div class="highlight">
<pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
</pre></div>
<p>Weights for hidden layer to output layer</p>
<div class="highlight">
<pre><span></span><span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_output</span><span class="p">)</span>
</pre></div>
<p>and bias terms for hidden and output layers</p>
<div class="highlight">
<pre><span></span><span class="n">B1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">))</span>
<span class="n">B2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_output</span><span class="p">))</span>
</pre></div>
<p><b>Exercise:</b> Calculate the output for this multi-layer network using the weights `W1` & `W2`, and the biases, `B1` & `B2`.</p>
<div class="highlight">
<pre><span></span><span class="n">input_layer_out</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">W1</span><span class="p">))</span> <span class="o">+</span> <span class="n">B1</span>
<span class="n">hidden_layer_out</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">input_layer_out</span><span class="p">,</span> <span class="n">W2</span><span class="p">))</span> <span class="o">+</span> <span class="n">B2</span>
<span class="k">print</span><span class="p">(</span><span class="n">hidden_layer_out</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([[0.4813]])

</pre>
<div class="highlight">
<pre><span></span><span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.4813</span><span class="p">]])</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">hidden_layer_out</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">expected</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">0.000305</span><span class="p">)</span>
</pre></div>
<p>If you did this correctly, you should see the output <code>tensor([[ 0.4813]])</code>.</p>
<p>The number of hidden units a parameter of the network, often called a <b>hyperparameter</b> to differentiate it from the weights and biases parameters. As you'll see later when we discuss training a neural network, the more hidden units a network has, and the more layers, the better able it is to learn from data and make accurate predictions.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org4084e46">
<h2 id="org4084e46">Numpy to Torch and back</h2>
<div class="outline-text-2" id="text-org4084e46">
<p>Special bonus section! PyTorch has a great feature for converting between Numpy arrays and Torch tensors. To create a tensor from a Numpy array, use <a href="https://pytorch.org/docs/stable/torch.html?highlight=from_numpy#torch.from_numpy">torch.from_numpy()</a>. To convert a tensor to a Numpy array, use the <a href="https://pytorch.org/docs/stable/tensors.html?highlight=numpy#torch.Tensor.numpy">.numpy()</a> method.</p>
<div class="highlight">
<pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.07665652 0.06831265 0.7607324 ]
 [0.71495335 0.34479699 0.67489027]
 [0.45834284 0.78789824 0.40383355]
 [0.28864364 0.21713754 0.62036028]]

</pre>
<div class="highlight">
<pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
<pre class="example">
tensor([[0.0767, 0.0683, 0.7607],
        [0.7150, 0.3448, 0.6749],
        [0.4583, 0.7879, 0.4038],
        [0.2886, 0.2171, 0.6204]], dtype=torch.float64)

</pre>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
<pre class="example">
[[0.07665652 0.06831265 0.7607324 ]
 [0.71495335 0.34479699 0.67489027]
 [0.45834284 0.78789824 0.40383355]
 [0.28864364 0.21713754 0.62036028]]

</pre>
<p>The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well.</p>
<p><i>Multiply PyTorch Tensor by 2, in place</i></p>
<div class="highlight">
<pre><span></span><span class="n">b</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>Numpy array matches new values from Tensor</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.15331305 0.1366253  1.52146479]
 [1.4299067  0.68959399 1.34978053]
 [0.91668568 1.57579648 0.80766711]
 [0.57728729 0.43427509 1.24072056]]

</pre></div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/sentiment_analysis/the-sentiment-analyzer/">The Sentiment Analyzer</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/sentiment_analysis/the-sentiment-analyzer/" rel="bookmark"><time class="published dt-published" datetime="2018-11-11T15:10:27-08:00" itemprop="datePublished" title="2018-11-11 15:10">2018-11-11 15:10</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/sentiment_analysis/the-sentiment-analyzer/#org57ba88c">Set Up</a></li>
<li><a href="/posts/nano/sentiment_analysis/the-sentiment-analyzer/#orga22080c">Encapsulate our neural network in a class</a></li>
<li><a href="/posts/nano/sentiment_analysis/the-sentiment-analyzer/#org0991885">Test The Network</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org57ba88c">
<h2 id="org57ba88c">Set Up</h2>
<div class="outline-text-2" id="text-org57ba88c"></div>
<div class="outline-3" id="outline-container-orgc46e6f5">
<h3 id="orgc46e6f5">Imports</h3>
<div class="outline-text-3" id="text-orgc46e6f5"></div>
<div class="outline-4" id="outline-container-org5c9e123">
<h4 id="org5c9e123">Python</h4>
<div class="outline-text-4" id="text-org5c9e123">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8477fa7">
<h4 id="org8477fa7">This Project</h4>
<div class="outline-text-4" id="text-org8477fa7">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org3728351">
<h3 id="org3728351">The Data</h3>
<div class="outline-text-3" id="text-org3728351">
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"reviews.pkl"</span><span class="p">)</span>
<span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">reviews</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgefba280">
<h3 id="orgefba280">The Labels</h3>
<div class="outline-text-3" id="text-orgefba280">
<p>A similar deal except casting the labels to upper case.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"labels.pkl"</span><span class="p">)</span>
<span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
<p><b>Note:</b> The data in <code>reviews.txt</code> we're using has already been preprocessed a bit and contains only lower case characters. If we were working from raw data, where we didn't know it was all lower case, we would want to add a step here to convert it. That's so we treat different variations of the same word, like `The`, `the`, and `THE`, all the same way.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga22080c">
<h2 id="orga22080c">Encapsulate our neural network in a class</h2>
<div class="outline-text-2" id="text-orga22080c">
<p>I'm going to try and break up the class so that I can make notes. You can't really do that in a notebook, though, so I'm going to tangle it out. The following Class is going to end up in a module named <code>sentiment_network</code>.</p>
<div class="highlight">
<pre><span></span><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">constants</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">review</span><span class="o">-</span><span class="n">vocabulary</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">review</span><span class="o">-</span><span class="n">vocabulary</span><span class="o">-</span><span class="n">size</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">label</span><span class="o">-</span><span class="n">vocabulary</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">label</span><span class="o">-</span><span class="n">vocabulary</span><span class="o">-</span><span class="n">size</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">word</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">index</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">label</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">index</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="n">nodes</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">weights</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">hidden</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">weights</span><span class="o">-</span><span class="n">hidden</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">output</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="n">layer</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">update</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="n">layer</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">get</span><span class="o">-</span><span class="n">target</span><span class="o">-</span><span class="k">for</span><span class="o">-</span><span class="n">label</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">sigmoid</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">sigmoid</span><span class="o">-</span><span class="n">output</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="n">derivative</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">train</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">test</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">sentiment</span><span class="o">-</span><span class="n">network</span><span class="o">-</span><span class="n">run</span><span class="o">&gt;&gt;</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orge8bb9d5">
<h3 id="orge8bb9d5">Imports</h3>
<div class="outline-text-3" id="text-orge8bb9d5">
<div class="highlight">
<pre><span></span><span class="c1"># From python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="p">)</span>
<span class="c1"># from pypi</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org06e7530">
<h3 id="org06e7530">Constants</h3>
<div class="outline-text-3" id="text-org06e7530">
<div class="highlight">
<pre><span></span><span class="n">SPLIT_ON_THIS</span> <span class="o">=</span> <span class="s2">" "</span>
<span class="n">Review</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="n">Label</span> <span class="o">=</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="n">Classification</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgca5a9b1">
<h3 id="orgca5a9b1">Sentiment Network Constructor</h3>
<div class="outline-text-3" id="text-orgca5a9b1">
<p>To make this more like a SKlearn implementation I'm not going to add the training and testing data at this point. This will break one of the examples given. Oh well.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">SentimentNetwork</span><span class="p">:</span>
    <span class="sd">"""A network to predict if a review is positive or negative</span>

<span class="sd">    Args:</span>
<span class="sd">     hidden_nodes: Number of nodes to create in the hidden layer</span>
<span class="sd">     learning_rate: Learning rate to use while training        </span>
<span class="sd">     output_nodes: Number of output nodes (should always be 1)</span>
<span class="sd">     tokenizer: what to split on</span>
<span class="sd">     verbose: whether to output update information</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">hidden_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> 
                 <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">output_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">tokenizer</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s2">" "</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Assign a seed to our random number generator to ensure we get</span>
        <span class="c1"># reproducable results during development </span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span> <span class="o">=</span> <span class="n">hidden_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span> <span class="o">=</span> <span class="n">output_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_label_vocabulary</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary_size</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_label_vocabulary_size</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_label_to_index</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_nodes</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_layer</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org44e447c">
<h3 id="org44e447c">The Review Vocabulary</h3>
<div class="outline-text-3" id="text-org44e447c">
<p>This takes the training reviews and tokenizes them so we have a set of unique tokens to work with. This requires that <code>self.reviews</code> and <code>self.tokenizer</code> are set.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">review_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sd">"""list of tokens in the reviews"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">review</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reviews</span><span class="p">:</span>
            <span class="n">vocabulary</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">review</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org640d043">
<h3 id="org640d043">The Review Vocabulary Size</h3>
<div class="outline-text-3" id="text-org640d043">
<p>This is the number of tokens we ended up with after tokenizing the training reviews.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">review_vocabulary_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""The amount of tokens in our reviews"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary_size</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">review_vocabulary</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_review_vocabulary_size</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc81f533">
<h3 id="orgc81f533">The Label Vocabulary</h3>
<div class="outline-text-3" id="text-orgc81f533">
<p>These are the labels - there should only be two in this case. This requires that <code>self.labels</code> has been set.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">label_vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sd">"""List of sentiment labels"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_label_vocabulary</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_label_vocabulary</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_label_vocabulary</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1095e95">
<h3 id="org1095e95">The Label Vocabulary Size</h3>
<div class="outline-text-3" id="text-org1095e95">
<p>The number of labels we ended up with.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">label_vocabulary_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""The amount of tokens in our labels"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_label_vocabulary_size</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_label_vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_vocabulary</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_label_vocabulary_size</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org14b760c">
<h3 id="org14b760c">The Word To Index Map</h3>
<div class="outline-text-3" id="text-org14b760c">
<p>This is a map to find the index in our review vocabulary where a word is. This requires that <code>self.review_vocabulary</code> has been set.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">word_to_index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""maps a word to the index in our review vocabulary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">word</span><span class="p">:</span> <span class="n">index</span>
            <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">review_vocabulary</span><span class="p">)}</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org992ddd9">
<h3 id="org992ddd9">The Label To Index Map</h3>
<div class="outline-text-3" id="text-org992ddd9">
<p>This finds the index where a label is in our vocabulary of labels. This requires that <code>self.label_vocabulary</code> has been set.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">label_to_index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""maps a label to the index in our label vocabulary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_label_to_index</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_label_to_index</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">label</span><span class="p">:</span> <span class="n">index</span>
            <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_vocabulary</span><span class="p">)}</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_label_to_index</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgee74ecd">
<h3 id="orgee74ecd">Input Nodes</h3>
<div class="outline-text-3" id="text-orgee74ecd">
<p>The number of input nodes is the size of our vocabulary built from the reviews. This requires <code>self.review_vocabulary</code> to have been set.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Number of input nodes"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_nodes</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">review_vocabulary</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_nodes</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9e2079a">
<h3 id="org9e2079a">Weight From the Input Layer To the Hidden Layer</h3>
<div class="outline-text-3" id="text-org9e2079a">
<p>This is a matrix with as many rows as the number of input nodes and as many columns as the number of hidden nodes. This relies on <code>self.input_nodes</code> and <code>self.hidden_nodes</code>.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">weights_input_to_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Weights for edges from input to hidden layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span>

<span class="nd">@weights_input_to_hidden.setter</span>
<span class="k">def</span> <span class="nf">weights_input_to_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""Set the weights"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span> <span class="o">=</span> <span class="n">weights</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org92d121e">
<h3 id="org92d121e">Weight From the Hidden Layer To the Output Layer</h3>
<div class="outline-text-3" id="text-org92d121e">
<p>This is a matrix with as many rows as the number of hidden nodes and as many columns as the number of output nodes (which should be 1). This depends of <code>self.hidden_nodes</code> and <code>self.output_nodes</code>.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">weights_hidden_to_output</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Weights for edges from hidden to output layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span>

<span class="nd">@weights_hidden_to_output.setter</span>
<span class="k">def</span> <span class="nf">weights_hidden_to_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""updates the weights"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span> <span class="o">=</span> <span class="n">weights</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0464187">
<h3 id="org0464187">The Input Layer</h3>
<div class="outline-text-3" id="text-org0464187">
<p>This is the layer where we will set the tokens for a particular review that we are going to categorize. This depends on <code>self.input_nodes</code>.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""The Input Layer for the review tokens"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_layer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_layer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_layer</span>

<span class="nd">@input_layer.setter</span>
<span class="k">def</span> <span class="nf">input_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""Set the input layer"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_layer</span> <span class="o">=</span> <span class="n">layer</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2a107a4">
<h3 id="org2a107a4">Update the Input Layer</h3>
<div class="outline-text-3" id="text-org2a107a4">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">update_input_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">review</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""Update the counts in the input layer</span>

<span class="sd">    Args:</span>
<span class="sd">     review: A movie review</span>
<span class="sd">    """</span>
    <span class="c1"># reset any previous inputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span> <span class="o">*=</span> <span class="mi">0</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">review</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">)</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
    <span class="n">counter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org285b97f">
<h3 id="org285b97f">Get the Target for the Label</h3>
<div class="outline-text-3" id="text-org285b97f">
<p>This converts a string label to an integer.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_target_for_label</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Convert a label to `0` or `1`.</span>
<span class="sd">    Args:</span>
<span class="sd">     label(string) - Either "POSITIVE" or "NEGATIVE".</span>
<span class="sd">    Returns:</span>
<span class="sd">     `0` or `1`.</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">label</span><span class="o">==</span><span class="s2">"POSITIVE"</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf9a5fce">
<h3 id="orgf9a5fce">The Sigmoid</h3>
<div class="outline-text-3" id="text-orgf9a5fce">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""calculates the sigmoid for the input</span>

<span class="sd">    Args:</span>
<span class="sd">     x: vector to calculate the sigmoid</span>

<span class="sd">    Returns:</span>
<span class="sd">     sigmoid of x</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org997ddf3">
<h3 id="org997ddf3">Sigmoid Derivative</h3>
<div class="outline-text-3" id="text-org997ddf3">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid_output_to_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the derivative if the sigmoid</span>

<span class="sd">    Args:</span>
<span class="sd">     output: the sigmoid output</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org76a3f2a">
<h3 id="org76a3f2a">Train the Network</h3>
<div class="outline-text-3" id="text-org76a3f2a">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_reviews</span><span class="p">:</span> <span class="n">Review</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">:</span> <span class="n">Label</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Trains the model</span>

<span class="sd">    Args:</span>
<span class="sd">     training_reviews: list of reviews</span>
<span class="sd">     training_labels: listo of labels for the reviews</span>

<span class="sd">    Returns:</span>
<span class="sd">     count of correct</span>
<span class="sd">    """</span>
    <span class="c1"># there are side-effects that require self.reviews and self.labels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reviews</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">training_reviews</span><span class="p">,</span> <span class="n">training_labels</span>

    <span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_reviews</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_labels</span><span class="p">))</span>
    <span class="n">correct_so_far</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>        
        <span class="c1"># Remember when we started for printing time statistics</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

    <span class="c1"># loop through all the given reviews and run a forward and backward pass,</span>
    <span class="c1"># updating weights for every item</span>
    <span class="n">reviews_labels</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">training_reviews</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">)</span>
    <span class="n">n_records</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_reviews</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">review</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reviews_labels</span><span class="p">):</span>
        <span class="c1"># feed-forward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_input_layer</span><span class="p">(</span><span class="n">review</span><span class="p">)</span>
        <span class="n">hidden_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">)</span>
        <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="n">hidden_inputs</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_outputs</span><span class="p">)</span>

        <span class="c1"># Backpropagation</span>
        <span class="c1"># we need to calculate the output_error separately</span>
        <span class="c1"># to update our correct count</span>
        <span class="n">output_error</span> <span class="o">=</span> <span class="n">output</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_target_for_label</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

        <span class="c1"># we applied a sigmoid to the output</span>
        <span class="c1"># so we need to apply the derivative</span>
        <span class="n">hidden_to_output_delta</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_error</span>
            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid_output_to_derivative</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>

        <span class="n">input_to_hidden_error</span> <span class="o">=</span> <span class="n">hidden_to_output_delta</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="c1"># we didn't apply a function to the inputs to the hidden layer</span>
        <span class="c1"># so we don't need a derivative</span>
        <span class="n">input_to_hidden_delta</span> <span class="o">=</span> <span class="n">input_to_hidden_error</span>

        <span class="c1"># our delta is based on the derivative which is heading</span>
        <span class="c1"># in the opposite direction of what we want so we need to negate it</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span> <span class="o">-=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>
            <span class="o">*</span> <span class="n">hidden_inputs</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden_to_output_delta</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span> <span class="o">-=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>
            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">input_to_hidden_delta</span><span class="p">))</span>

        <span class="k">if</span> <span class="p">((</span><span class="n">output</span> <span class="o">&lt;</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">label</span><span class="o">==</span><span class="s2">"NEGATIVE"</span><span class="p">)</span>
            <span class="ow">or</span> <span class="p">(</span><span class="n">output</span> <span class="o">&gt;=</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">label</span><span class="o">==</span><span class="s2">"POSITIVE"</span><span class="p">)):</span>
            <span class="n">correct_so_far</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">index</span> <span class="o">%</span> <span class="mi">1000</span><span class="p">:</span>
            <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
            <span class="n">reviews_per_second</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span><span class="o">/</span><span class="n">elapsed_time</span><span class="o">.</span><span class="n">seconds</span>
                                  <span class="k">if</span> <span class="n">elapsed_time</span><span class="o">.</span><span class="n">seconds</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span>
                <span class="s2">"Progress: {:.2f} %"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">index</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">training_reviews</span><span class="p">))</span>
                <span class="o">+</span> <span class="s2">" Speed(reviews/sec): {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reviews_per_second</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">" Error: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_error</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="o">+</span> <span class="s2">" #Correct: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">correct_so_far</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">" #Trained: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">" Training Accuracy: {:.2f} %"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">correct_so_far</span> <span class="o">*</span> <span class="mi">100</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
                <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">"Training Time: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">correct_so_far</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org58f98fd">
<h3 id="org58f98fd">Test The Model</h3>
<div class="outline-text-3" id="text-org58f98fd">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">testing_reviews</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">testing_labels</span><span class="p">:</span><span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Attempts to predict the labels for the given testing_reviews,</span>
<span class="sd">    and uses the test_labels to calculate the accuracy of those predictions.</span>

<span class="sd">    Returns:</span>
<span class="sd">     correct: number of correct predictions</span>
<span class="sd">    """</span>

    <span class="c1"># keep track of how many correct predictions we make</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># we'll time how many predictions per second we make</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

    <span class="c1"># Loop through each of the given reviews and call run to predict</span>
    <span class="c1"># its label.</span>
    <span class="n">reviews_and_labels</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">testing_reviews</span><span class="p">,</span> <span class="n">testing_labels</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="n">review</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reviews_and_labels</span><span class="p">):</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">review</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prediction</span> <span class="o">==</span> <span class="n">label</span><span class="p">:</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">index</span> <span class="o">%</span> <span class="mi">100</span><span class="p">:</span>
            <span class="n">elapsed_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
            <span class="n">reviews_per_second</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span><span class="o">/</span><span class="n">elapsed_time</span><span class="o">.</span><span class="n">seconds</span>
                                  <span class="k">if</span> <span class="n">elapsed_time</span><span class="o">.</span><span class="n">seconds</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

            <span class="k">print</span><span class="p">(</span>
                <span class="s2">"Progress: {:.2f}%"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="mi">100</span> <span class="o">*</span> <span class="n">index</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">testing_reviews</span><span class="p">))</span>
                <span class="o">+</span> <span class="s2">" Speed(reviews/sec): {:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">reviews_per_second</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">" #Correct: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">correct</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">" #Tested: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">" Testing Accuracy: {:.2f} %"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">correct</span> <span class="o">*</span> <span class="mi">100</span><span class="o">/</span><span class="p">(</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
            <span class="p">)</span>
    <span class="k">return</span> <span class="n">correct</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org81ed2f0">
<h3 id="org81ed2f0">Run a Prediction</h3>
<div class="outline-text-3" id="text-org81ed2f0">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">review</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Returns a POSITIVE or NEGATIVE prediction for the given review.</span>
<span class="sd">    """</span>
    <span class="n">review</span> <span class="o">=</span> <span class="n">review</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">update_input_layer</span><span class="p">(</span><span class="n">review</span><span class="p">)</span>
    <span class="n">hidden_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layer</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">)</span>
    <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="n">hidden_inputs</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">hidden_outputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">"POSITIVE"</span> <span class="k">if</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.5</span> <span class="k">else</span> <span class="s2">"NEGATIVE"</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0991885">
<h2 id="org0991885">Test The Network</h2>
<div class="outline-text-2" id="text-org0991885">
<p>So now we'll actually try and run the network to see how it does.</p>
<div class="highlight">
<pre><span></span><span class="o">%</span><span class="n">reload_ext</span> <span class="n">autoreload</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">sentiment_network</span> <span class="kn">import</span> <span class="n">SentimentNetwork</span>
</pre></div>
<p>We'll be using the last 1,000 labels to test the network and all but the last to train it.</p>
<div class="highlight">
<pre><span></span><span class="n">BOUNDARY</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1000</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">reviews</span><span class="p">[</span><span class="n">BOUNDARY</span><span class="p">:],</span><span class="n">labels</span><span class="p">[</span><span class="n">BOUNDARY</span><span class="p">:]</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>
</pre></div>
<pre class="example">
1000

</pre>
<div class="highlight">
<pre><span></span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">reviews</span><span class="p">[:</span><span class="n">BOUNDARY</span><span class="p">],</span><span class="n">labels</span><span class="p">[:</span><span class="n">BOUNDARY</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
</pre></div>
<pre class="example">
24000

</pre>
<p>Since I split this up into multiple posts I'm going to pickle up the data-sets to make sure that they're only being created once.</p>
<div class="highlight">
<pre><span></span><span class="n">pickles</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
               <span class="n">x_train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
<span class="k">for</span> <span class="n">potential_pickle</span><span class="p">,</span> <span class="n">collection</span> <span class="ow">in</span> <span class="n">pickles</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">potential_path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"{}.pkl"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">potential_pickle</span><span class="p">),</span> <span class="n">check_exists</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">potential_path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">potential_path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">writer</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">untrained</span> <span class="o">=</span> <span class="n">SentimentNetwork</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
<p>Run the following cell to actually train the network. During training, it will display the model's accuracy repeatedly as it trains so you can see how well it's doing.</p>
<div class="highlight">
<pre><span></span><span class="n">untrained</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00 % Speed(reviews/sec): 0.00 Error: [-0.5] #Correct: 1 #Trained: 1 Training Accuracy: 100.00 %
Progress: 4.17 % Speed(reviews/sec): 125.00 Error: [-0.50133709] #Correct: 492 #Trained: 1001 Training Accuracy: 49.15 %
Progress: 8.33 % Speed(reviews/sec): 153.85 Error: [-0.46896641] #Correct: 940 #Trained: 2001 Training Accuracy: 46.98 %
Progress: 12.50 % Speed(reviews/sec): 150.00 Error: [-0.76053545] #Correct: 1401 #Trained: 3001 Training Accuracy: 46.68 %
Progress: 16.67 % Speed(reviews/sec): 142.86 Error: [-0.5175674] #Correct: 1860 #Trained: 4001 Training Accuracy: 46.49 %
Progress: 20.83 % Speed(reviews/sec): 142.86 Error: [-0.7057053] #Correct: 2329 #Trained: 5001 Training Accuracy: 46.57 %
Progress: 25.00 % Speed(reviews/sec): 146.34 Error: [-0.87768714] #Correct: 2859 #Trained: 6001 Training Accuracy: 47.64 %
Progress: 29.17 % Speed(reviews/sec): 142.86 Error: [-0.42471556] #Correct: 3376 #Trained: 7001 Training Accuracy: 48.22 %
Progress: 33.33 % Speed(reviews/sec): 140.35 Error: [-0.25287871] #Correct: 3931 #Trained: 8001 Training Accuracy: 49.13 %
Progress: 37.50 % Speed(reviews/sec): 138.46 Error: [-0.13143902] #Correct: 4508 #Trained: 9001 Training Accuracy: 50.08 %
Progress: 41.67 % Speed(reviews/sec): 136.99 Error: [-0.30215181] #Correct: 5141 #Trained: 10001 Training Accuracy: 51.40 %
Progress: 45.83 % Speed(reviews/sec): 137.50 Error: [-0.83628373] #Correct: 5690 #Trained: 11001 Training Accuracy: 51.72 %
Progress: 50.00 % Speed(reviews/sec): 136.36 Error: [-0.2236724] #Correct: 6318 #Trained: 12001 Training Accuracy: 52.65 %
Progress: 54.17 % Speed(reviews/sec): 136.84 Error: [-0.00040756] #Correct: 6873 #Trained: 13001 Training Accuracy: 52.87 %
Progress: 58.33 % Speed(reviews/sec): 137.25 Error: [-0.24857157] #Correct: 7463 #Trained: 14001 Training Accuracy: 53.30 %
Progress: 62.50 % Speed(reviews/sec): 136.36 Error: [-0.56169307] #Correct: 8091 #Trained: 15001 Training Accuracy: 53.94 %
Progress: 66.67 % Speed(reviews/sec): 136.75 Error: [-0.30580514] #Correct: 8710 #Trained: 16001 Training Accuracy: 54.43 %
Progress: 70.83 % Speed(reviews/sec): 136.00 Error: [-0.85096669] #Correct: 9343 #Trained: 17001 Training Accuracy: 54.96 %
Progress: 75.00 % Speed(reviews/sec): 136.36 Error: [-0.0031485] #Correct: 9973 #Trained: 18001 Training Accuracy: 55.40 %
Progress: 79.17 % Speed(reviews/sec): 135.71 Error: [-0.73531052] #Correct: 10671 #Trained: 19001 Training Accuracy: 56.16 %
Progress: 83.33 % Speed(reviews/sec): 136.05 Error: [-0.14522187] #Correct: 11341 #Trained: 20001 Training Accuracy: 56.70 %
Progress: 87.50 % Speed(reviews/sec): 135.48 Error: [-0.38478658] #Correct: 11973 #Trained: 21001 Training Accuracy: 57.01 %
Progress: 91.67 % Speed(reviews/sec): 134.97 Error: [-0.39655627] #Correct: 12678 #Trained: 22001 Training Accuracy: 57.62 %
Progress: 95.83 % Speed(reviews/sec): 134.50 Error: [-0.55767025] #Correct: 13345 #Trained: 23001 Training Accuracy: 58.02 %
</pre>
<p>That most likely didn't train very well. Part of the reason may be because the learning rate is too high. Run the following cell to recreate the network with a smaller learning rate, `0.01`, and then train the new network.</p>
<div class="highlight">
<pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">SentimentNetwork</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00 % Speed(reviews/sec): 0.00 Error: [-0.5] #Correct: 1 #Trained: 1 Training Accuracy: 100.00 %
Progress: 4.17 % Speed(reviews/sec): 250.00 Error: [-0.73627527] #Correct: 482 #Trained: 1001 Training Accuracy: 48.15 %
Progress: 8.33 % Speed(reviews/sec): 333.33 Error: [-0.27663369] #Correct: 1065 #Trained: 2001 Training Accuracy: 53.22 %
Progress: 12.50 % Speed(reviews/sec): 333.33 Error: [-0.41620613] #Correct: 1743 #Trained: 3001 Training Accuracy: 58.08 %
Progress: 16.67 % Speed(reviews/sec): 333.33 Error: [-0.41925862] #Correct: 2378 #Trained: 4001 Training Accuracy: 59.44 %
Progress: 20.83 % Speed(reviews/sec): 333.33 Error: [-0.3792133] #Correct: 3022 #Trained: 5001 Training Accuracy: 60.43 %
Progress: 25.00 % Speed(reviews/sec): 333.33 Error: [-0.31493906] #Correct: 3670 #Trained: 6001 Training Accuracy: 61.16 %
Progress: 29.17 % Speed(reviews/sec): 333.33 Error: [-0.19472257] #Correct: 4380 #Trained: 7001 Training Accuracy: 62.56 %
Progress: 33.33 % Speed(reviews/sec): 333.33 Error: [-0.20326775] #Correct: 5068 #Trained: 8001 Training Accuracy: 63.34 %
Progress: 37.50 % Speed(reviews/sec): 333.33 Error: [-0.17244992] #Correct: 5751 #Trained: 9001 Training Accuracy: 63.89 %
Progress: 41.67 % Speed(reviews/sec): 333.33 Error: [-0.74943668] #Correct: 6475 #Trained: 10001 Training Accuracy: 64.74 %
Progress: 45.83 % Speed(reviews/sec): 333.33 Error: [-0.34768212] #Correct: 7171 #Trained: 11001 Training Accuracy: 65.18 %
Progress: 50.00 % Speed(reviews/sec): 333.33 Error: [-0.23588717] #Correct: 7895 #Trained: 12001 Training Accuracy: 65.79 %
Progress: 54.17 % Speed(reviews/sec): 325.00 Error: [-0.67639111] #Correct: 8634 #Trained: 13001 Training Accuracy: 66.41 %
Progress: 58.33 % Speed(reviews/sec): 325.58 Error: [-0.18425262] #Correct: 9360 #Trained: 14001 Training Accuracy: 66.85 %
Progress: 62.50 % Speed(reviews/sec): 326.09 Error: [-0.31647149] #Correct: 10083 #Trained: 15001 Training Accuracy: 67.22 %
Progress: 66.67 % Speed(reviews/sec): 326.53 Error: [-0.31838031] #Correct: 10791 #Trained: 16001 Training Accuracy: 67.44 %
Progress: 70.83 % Speed(reviews/sec): 326.92 Error: [-0.71363956] #Correct: 11494 #Trained: 17001 Training Accuracy: 67.61 %
Progress: 75.00 % Speed(reviews/sec): 327.27 Error: [-0.03786987] #Correct: 12237 #Trained: 18001 Training Accuracy: 67.98 %
Progress: 79.17 % Speed(reviews/sec): 327.59 Error: [-0.89039967] #Correct: 12995 #Trained: 19001 Training Accuracy: 68.39 %
Progress: 83.33 % Speed(reviews/sec): 327.87 Error: [-0.19787345] #Correct: 13741 #Trained: 20001 Training Accuracy: 68.70 %
Progress: 87.50 % Speed(reviews/sec): 328.12 Error: [-0.60033441] #Correct: 14484 #Trained: 21001 Training Accuracy: 68.97 %
Progress: 91.67 % Speed(reviews/sec): 323.53 Error: [-0.47631941] #Correct: 15242 #Trained: 22001 Training Accuracy: 69.28 %
Progress: 95.83 % Speed(reviews/sec): 323.94 Error: [-0.47388592] #Correct: 15995 #Trained: 23001 Training Accuracy: 69.54 %
Training Time: 0:01:15.489437
</pre>
<p>This actually did better, but let's see what a smaller learning rate will do.</p>
<div class="highlight">
<pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">SentimentNetwork</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
<pre class="example">
Progress: 0.00 % Speed(reviews/sec): 0.00 Error: [-0.5] #Correct: 1 #Trained: 1 Training Accuracy: 100.00 %
Progress: 4.17 % Speed(reviews/sec): 250.00 Error: [-0.42248049] #Correct: 472 #Trained: 1001 Training Accuracy: 47.15 %
Progress: 8.33 % Speed(reviews/sec): 333.33 Error: [-0.27087125] #Correct: 1046 #Trained: 2001 Training Accuracy: 52.27 %
Progress: 12.50 % Speed(reviews/sec): 333.33 Error: [-0.45852835] #Correct: 1708 #Trained: 3001 Training Accuracy: 56.91 %
Progress: 16.67 % Speed(reviews/sec): 333.33 Error: [-0.41728936] #Correct: 2334 #Trained: 4001 Training Accuracy: 58.34 %
Progress: 20.83 % Speed(reviews/sec): 333.33 Error: [-0.37365937] #Correct: 2959 #Trained: 5001 Training Accuracy: 59.17 %
Progress: 25.00 % Speed(reviews/sec): 315.79 Error: [-0.25350906] #Correct: 3595 #Trained: 6001 Training Accuracy: 59.91 %
Progress: 29.17 % Speed(reviews/sec): 318.18 Error: [-0.22273178] #Correct: 4292 #Trained: 7001 Training Accuracy: 61.31 %
Progress: 33.33 % Speed(reviews/sec): 320.00 Error: [-0.22148954] #Correct: 4985 #Trained: 8001 Training Accuracy: 62.30 %
Progress: 37.50 % Speed(reviews/sec): 321.43 Error: [-0.164888] #Correct: 5670 #Trained: 9001 Training Accuracy: 62.99 %
Progress: 41.67 % Speed(reviews/sec): 322.58 Error: [-0.70030978] #Correct: 6381 #Trained: 10001 Training Accuracy: 63.80 %
Progress: 45.83 % Speed(reviews/sec): 305.56 Error: [-0.37677934] #Correct: 7082 #Trained: 11001 Training Accuracy: 64.38 %
Progress: 50.00 % Speed(reviews/sec): 307.69 Error: [-0.25747753] #Correct: 7812 #Trained: 12001 Training Accuracy: 65.09 %
Progress: 54.17 % Speed(reviews/sec): 302.33 Error: [-0.66038851] #Correct: 8550 #Trained: 13001 Training Accuracy: 65.76 %
Progress: 58.33 % Speed(reviews/sec): 304.35 Error: [-0.21017589] #Correct: 9271 #Trained: 14001 Training Accuracy: 66.22 %
Progress: 62.50 % Speed(reviews/sec): 306.12 Error: [-0.32861519] #Correct: 9993 #Trained: 15001 Training Accuracy: 66.62 %
Progress: 66.67 % Speed(reviews/sec): 307.69 Error: [-0.31545046] #Correct: 10705 #Trained: 16001 Training Accuracy: 66.90 %
Progress: 70.83 % Speed(reviews/sec): 309.09 Error: [-0.70497608] #Correct: 11411 #Trained: 17001 Training Accuracy: 67.12 %
Progress: 75.00 % Speed(reviews/sec): 310.34 Error: [-0.04885612] #Correct: 12162 #Trained: 18001 Training Accuracy: 67.56 %
Progress: 79.17 % Speed(reviews/sec): 316.67 Error: [-0.79732231] #Correct: 12916 #Trained: 19001 Training Accuracy: 67.98 %
Progress: 83.33 % Speed(reviews/sec): 312.50 Error: [-0.2568252] #Correct: 13678 #Trained: 20001 Training Accuracy: 68.39 %
Progress: 87.50 % Speed(reviews/sec): 313.43 Error: [-0.59070143] #Correct: 14418 #Trained: 21001 Training Accuracy: 68.65 %
Progress: 91.67 % Speed(reviews/sec): 305.56 Error: [-0.42520887] #Correct: 15181 #Trained: 22001 Training Accuracy: 69.00 %
Progress: 95.83 % Speed(reviews/sec): 302.63 Error: [-0.50276096] #Correct: 15931 #Trained: 23001 Training Accuracy: 69.26 %
Training Time: 0:01:19.701444
</pre>
<p>Surprisingly it did around the same (maybe a little worse). It looks like tuning the learning rate isn't enough.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/sentiment_analysis/the-network-parts/">The Network Parts</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/sentiment_analysis/the-network-parts/" rel="bookmark"><time class="published dt-published" datetime="2018-11-11T14:44:07-08:00" itemprop="datePublished" title="2018-11-11 14:44">2018-11-11 14:44</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/sentiment_analysis/the-network-parts/#org9ede76d">Set Up</a></li>
<li><a href="/posts/nano/sentiment_analysis/the-network-parts/#org70bb370">The Data</a></li>
<li><a href="/posts/nano/sentiment_analysis/the-network-parts/#orge94b971">Transforming Text into Numbers</a></li>
<li><a href="/posts/nano/sentiment_analysis/the-network-parts/#org99de358">Creating the Input/Output Data</a></li>
</ul>
</div>
</div>
<p>This is an initial exploration of some of the parts that are going to make up the Neural Network as well as a little inspection of the data and how we're going to use it.</p>
<div class="outline-2" id="outline-container-org9ede76d">
<h2 id="org9ede76d">Set Up</h2>
<div class="outline-text-2" id="text-org9ede76d"></div>
<div class="outline-3" id="outline-container-org9800e41">
<h3 id="org9800e41">Imports</h3>
<div class="outline-text-3" id="text-org9800e41"></div>
<div class="outline-4" id="outline-container-org9c82f9f">
<h4 id="org9c82f9f">The Tangle</h4>
<div class="outline-text-4" id="text-org9c82f9f">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgbb18a49">
<h4 id="orgbb18a49">Python</h4>
<div class="outline-text-4" id="text-orgbb18a49">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org24df161">
<h4 id="org24df161">PyPi</h4>
<div class="outline-text-4" id="text-org24df161">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Graph</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge667b47">
<h4 id="orge667b47">This Project</h4>
<div class="outline-text-4" id="text-orge667b47">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPath</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orge386054">
<h3 id="orge386054">Loading The Pickles</h3>
<div class="outline-text-3" id="text-orge386054">
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"total_count.pkl"</span><span class="p">)</span>
<span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">total_counts</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0cbb71e">
<h3 id="org0cbb71e">Some Constants</h3>
<div class="outline-text-3" id="text-org0cbb71e">
<div class="highlight">
<pre><span></span><span class="n">SPLIT_ON_THIS</span> <span class="o">=</span> <span class="s2">" "</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org70bb370">
<h2 id="org70bb370">The Data</h2>
<div class="outline-text-2" id="text-org70bb370">
<p>The Reviews.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"reviews.txt"</span><span class="p">)</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"reviews.pkl"</span><span class="p">,</span> <span class="n">check_exists</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">output_path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="p">,</span><span class="s1">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
        <span class="n">reviews</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">output_path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">reviews</span><span class="p">,</span> <span class="n">writer</span><span class="p">)</span>
</pre></div>
<p>The labels.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"labels.txt"</span><span class="p">)</span>
<span class="n">output_path</span> <span class="o">=</span> <span class="n">DataPath</span><span class="p">(</span><span class="s2">"labels.pkl"</span><span class="p">,</span> <span class="n">check_exists</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">output_path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">is_file</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">()</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">output_path</span><span class="o">.</span><span class="n">from_folder</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">writer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orge94b971">
<h2 id="orge94b971">Transforming Text into Numbers</h2>
<div class="outline-text-2" id="text-orge94b971">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">plot_network</span><span class="p">():</span>
    <span class="sd">"""</span>
<span class="sd">    Creates a simplified plot of our network (simple_network.dot.png)</span>
<span class="sd">    """</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="n">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">attr</span><span class="p">(</span><span class="n">rankdir</span><span class="o">=</span><span class="s2">"LR"</span><span class="p">)</span>

    <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"a"</span><span class="p">,</span> <span class="s2">"horrible"</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"b"</span><span class="p">,</span> <span class="s2">"excellent"</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"c"</span><span class="p">,</span> <span class="s2">"terrible"</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"d"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"e"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"f"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"h"</span><span class="p">,</span> <span class="s2">"positive"</span><span class="p">)</span>

    <span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="s2">"ad"</span><span class="p">,</span> <span class="s2">"ae"</span><span class="p">,</span> <span class="s2">"af"</span><span class="p">,</span> <span class="s2">"ag"</span><span class="p">,</span>
                 <span class="s2">"bd"</span><span class="p">,</span> <span class="s2">"be"</span><span class="p">,</span> <span class="s2">"bf"</span><span class="p">,</span> <span class="s2">"bg"</span><span class="p">,</span>
                 <span class="s2">"cd"</span><span class="p">,</span> <span class="s2">"ce"</span> <span class="p">,</span> <span class="s2">"cf"</span><span class="p">,</span> <span class="s2">"cg"</span><span class="p">])</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="s2">"dh"</span><span class="p">,</span> <span class="s1">'eh'</span><span class="p">,</span> <span class="s1">'fh'</span><span class="p">,</span> <span class="s1">'gh'</span><span class="p">])</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">"graphs/simple_network.dot"</span><span class="p">)</span>
    <span class="n">graph</span>
    <span class="k">return</span>
</pre></div>
<p>This is one potential way to classify the sentiment of a review using a neural network. In this case if any of the terms (<i>horrible, excellent,</i> or <i>terrible</i>) exists the input is a one for that term and the output is the sum of the multiplication of the weights times the inputs.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org99de358">
<h2 id="org99de358">Creating the Input/Output Data</h2>
<div class="outline-text-2" id="text-org99de358"></div>
<div class="outline-3" id="outline-container-org4336598">
<h3 id="org4336598">The Vocabulary</h3>
<div class="outline-text-3" id="text-org4336598">
<p>We're going to create a "vocabulary" which is just a list of all the words in our reviews.</p>
<div class="highlight">
<pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">total_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
<p>Here's our vocabulary size.</p>
<div class="highlight">
<pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"{:,}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>
<span class="k">assert</span> <span class="n">vocab_size</span><span class="o">==</span><span class="mi">74074</span>
</pre></div>
<pre class="example">
74,074

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgc537e09">
<h3 id="orgc537e09">Layer 0</h3>
<div class="outline-text-3" id="text-orgc537e09">
<p>Now we're going to create a numpy array called <i>layer_0</i> and initialize it to all <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html">zeros</a>. This will represent our input layer, so it will be a 2-dimensional matrix with 1 row and <i>vocab_size</i> columns.</p>
<div class="highlight">
<pre><span></span><span class="n">layer_0</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
</pre></div>
<p>Now we can double-check the shape to make sure it matches what we're expecting.</p>
<div class="highlight">
<pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="n">layer_0</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"{}, {:,}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">))</span>
<span class="k">assert</span> <span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">74074</span><span class="p">)</span>
</pre></div>
<pre class="example">
1, 74,074

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgb58d82d">
<h3 id="orgb58d82d">Word 2 Index</h3>
<div class="outline-text-3" id="text-orgb58d82d">
<p><code>layer_0</code> contains one entry for every word in the vocabulary. We need to make sure we know the index of each word, so we'rec going to create a lookup table that stores the index of every word.</p>
<div class="highlight">
<pre><span></span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
</pre></div>
<p>Here's the first ten entries in the lookup table.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">"|Term| Index|"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"|-+-|"</span><span class="p">)</span>
<span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">word2index</span><span class="o">.</span><span class="n">keys</span><span class="p">())[:</span><span class="mi">10</span><span class="p">]</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"|{}|{}|"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">word2index</span><span class="p">[</span><span class="n">key</span><span class="p">]))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Term</th>
<th class="org-right" scope="col">Index</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">bromwell</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">high</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">is</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">a</td>
<td class="org-right">3</td>
</tr>
<tr>
<td class="org-left">cartoon</td>
<td class="org-right">4</td>
</tr>
<tr>
<td class="org-left">comedy</td>
<td class="org-right">5</td>
</tr>
<tr>
<td class="org-left">.</td>
<td class="org-right">6</td>
</tr>
<tr>
<td class="org-left">it</td>
<td class="org-right">7</td>
</tr>
<tr>
<td class="org-left">ran</td>
<td class="org-right">8</td>
</tr>
<tr>
<td class="org-left">at</td>
<td class="org-right">9</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-3" id="outline-container-org9982d65">
<h3 id="org9982d65">Update Input Layer</h3>
<div class="outline-text-3" id="text-org9982d65">
<p>The <code>update_input_layer</code> will count how many times each word is used in the review and then store those counts at the appropriate indices inside <code>layer_0</code>. To make this useable in other posts you have to pass in the <code>word2index</code> table, but in the actual Neural Network we're going to use a class so it will look a little different.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">update_input_layer</span><span class="p">(</span><span class="n">review</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">layer_0</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word2index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
    <span class="sd">""" Modify the global layer_0 to represent the vector form of review.</span>
<span class="sd">    The element at a given index of layer_0 should represent</span>
<span class="sd">    how many times the given word occurs in the review.</span>

<span class="sd">    Args:</span>
<span class="sd">       review: the string of the review</span>
<span class="sd">       layer_0: array representing layer 0</span>
<span class="sd">       word2index: dict mapping word to index in layer_0</span>
<span class="sd">    Returns:</span>
<span class="sd">        counter for the tokens (used for troubleshooting)</span>
<span class="sd">    """</span>
    <span class="c1"># clear out previous state by resetting the layer to be all 0s</span>
    <span class="n">layer_0</span> <span class="o">*=</span> <span class="mi">0</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">review</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">SPLIT_ON_THIS</span><span class="p">)</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>
    <span class="n">counter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">counter</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">layer_0</span><span class="p">[:,</span> <span class="n">word2index</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">counter</span>
</pre></div>
<p>Here's what happens when you update <code>layer_0</code> with the first review.</p>
<div class="highlight">
<pre><span></span><span class="n">update_input_layer</span><span class="p">(</span><span class="n">reviews</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">layer_0</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[4. 5. 4. ... 0. 0. 0.]]

</pre>
<p>It doesn't look exciting, but if we remember that we initialized the values as all zeros, then we can see that something is changing.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgf25d9a6">
<h3 id="orgf25d9a6">Get Target For Labels</h3>
<div class="outline-text-3" id="text-orgf25d9a6">
<p><code>get_target_for_labels</code> returns <code>0</code> or <code>1</code>, depending on whether the given label is <code>NEGATIVE</code> or <code>POSITIVE</code>, respectively. This will allow us to use the labels as we were given them and map them to numbers inside the neural net. An alternative might be to pre-process the labels or make this a dictionary.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_target_for_label</span><span class="p">(</span><span class="n">label</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Convert a label to `0` or `1`.</span>
<span class="sd">    Args:</span>
<span class="sd">       label(string) - Either "POSITIVE" or "NEGATIVE".</span>
<span class="sd">    Returns:</span>
<span class="sd">       `0` or `1`.</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">label</span><span class="o">==</span><span class="s2">"POSITIVE"</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>
<p>So, here's the first label.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
<pre class="example">
POSITIVE

</pre>
<p>And here's what we mapped it to.</p>
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">get_target_for_label</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<pre class="example">
1

</pre>
<p>And here we go with the second label.</p>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
NEGATIVE

</pre>
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">get_target_for_label</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">output</span> <span class="o">==</span> <span class="mi">0</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<pre class="example">
0

</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/sentiment_analysis/exploring-the-reviews-dataset/">Exploring the Reviews Dataset</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/sentiment_analysis/exploring-the-reviews-dataset/" rel="bookmark"><time class="published dt-published" datetime="2018-11-11T14:25:48-08:00" itemprop="datePublished" title="2018-11-11 14:25">2018-11-11 14:25</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/sentiment_analysis/exploring-the-reviews-dataset/#org474ca7e">Set Up</a></li>
<li><a href="/posts/nano/sentiment_analysis/exploring-the-reviews-dataset/#org8b78214">Lesson 1: Curate a Dataset</a></li>
<li><a href="/posts/nano/sentiment_analysis/exploring-the-reviews-dataset/#org1769fcb">Develop a Predictive Theory</a></li>
<li><a href="/posts/nano/sentiment_analysis/exploring-the-reviews-dataset/#org76c1071">Quick Theory Validation</a></li>
<li><a href="/posts/nano/sentiment_analysis/exploring-the-reviews-dataset/#org03c8105">Pickling</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org474ca7e">
<h2 id="org474ca7e">Set Up</h2>
<div class="outline-text-2" id="text-org474ca7e"></div>
<div class="outline-3" id="outline-container-org5510cba">
<h3 id="org5510cba">Imports</h3>
<div class="outline-text-3" id="text-org5510cba"></div>
<div class="outline-4" id="outline-container-org1e3db71">
<h4 id="org1e3db71">Python</h4>
<div class="outline-text-4" id="text-org1e3db71">
<div class="highlight">
<pre><span></span>from collections import Counter
import pickle
import textwrap
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org62a465e">
<h4 id="org62a465e">PyPi</h4>
<div class="outline-text-4" id="text-org62a465e">
<div class="highlight">
<pre><span></span>import numpy
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org05d8f60">
<h4 id="org05d8f60">This Project</h4>
<div class="outline-text-4" id="text-org05d8f60">
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPath
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org8b78214">
<h2 id="org8b78214">Lesson 1: Curate a Dataset</h2>
<div class="outline-text-2" id="text-org8b78214">
<p>The goal of this section is to become familiar with the data and perform any preprocessing that might be needed.</p>
</div>
<div class="outline-3" id="outline-container-orgd7834e7">
<h3 id="orgd7834e7">A Helper To Print</h3>
<div class="outline-text-3" id="text-orgd7834e7">
<div class="highlight">
<pre><span></span>def pretty_print_review_and_label(index: int, up_to: int=80) -&gt; None:
    """Prints the label and review

    Args:
     index: the index of the review in the data set
     up_to: number of characters in the review to show
    """
    print("|{}|{}|".format(labels[index], reviews[index][:up_to] + "..."))
    return
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge94f429">
<h3 id="orge94f429">The Reviews</h3>
<div class="outline-text-3" id="text-orge94f429">
<p>It's not really clear what he's doing here. I think he's stripping the newlines off of the reviews, so each review must be one line.</p>
<div class="highlight">
<pre><span></span>path = DataPath("reviews.txt")
with open(path.from_folder,'r') as reader:
    reviews = [line.rstrip() for line in reader]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org4bb5308">
<h3 id="org4bb5308">The Labels</h3>
<div class="outline-text-3" id="text-org4bb5308">
<p>A similar deal except casting the labels to upper case.</p>
<div class="highlight">
<pre><span></span>path = DataPath("labels.txt")
with open(path.from_folder,'r') as reader:
    labels = (line.rstrip() for line in reader)
    labels = [line.upper() for line in labels]
</pre></div>
<p><b>Note:</b> The data in <code>reviews.txt</code> we're using has already been preprocessed a bit and contains only lower case characters. If we were working from raw data, where we didn't know it was all lower case, we would want to add a step here to convert it. That's so we treat different variations of the same word, like `The`, `the`, and `THE`, all the same way.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgffff92d">
<h3 id="orgffff92d">How many reviews do we have?</h3>
<div class="outline-text-3" id="text-orgffff92d">
<div class="highlight">
<pre><span></span>print("{:,}".format(len(reviews)))
</pre></div>
<pre class="example">
25,000

</pre></div>
</div>
<div class="outline-3" id="outline-container-org7d2dac3">
<h3 id="org7d2dac3">What does a review look like?</h3>
<div class="outline-text-3" id="text-org7d2dac3">
<div class="highlight">
<pre><span></span>print("\n".join(textwrap.wrap(reviews[0], width=80)))
</pre></div>
<pre class="example">
bromwell high is a cartoon comedy . it ran at the same time as some other
programs about school life  such as  teachers  . my   years in the teaching
profession lead me to believe that bromwell high  s satire is much closer to
reality than is  teachers  . the scramble to survive financially  the insightful
students who can see right through their pathetic teachers  pomp  the pettiness
of the whole situation  all remind me of the schools i knew and their students .
when i saw the episode in which a student repeatedly tried to burn down the
school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a
classic line inspector i  m here to sack one of your teachers . student welcome
to bromwell high . i expect that many adults of my age think that bromwell high
is far fetched . what a pity that it isn  t
</pre>
<p>Kind of odd looking. It looks like the pre-processor did some bad things.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgd3cdb3a">
<h3 id="orgd3cdb3a">What does the label for that review look like?</h3>
<div class="outline-text-3" id="text-orgd3cdb3a">
<div class="highlight">
<pre><span></span>print(labels[0])
</pre></div>
<pre class="example">
POSITIVE

</pre></div>
</div>
<div class="outline-3" id="outline-container-org51bfff3">
<h3 id="org51bfff3">What are the labels available?</h3>
<div class="outline-text-3" id="text-org51bfff3">
<p>At this point we don't have pandas loaded, so I'll just use a set to look at the labels.</p>
<div class="highlight">
<pre><span></span>print(set(labels))
</pre></div>
<pre class="example">
{'NEGATIVE', 'POSITIVE'}

</pre>
<p>So there are two labels - "NEGATIVE" and "POSITIVE".</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1769fcb">
<h2 id="org1769fcb">Develop a Predictive Theory</h2>
<div class="outline-text-2" id="text-org1769fcb">
<p>The previous section gave us a rough idea of what's in the data set. Now we want to make a guess as to what the labels mean - why is a review labled POSITIVE or NEGATIVE?</p>
<div class="highlight">
<pre><span></span>print("|labels.txt| reviews.txt|")
print("|-+-|")
indices = (2137, 12816, 6267, 21934, 5297, 4998)
for index in indices:
    pretty_print_review_and_label(index)
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">labels.txt</th>
<th class="org-left" scope="col">reviews.txt</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">NEGATIVE</td>
<td class="org-left">this movie is terrible but it has some good effects ….</td>
</tr>
<tr>
<td class="org-left">POSITIVE</td>
<td class="org-left">adrian pasdar is excellent is this film . he makes a fascinating woman ….</td>
</tr>
<tr>
<td class="org-left">NEGATIVE</td>
<td class="org-left">comment this movie is impossible . is terrible very improbable bad interpretat…</td>
</tr>
<tr>
<td class="org-left">POSITIVE</td>
<td class="org-left">excellent episode movie ala pulp fiction . days suicides . it doesnt get more…</td>
</tr>
<tr>
<td class="org-left">NEGATIVE</td>
<td class="org-left">if you haven t seen this it s terrible . it is pure trash . i saw this about …</td>
</tr>
<tr>
<td class="org-left">POSITIVE</td>
<td class="org-left">this schiffer guy is a real genius the movie is of excellent quality and both e…</td>
</tr>
</tbody>
</table>
<p>If you look at the negative reviews, they all have the work 'terrible' in them, and the positives all have the workd 'excellent' in them. The theory then, is that the labels are based on whether a review has a key-word in it that makes it either positive or negative.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org76c1071">
<h2 id="org76c1071">Quick Theory Validation</h2>
<div class="outline-text-2" id="text-org76c1071">
<p>In this section we're going to test our theory that key-words identify whether a review is positive or negative using the <a href="https://docs.python.org/2/library/collections.html#collections.Counter">Counter</a> class and the <a href="https://docs.scipy.org/doc/numpy/reference/">numpy</a> library.</p>
</div>
<div class="outline-3" id="outline-container-org8e2a112">
<h3 id="org8e2a112">Word Counter</h3>
<div class="outline-text-3" id="text-org8e2a112">
<p>We'll create three <code>Counter</code> objects, one for words from postive reviews, one for words from negative reviews, and one for all the words.</p>
<div class="highlight">
<pre><span></span>positive_counts = Counter()
negative_counts = Counter()
total_counts = Counter()
</pre></div>
<p>Examine all the reviews. For each word in a positive review, increase the count for that word in both your positive counter and the total words counter; likewise, for each word in a negative review, increase the count for that word in both your negative counter and the total words counter.</p>
<p><b>Note:</b> Throughout these projects, you should use `split(' ')` to divide a piece of text (such as a review) into individual words. If you use `split()` instead, you'll get slightly different results than what the videos and solutions show.</p>
<p>The classifications in the <code>labels</code> list.</p>
<div class="highlight">
<pre><span></span>class Classification:
    positive = "POSITIVE"
    negative = "NEGATIVE"
</pre></div>
<p>What we are splitting on.</p>
<div class="highlight">
<pre><span></span>class Tokens:
    splitter = " "
</pre></div>
<div class="highlight">
<pre><span></span>with DataPath("labels.pkl").from_folder.open("rb") as reader:
    labels = pickle.load(reader)
</pre></div>
<div class="highlight">
<pre><span></span>for label, review in zip(labels, reviews):
    tokens = review.split(Tokens.splitter)
    total_counts.update(tokens)

    if label == Classification.positive:
        positive_counts.update(tokens)        
    else:
        negative_counts.update(tokens)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1d45f63">
<h3 id="org1d45f63">Most Common Words</h3>
<div class="outline-text-3" id="text-org1d45f63">
<p>Run the following two cells to list the words used in positive reviews and negative reviews, respectively, ordered from most to least commonly used.</p>
<p>Examine the counts of the most common words in positive reviews</p>
<div class="highlight">
<pre><span></span>print("|Token| Count|")
print("|-+-|")
for token, count in positive_counts.most_common(10):
    print("|{}|{:,}|".format(token, count))
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Token</th>
<th class="org-left" scope="col">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">&nbsp;</td>
<td class="org-left">518,327</td>
</tr>
<tr>
<td class="org-left">the</td>
<td class="org-left">173,324</td>
</tr>
<tr>
<td class="org-left">.</td>
<td class="org-left">159,654</td>
</tr>
<tr>
<td class="org-left">and</td>
<td class="org-left">89,722</td>
</tr>
<tr>
<td class="org-left">a</td>
<td class="org-left">83,688</td>
</tr>
<tr>
<td class="org-left">of</td>
<td class="org-left">76,855</td>
</tr>
<tr>
<td class="org-left">to</td>
<td class="org-left">66,746</td>
</tr>
<tr>
<td class="org-left">is</td>
<td class="org-left">57,245</td>
</tr>
<tr>
<td class="org-left">in</td>
<td class="org-left">50,215</td>
</tr>
<tr>
<td class="org-left">br</td>
<td class="org-left">49,235</td>
</tr>
</tbody>
</table>
<p>So, we probably don't want most of the most common tokens.</p>
<p>Examine the counts of the most common words in negative reviews</p>
<div class="highlight">
<pre><span></span>print("|Token| Count|")
print("|-+-|")
for token, count in negative_counts.most_common(10):
    print("|{}|{:,}|".format(token, count))
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Token</th>
<th class="org-left" scope="col">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">&nbsp;</td>
<td class="org-left">531,016</td>
</tr>
<tr>
<td class="org-left">.</td>
<td class="org-left">167,538</td>
</tr>
<tr>
<td class="org-left">the</td>
<td class="org-left">163,389</td>
</tr>
<tr>
<td class="org-left">a</td>
<td class="org-left">79,321</td>
</tr>
<tr>
<td class="org-left">and</td>
<td class="org-left">74,385</td>
</tr>
<tr>
<td class="org-left">of</td>
<td class="org-left">69,009</td>
</tr>
<tr>
<td class="org-left">to</td>
<td class="org-left">68,974</td>
</tr>
<tr>
<td class="org-left">br</td>
<td class="org-left">52,637</td>
</tr>
<tr>
<td class="org-left">is</td>
<td class="org-left">50,083</td>
</tr>
<tr>
<td class="org-left">it</td>
<td class="org-left">48,327</td>
</tr>
</tbody>
</table>
<p>As you can see, common words like "the" appear very often in both positive and negative reviews. Instead of finding the most common words in positive or negative reviews, what you really want are the words found in positive reviews more often than in negative reviews, and vice versa. To accomplish this, you'll need to calculate the <b>ratios</b> of word usage between positive and negative reviews.</p>
<p>Check all the words you've seen and calculate the ratio of postive to negative uses and store that ratio in <code>pos_neg_ratios</code>.</p>
<p>Hint: the positive-to-negative ratio for a given word can be calculated with `positive_counts[word] / float(negative_counts[word]+1)`. Notice the `+1` in the denominator –&nbsp;that ensures we don't divide by zero for words that are only seen in positive reviews.</p>
<p>Create a Counter object to store positive/negative ratios</p>
<div class="highlight">
<pre><span></span>pos_neg_ratios = Counter()
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfadef49">
<h3 id="orgfadef49">Positive to negative ratios</h3>
<div class="outline-text-3" id="text-orgfadef49">
<p>Calculate the ratios of positive and negative uses of the most common words</p>
<div class="highlight">
<pre><span></span>ratios = {element: positive_counts[element]/(negative_counts[element] + 1)
          for element in total_counts}
pos_neg_ratios.update(ratios)
</pre></div>
<div class="highlight">
<pre><span></span>with DataPath("pos_neg_ratios.pkl",
              check_exists=False).from_folder.open("wb") as writer:
    pickle.dump(pos_neg_ratios, writer)
</pre></div>
<p>Examine the ratios you've calculated for a few words:</p>
<div class="highlight">
<pre><span></span>print("Pos-to-neg ratio for 'the' = {:.2f}".format(pos_neg_ratios["the"]))
print("Pos-to-neg ratio for 'amazing' = {:.2f}".format(pos_neg_ratios["amazing"]))
print("Pos-to-neg ratio for 'terrible' = {:.2f}".format(pos_neg_ratios["terrible"]))
</pre></div>
<pre class="example">
Pos-to-neg ratio for 'the' = 1.06
Pos-to-neg ratio for 'amazing' = 4.02
Pos-to-neg ratio for 'terrible' = 0.18

</pre>
<p>Looking closely at the values you just calculated, we see the following:</p>
<ul class="org-ul">
<li>Words that you would expect to see more often in positive reviews - like "amazing" - have a ratio greater than 1. The more skewed a word is toward postive, the farther from 1 its positive-to-negative ratio will be.</li>
<li>Words that you would expect to see more often in negative reviews - like "terrible" - have positive values that are less than 1. The more skewed a word is toward negative, the closer to zero its positive-to-negative ratio will be.</li>
<li>Neutral words, which don't really convey any sentiment because you would expect to see them in all sorts of reviews – like "the" – have values very close to 1. A perfectly neutral word –&nbsp;one that was used in exactly the same number of positive reviews as negative reviews – would be almost exactly 1. The `+1` we suggested you add to the denominator slightly biases words toward negative, but it won't matter because it will be a tiny bias and later we'll be ignoring words that are too close to neutral anyway.</li>
</ul>
<p>Ok, the ratios tell us which words are used more often in postive or negative reviews, but the specific values we've calculated are a bit difficult to work with. A very positive word like "amazing" has a value above 4, whereas a very negative word like "terrible" has a value around 0.18. Those values aren't easy to compare for a couple of reasons:</p>
<ul class="org-ul">
<li>Right now, 1 is considered neutral, but the absolute value of the postive-to-negative ratios of very postive words is larger than the absolute value of the ratios for the very negative words. So there is no way to directly compare two numbers and see if one word conveys the same magnitude of positive sentiment as another word conveys negative sentiment. So we should center all the values around netural so the absolute value from neutral of the postive-to-negative ratio for a word would indicate how much sentiment (positive or negative) that word conveys.</li>
</ul>
<p><i>When comparing absolute values it's easier to do that around zero than one.</i></p>
<p>To fix these issues, we'll convert all of our ratios to new values using logarithms.</p>
<p>Go through all the ratios you calculated and convert them to logarithms. (i.e. use `np.log(ratio)`)</p>
<p>In the end, extremely positive and extremely negative words will have positive-to-negative ratios with similar magnitudes but opposite signs. Note that you have to create a new counter - the <code>update</code> method adds the new value to the previous values.</p>
<div class="highlight">
<pre><span></span>log_ratios = {}
for token, ratio in pos_neg_ratios.items():
    if ratio &gt; 1:
        log_ratios[token] = numpy.log(ratio)
    else:
        log_ratios[token] = -numpy.log(1/(ratio + 0.01))
positive_negative_log_ratios = Counter()
positive_negative_log_ratios.update(log_ratios)
</pre></div>
<p>Examine the new ratios you've calculated for the same words from before:</p>
<div class="highlight">
<pre><span></span>print("Pos-to-neg ratio for 'the' = {:.2f}".format(positive_negative_log_ratios["the"]))
print("Pos-to-neg ratio for 'amazing' = {:.2f}".format(positive_negative_log_ratios["amazing"]))
print("Pos-to-neg ratio for 'terrible' = {:.2f}".format(positive_negative_log_ratios["terrible"]))
</pre></div>
<pre class="example">
Pos-to-neg ratio for 'the' = 0.06
Pos-to-neg ratio for 'amazing' = 1.39
Pos-to-neg ratio for 'terrible' = -1.67

</pre>
<div class="highlight">
<pre><span></span>with DataPath("pos_neg_log_ratios.pkl",
              check_exists=False).from_folder.open("wb") as writer:
    pickle.dump(positive_negative_log_ratios, writer)
</pre></div>
<p>If everything worked, now you should see neutral words with values close to zero. In this case, "the" is near zero but slightly positive, so it was probably used in more positive reviews than negative reviews. But look at "amazing"'s ratio - it's above <code>1</code>, showing it is clearly a word with positive sentiment. And "terrible" has a similar score, but in the opposite direction, so it's below <code>-1</code>. It's now clear that both of these words are associated with specific, opposing sentiments.</p>
<p>Now run the following cells to see more ratios.</p>
<p>The first cell displays all the words, ordered by how associated they are with postive reviews. (Your notebook will most likely truncate the output so you won't actually see <b>all</b> the words in the list.)</p>
<p>The second cell displays the 30 words most associated with negative reviews by reversing the order of the first list and then looking at the first 30 words. (If you want the second cell to display all the words, ordered by how associated they are with negative reviews, you could just write `reversed(pos_neg_ratios.most_common())`.)</p>
<p>You should continue to see values similar to the earlier ones we checked –&nbsp;neutral words will be close to `0`, words will get more positive as their ratios approach and go above `1`, and words will get more negative as their ratios approach and go below `-1`. That's why we decided to use the logs instead of the raw ratios.</p>
<p>Here are the words most frequently seen in a review with a "POSITIVE" label.</p>
<div class="highlight">
<pre><span></span>print("|Token|Log Ratio|")
print("|-+-|")
for token, ratio in positive_negative_log_ratios.most_common(10):
    print("|{}|{:.2f}|".format(token, ratio))
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Token</th>
<th class="org-right" scope="col">Log Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">edie</td>
<td class="org-right">4.69</td>
</tr>
<tr>
<td class="org-left">antwone</td>
<td class="org-right">4.48</td>
</tr>
<tr>
<td class="org-left">din</td>
<td class="org-right">4.41</td>
</tr>
<tr>
<td class="org-left">gunga</td>
<td class="org-right">4.19</td>
</tr>
<tr>
<td class="org-left">goldsworthy</td>
<td class="org-right">4.17</td>
</tr>
<tr>
<td class="org-left">gypo</td>
<td class="org-right">4.09</td>
</tr>
<tr>
<td class="org-left">yokai</td>
<td class="org-right">4.09</td>
</tr>
<tr>
<td class="org-left">paulie</td>
<td class="org-right">4.08</td>
</tr>
<tr>
<td class="org-left">visconti</td>
<td class="org-right">3.93</td>
</tr>
<tr>
<td class="org-left">flavia</td>
<td class="org-right">3.93</td>
</tr>
</tbody>
</table>
<p>Ummm… okay.</p>
<div class="highlight">
<pre><span></span>print(positive_counts["edie"])
print(negative_counts["edie"])
</pre></div>
<pre class="example">
109
0

</pre>
<p>So the ones that are most positive appeared in the positive but not in the negative.</p>
<p>Here are the words most frequently seen in a review with a "NEGATIVE" label. The python slice notation is <code>list-name[first to include: first to exclude: step ]</code>.</p>
<div class="highlight">
<pre><span></span>print("|Token|Log Ratio|")
print("|-+-|")
for token, ratio in positive_negative_log_ratios.most_common()[:-11:-1]:
    print("|{}|{:.2f}|".format(token, ratio))
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Token</th>
<th class="org-right" scope="col">Log Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">whelk</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">pressurized</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">bellwood</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">mwuhahahaa</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">insulation</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">hoodies</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">yaks</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">raksha</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">deamon</td>
<td class="org-right">-4.61</td>
</tr>
<tr>
<td class="org-left">ziller</td>
<td class="org-right">-4.61</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span>print(positive_counts["whelk"])
print(negative_counts["whelk"])
</pre></div>
<pre class="example">
0
1

</pre>
<p>And the most negative counts just didn't appear in the positive counts, even if they only appeared once in the negative counts.</p>
<p>As with the positive reviews, it's actually hard to figure out exactly what the most common tokens for negative reviews are.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org2c952ac">
<h3 id="org2c952ac">Did our theory work?</h3>
<div class="outline-text-3" id="text-org2c952ac">
<p>Our theory was that key-words identify whether a review is positive or negative. There is some evidence for this, but really, it's not obvious that this is the case in general.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org03c8105">
<h2 id="org03c8105">Pickling</h2>
<div class="outline-text-2" id="text-org03c8105">
<p>Since the other posts in this section re-use some of this stuff it might make sense to pickle them.</p>
<div class="highlight">
<pre><span></span>with DataPath("total_counts.pkl", check_exists=False).from_folder.open("wb") as writer:
    pickle.dump(total_counts, writer)
</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/bike-sharing/bike-sharing-project-feedback/">Bike Sharing Project Feedback</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/bike-sharing/bike-sharing-project-feedback/" rel="bookmark"><time class="published dt-published" datetime="2018-11-05T12:55:10-08:00" itemprop="datePublished" title="2018-11-05 12:55">2018-11-05 12:55</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/bike-sharing/bike-sharing-project-feedback/#org71ffb0d">On the Number of Hidden Units</a></li>
<li><a href="/posts/nano/bike-sharing/bike-sharing-project-feedback/#org7ebbaa6">On the Learning Rate</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org71ffb0d">
<h2 id="org71ffb0d">On the Number of Hidden Units</h2>
<div class="outline-text-2" id="text-org71ffb0d">
<ul class="org-ul">
<li>Rule of thumb: halfway between number of inputs and outputs</li>
<li><a href="https://www.quora.com/How-do-I-decide-the-number-of-nodes-in-a-hidden-layer-of-a-neural-network-I-will-be-using-a-three-layer-model">Quora link</a></li>
</ul>
<p>\[8 \leq \text{number of hidden units} \leq \text{twice the number of inputs} \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-org7ebbaa6">
<h2 id="org7ebbaa6">On the Learning Rate</h2>
<div class="outline-text-2" id="text-org7ebbaa6">
<p>\[ 0.001 \leq \alpha \leq 0.1 \]</p>
<p>When considering the learning rate calculate \[\frac{\alpha}{\text{number of records}}\] and see if it's too small or too larg.e</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/sentiment_analysis/sentiment-classification-lectures/">Sentiment Classification Lectures</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/sentiment_analysis/sentiment-classification-lectures/" rel="bookmark"><time class="published dt-published" datetime="2018-11-04T14:17:10-08:00" itemprop="datePublished" title="2018-11-04 14:17">2018-11-04 14:17</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/sentiment_analysis/sentiment-classification-lectures/#org86528c1">Sentiment Classification & How To "Frame Problems" for a Neural Network</a></li>
<li><a href="/posts/nano/sentiment_analysis/sentiment-classification-lectures/#org7eb9da2">Set Up</a></li>
<li><a href="/posts/nano/sentiment_analysis/sentiment-classification-lectures/#orga7d1e43">Analysis: What's Going on in the Weights?</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org86528c1">
<h2 id="org86528c1">Sentiment Classification & How To "Frame Problems" for a Neural Network</h2>
<div class="outline-text-2" id="text-org86528c1">
<p>by Andrew Trask</p>
<ul class="org-ul">
<li><b>Twitter</b>: @iamtrask</li>
<li><b>Blog</b>: <a href="http://iamtrask.github.io">http://iamtrask.github.io</a></li>
</ul>
</div>
<div class="outline-3" id="outline-container-org403d7c5">
<h3 id="org403d7c5">What You Should Already Know</h3>
<div class="outline-text-3" id="text-org403d7c5">
<ul class="org-ul">
<li>neural networks, forward and back-propagation</li>
<li>stochastic gradient descent</li>
<li>mean squared error</li>
<li>and train/test splits</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orge438ca8">
<h3 id="orge438ca8">Where to Get Help if You Need it</h3>
<div class="outline-text-3" id="text-orge438ca8">
<ul class="org-ul">
<li>Re-watch previous Udacity Lectures</li>
<li>Leverage the recommended Course Reading Material - <a href="https://www.manning.com/books/grokking-deep-learning">Grokking Deep Learning</a></li>
<li>Shoot me Andrew a tweet @iamtrask</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7eb9da2">
<h2 id="org7eb9da2">Set Up</h2>
<div class="outline-text-2" id="text-org7eb9da2"></div>
<div class="outline-3" id="outline-container-org576bb76">
<h3 id="org576bb76">Debug</h3>
<div class="outline-text-3" id="text-org576bb76">
<div class="highlight">
<pre><span></span>%load_ext autoreload
%autoreload 2
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org86ffd82">
<h3 id="org86ffd82">Imports</h3>
<div class="outline-text-3" id="text-org86ffd82"></div>
<div class="outline-4" id="outline-container-org49a5560">
<h4 id="org49a5560">Python Standard Library</h4>
<div class="outline-text-4" id="text-org49a5560">
<div class="highlight">
<pre><span></span>from datetime import datetime
from functools import partial
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb2395eb">
<h4 id="orgb2395eb">From Pypi</h4>
<div class="outline-text-4" id="text-orgb2395eb">
<div class="highlight">
<pre><span></span>from graphviz import Graph
from tabulate import tabulate
import matplotlib.pyplot as pyplot
import numpy
import seaborn
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgef77c7c">
<h4 id="orgef77c7c">This Project</h4>
<div class="outline-text-4" id="text-orgef77c7c">
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPath
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd326937">
<h3 id="orgd326937">Tables</h3>
<div class="outline-text-3" id="text-orgd326937">
<div class="highlight">
<pre><span></span>table = partial(tabulate, tablefmt="orgtbl", headers="keys")
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org515ca99">
<h3 id="org515ca99">Printing</h3>
<div class="outline-text-3" id="text-org515ca99">
<div class="highlight">
<pre><span></span>%matplotlib inline
seaborn.set_style("whitegrid")
FIGURE_SIZE = (12, 10)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga7d1e43">
<h2 id="orga7d1e43">Analysis: What's Going on in the Weights?</h2>
<div class="outline-text-2" id="text-orga7d1e43">
<p>Let's start with a model that doesn't have any noise cancellation.</p>
<div class="highlight">
<pre><span></span>mlp_full = SentimentNoiseReduction(reviews=x_train, labels=y_train,
                                   lower_bound=0,
                                   polarity_cutoff=0,
                                   learning_rate=0.01)
</pre></div>
<div class="highlight">
<pre><span></span>mlp_full.train()
</pre></div>
<pre class="example">
Progress: 0.00 % Speed(reviews/sec): 0.00 Error: [-0.5] #Correct: 1 #Trained: 1 Training Accuracy: 100.00 %
Progress: 4.17 % Speed(reviews/sec): 100.00 Error: [-0.38320156] #Correct: 740 #Trained: 1001 Training Accuracy: 73.93 %
Progress: 8.33 % Speed(reviews/sec): 181.82 Error: [-0.26004622] #Correct: 1529 #Trained: 2001 Training Accuracy: 76.41 %
Progress: 12.50 % Speed(reviews/sec): 250.00 Error: [-0.40350302] #Correct: 2376 #Trained: 3001 Training Accuracy: 79.17 %
Progress: 16.67 % Speed(reviews/sec): 285.71 Error: [-0.23990249] #Correct: 3187 #Trained: 4001 Training Accuracy: 79.66 %
Progress: 20.83 % Speed(reviews/sec): 333.33 Error: [-0.14119144] #Correct: 4002 #Trained: 5001 Training Accuracy: 80.02 %
Progress: 25.00 % Speed(reviews/sec): 375.00 Error: [-0.06442389] #Correct: 4829 #Trained: 6001 Training Accuracy: 80.47 %
Progress: 29.17 % Speed(reviews/sec): 411.76 Error: [-0.03508728] #Correct: 5690 #Trained: 7001 Training Accuracy: 81.27 %
Progress: 33.33 % Speed(reviews/sec): 444.44 Error: [-0.05110633] #Correct: 6548 #Trained: 8001 Training Accuracy: 81.84 %
Progress: 37.50 % Speed(reviews/sec): 450.00 Error: [-0.07432703] #Correct: 7404 #Trained: 9001 Training Accuracy: 82.26 %
Progress: 41.67 % Speed(reviews/sec): 476.19 Error: [-0.26512013] #Correct: 8272 #Trained: 10001 Training Accuracy: 82.71 %
Progress: 45.83 % Speed(reviews/sec): 500.00 Error: [-0.14067275] #Correct: 9129 #Trained: 11001 Training Accuracy: 82.98 %
Progress: 50.00 % Speed(reviews/sec): 521.74 Error: [-0.01215903] #Correct: 9994 #Trained: 12001 Training Accuracy: 83.28 %
Progress: 54.17 % Speed(reviews/sec): 541.67 Error: [-0.33825111] #Correct: 10864 #Trained: 13001 Training Accuracy: 83.56 %
Progress: 58.33 % Speed(reviews/sec): 560.00 Error: [-0.00522004] #Correct: 11721 #Trained: 14001 Training Accuracy: 83.72 %
Progress: 62.50 % Speed(reviews/sec): 555.56 Error: [-0.49523538] #Correct: 12553 #Trained: 15001 Training Accuracy: 83.68 %
Progress: 66.67 % Speed(reviews/sec): 571.43 Error: [-0.20026672] #Correct: 13390 #Trained: 16001 Training Accuracy: 83.68 %
Progress: 70.83 % Speed(reviews/sec): 586.21 Error: [-0.20786817] #Correct: 14243 #Trained: 17001 Training Accuracy: 83.78 %
Progress: 75.00 % Speed(reviews/sec): 580.65 Error: [-0.03469862] #Correct: 15108 #Trained: 18001 Training Accuracy: 83.93 %
Progress: 79.17 % Speed(reviews/sec): 593.75 Error: [-0.99460657] #Correct: 15982 #Trained: 19001 Training Accuracy: 84.11 %
Progress: 83.33 % Speed(reviews/sec): 606.06 Error: [-0.0523489] #Correct: 16867 #Trained: 20001 Training Accuracy: 84.33 %
Progress: 87.50 % Speed(reviews/sec): 617.65 Error: [-0.28370015] #Correct: 17734 #Trained: 21001 Training Accuracy: 84.44 %
Progress: 91.67 % Speed(reviews/sec): 611.11 Error: [-0.33222958] #Correct: 18616 #Trained: 22001 Training Accuracy: 84.61 %
Progress: 95.83 % Speed(reviews/sec): 621.62 Error: [-0.17177784] #Correct: 19475 #Trained: 23001 Training Accuracy: 84.67 %
Training Time: 0:00:38.794351
</pre>
<p>Now here's a function to find the similarity of words in the vocabulary to a word, based on the dot product of the weights from the input layer to the hidden layer.</p>
<div class="highlight">
<pre><span></span>def get_most_similar_words(focus: str="horrible", count:int=10) -&gt; list:
    """Returns a list of similar words based on weights"""
    most_similar = Counter()
    for word in mlp_full.word_to_index:
        most_similar[word] = numpy.dot(
            mlp_full.weights_input_to_hidden[mlp_full.word_to_index[word]],
            mlp_full.weights_input_to_hidden[mlp_full.word_to_index[focus]])    
    return most_similar.most_common(count)
</pre></div>
<div class="highlight">
<pre><span></span>print(get_most_similar_words("excellent"))
</pre></div>
<pre class="example">
[('excellent', 0.14672474869646132), ('perfect', 0.12529721850063252), ('great', 0.1072983586254582), ('amazing', 0.10168346112776101), ('wonderful', 0.0971402564667566), ('best', 0.09640599864254018), ('today', 0.09064606014006837), ('fun', 0.08859560811231239), ('loved', 0.07914150763452406), ('definitely', 0.07693307843353574)]

</pre>
<p><i>excellent</i> was, ouf course, most similar to itself, but we can see that the network's weights are most similar to each other when the words are most similar to each other - the network has 'learned' what words are similar to <i>excellent</i> using the training set.</p>
<p>Now a negative example.</p>
<div class="highlight">
<pre><span></span>print(get_most_similar_words("terrible"))
</pre></div>
<pre class="example">
[('worst', 0.1761389721390966), ('awful', 0.12576492326546337), ('waste', 0.11989143949659276), ('poor', 0.10186721140388931), ('boring', 0.09740050873489904), ('terrible', 0.09719144477251088), ('bad', 0.08198016341605044), ('dull', 0.0812576973066953), ('worse', 0.07504920898991188), ('poorly', 0.07494303321254764)]

</pre>
<p>Once again, the more similar words were in sentiment, the closer the weights leading from their inputs became.</p>
<div class="highlight">
<pre><span></span>import matplotlib.colors as colors

words_to_visualize = list()
for word, ratio in pos_neg_ratios.most_common(500):
    if(word in mlp_full.word_to_index):
        words_to_visualize.append(word)

for word, ratio in list(reversed(pos_neg_ratios.most_common()))[0:500]:
    if(word in mlp_full.word_to_index):
        words_to_visualize.append(word)
</pre></div>
<div class="highlight">
<pre><span></span>pos = 0
neg = 0

colors_list = list()
vectors_list = list()
for word in words_to_visualize:
    if word in pos_neg_ratios.keys():
        vectors_list.append(mlp_full.weights_input_to_hidden[mlp_full.word_to_index[word]])
        if(pos_neg_ratios[word] &gt; 0):
            pos+=1
            colors_list.append("#00ff00")
        else:
            neg+=1
            colors_list.append("#000000")
</pre></div>
<div class="highlight">
<pre><span></span>from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=0)
words_top_ted_tsne = tsne.fit_transform(vectors_list)
</pre></div>
<p>p = figure(tools="pan,wheel_zoom,reset,save", toolbar_location="above", title="vector T-SNE for most polarized words")</p>
<p>source = ColumnDataSource(data=dict(x1=words_top_ted_tsne[:,0], x2=words_top_ted_tsne[:,1], names=words_to_visualize, color=colors_list))</p>
<p>p.scatter(x="x1", y="x2", size=8, source=source, fill_color="color")</p>
<p>word_labels = LabelSet(x="x1", y="x2", text="names", y_offset=6, text_font_size="8pt", text_color="#555555", source=source, text_align='center') p.add_layout(word_labels)</p>
<p>show(p)</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/notes/notes-on-the-deep-learning-revolution/">Notes on The Deep Learning Revolution</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/notes/notes-on-the-deep-learning-revolution/" rel="bookmark"><time class="published dt-published" datetime="2018-11-01T14:18:50-07:00" itemprop="datePublished" title="2018-11-01 14:18">2018-11-01 14:18</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/notes/notes-on-the-deep-learning-revolution/#orgbf67baa">Intelligence Reimagined (Where did this come from?)</a></li>
<li><a href="/posts/notes/notes-on-the-deep-learning-revolution/#orgdc92910">Many Ways To Learn (How does it work?)</a></li>
<li><a href="/posts/notes/notes-on-the-deep-learning-revolution/#org8ae3125">Technological and Scientific Impact (What has it done and what might it do?)</a></li>
<li><a href="/posts/notes/notes-on-the-deep-learning-revolution/#orgc4a11b0">Citation</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgbf67baa">
<h2 id="orgbf67baa">Intelligence Reimagined (Where did this come from?)</h2>
<div class="outline-text-2" id="text-orgbf67baa"></div>
<div class="outline-3" id="outline-container-org1c5330b">
<h3 id="org1c5330b">Timeline</h3>
<div class="outline-text-3" id="text-org1c5330b">
<ul class="org-ul">
<li>1956: Dartmouth Artificial Intelligence Summer Research Project - start of the field of Artificial Intelligence.</li>
<li>1962: Frank Rosenblatt publishes description of the Perceptron</li>
<li>1962: David Huble and Torsten Wiesel report first recordings of responses from neurons</li>
<li>1969: Marvin Minsky and Seymour Papert point out limits of perceptron, triggering the <i>AI Winter</i></li>
<li>1979: Geoffrey Hinton and James Anderson organize Parallel Models of Associative Memory workshop to gather researchers working on neural networks</li>
<li>1987: First Neural Information Processing Systems (NIPS) conference held, bringing machine learning reasearchers together</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orgeb8dbfb">
<h3 id="orgeb8dbfb">The Rise of Machine Learning</h3>
<div class="outline-text-3" id="text-orgeb8dbfb"></div>
<div class="outline-4" id="outline-container-orga15166e">
<h4 id="orga15166e">What is deep learning?</h4>
<div class="outline-text-4" id="text-orga15166e">
<p><i>Deep Learning</i> is a form of machine learning that uses data to train artificial neural networks to do things. When the field of artificial intelligence began in the 1950s there were two camps - one that believed the path to intelligenc lay in using formal logic and writing computer programs, and one that believe intelligence would come by learning directly from data. Deep Learning belongs to the second camp, and although it has been around for a long time, only once we had enough computational power and data was it able to compete.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org482ab93">
<h4 id="org482ab93">How did self-driving cars come about?</h4>
<div class="outline-text-4" id="text-org482ab93">
<ul class="org-ul">
<li>In 2005 a group from Stanford lead by Sebastian Thrun won the <a href="https://en.wikipedia.org/wiki/DARPA_Grand_Challenge_(2005)">DARPA Grand Challenge</a>. This was the second Darpa challenge and the first where (five) vehicles were able to finish.</li>
<li>Some see self-driving cars as a way to remake society:
<ul class="org-ul">
<li>no need to own a car, use a just-in-time service</li>
<li>No need for parking lots and so many lanes on the road</li>
<li>Travel time can be productive</li>
<li>Once one car learns something it can be taught to all the other cars so 'rare' events will be handled even if it is the first time a car sees the event.</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-4" id="outline-container-orgcd22133">
<h4 id="orgcd22133">How do machines translate languages?</h4>
<div class="outline-text-4" id="text-orgcd22133">
<p>Originally they worked using a statistical approach, looking for familiar word combinations and counts. Now they are able to keep longer sections of text which improves the translation because there is more seen in contetxt. The hope is that when they can be expanded to learn paragraphs or an author's entire body of work, then they can learn more subtleties and the poetry of the text.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org01c0fa3">
<h4 id="org01c0fa3">What's the big deal about speech recognition?</h4>
<div class="outline-text-4" id="text-org01c0fa3">
<p>Some people think that the next interface to our machines will be the human voice. There have already been demonstrations of live translations made using computer speech recognition and translation.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org1509f73">
<h4 id="org1509f73">How good is machine learning at playing poker?</h4>
<div class="outline-text-4" id="text-org1509f73">
<p>DeepStack played poker against professional poker players and beat all of them. This is important because the nature of the game means that every player is working with imperfect information (the unseen cards and the other players' cards). This could imply that machine learning could be used in other places where you don't have all the information, like politics and negotiations.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org29f1cbc">
<h4 id="org29f1cbc">Does artificial intelligence pose a threat to humanity?</h4>
<div class="outline-text-4" id="text-org29f1cbc">
<p>If you look an the areas where deep learning managed to outdo human competitors (e.g. Alpha Go), what eventually happened was that the human players were able to learn moves from the Artificial Intelligence that they would likely not have come up with themselves. This points the way to the immediate future of Artificial Intelligence. Although AI can sometimes outperform humans, the more open-ended the problem, the more it is likely that humans and machines can complement each other, with the machine creating outcomes we could never think of and the humans contributing the expertise needed as a human to solve human problems. AI is, so far, more of a complement to human intelligence, not a replacement for it.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7bd164e">
<h3 id="org7bd164e">The Rebirth of Artificial Intelligence</h3>
</div>
<div class="outline-3" id="outline-container-org1a4a159">
<h3 id="org1a4a159">The Dawn of Neural Networks</h3>
</div>
<div class="outline-3" id="outline-container-orgd6cc811">
<h3 id="orgd6cc811">Brain-style Computing</h3>
</div>
<div class="outline-3" id="outline-container-orgaf73792">
<h3 id="orgaf73792">Insights from the Visual System</h3>
</div>
</div>
<div class="outline-2" id="outline-container-orgdc92910">
<h2 id="orgdc92910">Many Ways To Learn (How does it work?)</h2>
<div class="outline-text-2" id="text-orgdc92910"></div>
<div class="outline-3" id="outline-container-org68b0a5f">
<h3 id="org68b0a5f">The Cocktail Party Problem</h3>
</div>
<div class="outline-3" id="outline-container-orgf246e8a">
<h3 id="orgf246e8a">The Hopfield Net and Boltzmann Machine</h3>
</div>
<div class="outline-3" id="outline-container-orgbb8a576">
<h3 id="orgbb8a576">Backpropagating Errors</h3>
</div>
<div class="outline-3" id="outline-container-org72781a9">
<h3 id="org72781a9">Convolutional Learning</h3>
</div>
<div class="outline-3" id="outline-container-org59b8e93">
<h3 id="org59b8e93">Reward Learning</h3>
</div>
<div class="outline-3" id="outline-container-org418cc1b">
<h3 id="org418cc1b">Neural Information Processing Systems</h3>
</div>
</div>
<div class="outline-2" id="outline-container-org8ae3125">
<h2 id="org8ae3125">Technological and Scientific Impact (What has it done and what might it do?)</h2>
<div class="outline-text-2" id="text-org8ae3125"></div>
<div class="outline-3" id="outline-container-org6a206d2">
<h3 id="org6a206d2">The Future of Machine Learning</h3>
</div>
<div class="outline-3" id="outline-container-orgcedcd70">
<h3 id="orgcedcd70">The Age of Algorithms</h3>
</div>
<div class="outline-3" id="outline-container-org29bd011">
<h3 id="org29bd011">Hello, Mr. Chips</h3>
</div>
<div class="outline-3" id="outline-container-org5952ae0">
<h3 id="org5952ae0">Inside Information</h3>
</div>
<div class="outline-3" id="outline-container-orgd47cfbc">
<h3 id="orgd47cfbc">Conscousness</h3>
</div>
<div class="outline-3" id="outline-container-org8cf659b">
<h3 id="org8cf659b">Nature Is Cleverer Than We Are</h3>
</div>
<div class="outline-3" id="outline-container-orge8d079f">
<h3 id="orge8d079f">Deep Intelligence</h3>
</div>
</div>
<div class="outline-2" id="outline-container-orgc4a11b0">
<h2 id="orgc4a11b0">Citation</h2>
<div class="outline-text-2" id="text-orgc4a11b0"></div>
<div class="outline-3" id="outline-container-org5ae2373">
<h3 id="org5ae2373">[TDLR] Sejnowski TJ. The deep learning revolution. MIT Press; 2018 Oct 23.</h3>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/notes/reading-list/">Reading List</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/notes/reading-list/" rel="bookmark"><time class="published dt-published" datetime="2018-11-01T13:34:48-07:00" itemprop="datePublished" title="2018-11-01 13:34">2018-11-01 13:34</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/notes/reading-list/#org6231c33">Books</a></li>
<li><a href="/posts/notes/reading-list/#org66c1897">Links</a></li>
<li><a href="/posts/notes/reading-list/#org37de1ca">Papers</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org6231c33">
<h2 id="org6231c33">Books</h2>
<div class="outline-text-2" id="text-org6231c33"></div>
<div class="outline-3" id="outline-container-org10c41dc">
<h3 id="org10c41dc">Deep Learning</h3>
<div class="outline-text-3" id="text-org10c41dc"></div>
<div class="outline-4" id="outline-container-org981c432">
<h4 id="org981c432">[TDLR] Sejnowski TJ. The deep learning revolution. MIT Press; 2018 Oct 23.</h4>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org66c1897">
<h2 id="org66c1897">Links</h2>
<div class="outline-text-2" id="text-org66c1897">
<ul class="org-ul">
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a> - halfway between <i>Grokking Deep Learning</i> and the <a href="http://www.deeplearningbook.org/">Deep Learning Textbook</a></li>
<li><a href="http://www.deeplearningbook.org/">Deep Learning</a> - textbook written by the guys who came up with it</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org37de1ca">
<h2 id="org37de1ca">Papers</h2>
<div class="outline-text-2" id="text-org37de1ca">
<ul class="org-ul">
<li><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a></li>
<li><a href="https://arxiv.org/pdf/1502.01852v1.pdf">Delving Deep Into Rectifiers</a></li>
<li><a href="https://arxiv.org/pdf/1502.03167v2.pdf">Batch Normalization</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/bike-sharing/bike-sharing-project-answers/">Bike Sharing Project Answers</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/bike-sharing/bike-sharing-project-answers/" rel="bookmark"><time class="published dt-published" datetime="2018-10-30T15:31:25-07:00" itemprop="datePublished" title="2018-10-30 15:31">2018-10-30 15:31</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/bike-sharing/bike-sharing-project-answers/#org773d838">Introduction</a></li>
<li><a href="/posts/nano/bike-sharing/bike-sharing-project-answers/#org95d0aed">Imports</a></li>
<li><a href="/posts/nano/bike-sharing/bike-sharing-project-answers/#orgdc6f231">The Neural Network</a></li>
<li><a href="/posts/nano/bike-sharing/bike-sharing-project-answers/#org5978068">The Hyper Parameters</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org773d838">
<h2 id="org773d838">Introduction</h2>
<div class="outline-text-2" id="text-org773d838">
<p>The Bike Sharing Project uses a neural network to predict daily ridership for a bike sharing service. The code is split into two parts - a jupyter notebook that you work with and a python file (<code>my_answers.py</code>) where you put the parts of the code that isn't provided. This creates the <code>my_answer.py</code> file.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org95d0aed">
<h2 id="org95d0aed">Imports</h2>
<div class="outline-text-2" id="text-org95d0aed">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgdc6f231">
<h2 id="orgdc6f231">The Neural Network</h2>
<div class="outline-text-2" id="text-orgdc6f231">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">"""Implementation of a neural network with one hidden layer</span>

<span class="sd">    Args:</span>
<span class="sd">     input_nodes: number of input nodes</span>
<span class="sd">     hidden_nodes: number of hidden nodes</span>
<span class="sd">     output_nodes: number of output_nodes</span>
<span class="sd">     learning_rate: rate at which to update the weights</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_nodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Set number of nodes in input, hidden and output layers.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span> <span class="o">=</span> <span class="n">input_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span> <span class="o">=</span> <span class="n">hidden_nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span> <span class="o">=</span> <span class="n">output_nodes</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>

        <span class="c1"># Initialize weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">return</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgd749523">
<h3 id="orgd749523">Input To Hidden Weights</h3>
<div class="outline-text-3" id="text-orgd749523">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">weights_input_to_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Array of weights from input layer to the hidden layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
            <span class="mf">0.0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">,</span> 
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span>
</pre></div>
<p>The unit-test tries to set the weights so we need a setter.</p>
<div class="highlight">
<pre><span></span><span class="nd">@weights_input_to_hidden.setter</span>
<span class="k">def</span> <span class="nf">weights_input_to_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""Sets the weights"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weights_input_to_hidden</span> <span class="o">=</span> <span class="n">weights</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org4d41b3c">
<h3 id="org4d41b3c">Hidden To Output Weights</h3>
<div class="outline-text-3" id="text-org4d41b3c">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">weights_hidden_to_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Array of weights for edges from hidden layer to output"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
            <span class="mf">0.0</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">,</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_nodes</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span>
</pre></div>
<p>Once again, this is for the unit-testing.</p>
<div class="highlight">
<pre><span></span><span class="nd">@weights_hidden_to_output.setter</span>
<span class="k">def</span> <span class="nf">weights_hidden_to_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sd">"""sets the weights for edges from hidden layer to output"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_weights_hidden_to_output</span> <span class="o">=</span> <span class="n">weights</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd027ec2">
<h3 id="orgd027ec2">Activation Function</h3>
<div class="outline-text-3" id="text-orgd027ec2">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">activation_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">"""A pass-through to the sigmoid"""</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org68992e6">
<h3 id="org68992e6">Sigmoid</h3>
<div class="outline-text-3" id="text-org68992e6">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="sd">"""Calculates the sigmoid of the value"""</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">value</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd9c3bfb">
<h3 id="orgd9c3bfb">Train</h3>
<div class="outline-text-3" id="text-orgd9c3bfb">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="sd">''' Train the network on batch of features and targets. </span>

<span class="sd">       Arguments</span>
<span class="sd">       ---------</span>

<span class="sd">       features: 2D array, each row is one data record, each column is a feature</span>
<span class="sd">       targets: 1D array of target values</span>

<span class="sd">    '''</span>
    <span class="n">n_records</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">delta_weights_i_h</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">delta_weights_h_o</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
   <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>            
        <span class="n">final_outputs</span><span class="p">,</span> <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_pass_train</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Implement the backpropagation function below</span>
        <span class="n">delta_weights_i_h</span><span class="p">,</span> <span class="n">delta_weights_h_o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backpropagation</span><span class="p">(</span>
            <span class="n">final_outputs</span><span class="p">,</span> <span class="n">hidden_outputs</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
            <span class="n">delta_weights_i_h</span><span class="p">,</span> <span class="n">delta_weights_h_o</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">update_weights</span><span class="p">(</span><span class="n">delta_weights_i_h</span><span class="p">,</span> <span class="n">delta_weights_h_o</span><span class="p">,</span> <span class="n">n_records</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf345927">
<h3 id="orgf345927">Forward Pass Train</h3>
<div class="outline-text-3" id="text-orgf345927">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">forward_pass_train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">''' Implement forward pass here </span>

<span class="sd">       Arguments</span>
<span class="sd">       ---------</span>
<span class="sd">       X: features batch</span>

<span class="sd">    '''</span>
    <span class="n">hidden_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">)</span>
    <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">hidden_inputs</span><span class="p">)</span>

    <span class="n">final_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hidden_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="p">)</span>
    <span class="n">final_outputs</span> <span class="o">=</span> <span class="n">final_inputs</span>
    <span class="k">return</span> <span class="n">final_outputs</span><span class="p">,</span> <span class="n">hidden_outputs</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgce25a6f">
<h3 id="orgce25a6f">Back Propagation</h3>
<div class="outline-text-3" id="text-orgce25a6f">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">backpropagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">final_outputs</span><span class="p">,</span> <span class="n">hidden_outputs</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">delta_weights_i_h</span><span class="p">,</span> <span class="n">delta_weights_h_o</span><span class="p">):</span>
    <span class="sd">''' Implement backpropagation</span>

<span class="sd">       Arguments</span>
<span class="sd">       ---------</span>
<span class="sd">       final_outputs: output from forward pass</span>
<span class="sd">       y: target (i.e. label) batch</span>
<span class="sd">       delta_weights_i_h: change in weights from input to hidden layers</span>
<span class="sd">       delta_weights_h_o: change in weights from hidden to output layers</span>

<span class="sd">    '''</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">final_outputs</span> <span class="o">-</span> <span class="n">y</span>

    <span class="n">hidden_error</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="p">,</span> <span class="n">error</span><span class="p">)</span>

    <span class="n">output_error_term</span> <span class="o">=</span> <span class="n">error</span>

    <span class="n">hidden_error_term</span> <span class="o">=</span> <span class="n">hidden_error</span> <span class="o">*</span> <span class="n">hidden_outputs</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hidden_outputs</span><span class="p">)</span>

    <span class="n">delta_weights_i_h</span> <span class="o">+=</span> <span class="o">-</span><span class="n">hidden_error_term</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

    <span class="n">delta_weights_h_o</span> <span class="o">+=</span> <span class="o">-</span><span class="n">output_error_term</span> <span class="o">*</span> <span class="n">hidden_outputs</span><span class="p">[:,</span><span class="bp">None</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">delta_weights_i_h</span><span class="p">,</span> <span class="n">delta_weights_h_o</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org4745637">
<h3 id="org4745637">Update Weights</h3>
<div class="outline-text-3" id="text-org4745637">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta_weights_i_h</span><span class="p">,</span> <span class="n">delta_weights_h_o</span><span class="p">,</span> <span class="n">n_records</span><span class="p">):</span>
    <span class="sd">''' Update weights on gradient descent step</span>

<span class="sd">       Arguments</span>
<span class="sd">       ---------</span>
<span class="sd">       delta_weights_i_h: change in weights from input to hidden layers</span>
<span class="sd">       delta_weights_h_o: change in weights from hidden to output layers</span>
<span class="sd">       n_records: number of records</span>

<span class="sd">    '''</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">delta_weights_h_o</span><span class="o">/</span><span class="n">n_records</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">delta_weights_i_h</span><span class="o">/</span><span class="n">n_records</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org33af958">
<h3 id="org33af958">Run</h3>
<div class="outline-text-3" id="text-org33af958">
<p><b>Warning:</b> The MSE function defined in the jupyter notebook won't work if you use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html">numpy.dot</a> instead of <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html">numpy.matmul</a>. You can make it work by passing in <code>axis=1</code> to <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html">numpy.mean</a> but I don't think you're allowed to change the things in the jupyter notebook.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
    <span class="sd">''' Run a forward pass through the network with input features </span>

<span class="sd">       Arguments</span>
<span class="sd">       ---------</span>
<span class="sd">       features: 1D array of feature values</span>
<span class="sd">    '''</span>

    <span class="n">hidden_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_input_to_hidden</span><span class="p">)</span>
    <span class="n">hidden_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_function</span><span class="p">(</span><span class="n">hidden_inputs</span><span class="p">)</span> 

    <span class="n">final_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">hidden_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_hidden_to_output</span><span class="p">)</span>
    <span class="n">final_outputs</span> <span class="o">=</span> <span class="n">final_inputs</span>        
    <span class="k">return</span> <span class="n">final_outputs</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5978068">
<h2 id="org5978068">The Hyper Parameters</h2>
<div class="outline-text-2" id="text-org5978068">
<div class="highlight">
<pre><span></span><span class="n">iterations</span> <span class="o">=</span> <span class="mi">7500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">hidden_nodes</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">output_nodes</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/nano/bike-sharing/the-bike-sharing-project/">The Bike Sharing Project</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/bike-sharing/the-bike-sharing-project/" rel="bookmark"><time class="published dt-published" datetime="2018-10-30T13:34:56-07:00" itemprop="datePublished" title="2018-10-30 13:34">2018-10-30 13:34</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org3d6f8cd">Introduction</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#orgb769e8b">Jupyter Setup</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org4adfb42">Imports</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org69f369c">Set Up</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org39af1f1">The Data</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org116ad83">Checking out the data</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org6d923e5">Dummy variables</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org49b43d6">Scaling target variables</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org0895995">Splitting the data into training, testing, and validation sets</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#orgc2093ee">Time to build the network</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org4fdbf06">Unit tests</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#org48c2294">Training the network</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#orgfdef998">Check out your predictions</a></li>
<li><a href="/posts/nano/bike-sharing/the-bike-sharing-project/#orga0d946d">More Variations</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org3d6f8cd">
<h2 id="org3d6f8cd">Introduction</h2>
<div class="outline-text-2" id="text-org3d6f8cd">
<p>This project builds a neural network and uses it to predict daily bike rental ridership.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgb769e8b">
<h2 id="orgb769e8b">Jupyter Setup</h2>
<div class="outline-text-2" id="text-orgb769e8b">
<p>This sets some "magic" jupyter values.</p>
<p>Display Matplotlib plots.</p>
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
</pre></div>
<p>Reload code from other modules that has changed (otherwise even if you re-run the import the changes won't get picked up).</p>
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')
</pre></div>
<p>I couldn't find any documentation on this other than people asking how to get it to work. I think it means <a href="https://matplotlib.org/users/prev_whats_new/whats_new_2.0.0.html#support-for-hidpi-retina-displays-in-the-nbagg-and-webagg-backends">to use a higher resolution if your display supports it</a>.</p>
<div class="highlight">
<pre><span></span># get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org4adfb42">
<h2 id="org4adfb42">Imports</h2>
<div class="outline-text-2" id="text-org4adfb42"></div>
<div class="outline-3" id="outline-container-orgf7ffd9f">
<h3 id="orgf7ffd9f">Python Standard Library</h3>
<div class="outline-text-3" id="text-orgf7ffd9f">
<div class="highlight">
<pre><span></span>from collections import namedtuple
from functools import partial
from datetime import datetime
import unittest
import sys
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgba9acc9">
<h3 id="orgba9acc9">From PyPi</h3>
<div class="outline-text-3" id="text-orgba9acc9">
<div class="highlight">
<pre><span></span>from graphviz import Digraph
from tabulate import tabulate
import numpy
import pandas
import matplotlib.pyplot as pyplot
import seaborn
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1df60e6">
<h3 id="org1df60e6">This Project</h3>
<div class="outline-text-3" id="text-org1df60e6">
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPath
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgef70d50">
<h3 id="orgef70d50">The Submission</h3>
<div class="outline-text-3" id="text-orgef70d50">
<p>The submission is set up so that you provide a separate python file where you implement the neural network, so this imports it.</p>
<div class="highlight">
<pre><span></span>from my_answers import (
    NeuralNetwork,
    iterations,
    learning_rate,
    hidden_nodes,
    output_nodes)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org69f369c">
<h2 id="org69f369c">Set Up</h2>
<div class="outline-text-2" id="text-org69f369c"></div>
<div class="outline-3" id="outline-container-org6ecfaff">
<h3 id="org6ecfaff">Tables</h3>
<div class="outline-text-3" id="text-org6ecfaff">
<div class="highlight">
<pre><span></span>table = partial(tabulate, tablefmt="orgtbl", headers="keys", showindex=False)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9d5ecd5">
<h3 id="org9d5ecd5">Plotting</h3>
<div class="outline-text-3" id="text-org9d5ecd5">
<div class="highlight">
<pre><span></span>seaborn.set_style("whitegrid", rc={"axes.grid": False})
FIGURE_SIZE = (12, 10)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org39af1f1">
<h2 id="org39af1f1">The Data</h2>
<div class="outline-text-2" id="text-org39af1f1">
<p>The data comes from the <a href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset">UCI Machine Learning Repository</a> (I think). It combines <a href="https://www.capitalbikeshare.com/system-data">Capital Bikeshare data</a>, <a href="http://www.freemeteo.com">Weather Data from i-Weather</a>, and <a href="https://dchr.dc.gov/page/holiday-schedules">Washington D.C. holiday information</a>. The authors note that becaus the bikes are tracked when they are checked out and when they arrive they have become a "virtual sensor network" that tracks how people move through the city (by shared bicycle, at least).</p>
<p>This first bit is the original path that you need for a submission, which is different from where I'm keeping it while working on this. I'm adding an <code>EXPECTED_DATA_PATH</code> variable because that's being checked in the unit-test for some reason, and I'll need to change it for the submission.</p>
<pre class="example">
data_path = 'Bike-Sharing-Dataset/hour.csv'
EXPECTED_DATA_PATH = data_path.lower()
</pre>
<p>This is where it's kept for this post.</p>
<div class="highlight">
<pre><span></span>path = DataPath("hour.csv")
data_path = str(path.from_folder)
EXPECTED_DATA_PATH = data_path.lower()
print(path.from_folder)
</pre></div>
<pre class="example">
../../../data/bike-sharing/hour.csv

</pre>
<div class="highlight">
<pre><span></span>rides = pandas.read_csv(data_path)
</pre></div>
<div class="highlight">
<pre><span></span>print(table(rides.head()))
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">instant</th>
<th class="org-right" scope="col">dteday</th>
<th class="org-right" scope="col">season</th>
<th class="org-right" scope="col">yr</th>
<th class="org-right" scope="col">mnth</th>
<th class="org-right" scope="col">hr</th>
<th class="org-right" scope="col">holiday</th>
<th class="org-right" scope="col">weekday</th>
<th class="org-right" scope="col">workingday</th>
<th class="org-right" scope="col">weathersit</th>
<th class="org-right" scope="col">temp</th>
<th class="org-right" scope="col">atemp</th>
<th class="org-right" scope="col">hum</th>
<th class="org-right" scope="col">windspeed</th>
<th class="org-right" scope="col">casual</th>
<th class="org-right" scope="col">registered</th>
<th class="org-right" scope="col">cnt</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.24</td>
<td class="org-right">0.2879</td>
<td class="org-right">0.81</td>
<td class="org-right">0</td>
<td class="org-right">3</td>
<td class="org-right">13</td>
<td class="org-right">16</td>
</tr>
<tr>
<td class="org-right">2</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.22</td>
<td class="org-right">0.2727</td>
<td class="org-right">0.8</td>
<td class="org-right">0</td>
<td class="org-right">8</td>
<td class="org-right">32</td>
<td class="org-right">40</td>
</tr>
<tr>
<td class="org-right">3</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.22</td>
<td class="org-right">0.2727</td>
<td class="org-right">0.8</td>
<td class="org-right">0</td>
<td class="org-right">5</td>
<td class="org-right">27</td>
<td class="org-right">32</td>
</tr>
<tr>
<td class="org-right">4</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">3</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.24</td>
<td class="org-right">0.2879</td>
<td class="org-right">0.75</td>
<td class="org-right">0</td>
<td class="org-right">3</td>
<td class="org-right">10</td>
<td class="org-right">13</td>
</tr>
<tr>
<td class="org-right">5</td>
<td class="org-right">2011-01-01</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">4</td>
<td class="org-right">0</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.24</td>
<td class="org-right">0.2879</td>
<td class="org-right">0.75</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span>print(len(rides.dteday.unique()))
</pre></div>
<pre class="example">
731

</pre>
<div class="highlight">
<pre><span></span>print(table(rides.describe(), showindex=True))
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">&nbsp;</th>
<th class="org-right" scope="col">instant</th>
<th class="org-right" scope="col">season</th>
<th class="org-right" scope="col">yr</th>
<th class="org-right" scope="col">mnth</th>
<th class="org-right" scope="col">hr</th>
<th class="org-right" scope="col">holiday</th>
<th class="org-right" scope="col">weekday</th>
<th class="org-right" scope="col">workingday</th>
<th class="org-right" scope="col">weathersit</th>
<th class="org-right" scope="col">temp</th>
<th class="org-right" scope="col">atemp</th>
<th class="org-right" scope="col">hum</th>
<th class="org-right" scope="col">windspeed</th>
<th class="org-right" scope="col">casual</th>
<th class="org-right" scope="col">registered</th>
<th class="org-right" scope="col">cnt</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">count</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
<td class="org-right">17379</td>
</tr>
<tr>
<td class="org-left">mean</td>
<td class="org-right">8690</td>
<td class="org-right">2.50164</td>
<td class="org-right">0.502561</td>
<td class="org-right">6.53778</td>
<td class="org-right">11.5468</td>
<td class="org-right">0.0287704</td>
<td class="org-right">3.00368</td>
<td class="org-right">0.682721</td>
<td class="org-right">1.42528</td>
<td class="org-right">0.496987</td>
<td class="org-right">0.475775</td>
<td class="org-right">0.627229</td>
<td class="org-right">0.190098</td>
<td class="org-right">35.6762</td>
<td class="org-right">153.787</td>
<td class="org-right">189.463</td>
</tr>
<tr>
<td class="org-left">std</td>
<td class="org-right">5017.03</td>
<td class="org-right">1.10692</td>
<td class="org-right">0.500008</td>
<td class="org-right">3.43878</td>
<td class="org-right">6.91441</td>
<td class="org-right">0.167165</td>
<td class="org-right">2.00577</td>
<td class="org-right">0.465431</td>
<td class="org-right">0.639357</td>
<td class="org-right">0.192556</td>
<td class="org-right">0.17185</td>
<td class="org-right">0.19293</td>
<td class="org-right">0.12234</td>
<td class="org-right">49.305</td>
<td class="org-right">151.357</td>
<td class="org-right">181.388</td>
</tr>
<tr>
<td class="org-left">min</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.02</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>
<tr>
<td class="org-left">25%</td>
<td class="org-right">4345.5</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">4</td>
<td class="org-right">6</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0.34</td>
<td class="org-right">0.3333</td>
<td class="org-right">0.48</td>
<td class="org-right">0.1045</td>
<td class="org-right">4</td>
<td class="org-right">34</td>
<td class="org-right">40</td>
</tr>
<tr>
<td class="org-left">50%</td>
<td class="org-right">8690</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">7</td>
<td class="org-right">12</td>
<td class="org-right">0</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0.5</td>
<td class="org-right">0.4848</td>
<td class="org-right">0.63</td>
<td class="org-right">0.194</td>
<td class="org-right">17</td>
<td class="org-right">115</td>
<td class="org-right">142</td>
</tr>
<tr>
<td class="org-left">75%</td>
<td class="org-right">13034.5</td>
<td class="org-right">3</td>
<td class="org-right">1</td>
<td class="org-right">10</td>
<td class="org-right">18</td>
<td class="org-right">0</td>
<td class="org-right">5</td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">0.66</td>
<td class="org-right">0.6212</td>
<td class="org-right">0.78</td>
<td class="org-right">0.2537</td>
<td class="org-right">48</td>
<td class="org-right">220</td>
<td class="org-right">281</td>
</tr>
<tr>
<td class="org-left">max</td>
<td class="org-right">17379</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">12</td>
<td class="org-right">23</td>
<td class="org-right">1</td>
<td class="org-right">6</td>
<td class="org-right">1</td>
<td class="org-right">4</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0.8507</td>
<td class="org-right">367</td>
<td class="org-right">886</td>
<td class="org-right">977</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span>print(len(rides.dteday.unique()) * 24)
</pre></div>
<pre class="example">
17544

</pre>
<p>So there appear to be some hours missing, since there aren't enough rows in the data set.</p>
<div class="highlight">
<pre><span></span>print("First Hour: {} {}".format(
    rides.dteday.min(),
    rides[rides.dteday == rides.dteday.min()].hr.min()))
print("Last Hour: {} {}".format(
    rides.dteday.max(),
    rides[rides.dteday == rides.dteday.max()].hr.max()))
</pre></div>
<pre class="example">
First Hour: 2011-01-01 0
Last Hour: 2012-12-31 23

</pre>
<p>Well, that's odd. It looks like the span is complete, why are there missing hours?</p>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
counts = rides.groupby(["dteday"]).hr.count()
axe.set_title("Hours Recorded Per Day")
axe.set_xlabel("Day")
axe.set_ylabel("Count")
ax = axe.plot(range(len(counts.index)), counts.values, "o", markerfacecolor='None')
</pre></div>
<div class="figure">
<p><img alt="date_hours.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/date_hours.png"></p>
</div>
<p>So it looks like some days they didn't manage to record all the hours.</p>
<p>Assuming this is the UC Irvine dataset, this is the description of the variables.</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Variable</th>
<th class="org-left" scope="col">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">instant</td>
<td class="org-left">record index</td>
</tr>
<tr>
<td class="org-left">dteday</td>
<td class="org-left">date</td>
</tr>
<tr>
<td class="org-left">season</td>
<td class="org-left">season (1:spring, 2:summer, 3:fall, 4:winter)</td>
</tr>
<tr>
<td class="org-left">yr</td>
<td class="org-left">year (0: 2011, 1:2012)</td>
</tr>
<tr>
<td class="org-left">mnth</td>
<td class="org-left">month (1 to 12)</td>
</tr>
<tr>
<td class="org-left">hr</td>
<td class="org-left">hour (0 to 23)</td>
</tr>
<tr>
<td class="org-left">holiday</td>
<td class="org-left">whether day is holiday or not (extracted from <a href="https://dchr.dc.gov/page/holiday-schedules">Washington D.C. holiday information</a>)</td>
</tr>
<tr>
<td class="org-left">weekday</td>
<td class="org-left">day of the week (0 to 6)</td>
</tr>
<tr>
<td class="org-left">workingday</td>
<td class="org-left">if day is neither weekend nor holiday is 1, otherwise is 0.</td>
</tr>
<tr>
<td class="org-left">weathersit</td>
<td class="org-left">Weather (1, 2, 3, or 4) (see next table)</td>
</tr>
<tr>
<td class="org-left">temp</td>
<td class="org-left">Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)</td>
</tr>
<tr>
<td class="org-left">atemp</td>
<td class="org-left">Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)</td>
</tr>
<tr>
<td class="org-left">hum</td>
<td class="org-left">Normalized humidity. The values are divided to 100 (max)</td>
</tr>
<tr>
<td class="org-left">windspeed</td>
<td class="org-left">Normalized wind speed. The values are divided to 67 (max)</td>
</tr>
<tr>
<td class="org-left">casual</td>
<td class="org-left">count of casual users</td>
</tr>
<tr>
<td class="org-left">registered</td>
<td class="org-left">count of registered users</td>
</tr>
<tr>
<td class="org-left">cnt</td>
<td class="org-left">count of total rental bikes including both casual and registered</td>
</tr>
</tbody>
</table>
<p><code>weathersit</code></p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Value</th>
<th class="org-left" scope="col">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">Clear, Few clouds, Partly cloudy, Partly cloudy</td>
</tr>
<tr>
<td class="org-right">2</td>
<td class="org-left">Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist</td>
</tr>
<tr>
<td class="org-right">3</td>
<td class="org-left">Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds</td>
</tr>
<tr>
<td class="org-right">4</td>
<td class="org-left">Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-2" id="outline-container-org116ad83">
<h2 id="org116ad83">Checking out the data</h2>
<div class="outline-text-2" id="text-org116ad83">
<p>This dataset has the number of riders for each hour of each day from January 1, 2011 to December 31, 2012. The number of riders is split between casual and registered and summed up in the <code>cnt</code> column. You can see the first few rows of the data above.</p>
<p>Below is a plot showing the number of bike riders over the first 10 days or so in the data set (some days don't have exactly 24 entries in the data set, so it's not exactly 10 days). You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model.</p>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Rides For the First Ten Days")
first_ten = rides[:24*10]
plot_lines = axe.plot(range(len(first_ten)), first_ten.cnt, label="Count")
lines = axe.plot(range(len(first_ten)), first_ten.cnt,
                 '.',
                 markeredgecolor="r")
axe.set_xlabel("Day")
legend = axe.legend(plot_lines, ["Count"], loc="upper left")
</pre></div>
<div class="figure">
<p><img alt="riders_first_ten_days.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/riders_first_ten_days.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org6d923e5">
<h2 id="org6d923e5">Dummy variables</h2>
<div class="outline-text-2" id="text-org6d923e5">
<p>Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to <a href="https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html">get_dummies</a>.</p>
<div class="highlight">
<pre><span></span>dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']
for each in dummy_fields:
    dummies = pandas.get_dummies(rides[each], prefix=each, drop_first=False)
    rides = pandas.concat([rides, dummies], axis=1)

fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', 
                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']
data = rides.drop(fields_to_drop, axis=1)
</pre></div>
<div class="highlight">
<pre><span></span>print(data.head())
</pre></div>
<pre class="example">
   yr  holiday  temp   hum  windspeed  casual  registered  cnt  season_1  \
0   0        0  0.24  0.81        0.0       3          13   16         1   
1   0        0  0.22  0.80        0.0       8          32   40         1   
2   0        0  0.22  0.80        0.0       5          27   32         1   
3   0        0  0.24  0.75        0.0       3          10   13         1   
4   0        0  0.24  0.75        0.0       0           1    1         1   

   season_2    ...      hr_21  hr_22  hr_23  weekday_0  weekday_1  weekday_2  \
0         0    ...          0      0      0          0          0          0   
1         0    ...          0      0      0          0          0          0   
2         0    ...          0      0      0          0          0          0   
3         0    ...          0      0      0          0          0          0   
4         0    ...          0      0      0          0          0          0   

   weekday_3  weekday_4  weekday_5  weekday_6  
0          0          0          0          1  
1          0          0          0          1  
2          0          0          0          1  
3          0          0          0          1  
4          0          0          0          1  

[5 rows x 59 columns]
</pre></div>
</div>
<div class="outline-2" id="outline-container-org49b43d6">
<h2 id="org49b43d6">Scaling target variables</h2>
<div class="outline-text-2" id="text-org49b43d6">
<p>To make training the network easier, we'll standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1.</p>
<p>The scaling factors are saved so we can go backwards when we use the network for predictions.</p>
<div class="highlight">
<pre><span></span>quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']
# Store scalings in a dictionary so we can convert back later
scaled_features = {}
for each in quant_features:
    mean, std = data[each].mean(), data[each].std()
    scaled_features[each] = [mean, std]
    data.loc[:, each] = (data[each] - mean)/std
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org0895995">
<h2 id="org0895995">Splitting the data into training, testing, and validation sets</h2>
<div class="outline-text-2" id="text-org0895995">
<p>We'll save the data for the last approximately 21 days to use as a test set after we've trained the network. We'll use this set to make predictions and compare them with the actual number of riders.</p>
<p>Save data for approximately the last 21 days.</p>
<div class="highlight">
<pre><span></span>LAST_TWENTY_ONE = -21 * 24 
test_data = data[LAST_TWENTY_ONE:]
</pre></div>
<p>Now remove the test data from the data set .</p>
<div class="highlight">
<pre><span></span>data = data[:LAST_TWENTY_ONE]
</pre></div>
<p>Separate the data into features and targets.</p>
<div class="highlight">
<pre><span></span>target_fields = ['cnt', 'casual', 'registered']
features, targets = data.drop(target_fields, axis=1), data[target_fields]
test_features, test_targets = (test_data.drop(target_fields, axis=1),
                               test_data[target_fields])
</pre></div>
<p>We'll split the data into two sets, one for training and one for validating as the network is being trained. Since this is time series data, we'll train on historical data, then try to predict on future data (the validation set).</p>
<p>Hold out the last 60 days or so of the remaining data as a validation set</p>
<div class="highlight">
<pre><span></span>LAST_SIXTY = -60 * 24
train_features, train_targets = features[:LAST_SIXTY], targets[:LAST_SIXTY]
val_features, val_targets = features[LAST_SIXTY:], targets[LAST_SIXTY:]
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc2093ee">
<h2 id="orgc2093ee">Time to build the network</h2>
<div class="outline-text-2" id="text-orgc2093ee">
<p>Below you'll build your network. We've built out the structure. You'll implement both the forward pass and backwards pass through the network. You'll also set the hyperparameters: the learning rate, the number of hidden units, and the number of training passes.</p>
<div class="highlight">
<pre><span></span>graph = Digraph(comment="Neural Network", format="png")
graph.attr(rankdir="LR")

with graph.subgraph(name="cluster_input") as cluster:
    cluster.attr(label="Input")
    cluster.node("a", "")
    cluster.node("b", "")
    cluster.node("c", "")

with graph.subgraph(name="cluster_hidden") as cluster:
    cluster.attr(label="Hidden")
    cluster.node("d", "")
    cluster.node("e", "")
    cluster.node("f", "")
    cluster.node("g", "")

with graph.subgraph(name="cluster_output") as cluster:
    cluster.attr(label="Output")
    cluster.node("h", "")


graph.edges(["ad", "ae", "af", "ag",
             "bd", "be", "bf", "bg",
             "cd", "ce", "cf", "cg"])

graph.edges(["dh", 'eh', "fh", "gh"])

graph.render("graphs/network.dot")
graph
</pre></div>
<div class="figure">
<p><img alt="network.dot.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/network.dot.png"></p>
</div>
<p>The network has two layers, a hidden layer and an output layer. The hidden layer will use the sigmoid function for activations. The output layer has only one node and is used for the regression, the output of the node is the same as the input of the node. That is, the activation function is \(f(x)=x\). A function that takes the input signal and generates an output signal, but takes into account the threshold, is called an activation function. We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer. This process is called <b>forward propagation</b>.</p>
<p>We use the weights to propagate signals forward from the input to the output layers in a neural network. We use the weights to also propagate error backwards from the output back into the network to update our weights. This is called <b>backpropagation</b>.</p>
<p><b>Hint:</b> You'll need the derivative of the output activation function (\(f(x) = x\)) for the backpropagation implementation. If you aren't familiar with calculus, this function is equivalent to the equation \(y = x\). What is the slope of that equation? That is the derivative of \(f(x)\).</p>
<p>Below, you have these tasks:</p>
<ol class="org-ol">
<li>Implement the sigmoid function to use as the activation function. Set `self.activation_function` in `__init__` to your sigmoid function.</li>
<li>Implement the forward pass in the `train` method.</li>
<li>Implement the backpropagation algorithm in the `train` method, including calculating the output error.</li>
<li>Implement the forward pass in the `run` method.</li>
</ol>
<p>In the my_answers.py file, fill out the TODO sections as specified</p>
<pre class="example">
from my_answers import NeuralNetwork
</pre></div>
<div class="outline-3" id="outline-container-org3706f37">
<h3 id="org3706f37">Mean Squared Error</h3>
<div class="outline-text-3" id="text-org3706f37">
<div class="highlight">
<pre><span></span>def MSE(y, Y):
    return numpy.mean((y-Y)**2)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4fdbf06">
<h2 id="org4fdbf06">Unit tests</h2>
<div class="outline-text-2" id="text-org4fdbf06">
<p>Run these unit tests to check the correctness of your network implementation. This will help you be sure your network was implemented correctly befor you starting trying to train it. These tests must all be successful to pass the project.</p>
<div class="highlight">
<pre><span></span>inputs = numpy.array([[0.5, -0.2, 0.1]])
targets = numpy.array([[0.4]])

test_w_i_h = numpy.array([[0.1, -0.2],
                          [0.4, 0.5],
                          [-0.3, 0.2]])
test_w_h_o = numpy.array([[0.3],
                          [-0.1]])
</pre></div>
</div>
<div class="outline-3" id="outline-container-org71de600">
<h3 id="org71de600">The TestMethods Class</h3>
<div class="outline-text-3" id="text-org71de600">
<div class="highlight">
<pre><span></span>class TestMethods(unittest.TestCase):

    ##########
    # Unit tests for data loading
    ##########

    def test_data_path(self):
        # Test that file path to dataset has been unaltered
        self.assertTrue(data_path.lower() == EXPECTED_DATA_PATH)

    def test_data_loaded(self):
        # Test that data frame loaded
        self.assertTrue(isinstance(rides, pandas.DataFrame))

    ##########
    # Unit tests for network functionality
    ##########

    def test_activation(self):
        network = NeuralNetwork(3, 2, 1, 0.5)
        # Test that the activation function is a sigmoid
        self.assertTrue(numpy.all(network.activation_function(0.5) == 1/(1+numpy.exp(-0.5))))

    def test_train(self):
        # Test that weights are updated correctly on training
        network = NeuralNetwork(3, 2, 1, 0.5)
        network.weights_input_to_hidden = test_w_i_h.copy()
        network.weights_hidden_to_output = test_w_h_o.copy()

        network.train(inputs, targets)
        expected = numpy.array([[ 0.37275328], 
                                [-0.03172939]])
        actual = network.weights_hidden_to_output
        self.assertTrue(
            numpy.allclose(expected, actual),
            "(weights hidden to output) Expected {} Actual: {}".format(
                expected, actual))
        expected = numpy.array([[ 0.10562014, -0.20185996], 
                                [0.39775194, 0.50074398], 
                                [-0.29887597, 0.19962801]])
        actual = network.weights_input_to_hidden
        self.assertTrue(
            numpy.allclose(actual,
                           expected), #, 0.1),
            "(weights input to hidden) Expected: {} Actual: {}".format(
                expected,
                actual))
        return

    def test_run(self):
        # Test correctness of run method
        network = NeuralNetwork(3, 2, 1, 0.5)
        network.weights_input_to_hidden = test_w_i_h.copy()
        network.weights_hidden_to_output = test_w_h_o.copy()

        self.assertTrue(numpy.allclose(network.run(inputs), 0.09998924))

suite = unittest.TestLoader().loadTestsFromModule(TestMethods())
unittest.TextTestRunner().run(suite)
</pre></div>
<pre class="example">
.....
----------------------------------------------------------------------
Ran 5 tests in 0.006s

OK

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org48c2294">
<h2 id="org48c2294">Training the network</h2>
<div class="outline-text-2" id="text-org48c2294">
<p>Here you'll set the hyperparameters for the network. The strategy here is to find hyperparameters such that the error on the training set is low, but you're not overfitting to the data. If you train the network too long or have too many hidden nodes, it can become overly specific to the training set and will fail to generalize to the validation set. That is, the loss on the validation set will start increasing as the training set loss drops.</p>
<p>You'll also be using a method know as Stochastic Gradient Descent (SGD) to train the network. The idea is that for each training pass, you grab a random sample of the data instead of using the whole data set. You use many more training passes than with normal gradient descent, but each pass is much faster. This ends up training the network more efficiently. You'll learn more about SGD later.</p>
</div>
<div class="outline-3" id="outline-container-org6b8f336">
<h3 id="org6b8f336">Choose the number of iterations</h3>
<div class="outline-text-3" id="text-org6b8f336">
<p>This is the number of batches of samples from the training data we'll use to train the network. The more iterations you use, the better the model will fit the data. However, this process can have sharply diminishing returns and can waste computational resources if you use too many iterations. You want to find a number here where the network has a low training loss, and the validation loss is at a minimum. The ideal number of iterations would be a level that stops shortly after the validation loss is no longer decreasing.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgec94c87">
<h3 id="orgec94c87">Choose the learning rate</h3>
<div class="outline-text-3" id="text-orgec94c87">
<p>This scales the size of weight updates. If this is too big, the weights tend to explode and the network fails to fit the data. Normally a good choice to start at is 0.1; however, if you effectively divide the learning rate by n_records, try starting out with a learning rate of 1. In either case, if the network has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the neural network to converge.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org288a2f6">
<h3 id="org288a2f6">Choose the number of hidden nodes</h3>
<div class="outline-text-3" id="text-org288a2f6">
<p>In a model where all the weights are optimized, the more hidden nodes you have, the more accurate the predictions of the model will be. (A fully optimized model could have weights of zero, after all.) However, the more hidden nodes you have, the harder it will be to optimize the weights of the model, and the more likely it will be that suboptimal weights will lead to overfitting. With overfitting, the model will memorize the training data instead of learning the true pattern, and won't generalize well to unseen data.</p>
<p>Try a few different numbers and see how it affects the performance. You can look at the losses dictionary for a metric of the network performance. If the number of hidden units is too low, then the model won't have enough space to learn and if it is too high there are too many options for the direction that the learning can take. The trick here is to find the right balance in number of hidden units you choose. You'll generally find that the best number of hidden nodes to use ends up being between the number of input and output nodes.</p>
<p>Set the hyperparameters in you myanswers.py file:</p>
<pre class="example">
from my_answers import iterations, learning_rate, hidden_nodes, output_nodes
</pre>
<div class="highlight">
<pre><span></span>N_i = train_features.shape[1]
</pre></div>
<div class="highlight">
<pre><span></span>network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)
losses = {'train':[], 'validation':[]}
print("Inputs: {}, Hidden: {}, Output: {}, Learning Rate: {}".format(
    N_i,
    hidden_nodes,
    output_nodes,
    learning_rate))
print("Starting {} repetitions".format(iterations))
for iteration in range(iterations):
    # Go through a random batch of 128 records from the training data set
    batch = numpy.random.choice(train_features.index, size=128)
    X, y = train_features.loc[batch].values, train_targets.loc[batch]['cnt']

    network.train(X, y)

    # Printing out the training progress
    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)
    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)
    if not iteration % 500:
        sys.stdout.write("\nProgress: {:2.1f}".format(100 * iteration/iterations)
                         + "% ... Training loss: " 
                         + "{:.5f}".format(train_loss)
                         + " ... Validation loss: {:.5f}".format(val_loss))
        sys.stdout.flush()

    losses['train'].append(train_loss)
    losses['validation'].append(val_loss)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4
Starting 7500 repetitions

Progress: 0.0% ... Training loss: 1.09774 ... Validation loss: 1.74283
Progress: 6.7% ... Training loss: 0.27687 ... Validation loss: 0.44356
Progress: 13.3% ... Training loss: 0.24134 ... Validation loss: 0.42289
Progress: 20.0% ... Training loss: 0.20681 ... Validation loss: 0.38749
Progress: 26.7% ... Training loss: 0.16536 ... Validation loss: 0.31655
Progress: 33.3% ... Training loss: 0.13105 ... Validation loss: 0.25414
Progress: 40.0% ... Training loss: 0.10072 ... Validation loss: 0.21108
Progress: 46.7% ... Training loss: 0.08929 ... Validation loss: 0.18401
Progress: 53.3% ... Training loss: 0.07844 ... Validation loss: 0.16669
Progress: 60.0% ... Training loss: 0.07380 ... Validation loss: 0.15336
Progress: 66.7% ... Training loss: 0.07580 ... Validation loss: 0.18654
Progress: 73.3% ... Training loss: 0.06308 ... Validation loss: 0.15848
Progress: 80.0% ... Training loss: 0.06632 ... Validation loss: 0.17960
Progress: 86.7% ... Training loss: 0.05954 ... Validation loss: 0.15988
Progress: 93.3% ... Training loss: 0.05809 ... Validation loss: 0.16016
</pre>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time")
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="losses.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/losses.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgfdef998">
<h2 id="orgfdef998">Check out your predictions</h2>
<div class="outline-text-2" id="text-orgfdef998">
<p>Here, use the test data to view how well your network is modeling the data. If something is completely wrong here, make sure each step in your network is implemented correctly.</p>
<div class="highlight">
<pre><span></span>fig, axe = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
axe.plot(expected, '.', label='Data')
axe.plot(expected, linestyle="--", color="tab:blue", label=None)
axe.plot(predictions,linestyle="--", color="tab:orange", label=None)
axe.plot(predictions, ".", label='Prediction')
axe.set_xlim(right=len(predictions))
legend = axe.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
axe.set_xticks(numpy.arange(len(dates))[12::24])
_ = axe.set_xticklabels(dates[12::24], rotation=45)
</pre></div>
<div class="figure">
<p><img alt="count.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/count.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-org05da6a4">
<h3 id="org05da6a4">How well does the model predict the data?</h3>
<div class="outline-text-3" id="text-org05da6a4">
<p>It looks like it does better initially and then over-predicts the peaks later on.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org01d43c0">
<h3 id="org01d43c0">Where does it fail?</h3>
<div class="outline-text-3" id="text-org01d43c0">
<p>It doesn't anticipate the drop-off in ridershp as the holidays come around.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org08a5630">
<h3 id="org08a5630">Why does it fail where it does?</h3>
<div class="outline-text-3" id="text-org08a5630">
<p>Although there might be holidays noted (at least for Christmas), it probably isn't reflecting the extreme change in behavior that the holidays bring about in the United States.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga0d946d">
<h2 id="orga0d946d">More Variations</h2>
<div class="outline-text-2" id="text-orga0d946d">
<div class="highlight">
<pre><span></span>def train_this(hidden_nodes:int, learning_rate:float,
               output_nodes:int=1, 
               input_nodes: int=N_i,
               repetitions: int=100,
               emit: bool=True):
    """Trains the network using the given values

    Args:
     hidden_nodes: number of nodes in the hidden layer
     learning_rate: amount to change the weights during backpropagation
     output_nodes: number of nodes in the output layer
     input_nodes: number of nodes in the input layer
     repetitions: number of times to train the model
     emit: print information

    Returns:
     test error, losses: MSE against test, dict of losses
    """
    network = NeuralNetwork(input_nodes, hidden_nodes, output_nodes,
                            learning_rate)
    losses = {'train':[], 'validation':[]}
    last_validation_loss = -1
    if emit:        
        print(
            ("Inputs: {}, Hidden: {}, Output: {}, Learning Rate: {}, "
             "Repetitions: {}").format(
                 input_nodes,
                 hidden_nodes,
                 output_nodes,
                 learning_rate, 
                 repetitions))
    reported = False
    for iteration in range(repetitions):
        # Go through a random batch of 128 records from the training data set
        batch = numpy.random.choice(train_features.index, size=128)
        X, y = (train_features.iloc[batch].values,
                train_targets.iloc[batch]['cnt'])
        network.train(X, y)

        train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)
        val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)
        losses['train'].append(train_loss)
        losses['validation'].append(val_loss)
        if last_validation_loss == -1:
            last_validation_loss = val_loss[0] 
        if val_loss[0] &gt; last_validation_loss and not reported:
            reported = True
            if emit:
                print("Repetition {} Validation Loss went up by {}".format(
                iteration + 1,
                val_loss[0] - last_validation_loss))
        last_validation_loss = val_loss[0]

    predictions = network.run(test_features)
    expected = (test_targets['cnt']).values
    test_error = MSE(predictions.T, expected)[0]
    if emit:
        print(("Training Error: {:.5f}, "
               "Validation Error: {:.5f}, "
               "Test Error: {:.2f}").format(
                   losses["train"][-1][0],
                   losses["validation"][-1][0],
                   test_error))
    return test_error, losses, network
</pre></div>
<div class="highlight">
<pre><span></span>Parameters = namedtuple(
    "Parameters",
    "hidden_nodes learning_rate trials losses test_error network".split())
</pre></div>
<div class="highlight">
<pre><span></span>def grid_search(hidden_nodes: list, learning_rates: list, trials: list,
                max_train_error = 0.09, max_validation_error=0.18,
                emit_training:bool=False):
    """does a search for the best parameters

    Args:
     hidden_nodes: list of number of hidden nodes
     learning rates: list of how much to update the weights
     trials: list of number of times to train
     max_train_error: upper ceiling for training error
     max_validation_error: upper ceilining for acceptable validation error
     emit_training: print the statements during training
    """
    best = 1000
    if not type(trials) is list:
        trials = [trials]
    for node_count in hidden_nodes:
        for rate in learning_rates:
            for trial in trials:
                test_error, losses, network = train_this(node_count, rate,
                                                         repetitions=trial,
                                                         emit=emit_training)
                if test_error &lt; best:
                    print("New Best: {:.2f} (Hidden: {}, Learning Rate: {:.2f})".format(
                        test_error,
                        node_count,
                        rate))
                    best = test_error
                    best_parameters = Parameters(hidden_nodes=node_count,
                                                 learning_rate=rate,
                                                 trials=trials,
                                                 losses=losses,
                                                 test_error=test_error,
                                                 network=network)
                print()
    return best_parameters
</pre></div>
<div class="highlight">
<pre><span></span>parameters = grid_search(
    hidden_nodes=[14, 28, 42, 56],
    learning_rates=[0.1, 0.01, 0.001],
    trials=200)
</pre></div>
<pre class="example">
New Best: 0.53 (Hidden: 14, Learning Rate: 0.10)



New Best: 0.47 (Hidden: 28, Learning Rate: 0.10)



New Best: 0.44 (Hidden: 42, Learning Rate: 0.10)



New Best: 0.42 (Hidden: 56, Learning Rate: 0.10)



</pre>
<div class="highlight">
<pre><span></span>parameters = grid_search([42, 56], [0.1, 0.01, 0.001], trials=300)
</pre></div>
<pre class="example">
New Best: 0.45 (Hidden: 42, Learning Rate: 0.10)



New Best: 0.42 (Hidden: 56, Learning Rate: 0.10)




</pre>
<p>So, I wouldn't have guessed it, but the 56 node model does best with a reasonably large learning rate.</p>
<div class="highlight">
<pre><span></span>parameters = grid_search([56, 112], [0.1, 0.2], trials=200)
</pre></div>
<pre class="example">
New Best: 0.44 (Hidden: 56, Learning Rate: 0.10)

New Best: 0.41 (Hidden: 56, Learning Rate: 0.20)




</pre>
<p>Weird.</p>
<div class="highlight">
<pre><span></span>parameters = grid_search([56], [0.2, 0.3], trials=300, emit_training=True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 300
Repetition 2 Validation Loss went up by 1.2119413344400733
Training Error: 0.42973, Validation Error: 0.71298, Test Error: 0.36
New Best: 0.36 (Hidden: 56, Learning Rate: 0.20)

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.3, Repetitions: 300
Repetition 2 Validation Loss went up by 47.942730166612094
Training Error: 0.69836, Validation Error: 1.20312, Test Error: 0.63


</pre>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time")
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
losses = parameters.losses
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="better_losses.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/better_losses.png"></p>
</div>
<p>It looks like going over 100 doesn't really help the model a lot, or at all, really.</p>
<div class="highlight">
<pre><span></span>parameters = grid_search([56], [0.2, 0.3], trials=300, emit_training=True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 300
Repetition 2 Validation Loss went up by 0.29155647125413564
Training Error: 0.46915, Validation Error: 0.77756, Test Error: 0.36
New Best: 0.36 (Hidden: 56, Learning Rate: 0.20)

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.3, Repetitions: 300
Repetition 2 Validation Loss went up by 68.10510030432465
Training Error: 0.63604, Validation Error: 1.07500, Test Error: 0.58


</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([56], [0.2], 2000)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
New Best: 0.27 (Hidden: 56, Learning Rate: 0.20)

Elapsed: 0:02:20.654989

</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([56], [0.2], 1000)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
New Best: 0.29 (Hidden: 56, Learning Rate: 0.20)

Elapsed: 0:01:51.175404

</pre>
<p>I just checked the rubric and you need a training loss below 0.09 and a validation loss below 0.18, regardless of the test loss.</p>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28, 42, 56], [0.01, 0.1, 0.2], 100, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 10 Validation Loss went up by 0.0005887216742490597
Training Error: 0.92157, Validation Error: 1.36966, Test Error: 0.69
New Best: 0.69 (Hidden: 28, Learning Rate: 0.01)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 7 Validation Loss went up by 0.06008857123483735
Training Error: 0.65584, Validation Error: 1.09581, Test Error: 0.52
New Best: 0.52 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 4 Validation Loss went up by 0.010135891697978794
Training Error: 0.61391, Validation Error: 0.99344, Test Error: 0.47
New Best: 0.47 (Hidden: 28, Learning Rate: 0.20)

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 2 Validation Loss went up by 0.0016900520112179684
Training Error: 0.87851, Validation Error: 1.31872, Test Error: 0.66

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 3 Validation Loss went up by 0.08058311852052547
Training Error: 0.66637, Validation Error: 1.09371, Test Error: 0.53

Inputs: 56, Hidden: 42, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 3 Validation Loss went up by 0.08181869722819135
Training Error: 0.60174, Validation Error: 0.99664, Test Error: 0.47

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 4 Validation Loss went up by 0.0015646480595301604
Training Error: 0.93732, Validation Error: 1.36061, Test Error: 0.72

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.03097966349747283
Training Error: 0.67087, Validation Error: 1.07306, Test Error: 0.51

Inputs: 56, Hidden: 56, Output: 1, Learning Rate: 0.2, Repetitions: 100
Repetition 2 Validation Loss went up by 9.947099886932289
Training Error: 0.65815, Validation Error: 1.17712, Test Error: 0.52

Elapsed: 0:00:47.842652
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.01, 0.1, 0.2], 1000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.01, Repetitions: 1000
Repetition 2 Validation Loss went up by 0.002750823237845479
Training Error: 0.71460, Validation Error: 1.28385, Test Error: 0.59
New Best: 0.59 (Hidden: 28, Learning Rate: 0.01)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 1000
Repetition 5 Validation Loss went up by 0.09885352252565549
Training Error: 0.31086, Validation Error: 0.48591, Test Error: 0.33
New Best: 0.33 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 1000
Repetition 2 Validation Loss went up by 0.09560269140958688
Training Error: 0.28902, Validation Error: 0.44905, Test Error: 0.33

Elapsed: 0:01:59.136160
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.1, 0.2], 2000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 2000
Repetition 2 Validation Loss went up by 0.06973195973646384
Training Error: 0.28625, Validation Error: 0.45083, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.10)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 2000
Repetition 3 Validation Loss went up by 0.037295970545350166
Training Error: 0.26864, Validation Error: 0.43831, Test Error: 0.32

Elapsed: 0:02:35.122622
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.05], 4000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.1, Repetitions: 4000
Repetition 4 Validation Loss went up by 0.039021584745929205
Training Error: 0.27045, Validation Error: 0.44738, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.10)

Elapsed: 0:02:36.990482

</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.2], 5000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 5000
Repetition 4 Validation Loss went up by 0.32617394730848104
Training Error: 0.18017, Validation Error: 0.32432, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.20)

Elapsed: 0:03:05.664176

</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.2, 0.3], 6000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.2, Repetitions: 6000
Repetition 3 Validation Loss went up by 0.18572519005609722
Training Error: 0.22969, Validation Error: 0.38789, Test Error: 0.35
New Best: 0.35 (Hidden: 28, Learning Rate: 0.20)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 6000
Repetition 3 Validation Loss went up by 1.6850265407570482
Training Error: 0.08168, Validation Error: 0.20003, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.30)

Elapsed: 0:07:30.082137
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.3, 0.4], 7000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 7000
Repetition 3 Validation Loss went up by 0.4652683795507646
Training Error: 0.07100, Validation Error: 0.19299, Test Error: 0.29
New Best: 0.29 (Hidden: 28, Learning Rate: 0.30)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 7000
Repetition 2 Validation Loss went up by 8.612689644792866
Training Error: 0.05771, Validation Error: 0.19188, Test Error: 0.22
New Best: 0.22 (Hidden: 28, Learning Rate: 0.40)

Elapsed: 0:09:06.729922
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.4, 0.5], 7500, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 7500
Repetition 3 Validation Loss went up by 3.6207932112624386
Training Error: 0.05942, Validation Error: 0.13644, Test Error: 0.16
New Best: 0.16 (Hidden: 28, Learning Rate: 0.40)

Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.5, Repetitions: 7500
Repetition 2 Validation Loss went up by 4.101532160572686
Training Error: 0.05710, Validation Error: 0.14214, Test Error: 0.22

Elapsed: 0:09:56.116403
</pre>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.4], 8000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.4, Repetitions: 8000
Repetition 2 Validation Loss went up by 0.6450181997021212
Training Error: 0.05479, Validation Error: 0.14289, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.40)

Elapsed: 0:05:18.546979

</pre>
<p>That did worse so it probably overtrains at the 0.4 learning rate. What about 0.3?</p>
<div class="highlight">
<pre><span></span>start = datetime.now()
parameters = grid_search([28], [0.3], 8000, emit_training=True)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 28, Output: 1, Learning Rate: 0.3, Repetitions: 8000
Repetition 2 Validation Loss went up by 0.3336478297258907
Training Error: 0.06670, Validation Error: 0.16918, Test Error: 0.24
New Best: 0.24 (Hidden: 28, Learning Rate: 0.30)

Elapsed: 0:05:06.761537

</pre>
<p>So this did worse than a learning rate of 0.5 at 7500 and much worse than 0.4 at 7500, so maybe that is the optimal (0.4 at 7,500) I'm chasing. It seems king of arbitrary, but it works for the assignment.</p>
<p>The submission is timing out for some reason (it only takes 5 minutes to run but the error says it took more than 7 minutes). I might have to try some compromise runtime.</p>
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots(figsize=FIGURE_SIZE)
axe.set_title("Error Over Time (Hidden: {} Learning Rate: {})".format(parameters.hidden_nodes, parameters.learning_rate))
axe.set_ylabel("MSE")
axe.set_xlabel("Repetition")
losses = parameters.losses
axe.plot(range(len(losses["train"])), losses['train'], label='Training loss')
lines = axe.plot(range(len(losses["validation"])), losses['validation'], label='Validation loss')
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="found_losses.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/found_losses.png"></p>
</div>
<div class="highlight">
<pre><span></span>fig, ax = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = parameters.network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
ax.plot(expected, '.', label='Data')
ax.plot(predictions.values, ".", label='Prediction')
ax.set_xlim(right=len(predictions))
legend = ax.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
ax.set_xticks(numpy.arange(len(dates))[12::24])
_ = ax.set_xticklabels(dates[12::24], rotation=45)
</pre></div>
<div class="figure">
<p><img alt="best_count.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/best_count.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-orge30bed8">
<h3 id="orge30bed8">How well does the model predict the data?</h3>
<div class="outline-text-3" id="text-orge30bed8">
<p>The model seems to not be able to capture all the variations in the data. I don't think it really did well at all.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org4600f83">
<h3 id="org4600f83">Where does it fail?</h3>
</div>
<div class="outline-3" id="outline-container-orgce896ea">
<h3 id="orgce896ea">Why does it fail where it does?</h3>
<div class="outline-text-3" id="text-orgce896ea">
<div class="highlight">
<pre><span></span>better_network = train_this(parameters.hidden_nodes, parameters.learning_rate,
                            repetitions=150, emit=True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 19, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 43 Validation Loss went up by 0.0008309723799344582
Training Error: 0.92939, Validation Error: 1.44647, Test Error: 0.62

</pre>
<div class="highlight">
<pre><span></span>more_parameters = grid_search([10, 20],
                                [0.1, 0.01],
                                trials=list(range(50, 225, 25)),
                                emit_training=True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 50
Repetition 10 Validation Loss went up by 0.013827968028145676
Training Error: 0.92764, Validation Error: 1.45833, Test Error: 0.64
New Best: 0.64 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 75
Repetition 11 Validation Loss went up by 0.01268617480459655
Training Error: 0.95884, Validation Error: 1.45576, Test Error: 0.67

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.004302293010363778
Training Error: 0.96904, Validation Error: 1.32713, Test Error: 0.74

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 125
Repetition 2 Validation Loss went up by 0.00011567129175471536
Training Error: 0.96480, Validation Error: 1.38193, Test Error: 0.70

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 150
Repetition 2 Validation Loss went up by 0.00525072377638347
Training Error: 0.95649, Validation Error: 1.29823, Test Error: 0.79

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 175
Repetition 6 Validation Loss went up by 0.0184187066638537
Training Error: 0.94033, Validation Error: 1.48648, Test Error: 0.64
New Best: 0.64 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 200
Repetition 4 Validation Loss went up by 0.006479207709029211
Training Error: 0.96348, Validation Error: 1.38834, Test Error: 0.67

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 50
Repetition 8 Validation Loss went up by 0.00034037805450659597
Training Error: 0.99148, Validation Error: 1.50023, Test Error: 0.71

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 75
Repetition 26 Validation Loss went up by 7.26803968935652e-05
Training Error: 0.94736, Validation Error: 1.42677, Test Error: 0.69

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 55 Validation Loss went up by 0.0005170583384894734
Training Error: 0.92258, Validation Error: 1.52912, Test Error: 0.64

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 125
Repetition 14 Validation Loss went up by 4.7182200476170166e-05
Training Error: 0.96801, Validation Error: 1.43898, Test Error: 0.69

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 25 Validation Loss went up by 0.00025169631184263075
Training Error: 0.94769, Validation Error: 1.28473, Test Error: 0.79

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 175
Repetition 2 Validation Loss went up by 0.0011783405140612935
Training Error: 0.95784, Validation Error: 1.38248, Test Error: 0.75

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 200
Repetition 24 Validation Loss went up by 6.126094364189427e-05
Training Error: 0.95839, Validation Error: 1.29311, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 50
Repetition 4 Validation Loss went up by 0.04115757797958697
Training Error: 0.97326, Validation Error: 1.42168, Test Error: 0.72

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 75
Repetition 3 Validation Loss went up by 0.004544958155855872
Training Error: 0.97079, Validation Error: 1.38326, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.004455074996461583
Training Error: 0.95492, Validation Error: 1.29813, Test Error: 0.80

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 125
Repetition 3 Validation Loss went up by 0.038516428472006314
Training Error: 0.96063, Validation Error: 1.41134, Test Error: 0.68

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 150
Repetition 4 Validation Loss went up by 0.005238556857821708
Training Error: 0.96022, Validation Error: 1.58371, Test Error: 0.67

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 175
Repetition 4 Validation Loss went up by 0.03291822053421889
Training Error: 0.95138, Validation Error: 1.39820, Test Error: 0.67

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 200
Repetition 2 Validation Loss went up by 0.01721971957045554
Training Error: 0.96676, Validation Error: 1.33802, Test Error: 0.72

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 50
Repetition 2 Validation Loss went up by 0.0016568049054144218
Training Error: 0.90793, Validation Error: 1.36444, Test Error: 0.78

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 75
Repetition 24 Validation Loss went up by 1.0669002188823384e-05
Training Error: 0.93910, Validation Error: 1.43277, Test Error: 0.61
New Best: 0.61 (Hidden: 20, Learning Rate: 0.01)

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 13 Validation Loss went up by 0.0006819932351833646
Training Error: 0.95953, Validation Error: 1.30219, Test Error: 0.79

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 125
Repetition 2 Validation Loss went up by 0.001983487952677443
Training Error: 0.96503, Validation Error: 1.34180, Test Error: 0.76

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 150
Repetition 2 Validation Loss went up by 0.0014568870668274503
Training Error: 0.95968, Validation Error: 1.34224, Test Error: 0.73

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 175
Repetition 22 Validation Loss went up by 0.0009898893960744726
Training Error: 0.94891, Validation Error: 1.25502, Test Error: 0.81

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 200
Repetition 24 Validation Loss went up by 0.0003693045874550993
Training Error: 0.96222, Validation Error: 1.29511, Test Error: 0.75

</pre>
<div class="highlight">
<pre><span></span>best_parameters = grid_search([10, 20, 30, 40], [0.1, 0.01], 100, True)
</pre></div>
<pre class="example">
Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.006104362102346439
Training Error: 0.96812, Validation Error: 1.32203, Test Error: 0.76
New Best: 0.76 (Hidden: 10, Learning Rate: 0.10)

Inputs: 56, Hidden: 10, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 16 Validation Loss went up by 0.00020577471034299855
Training Error: 0.96607, Validation Error: 1.30376, Test Error: 0.79

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.018123806353317784
Training Error: 0.95858, Validation Error: 1.28351, Test Error: 0.76
New Best: 0.76 (Hidden: 20, Learning Rate: 0.10)

Inputs: 56, Hidden: 20, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 29 Validation Loss went up by 0.0029168059867459295
Training Error: 0.94895, Validation Error: 1.45710, Test Error: 0.66
New Best: 0.66 (Hidden: 20, Learning Rate: 0.01)

Inputs: 56, Hidden: 30, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 2 Validation Loss went up by 0.045063972993018675
Training Error: 0.96892, Validation Error: 1.50312, Test Error: 0.69

Inputs: 56, Hidden: 30, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 14 Validation Loss went up by 7.233598065781166e-05
Training Error: 0.96606, Validation Error: 1.32913, Test Error: 0.80

Inputs: 56, Hidden: 40, Output: 1, Learning Rate: 0.1, Repetitions: 100
Repetition 3 Validation Loss went up by 0.05076336638491519
Training Error: 0.95920, Validation Error: 1.45843, Test Error: 0.68

Inputs: 56, Hidden: 40, Output: 1, Learning Rate: 0.01, Repetitions: 100
Repetition 7 Validation Loss went up by 0.0006550737027899434
Training Error: 0.97148, Validation Error: 1.36548, Test Error: 0.79

</pre>
<div class="highlight">
<pre><span></span>fig, axe = pyplot.subplots(figsize=FIGURE_SIZE)

mean, std = scaled_features['cnt']
predictions = best_parameters.network.run(test_features) * std + mean
expected = (test_targets['cnt'] * std + mean).values
axe.set_title("{} Hidden and Learning Rate: {}".format(best_parameters.hidden_nodes,
                                                       best_parameters.learning_rate))
axe.plot(expected, '.', label='Data')
axe.plot(predictions.values, ".", label='Prediction')
axe.set_xlim(right=len(predictions))
legend = axe.legend()

dates = pandas.to_datetime(rides.loc[test_data.index]['dteday'])
dates = dates.apply(lambda d: d.strftime('%b %d'))
axe.set_xticks(numpy.arange(len(dates))[12::24])
_ = axe.set_xticklabels(dates[12::24], rotation=45)
</pre></div>
<div class="figure">
<p><img alt="even_bester_count.png" src="/posts/nano/bike-sharing/the-bike-sharing-project/even_bester_count.png"></p>
</div>
</div>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="/index-4.html" rel="prev">Newer posts</a></li>
<li class="next"><a href="/index-2.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer">Contents © 2020 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
