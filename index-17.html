<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neurotic Networking (old posts, page 17) | Neurotic Networking</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/index-17.html" rel="canonical">
<link href="index-18.html" rel="prev" type="text/html">
<link href="index-16.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="."><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/deep-n-grams/">Deep N-Grams</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/deep-n-grams/" rel="bookmark"><time class="published dt-published" datetime="2021-01-05T16:30:51-08:00" itemprop="datePublished" title="2021-01-05 16:30">2021-01-05 16:30</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div class="outline-2" id="outline-container-orgbf61ecf">
<h2 id="orgbf61ecf">Deep N-Grams</h2>
<div class="outline-text-2" id="text-orgbf61ecf">
<p>This is an exploration of Recurrent Neural Networks (RNN) using <a href="https://github.com/google/trax">trax</a>. We're going to predict the next set of characters in a sentence given the previous characters.</p>
<p>Since this is so long I'm going to break it up into separate posts.</p>
<ul class="org-ul">
<li><a href="posts/nlp/deep-n-grams-loading-the-data/">Loading the Data</a>: Load the data and convert it to tensors</li>
<li><a href="posts/nlp/deep-n-grams-batch-generation/">Generating Data</a>: Create a batch generator for the tensors</li>
<li><a href="posts/nlp/deep-n-grams-creating-the-model/">Creating the Model</a>: Create a Gated Recurrent Unit (GRU) model</li>
<li><a href="posts/nlp/deep-n-grams-training-the-model/">Training the Model</a>: Train the model</li>
<li><a href="posts/nlp/deep-n-grams-evaluating-the-model/">Evaluating the Model</a>: Evaluate the model's perplexity</li>
<li><a href="posts/nlp/deep-n-grams-generating-sentences/">Generating Sentences</a>: Generate new sentences using the model</li>
</ul>
<p>First up: - <a href="posts/nlp/deep-n-grams-loading-the-data/">Loading the Data</a>.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/trax-gru-model/">Trax GRU Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/trax-gru-model/" rel="bookmark"><time class="published dt-published" datetime="2021-01-04T18:49:01-08:00" itemprop="datePublished" title="2021-01-04 18:49">2021-01-04 18:49</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/trax-gru-model/#orgce3e9ba">Creating a GRU Model Using Trax</a>
<ul>
<li><a href="posts/nlp/trax-gru-model/#org7e21693">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/trax-gru-model/#orge31a933">Middle</a>
<ul>
<li><a href="posts/nlp/trax-gru-model/#org652c6ab">Trax Review</a></li>
<li><a href="posts/nlp/trax-gru-model/#org96133b3">The GRU Model</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgce3e9ba">
<h2 id="orgce3e9ba">Creating a GRU Model Using Trax</h2>
<div class="outline-text-2" id="text-orgce3e9ba"></div>
<div class="outline-3" id="outline-container-org7e21693">
<h3 id="org7e21693">Imports</h3>
<div class="outline-text-3" id="text-org7e21693">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">import</span> <span class="nn">trax</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orge31a933">
<h2 id="orge31a933">Middle</h2>
<div class="outline-text-2" id="text-orge31a933"></div>
<div class="outline-3" id="outline-container-org652c6ab">
<h3 id="org652c6ab">Trax Review</h3>
<div class="outline-text-3" id="text-org652c6ab">
<p>Trax allows us to define neural network architectures by stacking layers (similarly to other libraries such as Keras). For this the <code>Serial()</code> is often used as it is a combinator that allows us to stack layers serially using function composition.</p>
<p>Next we'll look at a simple vanilla NN architecture containing 1 hidden(dense) layer with 128 cells and output (dense) layer with 10 cells on which we apply the final layer of <code>LogSoftMax</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">simple</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
<p>Each of the layers within the <code>Serial</code> combinator layer is considered a sublayer. Notice that unlike similar libraries, <b>in Trax the activation functions are considered layers.</b> To know more about the <code>Serial</code> layer check out the <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">documentation for it</a>.</p>
<p>Here's the representation for it.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">simple</span><span class="p">)</span>
</pre></div>
<pre class="example">
Serial[
  Dense_128
  Serial[
    Relu
  ]
  Dense_10
  LogSoftmax
]
</pre>
<p>Printing the model gives you the exact same information as the model's definition itself.</p>
<p>By just looking at the definition you can clearly see what is going on inside the neural network. Trax is very straightforward in the way a network is defined.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org96133b3">
<h3 id="org96133b3">The GRU Model</h3>
<div class="outline-text-3" id="text-org96133b3">
<p>To create a <code>GRU</code> model you will need to be familiar with the following layers (Documentation link attached with each layer name):</p>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight"><code>ShiftRight</code></a>: Shifts the tensor to the right by padding on axis 1. The <code>mode</code> should be specified and it refers to the context in which the model is being used. Possible values are: 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to "train".</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding"><code>Embedding</code></a> Maps discrete tokens to vectors. It will have shape <code>(vocabulary length X dimension of output vectors)</code>. The dimension of output vectors (also called <code>d_feature</code>) is the number of elements in the word embedding.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.GRU"><code>GRU</code></a> The GRU layer. It leverages another Trax layer called <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.GRUCell"><code>GRUCell</code></a>. The number of GRU units should be specified and should match the number of elements in the word embedding. If you want to stack two consecutive GRU layers, it can be done by using python's list comprehension.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense"><code>Dense</code></a> Vanilla Dense layer.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax"><code>LogSoftMax</code></a> Log Softmax function.</li>
</ul>
<p>Putting everything together the GRU model looks like this.</p>
<div class="highlight">
<pre><span></span><span class="n">mode</span> <span class="o">=</span> <span class="s1">'train'</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">model_dimension</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">GRU</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">ShiftRight</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_feature</span><span class="o">=</span><span class="n">model_dimension</span><span class="p">),</span>
      <span class="p">[</span><span class="n">layers</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="n">model_dimension</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)],</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">),</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
    <span class="p">)</span>
</pre></div>
<p>Next is a helper function that prints information for every layer (sublayer within <code>Serial</code>).</p>
<p><i>Try changing the parameters defined before the GRU model and see how it changes.</i></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">show_layers</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">layer_prefix</span><span class="o">=</span><span class="s2">"Serial.sublayers"</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Total layers: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'========'</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">layer_prefix</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">sublayers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">show_layers</span><span class="p">(</span><span class="n">GRU</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orgb0a42af">
Total layers: 6

========
Serial.sublayers_0: Serial[
  ShiftRight(1)
]

========
Serial.sublayers_1: Embedding_256_512

========
Serial.sublayers_2: GRU_512

========
Serial.sublayers_3: GRU_512

========
Serial.sublayers_4: Dense_256

========
Serial.sublayers_5: LogSoftmax
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">GRU</span><span class="p">)</span>
</pre></div>
<pre class="example">
Serial[
  Serial[
    ShiftRight(1)
  ]
  Embedding_256_512
  GRU_512
  GRU_512
  Dense_256
  LogSoftmax
]
</pre>
<p>Interesting that it inserted a second Serial for the ShiftRight…</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/vanilla-rnns-and-grus/">Vanilla RNNs and GRUs</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/vanilla-rnns-and-grus/" rel="bookmark"><time class="published dt-published" datetime="2021-01-01T20:21:58-08:00" itemprop="datePublished" title="2021-01-01 20:21">2021-01-01 20:21</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#orgf401a12">Vanilla RNNs, GRUs and the <code>scan</code> function</a>
<ul>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#orgd653534">Imports</a></li>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#org474947f">Set Up</a>
<ul>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#org34d51d4">The Sigmoid Function</a></li>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#org8e89224">Collections</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#org48aa67b">Middle</a>
<ul>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#orga0a86ac">The Forward Method For Vanilla RNNs and GRUs</a>
<ul>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#orgfb5eae0">The Forward Method For Vanilla RNNs</a></li>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#org04f6d8b">The Forward Method For GRUs</a></li>
</ul>
</li>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#org31a1c91">Part 2: Implementation of the <code>scan</code> function</a></li>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#org45cadf3">Comparing Vanilla RNNs and GRUs</a>
<ul>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#org4a1997f">Vanilla RNNs</a></li>
<li><a href="posts/nlp/vanilla-rnns-and-grus/#orgabe5c0c">GRUs</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf401a12">
<h2 id="orgf401a12">Vanilla RNNs, GRUs and the <code>scan</code> function</h2>
<div class="outline-text-2" id="text-orgf401a12"></div>
<div class="outline-3" id="outline-container-orgd653534">
<h3 id="orgd653534">Imports</h3>
<div class="outline-text-3" id="text-orgd653534">
<div class="highlight">
<pre><span></span><span class="c1"># from python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">perf_counter</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">expect</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">random</span>

<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org474947f">
<h3 id="org474947f">Set Up</h3>
<div class="outline-text-3" id="text-org474947f"></div>
<div class="outline-4" id="outline-container-org34d51d4">
<h4 id="org34d51d4">The Sigmoid Function</h4>
<div class="outline-text-4" id="text-org34d51d4">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the sigmoid of x</span>

<span class="sd">    Args:</span>
<span class="sd">     x: the array (or float) to get the sigmoid for</span>

<span class="sd">    Returns:</span>
<span class="sd">     the sigmoid of x</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8e89224">
<h4 id="org8e89224">Collections</h4>
<div class="outline-text-4" id="text-org8e89224">
<p>These are going to hold the arrays that we are using for calculation.</p>
<div class="highlight">
<pre><span></span><span class="n">Weights</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Weights"</span><span class="p">,</span> <span class="s2">"w1 w2 w3 b1 b2 b3"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="n">Inputs</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Inputs"</span><span class="p">,</span> <span class="s2">"X hidden_state"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org48aa67b">
<h2 id="org48aa67b">Middle</h2>
<div class="outline-text-2" id="text-org48aa67b"></div>
<div class="outline-3" id="outline-container-orga0a86ac">
<h3 id="orga0a86ac">The Forward Method For Vanilla RNNs and GRUs</h3>
<div class="outline-text-3" id="text-orga0a86ac">
<p>In this part of the notebook, we'll look at the implementation of the forward method for a vanilla RNN and implement that same method for a GRU. For this excercise we'll use a set of random weights and variables with the following dimensions:</p>
<ul class="org-ul">
<li>Embedding size (<code>emb</code>) : 128</li>
<li>Hidden state size (<code>h_dim</code>) : (16,1)</li>
</ul>
<p>The weights <code>w_</code> and biases <code>b_</code> are initialized with dimensions (<code>h_dim</code>, <code>emb + h_dim</code>) and (<code>h_dim</code>, 1). We expect the hidden state <code>h_t</code> to be a column vector with size (<code>h_dim</code>,1) and the initial hidden state <code>h_0</code> is a vector of zeros.</p>
<p>Now we'll set up the variables for the dimensions.</p>
<div class="highlight">
<pre><span></span><span class="n">Dimension</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">embedding</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">hidden_variables</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">hidden_state</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>    
<span class="p">)</span>
</pre></div>
<p>Now we'll initialize the various arrays.</p>
<div class="highlight">
<pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">Weights</span><span class="p">(</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span>
        <span class="p">(</span><span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span>
         <span class="n">Dimension</span><span class="o">.</span><span class="n">embedding</span> <span class="o">+</span> <span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)),</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span>
        <span class="p">(</span><span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span>
         <span class="n">Dimension</span><span class="o">.</span><span class="n">embedding</span> <span class="o">+</span> <span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)),</span>
    <span class="n">w3</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span>
        <span class="p">(</span><span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span>
         <span class="n">Dimension</span><span class="o">.</span><span class="n">embedding</span> <span class="o">+</span> <span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)),</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>  
<span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">Inputs</span><span class="p">(</span>
    <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">Dimension</span><span class="o">.</span><span class="n">hidden_variables</span><span class="p">,</span> <span class="n">Dimension</span><span class="o">.</span><span class="n">embedding</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgfb5eae0">
<h4 id="orgfb5eae0">The Forward Method For Vanilla RNNs</h4>
<div class="outline-text-4" id="text-orgfb5eae0">
<p>The vanilla RNN cell is quite straight forward.</p>
<p>The computations made in a vanilla RNN cell are equivalent to the following equations:</p>
\begin{equation} h^{\langle t \rangle}=g(W_{h}[h^{\langle t-1 \rangle},x^{\langle t \rangle}] + b_h) \label{eq: htRNN} \end{equation} \begin{equation} \hat{y}^{\langle t \rangle}=g(W_{yh}h^{\langle t \rangle} + b_y) \label{eq: ytRNN} \end{equation}
<p>Where \([h^{\langle t-1 \rangle},x^{\langle t \rangle}]\) means that \(h^{\langle t-1 \rangle}\) and \(x^{\langle t \rangle}\) are concatenated together.</p>
<p>Here's the implementation of the forward method for a vanilla RNN.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">forward_vanilla_RNN</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Forward propagation for a a single vanilla RNN cell</span>

<span class="sd">    Args:</span>
<span class="sd">     inputs: collection of x and the hidden state</span>
<span class="sd">     weights: collections of weights and biases</span>

<span class="sd">    Returns:</span>
<span class="sd">     hidden state twice (so we don't have to implement y for the scan)</span>
<span class="sd">    """</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="n">w1</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">__</span> <span class="o">=</span> <span class="n">weights</span>
    <span class="n">h_t</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span>
                    <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">hidden_state</span><span class="p">,</span>
                                       <span class="n">x</span><span class="p">]))</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">h_t</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">h_t</span>
</pre></div>
<p>As you can see, we omitted the computation of \(\hat{y}^{\langle t \rangle}\). This was done for the sake of simplicity, so you can focus on the way that hidden states are updated here and in the GRU cell.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org04f6d8b">
<h4 id="org04f6d8b">The Forward Method For GRUs</h4>
<div class="outline-text-4" id="text-org04f6d8b">
<p>A GRU cell has more computations than the ones that vanilla RNNs have.</p>
<p>GRUs have relevance \(\Gamma_r\) and update \(\Gamma_u\) gates that control how the hidden state \(h^{\langle t \rangle}\) is updated on every time step. With these gates, GRUs are capable of keeping relevant information in the hidden state even for long sequences. The equations needed for the forward method in GRUs are:</p>
\begin{equation} \Gamma_r=\sigma{(W_r[h^{\langle t-1\rangle}, x^{\langle t\rangle}]+b_r)} \end{equation} \begin{equation} \Gamma_u=\sigma{(W_u[h^{\langle t-1\rangle}, x^{\langle t\rangle}]+b_u)} \end{equation} \begin{equation} c^{\langle t\rangle}=\tanh{(W_h[\Gamma_r*h^{\langle t-1\rangle},x^{\langle t\rangle}]+b_h)} \end{equation} \begin{equation} h^{\langle t\rangle}=\Gamma_u*c^{\langle t\rangle}+(1-\Gamma_u)*h^{\langle t-1\rangle} \end{equation}
<p>In the next cell, we'll implement the forward method for a GRU cell by computing the update <code>u</code> and relevance <code>r</code> gates, and the candidate hidden state <code>c</code>.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">forward_GRU</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Forward propagation for a single GRU cell</span>

<span class="sd">    Args: </span>
<span class="sd">     inputs: collection of (x, h_t)</span>
<span class="sd">     weights: tuple of weights</span>

<span class="sd">    Returns:</span>
<span class="sd">     updated hidden weights twice</span>
<span class="sd">    """</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">h_t</span> <span class="o">=</span> <span class="n">inputs</span>

    <span class="c1"># weights.</span>
    <span class="n">wu</span><span class="p">,</span> <span class="n">wr</span><span class="p">,</span> <span class="n">wc</span><span class="p">,</span> <span class="n">bu</span><span class="p">,</span> <span class="n">br</span><span class="p">,</span> <span class="n">bc</span> <span class="o">=</span> <span class="n">weights</span>

    <span class="c1"># Update gate</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wu</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">h_t</span><span class="p">,</span> <span class="n">x</span><span class="p">]))</span> <span class="o">+</span> <span class="n">bu</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>

    <span class="c1"># Relevance gate</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wr</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">h_t</span><span class="p">,</span> <span class="n">x</span><span class="p">]))</span> <span class="o">+</span> <span class="n">br</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

    <span class="c1"># Candidate hidden state </span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">wc</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">r</span> <span class="o">*</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">x</span><span class="p">]))</span> <span class="o">+</span> <span class="n">bc</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

    <span class="c1"># New Hidden state h_t</span>
    <span class="n">h_t</span> <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">*</span> <span class="n">h_t</span>
    <span class="k">return</span> <span class="n">h_t</span><span class="p">,</span> <span class="n">h_t</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgfdecc67"></a>A Check<br>
<div class="outline-text-5" id="text-orgfdecc67">
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">forward_GRU</span><span class="p">([</span><span class="n">inputs</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">inputs</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">],</span> <span class="n">weights</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">9.77779014e-01</span><span class="p">],</span>
                        <span class="p">[</span><span class="o">-</span><span class="mf">9.97986240e-01</span><span class="p">],</span>
                        <span class="p">[</span><span class="o">-</span><span class="mf">5.19958083e-01</span><span class="p">],</span>
                        <span class="p">[</span><span class="o">-</span><span class="mf">9.99999886e-01</span><span class="p">],</span>
                        <span class="p">[</span><span class="o">-</span><span class="mf">9.99707004e-01</span><span class="p">],</span>
                        <span class="p">[</span><span class="o">-</span><span class="mf">3.02197037e-04</span><span class="p">],</span>
                        <span class="p">[</span><span class="o">-</span><span class="mf">9.58733503e-01</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">2.10804828e-02</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">9.77365398e-05</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">9.99833090e-01</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">1.63200940e-08</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">8.51874303e-01</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">5.21399924e-02</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">2.15495959e-02</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">9.99878828e-01</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">9.77165472e-01</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org90dbf88">
[[ 9.77779014e-01]
 [-9.97986240e-01]
 [-5.19958083e-01]
 [-9.99999886e-01]
 [-9.99707004e-01]
 [-3.02197037e-04]
 [-9.58733503e-01]
 [ 2.10804828e-02]
 [ 9.77365398e-05]
 [ 9.99833090e-01]
 [ 1.63200940e-08]
 [ 8.51874303e-01]
 [ 5.21399924e-02]
 [ 2.15495959e-02]
 [ 9.99878828e-01]
 [ 9.77165472e-01]]
</pre></div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org31a1c91">
<h3 id="org31a1c91">Part 2: Implementation of the <code>scan</code> function</h3>
<div class="outline-text-3" id="text-org31a1c91">
<p>The <code>scan</code> function is used for forward propagation in RNNs. It takes as inputs:</p>
<ul class="org-ul">
<li><code>fn</code> : the function to be called recurrently (i.e. <code>forward_GRU</code>)</li>
<li><code>elems</code> : the list of inputs for each time step (<code>X</code>)</li>
<li><code>weights</code> : the parameters needed to compute <code>fn</code></li>
<li><code>h_0</code> : the initial hidden state</li>
</ul>
<p><code>scan</code> goes through all the elements <code>x</code> in <code>elems</code>, calls the function <code>fn</code> with arguments ([=x=, <code>h_t=],=weights</code>), stores the computed hidden state <code>h_t</code> and appends the result to a list <code>ys</code>. Complete the following cell by calling <code>fn</code> with arguments ([=x=, <code>h_t=],=weights</code>).</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">scan</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">elems</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">h_0</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Forward propagation for RNNs</span>

<span class="sd">    Args:</span>
<span class="sd">     function: callable that updates the hidden state</span>
<span class="sd">      elems: input (x)</span>
<span class="sd">      weights: collection of weights</span>
<span class="sd">      h_0: the initial hidden weights</span>
<span class="sd">    """</span>
    <span class="n">h_t</span> <span class="o">=</span> <span class="n">h_0</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">elems</span><span class="p">:</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">h_t</span> <span class="o">=</span> <span class="n">fn</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">h_t</span><span class="p">],</span> <span class="n">weights</span><span class="p">)</span>
        <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ys</span><span class="p">,</span> <span class="n">h_t</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org45cadf3">
<h3 id="org45cadf3">Comparing Vanilla RNNs and GRUs</h3>
<div class="outline-text-3" id="text-org45cadf3">
<p>You have already seen how forward propagation is computed for vanilla RNNs and GRUs. As a quick recap, you need to have a forward method for the recurrent cell and a function like <code>scan</code> to go through all the elements from a sequence using a forward method. You saw that GRUs performed more computations than vanilla RNNs, and you can check that they have 3 times more parameters. In the next two cells, we compute forward propagation for a sequence with 256 time steps (<code>T</code>) for an RNN and a GRU with the same hidden state <code>h_t</code> size (=h_dim==16).</p>
</div>
<div class="outline-4" id="outline-container-org4a1997f">
<h4 id="org4a1997f">Vanilla RNNs</h4>
<div class="outline-text-4" id="text-org4a1997f">
<p>We'll train the RNN and also time it.</p>
<div class="highlight">
<pre><span></span><span class="n">tick</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
<span class="n">ys</span><span class="p">,</span> <span class="n">h_T</span> <span class="o">=</span> <span class="n">scan</span><span class="p">(</span><span class="n">forward_vanilla_RNN</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)</span>
<span class="n">tock</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
<span class="n">RNN_time</span><span class="o">=</span><span class="p">(</span><span class="n">tock</span><span class="o">-</span><span class="n">tick</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"It took </span><span class="si">{</span><span class="n">RNN_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">ms to run the forward method for the vanilla RNN."</span><span class="p">)</span>
</pre></div>
<pre class="example">
It took 2.03ms to run the forward method for the vanilla RNN.
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgabe5c0c">
<h4 id="orgabe5c0c">GRUs</h4>
<div class="outline-text-4" id="text-orgabe5c0c">
<div class="highlight">
<pre><span></span><span class="n">tick</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
<span class="n">ys</span><span class="p">,</span> <span class="n">h_T</span> <span class="o">=</span> <span class="n">scan</span><span class="p">(</span><span class="n">forward_GRU</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">hidden_state</span><span class="p">)</span>
<span class="n">tock</span> <span class="o">=</span> <span class="n">perf_counter</span><span class="p">()</span>
<span class="n">GRU_time</span><span class="o">=</span><span class="p">(</span><span class="n">tock</span> <span class="o">-</span> <span class="n">tick</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"It took </span><span class="si">{</span><span class="n">GRU_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">ms to run the forward method for the GRU."</span><span class="p">)</span>
</pre></div>
<pre class="example">
It took 5.48ms to run the forward method for the GRU.
</pre>
<p>GRUs take more time to compute. This means that training and prediction would take more time for a GRU than for a vanilla RNN. However, GRUs allow you to propagate relevant information even for long sequences, so when selecting an architecture for NLP we should assess the tradeoff between computational time and performance.</p>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/jax-numpy-and-perplexity/">Jax, Numpy, and Perplexity</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/jax-numpy-and-perplexity/" rel="bookmark"><time class="published dt-published" datetime="2020-12-31T21:41:39-08:00" itemprop="datePublished" title="2020-12-31 21:41">2020-12-31 21:41</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/jax-numpy-and-perplexity/#orged8f697">Beginning</a>
<ul>
<li><a href="posts/nlp/jax-numpy-and-perplexity/#orga207ac1">Imports</a></li>
<li><a href="posts/nlp/jax-numpy-and-perplexity/#org19b081d">Set Up</a>
<ul>
<li><a href="posts/nlp/jax-numpy-and-perplexity/#org5d6dc68">The Data Paths</a></li>
<li><a href="posts/nlp/jax-numpy-and-perplexity/#org6f8fb6f">The Random Seed</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/jax-numpy-and-perplexity/#org324bcd6">Middle</a>
<ul>
<li><a href="posts/nlp/jax-numpy-and-perplexity/#orgf86673c">Numpy vs Trax</a></li>
<li><a href="posts/nlp/jax-numpy-and-perplexity/#org0c1655b">Calculating Perplexity</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orged8f697">
<h2 id="orged8f697">Beginning</h2>
<div class="outline-text-2" id="text-orged8f697"></div>
<div class="outline-3" id="outline-container-orga207ac1">
<h3 id="orga207ac1">Imports</h3>
<div class="outline-text-3" id="text-orga207ac1">
<p><b>Note to future self:</b> The default jax installation from <code>pip</code> is CPU only, to get it to run on the GPU (which seems to be the main reason to use it) you need to specify it. Right now the command is:</p>
<pre class="example" id="org759d699">
pip install jaxlib==0.1.57+cuda111 -f https://storage.googleapis.com/jax-releases/jax_releases.html
</pre>
<p>Where <code>cuda111</code> refers to the fact that I have cuda 11.1 installed on the server, so I need that version. See the <a href="https://github.com/google/jax#installation">installation instructions</a> for more information (and to see if anything changes).</p>
<div class="highlight">
<pre><span></span><span class="c1"># from python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">trax</span>
<span class="kn">import</span> <span class="nn">trax.fastmath.numpy</span> <span class="k">as</span> <span class="nn">trax_numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org19b081d">
<h3 id="org19b081d">Set Up</h3>
<div class="outline-text-3" id="text-org19b081d"></div>
<div class="outline-4" id="outline-container-org5d6dc68">
<h4 id="org5d6dc68">The Data Paths</h4>
<div class="outline-text-4" id="text-org5d6dc68">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Paths</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">targets</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"RNN_TARGETS"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(),</span>
    <span class="n">predictions</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"RNN_PREDICTIONS"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6f8fb6f">
<h4 id="org6f8fb6f">The Random Seed</h4>
<div class="outline-text-4" id="text-org6f8fb6f">
<div class="highlight">
<pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># trax no longer has a global seed setting - pass it to the training.Loop</span>
<span class="c1"># trax.supervised.trainer_lib.init_random_number_generators(SEED)</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org324bcd6">
<h2 id="org324bcd6">Middle</h2>
<div class="outline-text-2" id="text-org324bcd6"></div>
<div class="outline-3" id="outline-container-orgf86673c">
<h3 id="orgf86673c">Numpy vs Trax</h3>
<div class="outline-text-3" id="text-orgf86673c">
<p>One important change to take into consideration is that the types of the resulting objects will be different depending on the version of numpy. With regular numpy you get <code>numpy.ndarray</code> but with Trax's numpy you will get <code>jax.interpreters.xla.DeviceArray</code>. These two types map to each other. So if you find some error logs mentioning DeviceArray type, don't worry about it, treat it like you would treat an ndarray and march ahead.</p>
<p>You can get a randomized numpy array by using the <code>numpy.random.random()</code> function.</p>
<p>This is one of the functionalities that Trax's numpy does not currently support in the same way as the regular numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The regular numpy array looks like this:</span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{</span><span class="n">numpy_array</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"It is of type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org083bcb1">
The regular numpy array looks like this:

 [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514
  0.10108656 0.92848807 0.60910917 0.59655344]
 [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249
  0.58940123 0.04993276 0.56179184 0.76635847]
 [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549
  0.16242374 0.70937058 0.16062408 0.81077677]
 [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613
  0.67483453 0.78238275 0.69520163 0.32895445]
 [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492
  0.39807245 0.72790532 0.86333097 0.02616954]]

It is of type: &lt;class 'numpy.ndarray'&gt;
</pre>
<p>You can easily cast regular numpy arrays or lists into trax numpy arrays using the <code>trax.fastmath.numpy.array()</code> function:</p>
<div class="highlight">
<pre><span></span><span class="n">trax_numpy_array</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The trax numpy array looks like this:</span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{</span><span class="n">trax_numpy_array</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"It is of type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">trax_numpy_array</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org97aa771">
The trax numpy array looks like this:

 [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514
  0.10108656 0.9284881  0.60910916 0.59655344]
 [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246
  0.58940125 0.04993276 0.56179184 0.7663585 ]
 [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255
  0.16242374 0.7093706  0.16062407 0.81077677]
 [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613
  0.67483455 0.7823827  0.69520164 0.32895446]
 [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493
  0.39807245 0.72790533 0.86333096 0.02616954]]

It is of type: &lt;class 'jax.interpreters.xla._DeviceArray'&gt;
</pre>
<p>The previous section was a quick look at Trax's numpy. However this notebook also aims to teach you how you can calculate the perplexity of a trained model.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org0c1655b">
<h3 id="org0c1655b">Calculating Perplexity</h3>
<div class="outline-text-3" id="text-org0c1655b">
<p>The <i>perplexity</i> is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as:</p>
<p>\[ P(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}} \]</p>
<p>As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our <code>RNN</code>, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:</p>
\begin{align} log P(W) &amp;= {log\left(\sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}}\right)} \\ &amp;= {log\left({\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}}\right)^{\frac{1}{N}}} \\ &amp;= {log\left({\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\right)^{-\frac{1}{N}}} \\ &amp;= -\frac{1}{N}{log\left({\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\right)} \\ &amp;= -\frac{1}{N}{\left({\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\right)} \end{align}
<p>We're going to use some pre-made arrays.</p>
<div class="highlight">
<pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Paths</span><span class="o">.</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Paths</span><span class="o">.</span><span class="n">targets</span><span class="p">)</span>
</pre></div>
<p>Now we'll cast the numpy arrays to jax.interpreters.xla.DeviceArrays.</p>
<div class="highlight">
<pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'predictions has shape: </span><span class="si">{</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'targets has shape: </span><span class="si">{</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
predictions has shape: (32, 64, 256)
targets has shape: (32, 64)
</pre>
<p>Notice that the predictions have an extra dimension - this is the same length as the size of the vocabulary used. Because of this you will need a way of reshaping <code>targets</code> to match this shape. For this we will use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.one_hot">trax.layers.one_hot</a>.</p>
<p>Also note that we can get the size of the last dimension using <code>predictions.shape[-1]</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">reshaped_targets</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span> <span class="n">n_categories</span><span class="o">=</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'reshaped_targets has shape: </span><span class="si">{</span><span class="n">reshaped_targets</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
reshaped_targets has shape: (32, 64, 256)
</pre>
<p>By calculating the product of the predictions and the reshaped targets and summing across the last dimension, we can compute the total log perplexity.</p>
<div class="highlight">
<pre><span></span><span class="n">total_log_perplexity</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">predictions</span> <span class="o">*</span> <span class="n">reshaped_targets</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>Now you will need to account for the padding so this metric is not artificially deflated (since a lower perplexity means a better model). To identify which elements are padding and which are not, you can use <code>np.equal()</code> and get a tensor with <code>True</code> in the positions of actual values and <code>False</code> where there are paddings.</p>
<div class="highlight">
<pre><span></span><span class="n">equals_zero</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">equals_zero</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[False False False ...  True  True  True]
 [False False False ...  True  True  True]
 [False False False ...  True  True  True]
 ...
 [False False False ...  True  True  True]
 [False False False ...  True  True  True]
 [False False False ...  True  True  True]]
</pre>
<p><code>equals_zero</code> is a boolean array that has <code>True</code> wherever the cell had a 0 and <code>False</code> everywhere else. To make it numeric we can subtract the boolean array from 1 (generally in python True is treated as 1 and False as 0).</p>
<div class="highlight">
<pre><span></span><span class="n">non_pad</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">equals_zero</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'non_pad has shape: </span><span class="si">{</span><span class="n">non_pad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'non_pad looks like this: </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">non_pad</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org3503f0f">
non_pad has shape: (32, 64)

non_pad looks like this: 

 [[1. 1. 1. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]
 ...
 [1. 1. 1. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]]
</pre>
<p>Now if we multiply <code>total_log_perplexity</code> by the <code>non_pad</code> we'll zero-out all the entries in <code>total_log_perplexity</code> where <code>non_pad</code> has zero.</p>
<div class="highlight">
<pre><span></span><span class="n">real_log_perplexity</span> <span class="o">=</span> <span class="n">total_log_perplexity</span> <span class="o">*</span> <span class="n">non_pad</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'real perplexity still has shape: </span><span class="si">{</span><span class="n">real_log_perplexity</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
real perplexity still has shape: (32, 64)
</pre>
<p>We can check the effect of filtering out the padding by looking at the two log perplexity tensors.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'log perplexity tensor before filtering padding: </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">total_log_perplexity</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'log perplexity tensor after filtering padding: </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">real_log_perplexity</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org10f067f">
log perplexity tensor before filtering padding: 

 [[ -5.396545    -1.0311184   -0.66916656 ... -22.37673    -23.18771
  -21.843483  ]
 [ -4.5857706   -1.1341286   -8.538033   ... -20.15686    -26.837097
  -23.57502   ]
 [ -5.2223887   -1.2824144   -0.17312431 ... -21.328228   -19.854412
  -33.88444   ]
 ...
 [ -5.396545   -17.291681    -4.360766   ... -20.825802   -21.065838
  -22.443115  ]
 [ -5.9313164  -14.247417    -0.2637329  ... -26.743248   -18.38433
  -22.355278  ]
 [ -5.670536    -0.10595131   0.         ... -23.332523   -28.087376
  -23.878807  ]]

log perplexity tensor after filtering padding: 

 [[ -5.396545    -1.0311184   -0.66916656 ...  -0.          -0.
   -0.        ]
 [ -4.5857706   -1.1341286   -8.538033   ...  -0.          -0.
   -0.        ]
 [ -5.2223887   -1.2824144   -0.17312431 ...  -0.          -0.
   -0.        ]
 ...
 [ -5.396545   -17.291681    -4.360766   ...  -0.          -0.
   -0.        ]
 [ -5.9313164  -14.247417    -0.2637329  ...  -0.          -0.
   -0.        ]
 [ -5.670536    -0.10595131   0.         ...  -0.          -0.
   -0.        ]]
</pre>
<p>To get a single average log perplexity across all the elements in the batch you can sum across both dimensions and divide by the number of elements. Note that the result will be the negative of the real log perplexity of the model.</p>
<div class="highlight">
<pre><span></span><span class="n">log_perplexity</span> <span class="o">=</span> <span class="o">-</span><span class="n">trax_numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">real_log_perplexity</span><span class="p">)</span> <span class="o">/</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">non_pad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"log perplexity: </span><span class="si">{</span><span class="n">log_perplexity</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">, "</span>
      <span class="sa">f</span><span class="s2">"perplexity: </span><span class="si">{</span><span class="n">trax_numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_perplexity</span><span class="p">)</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
log perplexity: 2.3281, perplexity: 10.2586
</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/hidden-state-activation/">Hidden State Activation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/hidden-state-activation/" rel="bookmark"><time class="published dt-published" datetime="2020-12-30T19:02:56-08:00" itemprop="datePublished" title="2020-12-30 19:02">2020-12-30 19:02</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/hidden-state-activation/#org260ed18">Hidden State Activation</a>
<ul>
<li><a href="posts/nlp/hidden-state-activation/#org8c8dbd3">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/hidden-state-activation/#org24cee19">Middle</a>
<ul>
<li><a href="posts/nlp/hidden-state-activation/#org7818da2">Joining</a>
<ul>
<li><a href="posts/nlp/hidden-state-activation/#org45efa24">Weights: Horizontal Concatenation</a></li>
<li><a href="posts/nlp/hidden-state-activation/#org02d5c82">Hidden State & Inputs: Vertical Concatenation</a></li>
<li><a href="posts/nlp/hidden-state-activation/#org1a07140">Option 1: concatenate - Rows</a></li>
<li><a href="posts/nlp/hidden-state-activation/#org1e4da29">Option 2: vstack</a></li>
</ul>
</li>
<li><a href="posts/nlp/hidden-state-activation/#org5b1a0dc">Verify Formulas</a>
<ul>
<li><a href="posts/nlp/hidden-state-activation/#orge3ac2c9">The Data</a></li>
<li><a href="posts/nlp/hidden-state-activation/#org384ec7c">Formula 1</a></li>
<li><a href="posts/nlp/hidden-state-activation/#org3084c1a">Formula 2</a></li>
<li><a href="posts/nlp/hidden-state-activation/#orgbd52f6f">Verification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org260ed18">
<h2 id="org260ed18">Hidden State Activation</h2>
<div class="outline-text-2" id="text-org260ed18">
<p>This is the hidden state activation function for a vanilla RNN.</p>
<p>\[ h^{\langle t\rangle}=g(W_{h}[h^{\langle t-1\rangle},x^{\langle t\rangle}] + b_h) \]</p>
<p>Which is another way of writing this:</p>
<p>\[ h^{\langle t\rangle}=g(W_{hh}h^{\langle t-1\rangle} \oplus W_{hx}x^{\langle t\rangle} + b_h) \]</p>
<p>Where</p>
<ul class="org-ul">
<li>\(W_{h}\) in the first formula is denotes the <b>horizontal</b> concatenation of \(W_{hh}\) and \(W_{hx}\) from the second formula.</li>
<li>\(W_{h}\) in the first formula is then multiplied by \([h^{\langle t-1\rangle},x^{\langle t\rangle}]\), another concatenation of parameters from the second formula but this time in a different direction, i.e <b>vertical</b>.</li>
</ul>
<p>Let us see what this means computationally.</p>
</div>
<div class="outline-3" id="outline-container-org8c8dbd3">
<h3 id="org8c8dbd3">Imports</h3>
<div class="outline-text-3" id="text-org8c8dbd3">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org24cee19">
<h2 id="org24cee19">Middle</h2>
<div class="outline-text-2" id="text-org24cee19"></div>
<div class="outline-3" id="outline-container-org7818da2">
<h3 id="org7818da2">Joining</h3>
<div class="outline-text-3" id="text-org7818da2"></div>
<div class="outline-4" id="outline-container-org45efa24">
<h4 id="org45efa24">Weights: Horizontal Concatenation</h4>
<div class="outline-text-4" id="text-org45efa24">
<p>A join along the vertical boundary is called a <b>horizontal concatenation</b> or <b>horizontal stack</b>.</p>
<p>Visually, it looks like this:- \(W_h = \left [ W_{hh} \ | \ W_{hx} \right ]\).</p>
<p>We'll look at two different ways to achieve this using numpy.</p>
<p><b>Note:</b> <i>The values used to populate the arrays, below, have been chosen to aid in visual illustration only. They are NOT what you'd expect to use building a model, which would typically be random variables instead.</i></p>
<p>First create some dummy data. The <a href="https://numpy.org/doc/stable/reference/generated/numpy.full.html">numpy.full</a> function creates an array of a given shape that all has the same values. Our first array is almost like <code>numpy.ones</code> except it uses the dtype of the number you pass in so it will be integers, not floats.</p>
<div class="highlight">
<pre><span></span><span class="n">w_hh</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">w_hx</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">9</span><span class="p">)</span>
</pre></div>
<p>We could use some random initializations, but it would make it harder to see the joins.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Data --</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"w_hh :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w_hh</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"w_hh shape :"</span><span class="p">,</span> <span class="n">w_hh</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"w_hx :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w_hx</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"w_hx shape :"</span><span class="p">,</span> <span class="n">w_hx</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org1cd8b23">
-- Data --

w_hh :
[[1 1]
 [1 1]
 [1 1]]
w_hh shape : (3, 2) 

w_hx :
[[9 9 9]
 [9 9 9]
 [9 9 9]]
w_hx shape : (3, 3) 
</pre></div>
<ul class="org-ul">
<li><a id="org7be4dda"></a>Option 1: concatenate - horizontal<br>
<div class="outline-text-5" id="text-org7be4dda">
<p>First we'll use <a href="https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html">numpy.concatenate</a>.</p>
<div class="highlight">
<pre><span></span><span class="n">ROWS</span><span class="p">,</span> <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">w_h1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">w_hx</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">COLUMNS</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"option 1 : concatenate</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"w_h :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w_h1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"w_h shape :"</span><span class="p">,</span> <span class="n">w_h1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
option 1 : concatenate

w_h :
[[1 1 9 9 9]
 [1 1 9 9 9]
 [1 1 9 9 9]]
w_h shape : (3, 5) 

</pre></div>
</li>
<li><a id="orgdb582f8"></a>Option 2: hstack<br>
<div class="outline-text-5" id="text-orgdb582f8">
<p>Now we'll try <a href="https://numpy.org/doc/stable/reference/generated/numpy.hstack.html">numpy.hstack</a>.</p>
<div class="highlight">
<pre><span></span><span class="n">w_h2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">w_hx</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"option 2 : hstack</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"w_h :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w_h2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"w_h shape :"</span><span class="p">,</span> <span class="n">w_h2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
option 2 : hstack

w_h :
[[1 1 9 9 9]
 [1 1 9 9 9]
 [1 1 9 9 9]]
w_h shape : (3, 5)
</pre>
<p>As you can see, <code>hstack</code> gives you the same thing as <code>concatenate</code> along columns, <code>concatenate</code> also allows you to concatenate along rows and is more general than <code>hstack</code>. Although <code>hstack</code> might be more intuitive.</p>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org02d5c82">
<h4 id="org02d5c82">Hidden State & Inputs: Vertical Concatenation</h4>
<div class="outline-text-4" id="text-org02d5c82">
<p>Joining along a horizontal boundary is called a vertical concatenation or vertical stack. Visually it looks like this:</p>
<p>\[ [h^{\langle t-1\rangle},x^{\langle t\rangle}] = \left[ \frac{h^{\langle t-1\rangle}}{x^{\langle t\rangle}} \right] \]</p>
<p>We'll look at two different ways to achieve this using numpy.</p>
<p>First create some more dummy data.</p>
<div class="highlight">
<pre><span></span><span class="n">h_t_prev</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">9</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Data --</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"h_t_prev :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h_t_prev</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"h_t_prev shape :"</span><span class="p">,</span> <span class="n">h_t_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x_t :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x_t shape :"</span><span class="p">,</span> <span class="n">x_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orgcd6c878">
-- Data --

h_t_prev :
[[1]
 [1]]
h_t_prev shape : (2, 1) 

x_t :
[[9]
 [9]
 [9]]
x_t shape : (3, 1) 
</pre></div>
</div>
<div class="outline-4" id="outline-container-org1a07140">
<h4 id="org1a07140">Option 1: concatenate - Rows</h4>
<div class="outline-text-4" id="text-org1a07140">
<div class="highlight">
<pre><span></span><span class="n">ax_1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
    <span class="p">(</span><span class="n">h_t_prev</span><span class="p">,</span> <span class="n">x_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROWS</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"option 1 : concatenate</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"ax_1 :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ax_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"ax_1 shape :"</span><span class="p">,</span> <span class="n">ax_1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
option 1 : concatenate

ax_1 :
[[1]
 [1]
 [9]
 [9]
 [9]]
ax_1 shape : (5, 1) 

</pre></div>
</div>
<div class="outline-4" id="outline-container-org1e4da29">
<h4 id="org1e4da29">Option 2: vstack</h4>
<div class="outline-text-4" id="text-org1e4da29">
<p><a href="https://numpy.org/doc/stable/reference/generated/numpy.vstack.html#numpy.vstack">vstack</a> is much like <code>hstack</code> except instead of inserting columns it appends rows, more of what the word <i>stack</i> would seem to suggest.</p>
<div class="highlight">
<pre><span></span><span class="n">ax_2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">h_t_prev</span><span class="p">,</span> <span class="n">x_t</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"option 2 : vstack</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"ax_2 :"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ax_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"ax_2 shape :"</span><span class="p">,</span> <span class="n">ax_2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
option 2 : vstack

ax_2 :
[[1]
 [1]
 [9]
 [9]
 [9]]
ax_2 shape : (5, 1)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5b1a0dc">
<h3 id="org5b1a0dc">Verify Formulas</h3>
<div class="outline-text-3" id="text-org5b1a0dc">
<p>Now that we know how to do the concatenations, horizontal and vertical, let's verify that the two formulas produce the same result.</p>
<ul class="org-ul">
<li><b>Formula 1:</b> \(h^{\langle t\rangle}=g(W_{h}[h^{\langle t-1\rangle},x^{\langle t\rangle}] + b_h)\)</li>
<li><b>Formula 2:</b> \(h^{\langle t\rangle}=g(W_{hh}h^{\langle t-1\rangle} \oplus W_{hx}x^{\langle t\rangle} + b_h)\)</li>
</ul>
<p>We want to assure ourselves that <b>Formula 1</b> \(\Leftrightarrow\) <b>Formula 2</b>.</p>
<p>We will initially ignore the bias term \(b_h\) and the activation function <i>g( )</i> because the transformation will be identical for each formula. So what we really want to compare is the result of the following parameters inside each formula:</p>
<p>\[ W_{h}[h^{\langle t-1\rangle},x^{\langle t\rangle}] \quad \Leftrightarrow \quad W_{hh}h^{\langle t-1\rangle} \oplus W_{hx}x^{\langle t\rangle} \]</p>
<p>We'll see how to do this using matrix multiplication combined with the data and techniques (stacking/concatenating) from above.</p>
</div>
<div class="outline-4" id="outline-container-orge3ac2c9">
<h4 id="orge3ac2c9">The Data</h4>
<div class="outline-text-4" id="text-orge3ac2c9">
<div class="highlight">
<pre><span></span><span class="n">w_hh</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">w_hx</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">h_t_prev</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_t</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">9</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org384ec7c">
<h4 id="org384ec7c">Formula 1</h4>
<div class="outline-text-4" id="text-org384ec7c">
<div class="highlight">
<pre><span></span><span class="n">stack_1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">w_hx</span><span class="p">))</span>
<span class="n">stack_2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">h_t_prev</span><span class="p">,</span> <span class="n">x_t</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Formula 1"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Term1:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span><span class="n">stack_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Term2:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span><span class="n">stack_2</span><span class="p">)</span>
<span class="n">formula_1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">stack_1</span><span class="p">,</span>
                         <span class="n">stack_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Output:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">formula_1</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org2d850e4">

Formula 1
Term1:
 [[1 1 9 9 9]
 [1 1 9 9 9]
 [1 1 9 9 9]]
Term2:
 [[1]
 [1]
 [9]
 [9]
 [9]]
Output:
[[245]
 [245]
 [245]]
</pre></div>
</div>
<div class="outline-4" id="outline-container-org3084c1a">
<h4 id="org3084c1a">Formula 2</h4>
<div class="outline-text-4" id="text-org3084c1a">
<div class="highlight">
<pre><span></span><span class="n">term_1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w_hh</span><span class="p">,</span> <span class="n">h_t_prev</span><span class="p">)</span>
<span class="n">term_2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w_hx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Formula 2"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Term1:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">term_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Term2:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">term_2</span><span class="p">)</span>

<span class="n">formula_2</span> <span class="o">=</span> <span class="n">term_1</span> <span class="o">+</span> <span class="n">term_2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Output:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">formula_2</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orgbbce7f8">

Formula 2
Term1:
 [[2]
 [2]
 [2]]
Term2:
 [[243]
 [243]
 [243]]

Output:
[[245]
 [245]
 [245]] 
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgbd52f6f">
<h4 id="orgbd52f6f">Verification</h4>
<div class="outline-text-4" id="text-orgbd52f6f">
<p><a href="https://numpy.org/doc/stable/reference/generated/numpy.allclose.html">np.allclose</a> checks that each entry in one array is within a certain tolerance of the corresponding entry in another. For this example we're using integers, so you could probably use <code>all(a == b)</code> but otherwise, when you have floats, it's better to use <code>allclose</code> since floats won't always be exact.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Verify --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Results are the same :"</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">formula_1</span><span class="p">,</span> <span class="n">formula_2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Also the same: </span><span class="si">{</span><span class="nb">all</span><span class="p">(</span><span class="n">formula_1</span><span class="o">==</span><span class="n">formula_2</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Verify --
Results are the same : True
Also the same: True
</pre>
<p>Now we'll add a sigmoid activation function and bias term as a final check so we can see how this would work in action.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the sigmoid of x</span>

<span class="sd">    Args:</span>
<span class="sd">     x: numpy array or list or float</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">bias</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">((</span><span class="n">formula_1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Formula 1 Output:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">formula_1</span> <span class="o">+</span> <span class="n">bias</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Formula 2 Output:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">formula_2</span> <span class="o">+</span> <span class="n">bias</span><span class="p">))</span>

<span class="k">assert</span> <span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">formula_1</span> <span class="o">+</span> <span class="n">bias</span><span class="p">),</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">formula_2</span> <span class="o">+</span> <span class="n">bias</span><span class="p">))</span>
</pre></div>
<pre class="example">
Formula 1 Output:
 [[1.]
 [1.]
 [1.]]
Formula 2 Output:
 [[1.]
 [1.]
 [1.]]
</pre></div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/tensorflow-docker-setup/">Tensorflow Docker Setup</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/tensorflow-docker-setup/" rel="bookmark"><time class="published dt-published" datetime="2020-12-27T14:12:26-08:00" itemprop="datePublished" title="2020-12-27 14:12">2020-12-27 14:12</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/tensorflow-docker-setup/#orgbadd54d">Beginning</a></li>
<li><a href="posts/tensorflow-docker-setup/#org8df9afa">Setting Up</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgbadd54d">
<h2 id="orgbadd54d">Beginning</h2>
<div class="outline-text-2" id="text-orgbadd54d">
<p>I recently re-started using tensorflow and the python interpreter kept crashing. It appears that they compiled the latest version to require <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#Advanced_Vector_Extensions_2">AVX2</a> and the server I was using has <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#Advanced_Vector_Extensions_2">AVX</a> but not AVX2. I couldn't find any documentation about this requirement, but running the code on a different machine that has both AVX and AVX2 got rid of the problem. This might be a transient problem, as the nightly build doesn't crash on either machine, but trying to run the nightly build with other code is a nightmare as it seems that every framework related to tensorflow tries to revert the version back to the broken one, so I gave up and changed machines. The process of setting up cuda and tensorflow over and over again proved difficult, as there's different ways to do it (through apt, using nvidia installers, building from source) and each presents a different problem. The version apt installs, for instance puts the folders in places the tensorflow <code>configure.py</code> file can't figure out (if you build tensorflow from source) and using the nvidia debian package for cudnn left my packages in a broken state, as it was trying to install something that then broke another packages requirements… Anyway, I'm going to try and avoid building tensorflow from source and run everything from docker containers.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org8df9afa">
<h2 id="org8df9afa">Setting Up</h2>
<div class="outline-text-2" id="text-org8df9afa">
<p>I don't know for sure that this is necessary, but I followed <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker">nvidia's docker installation</a> instructions. If nothing else you can use it to check that the setup works. After that I setup tensorflow's container with a dockerfile:</p>
<div class="highlight">
<pre><span></span>FROM tensorflow/tensorflow:latest-gpu-py3-jupyter
RUN apt-get update &amp;& \
        apt-get install openssh-server --yes &amp;& \
        echo "Adding neurotic user" &amp;& \
        useradd --create-home --shell /bin/bash neurotic
COPY authorized_keys /home/neurotic/.ssh/
ENTRYPOINT service ssh restart &amp;& bash
</pre></div>
<p>The latest tensorflow container comes with python 2.7 as the default for some reason, and all the dependencies are installed with it in mind so to get python 3 (3.6 as of now) you need to specify the <code>py3</code> tag like I did in the from line. Additionally I use ssh-forwarding for jupyter kernels so I can work in emacs with them so I installed the ssh-server and also created a non-root user to run jupyter. The last line <code>ENTRYPOINT service ssh restart &amp;& bash</code> makes sure the ssh-server is running and opens up a bash shell. To build the container I used this command:</p>
<div class="highlight">
<pre><span></span>docker build -t neurotic-tensorflow .
</pre></div>
<p>This creates an image named <code>neurotic-tensorflow</code>. To run it I use this command:</p>
<div class="highlight">
<pre><span></span>docker run --gpus all -p 2222:22 --name data-neurotic \
       --mount type=bind,source=$HOME/projects/neurotic-networks,target=/home/neurotic/neurotic-networks \
       --mount type=bind,source=/media/data,target=/home/neurotic/data \
       -it neurotic-tensorflow bash
</pre></div>
<p>The <code>--gpus all</code> makes the GPUs available. The <code>-p 2222:22</code> flag maps the ssh-server in the container to port 2222 on the host. This allows you to ssh into the container using <code>ssh neurotic@localhost -p 2222</code> without knowing the IP address of the container. You can also grab the IP address and then ssh into it like it's another machine on the network:</p>
<div class="highlight">
<pre><span></span>docker inspect --format "{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}" data-neurotic
</pre></div>
<p>Where <code>data-neurotic</code> is the name given to the container in the <code>docker run</code> command, but the advantage of the port mapping is that:</p>
<ul class="org-ul">
<li>You don't need to know the address of the container if you are on the host machine.</li>
<li>You can ssh into the container from another machine by substituting the host's IP address for <code>localhost</code> in the ssh command</li>
</ul>
<p>The <code>mount</code> options mount some folders into the container so we can share files.</p>
<p>Once you've run it you can restart it at any time using:</p>
<div class="highlight">
<pre><span></span>docker start data-neurotic
</pre></div>
<p>And if you need to run something as root you can attach the running container.</p>
<div class="highlight">
<pre><span></span>docker attach data-neurotic
</pre></div>
<p><b>NOTE:</b> The python 3 container has cuda 10.1 installed but the latest version of tensorflow expects 11.0 - and tensorflow seems to use hard-coded names. So to make it work you either have to upgrade cuda or symlink the file and rename it to look like the newer version.</p>
<div class="highlight">
<pre><span></span>ln -s /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudart.so.10.1 /usr/lib/x86_64-linux-gnu/libcudart.so.11.0
</pre></div>
<p>Tensorflow dependencies are incredibly convoluted and broken all over the place.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/sentiment-analysis-testing-the-model/">Sentiment Analysis: Testing the Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/sentiment-analysis-testing-the-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-23T15:52:18-08:00" itemprop="datePublished" title="2020-12-23 15:52">2020-12-23 15:52</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#orgf53aeff">Beginning</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#org6251883">Imports</a></li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#orgde1058b">Set Up</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#orga314dfc">Download</a></li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#org1fc39e0">The Data Generators</a></li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#org21ffdc5">The Model Builder</a></li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#orgbd2866f">The Accuracy</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#org7742de8">Middle</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#orgaead032">Testing the model on Validation Data</a></li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#orgc6af298">Testing Some Custom Input</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#org84c31e7">A Positive Sentence</a></li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#org010550b">A Negative Sentence</a></li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#org6afd4f7">On Pooh</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#orgecc1748">On Deep Nets</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/#org73bd3b7">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf53aeff">
<h2 id="orgf53aeff">Beginning</h2>
<div class="outline-text-2" id="text-orgf53aeff">
<p>Having trained our Deep Learning model for Sentiment Analysis <a href="posts/nlp/sentiment-analysis-training-the-model/">previously</a> we're now going to test how well it did.</p>
</div>
<div class="outline-3" id="outline-container-org6251883">
<h3 id="org6251883">Imports</h3>
<div class="outline-text-3" id="text-org6251883">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">trax.fastmath.numpy</span> <span class="k">as</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">trax.layers</span> <span class="k">as</span> <span class="nn">trax_layers</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.sentiment_network</span> <span class="kn">import</span> <span class="n">SentimentNetwork</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.tensor_generator</span> <span class="kn">import</span> <span class="n">TensorBuilder</span><span class="p">,</span> <span class="n">TensorGenerator</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgde1058b">
<h3 id="orgde1058b">Set Up</h3>
<div class="outline-text-3" id="text-orgde1058b"></div>
<div class="outline-4" id="outline-container-orga314dfc">
<h4 id="orga314dfc">Download</h4>
<div class="outline-text-4" id="text-orga314dfc">
<p>This is because of all the trouble getting trax and tensorflow working with CUDA means I have to keep re-building the Docker container I'm using.</p>
<div class="highlight">
<pre><span></span><span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/data/datasets/nltk_data/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">"twitter_samples"</span><span class="p">,</span> <span class="n">download_dir</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">data_path</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1fc39e0">
<h4 id="org1fc39e0">The Data Generators</h4>
<div class="outline-text-4" id="text-org1fc39e0">
<div class="highlight">
<pre><span></span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">TensorBuilder</span><span class="p">()</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">TensorGenerator</span><span class="p">,</span> <span class="n">converter</span><span class="p">,</span>
                                     <span class="n">positive_data</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">positive_training</span><span class="p">,</span>
                                     <span class="n">negative_data</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">negative_training</span><span class="p">,</span>
                                     <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">valid_generator</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">TensorGenerator</span><span class="p">,</span>
                          <span class="n">converter</span><span class="p">,</span>
                          <span class="n">positive_data</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">positive_validation</span><span class="p">,</span>
                          <span class="n">negative_data</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">negative_validation</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>

<span class="n">TRAINING_GENERATOR</span><span class="o">=</span><span class="n">train_generator</span><span class="p">()</span>
<span class="n">VALIDATION_GENERATOR</span> <span class="o">=</span> <span class="n">valid_generator</span><span class="p">()</span>
<span class="n">SIZE_OF_VOCABULARY</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">converter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">TRAINING_LOOPS</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">OUTPUT_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">OUTPUT_PATH</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
    <span class="n">OUTPUT_PATH</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org21ffdc5">
<h4 id="org21ffdc5">The Model Builder</h4>
<div class="outline-text-4" id="text-org21ffdc5">
<div class="highlight">
<pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">SentimentNetwork</span><span class="p">(</span>
    <span class="n">training_generator</span><span class="o">=</span><span class="n">TRAINING_GENERATOR</span><span class="p">,</span>
    <span class="n">validation_generator</span><span class="o">=</span><span class="n">VALIDATION_GENERATOR</span><span class="p">,</span>
    <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">SIZE_OF_VOCABULARY</span><span class="p">,</span>
    <span class="n">training_loops</span><span class="o">=</span><span class="n">TRAINING_LOOPS</span><span class="p">,</span>
    <span class="n">output_path</span><span class="o">=</span><span class="n">OUTPUT_PATH</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
<pre class="example" id="orgf954458">
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)

Step    110: Ran 10 train steps in 4.89 secs
Step    110: train CrossEntropyLoss |  0.00662578
Step    110: eval  CrossEntropyLoss |  0.00139236
Step    110: eval          Accuracy |  1.00000000

Step    120: Ran 10 train steps in 2.61 secs
Step    120: train CrossEntropyLoss |  0.03323080
Step    120: eval  CrossEntropyLoss |  0.00684100
Step    120: eval          Accuracy |  1.00000000

Step    130: Ran 10 train steps in 1.27 secs
Step    130: train CrossEntropyLoss |  0.11124543
Step    130: eval  CrossEntropyLoss |  0.00011413
Step    130: eval          Accuracy |  1.00000000

Step    140: Ran 10 train steps in 0.71 secs
Step    140: train CrossEntropyLoss |  0.03609489
Step    140: eval  CrossEntropyLoss |  0.00000590
Step    140: eval          Accuracy |  1.00000000

Step    150: Ran 10 train steps in 1.92 secs
Step    150: train CrossEntropyLoss |  0.08605278
Step    150: eval  CrossEntropyLoss |  0.00003427
Step    150: eval          Accuracy |  1.00000000

Step    160: Ran 10 train steps in 1.31 secs
Step    160: train CrossEntropyLoss |  0.04926774
Step    160: eval  CrossEntropyLoss |  0.00003597
Step    160: eval          Accuracy |  1.00000000

Step    170: Ran 10 train steps in 1.30 secs
Step    170: train CrossEntropyLoss |  0.00986138
Step    170: eval  CrossEntropyLoss |  0.00026259
Step    170: eval          Accuracy |  1.00000000

Step    180: Ran 10 train steps in 0.76 secs
Step    180: train CrossEntropyLoss |  0.00773767
Step    180: eval  CrossEntropyLoss |  0.00038017
Step    180: eval          Accuracy |  1.00000000

Step    190: Ran 10 train steps in 1.35 secs
Step    190: train CrossEntropyLoss |  0.00555876
Step    190: eval  CrossEntropyLoss |  0.00000706
Step    190: eval          Accuracy |  1.00000000

Step    200: Ran 10 train steps in 0.76 secs
Step    200: train CrossEntropyLoss |  0.00381955
Step    200: eval  CrossEntropyLoss |  0.00000122
Step    200: eval          Accuracy |  1.00000000
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgbd2866f">
<h4 id="orgbd2866f">The Accuracy</h4>
<div class="outline-text-4" id="text-orgbd2866f">
<p>This is from the last post. I havent' figured out how to arrange all the code yet.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">y_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Compute a batch accuracy</span>

<span class="sd">    Args: </span>
<span class="sd">       preds: a tensor of shape (dim_batch, output_dim) </span>
<span class="sd">       y: a tensor of shape (dim_batch,) with the true labels</span>
<span class="sd">       y_weights: a n.ndarray with the a weight for each example</span>

<span class="sd">    Returns: </span>
<span class="sd">       accuracy: a float between 0-1 </span>
<span class="sd">       weighted_num_correct (np.float32): Sum of the weighted correct predictions</span>
<span class="sd">       sum_weights (np.float32): Sum of the weights</span>
<span class="sd">    """</span>
    <span class="c1"># Create an array of booleans, </span>
    <span class="c1"># True if the probability of positive sentiment is greater than</span>
    <span class="c1"># the probability of negative sentiment</span>
    <span class="c1"># else False</span>
    <span class="n">is_pos</span> <span class="o">=</span>  <span class="n">preds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">preds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="c1"># convert the array of booleans into an array of np.int32</span>
    <span class="n">is_pos_int</span> <span class="o">=</span> <span class="n">is_pos</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="c1"># compare the array of predictions (as int32) with the target (labels) of type int32</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">is_pos_int</span> <span class="o">==</span> <span class="n">y</span>

    <span class="c1"># Count the sum of the weights.</span>
    <span class="n">sum_weights</span> <span class="o">=</span> <span class="n">y_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># convert the array of correct predictions (boolean) into an arrayof np.float32</span>
    <span class="n">correct_float</span> <span class="o">=</span> <span class="n">correct</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Multiply each prediction with its corresponding weight.</span>
    <span class="n">weighted_correct_float</span> <span class="o">=</span> <span class="n">correct_float</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_weights</span><span class="p">)</span>

    <span class="c1"># Sum up the weighted correct predictions (of type np.float32), to go in the</span>
    <span class="c1"># denominator.</span>
    <span class="n">weighted_num_correct</span> <span class="o">=</span> <span class="n">weighted_correct_float</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Divide the number of weighted correct predictions by the sum of the</span>
    <span class="c1"># weights.</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">weighted_num_correct</span><span class="o">/</span><span class="n">sum_weights</span>

    <span class="k">return</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">weighted_num_correct</span><span class="p">,</span> <span class="n">sum_weights</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7742de8">
<h2 id="org7742de8">Middle</h2>
<div class="outline-text-2" id="text-org7742de8"></div>
<div class="outline-3" id="outline-container-orgaead032">
<h3 id="orgaead032">Testing the model on Validation Data</h3>
<div class="outline-text-3" id="text-orgaead032">
<p>Now we'll test our model's prediction accuracy on validation data.</p>
<p>This program will take in a data generator and the model.</p>
<ul class="org-ul">
<li>The generator allows us to get batches of data. You can use it with a <code>for</code> loop:</li>
</ul>
<pre class="example" id="orgac5c62f">
for batch in iterator: 
   # do something with that batch
</pre>
<p><code>batch</code> has dimensions <code>(X, Y, weights)</code>.</p>
<ul class="org-ul">
<li>Column 0 corresponds to the tweet as a tensor (input).</li>
<li>Column 1 corresponds to its target (actual label, positive or negative sentiment).</li>
<li>Column 2 corresponds to the weights associated (example weights)</li>
<li>You can feed the tweet into model and it will return the predictions for the batch.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: test_model</span>
<span class="k">def</span> <span class="nf">test_model</span><span class="p">(</span><span class="n">generator</span><span class="p">:</span> <span class="n">TensorGenerator</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Calculate the accuracy of the model</span>

<span class="sd">    Args: </span>
<span class="sd">       generator: an iterator instance that provides batches of inputs and targets</span>
<span class="sd">       model: a model instance </span>
<span class="sd">    Returns: </span>
<span class="sd">       accuracy: float corresponding to the accuracy</span>
<span class="sd">    """</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">total_num_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_num_pred</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">generator</span><span class="p">:</span> 

        <span class="c1"># Retrieve the inputs from the batch</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Retrieve the targets (actual labels) from the batch</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Retrieve the example weight.</span>
        <span class="n">example_weight</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="c1"># Make predictions using the inputs</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># Calculate accuracy for the batch by comparing its predictions and targets</span>
        <span class="n">batch_accuracy</span><span class="p">,</span> <span class="n">batch_num_correct</span><span class="p">,</span> <span class="n">batch_num_pred</span> <span class="o">=</span> <span class="n">compute_accuracy</span><span class="p">(</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">example_weight</span><span class="p">)</span>

        <span class="c1"># Update the total number of correct predictions</span>
        <span class="c1"># by adding the number of correct predictions from this batch</span>
        <span class="n">total_num_correct</span> <span class="o">+=</span> <span class="n">batch_num_correct</span>

        <span class="c1"># Update the total number of predictions </span>
        <span class="c1"># by adding the number of predictions made for the batch</span>
        <span class="n">total_num_pred</span> <span class="o">+=</span> <span class="n">batch_num_pred</span>

    <span class="c1"># Calculate accuracy over all examples</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">total_num_correct</span><span class="o">/</span><span class="n">total_num_pred</span>

    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1"># DO NOT EDIT THIS CELL</span>
<span class="c1"># testing the accuracy of your model: this takes around 20 seconds</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">training_loop</span><span class="o">.</span><span class="n">eval_model</span>

<span class="c1"># we used all the data for the training and validation (oops)</span>
<span class="c1"># so we don't have any test data. Fix that later</span>
<span class="c1">#accuracy = test_model(VALIDATION_GENERATOR, model)</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">valid_generator</span><span class="p">(</span><span class="n">infinite</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">test_model</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'The accuracy of your model on the validation set is </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">'</span><span class="p">,</span> <span class="p">)</span>
</pre></div>
<pre class="example">
The accuracy of your model on the validation set is 0.9995
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgc6af298">
<h3 id="orgc6af298">Testing Some Custom Input</h3>
<div class="outline-text-3" id="text-orgc6af298">
<p>Finally, let's test some custom input. You will see that deepnets are more powerful than the older methods we have used before. Although we got close to 100% accuracy using Naive Bayes and Logistic Regression, that was because the task was way easier.</p>
<p>This is used to predict on a new sentence.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">sentence</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Predicts the sentiment of the sentence</span>

<span class="sd">    Args:</span>
<span class="sd">     sentence to get the sentiment for</span>

<span class="sd">    Returns:</span>
<span class="sd">     predictions, sentiment</span>
<span class="sd">    """</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">converter</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>

    <span class="c1"># Batch size 1, add dimension for batch, to work with the model</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>

    <span class="c1"># predict with the model</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Turn probabilities into categories</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">probabilities</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

    <span class="n">sentiment</span> <span class="o">=</span> <span class="s2">"positive"</span> <span class="k">if</span> <span class="n">prediction</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">"negative"</span>

    <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">sentiment</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe"</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">converter</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org84c31e7">
<h4 id="org84c31e7">A Positive Sentence</h4>
<div class="outline-text-4" id="text-org84c31e7">
<div class="highlight">
<pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe"</span>
<span class="n">tmp_pred</span><span class="p">,</span> <span class="n">tmp_sentiment</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The sentiment of the sentence </span><span class="se">\n</span><span class="s2">***</span><span class="se">\n\"</span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="se">\"\n</span><span class="s2">***</span><span class="se">\n</span><span class="s2">is </span><span class="si">{</span><span class="n">tmp_sentiment</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
</pre></div>
<pre class="example">
The sentiment of the sentence 
***
"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe"
***
is positive.
</pre></div>
</div>
<div class="outline-4" id="outline-container-org010550b">
<h4 id="org010550b">A Negative Sentence</h4>
<div class="outline-text-4" id="text-org010550b">
<div class="highlight">
<pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">"I hated my day, it was the worst, I'm so sad."</span>
<span class="n">tmp_pred</span><span class="p">,</span> <span class="n">tmp_sentiment</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The sentiment of the sentence </span><span class="se">\n</span><span class="s2">***</span><span class="se">\n\"</span><span class="si">{</span><span class="n">sentence</span><span class="si">}</span><span class="se">\"\n</span><span class="s2">***</span><span class="se">\n</span><span class="s2">is </span><span class="si">{</span><span class="n">tmp_sentiment</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
</pre></div>
<pre class="example">
The sentiment of the sentence 
***
"I hated my day, it was the worst, I'm so sad."
***
is negative.
</pre>
<p>Notice that the model works well even for complex sentences.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org6afd4f7">
<h4 id="org6afd4f7">On Pooh</h4>
<div class="outline-text-4" id="text-org6afd4f7">
<div class="highlight">
<pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="s2">"Oh, bother!"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">predict</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Oh, bother!: (0, 'negative')
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgecc1748">
<h3 id="orgecc1748">On Deep Nets</h3>
<div class="outline-text-3" id="text-orgecc1748">
<p>Deep nets allow you to understand and capture dependencies that you would have not been able to capture with a simple linear regression, or logistic regression.</p>
<ul class="org-ul">
<li>It also allows you to better use pre-trained embeddings for classification and tends to generalize better.</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org73bd3b7">
<h2 id="org73bd3b7">End</h2>
<div class="outline-text-2" id="text-org73bd3b7">
<p>So, there you have it, a Deep Learning Model for Sentiment Analysis built using Trax. Here are the prior posts in this series.</p>
<ul class="org-ul">
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/">Introduction</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/">Loading the Data</a></li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/">Defining the Model</a></li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/">Training the Model</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/sentiment-analysis-training-the-model/">Sentiment Analysis: Training the Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/sentiment-analysis-training-the-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-23T15:49:53-08:00" itemprop="datePublished" title="2020-12-23 15:49">2020-12-23 15:49</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org347ed02">Training the Model</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org6e87912">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org05f2a33">Middle</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org7a1f8a8">The Dataset</a></li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#orgd8122e8">Here's the Model</a></li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org333011c">Bundle It Up</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#orgee20afd">Imports</a></li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org1d278a2">The Trainer</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#orge8ee1b9">Practice In Making Predictions</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org773ca01">Compare 1 to 1 rather than comparing True to 1.</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org3da5d51">Evaluation</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#orgfbdd76e">5.1 Computing the accuracy of a batch</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/#org80f15c0">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org347ed02">
<h2 id="org347ed02">Training the Model</h2>
<div class="outline-text-2" id="text-org347ed02">
<p>In the <a href="posts/nlp/sentiment-analysis-defining-the-model/">previous post</a> we defined our Deep Learning model for Sentiment Analysis. Now we'll turn to training it on our data.</p>
<p>To train a model on a task, Trax defines an abstraction <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask"><code>trax.supervised.training.TrainTask</code></a> which packages the training data, loss and optimizer (among other things) together into an object.</p>
<p>Similarly to training a model, Trax defines an abstraction <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask"><code>trax.supervised.training.EvalTask</code></a> which packages the eval data and metrics (among other things) into another object.</p>
<p>The final piece tying things together is the <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop"><code>trax.supervised.training.Loop</code></a> abstraction that is a very simpl eand flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints. Using <code>Loop</code> will save you a lot of code compared to always writing the training loop by hand, like you did in courses 1 and 2. More importantly, you are less likely to have a bug in that code that would ruin your training.</p>
</div>
<div class="outline-3" id="outline-container-org6e87912">
<h3 id="org6e87912">Imports</h3>
<div class="outline-text-3" id="text-org6e87912">
<div class="highlight">
<pre><span></span><span class="c1"># from python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">trax.supervised</span> <span class="kn">import</span> <span class="n">training</span>

<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">trax</span>
<span class="kn">import</span> <span class="nn">trax.layers</span> <span class="k">as</span> <span class="nn">trax_layers</span>
<span class="kn">import</span> <span class="nn">trax.fastmath.numpy</span> <span class="k">as</span> <span class="nn">numpy</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.tensor_generator</span> <span class="kn">import</span> <span class="n">TensorBuilder</span><span class="p">,</span> <span class="n">TensorGenerator</span>
</pre></div>
<p>This next part (re-downloading the dataset) is just because I have to keep setting up new containers to get trax to work…</p>
<div class="highlight">
<pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">"twitter_samples"</span><span class="p">,</span> <span class="n">download_dir</span><span class="o">=</span><span class="s2">"/home/neurotic/data/datasets/nltk_data/"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org05f2a33">
<h2 id="org05f2a33">Middle</h2>
<div class="outline-text-2" id="text-org05f2a33"></div>
<div class="outline-3" id="outline-container-org7a1f8a8">
<h3 id="org7a1f8a8">The Dataset</h3>
<div class="outline-text-3" id="text-org7a1f8a8">
<div class="highlight">
<pre><span></span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">converter</span> <span class="o">=</span> <span class="n">TensorBuilder</span><span class="p">()</span>


<span class="n">train_generator</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">TensorGenerator</span><span class="p">,</span> <span class="n">converter</span><span class="p">,</span>
                                     <span class="n">positive_data</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">positive_training</span><span class="p">,</span>
                                     <span class="n">negative_data</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">negative_training</span><span class="p">,</span>
                                     <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">training_generator</span> <span class="o">=</span> <span class="n">train_generator</span><span class="p">()</span>

<span class="n">valid_generator</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">TensorGenerator</span><span class="p">,</span>
                          <span class="n">converter</span><span class="p">,</span>
                          <span class="n">positive_data</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">positive_validation</span><span class="p">,</span>
                          <span class="n">negative_data</span><span class="o">=</span><span class="n">converter</span><span class="o">.</span><span class="n">negative_validation</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="n">validation_generator</span> <span class="o">=</span> <span class="n">valid_generator</span><span class="p">()</span>

<span class="n">size_of_vocabulary</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">converter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd8122e8">
<h3 id="orgd8122e8">Here's the Model</h3>
<div class="outline-text-3" id="text-orgd8122e8">
<p>This was defined in the last post. It seems like too much trouble not to just copy it over.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">classifier</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">size_of_vocabulary</span><span class="p">,</span>
               <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
               <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">"""Creates the classifier model</span>

<span class="sd">    Args:</span>
<span class="sd">     vocab_size: number of tokens in the training vocabulary</span>
<span class="sd">     embedding_dim: output dimension for the Embedding layer</span>
<span class="sd">     output_dim: dimension for the Dense layer</span>

<span class="sd">    Returns:</span>
<span class="sd">     the composed layer-model</span>
<span class="sd">    """</span>
    <span class="n">embed_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="c1"># Size of the vocabulary</span>
        <span class="n">d_feature</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>  <span class="c1"># Embedding dimension</span>

    <span class="n">mean_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">dense_output_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span> <span class="o">=</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="n">log_softmax_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
      <span class="n">embed_layer</span><span class="p">,</span>
      <span class="n">mean_layer</span><span class="p">,</span>
      <span class="n">dense_output_layer</span><span class="p">,</span>
      <span class="n">log_softmax_layer</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
<p>Now to train the model.</p>
<p>First define the <code>TrainTask</code>, <code>EvalTask</code> and <code>Loop</code> in preparation to training the model.</p>
<div class="highlight">
<pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">271</span><span class="p">)</span>

<span class="c1"># train_generator(batch_size=batch_size, shuffle=True),</span>

<span class="n">train_task</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">TrainTask</span><span class="p">(</span>
    <span class="n">labeled_data</span><span class="o">=</span><span class="n">training_generator</span><span class="p">,</span>
    <span class="n">loss_layer</span><span class="o">=</span><span class="n">trax_layers</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">trax</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="n">n_steps_per_checkpoint</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eval_task</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">EvalTask</span><span class="p">(</span>
    <span class="n">labeled_data</span><span class="o">=</span><span class="n">validation_generator</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">trax_layers</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">()],</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">()</span>
</pre></div>
<p>This defines a model trained using <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss">tl.CrossEntropyLoss</a> optimized with the <a href="https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam">trax.optimizers.Adam</a> optimizer, all the while tracking the accuracy using <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy">tl.Accuracy</a> metric. We also track <code>tl.CrossEntropyLoss</code> on the validation set.</p>
<p>Now let's make an output directory and train the model.</p>
<div class="highlight">
<pre><span></span><span class="n">output_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">output_path</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
    <span class="n">output_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">classifier</span><span class="p">,</span> <span class="n">train_task</span><span class="p">,</span> <span class="n">eval_task</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">):</span>
    <span class="sd">"""Create and run the training loop</span>

<span class="sd">    Args: </span>
<span class="sd">       classifier - the model you are building</span>
<span class="sd">       train_task - Training task</span>
<span class="sd">       eval_task - Evaluation task</span>
<span class="sd">       n_steps - the evaluation steps</span>
<span class="sd">       output_dir - folder to save your files</span>
<span class="sd">    Returns:</span>
<span class="sd">       trainer -  trax trainer</span>
<span class="sd">    """</span>
    <span class="n">training_loop</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">Loop</span><span class="p">(</span>
                                <span class="n">model</span><span class="o">=</span><span class="n">classifier</span><span class="p">,</span> <span class="c1"># The learning model</span>
                                <span class="n">tasks</span><span class="o">=</span><span class="n">train_task</span><span class="p">,</span> <span class="c1"># The training task</span>
                                <span class="n">eval_tasks</span> <span class="o">=</span> <span class="n">eval_task</span><span class="p">,</span> <span class="c1"># The evaluation task</span>
                                <span class="n">output_dir</span> <span class="o">=</span> <span class="n">output_dir</span><span class="p">)</span> <span class="c1"># The output directory</span>

    <span class="n">training_loop</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span><span class="p">)</span>
    <span class="c1"># Return the training_loop, since it has the model.</span>
    <span class="k">return</span> <span class="n">training_loop</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">training_loop</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_task</span><span class="p">,</span> <span class="n">eval_task</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">output_path</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orgc25dd1c">

Step    110: Ran 10 train steps in 6.06 secs
Step    110: train CrossEntropyLoss |  0.00527583
Step    110: eval  CrossEntropyLoss |  0.00304692
Step    110: eval          Accuracy |  1.00000000

Step    120: Ran 10 train steps in 2.06 secs
Step    120: train CrossEntropyLoss |  0.02130376
Step    120: eval  CrossEntropyLoss |  0.00000677
Step    120: eval          Accuracy |  1.00000000

Step    130: Ran 10 train steps in 0.75 secs
Step    130: train CrossEntropyLoss |  0.01026674
Step    130: eval  CrossEntropyLoss |  0.00424393
Step    130: eval          Accuracy |  1.00000000

Step    140: Ran 10 train steps in 1.33 secs
Step    140: train CrossEntropyLoss |  0.00172522
Step    140: eval  CrossEntropyLoss |  0.00004072
Step    140: eval          Accuracy |  1.00000000

Step    150: Ran 10 train steps in 0.77 secs
Step    150: train CrossEntropyLoss |  0.00002847
Step    150: eval  CrossEntropyLoss |  0.00000232
Step    150: eval          Accuracy |  1.00000000

Step    160: Ran 10 train steps in 0.78 secs
Step    160: train CrossEntropyLoss |  0.00002123
Step    160: eval  CrossEntropyLoss |  0.00104654
Step    160: eval          Accuracy |  1.00000000

Step    170: Ran 10 train steps in 0.79 secs
Step    170: train CrossEntropyLoss |  0.00001706
Step    170: eval  CrossEntropyLoss |  0.00000080
Step    170: eval          Accuracy |  1.00000000

Step    180: Ran 10 train steps in 0.83 secs
Step    180: train CrossEntropyLoss |  0.00001554
Step    180: eval  CrossEntropyLoss |  0.00000989
Step    180: eval          Accuracy |  1.00000000

Step    190: Ran 10 train steps in 0.85 secs
Step    190: train CrossEntropyLoss |  0.00639312
Step    190: eval  CrossEntropyLoss |  0.00255337
Step    190: eval          Accuracy |  1.00000000

Step    200: Ran 10 train steps in 0.85 secs
Step    200: train CrossEntropyLoss |  0.00124322
Step    200: eval  CrossEntropyLoss |  0.02190475
Step    200: eval          Accuracy |  1.00000000
</pre></div>
</div>
<div class="outline-3" id="outline-container-org333011c">
<h3 id="org333011c">Bundle It Up</h3>
<div class="outline-text-3" id="text-org333011c">
<div class="highlight">
<pre><span></span><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>


<span class="o">&lt;&lt;</span><span class="n">model</span><span class="o">-</span><span class="n">trainer</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">the</span><span class="o">-</span><span class="n">model</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">training</span><span class="o">-</span><span class="n">task</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="nb">eval</span><span class="o">-</span><span class="n">task</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">training</span><span class="o">-</span><span class="n">loop</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">fit</span><span class="o">-</span><span class="n">the</span><span class="o">-</span><span class="n">model</span><span class="o">&gt;&gt;</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgee20afd">
<h4 id="orgee20afd">Imports</h4>
<div class="outline-text-4" id="text-orgee20afd">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">trax.supervised</span> <span class="kn">import</span> <span class="n">training</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">trax</span>
<span class="kn">import</span> <span class="nn">trax.layers</span> <span class="k">as</span> <span class="nn">trax_layers</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1d278a2">
<h4 id="org1d278a2">The Trainer</h4>
<div class="outline-text-4" id="text-org1d278a2">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">SentimentNetwork</span><span class="p">:</span>
    <span class="sd">"""Builds and Trains the Sentiment Analysis Model</span>

<span class="sd">    Args:</span>
<span class="sd">     training_generator: generator of training batches</span>
<span class="sd">     validation_generator: generator of validation batches</span>
<span class="sd">     vocabulary_size: number of tokens in the training vocabulary</span>
<span class="sd">     training_loops: number of times to run the training loop</span>
<span class="sd">     output_path: path to where to store the model</span>
<span class="sd">     embedding_dimension: output dimension for the Embedding layer</span>
<span class="sd">     output_dimension: dimension for the Dense layer</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">training_generator</span><span class="p">:</span> <span class="nb">object</span>
    <span class="n">validation_generator</span><span class="p">:</span> <span class="nb">object</span>
    <span class="n">training_loops</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">output_path</span><span class="p">:</span> <span class="n">Path</span>
    <span class="n">embedding_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">256</span>
    <span class="n">output_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span>
    <span class="n">_model</span><span class="p">:</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_training_task</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">TrainTask</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_evaluation_task</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">EvalTask</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_training_loop</span><span class="p">:</span> <span class="n">training</span><span class="o">.</span><span class="n">Loop</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgac32250"></a>The Model<br>
<div class="outline-text-5" id="text-orgac32250">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">"""The Embeddings model"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
            <span class="n">trax_layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
                <span class="n">vocab_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">,</span>
                <span class="n">d_feature</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_dimension</span><span class="p">),</span>
            <span class="n">trax_layers</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">trax_layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dimension</span><span class="p">),</span>
            <span class="n">trax_layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(),</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>
</pre></div>
</div>
</li>
<li><a id="orga7c288f"></a>The Training Task<br>
<div class="outline-text-5" id="text-orga7c288f">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">training_task</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">training</span><span class="o">.</span><span class="n">TrainTask</span><span class="p">:</span>
    <span class="sd">"""The training task for training the model"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training_task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training_task</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">TrainTask</span><span class="p">(</span>
            <span class="n">labeled_data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_generator</span><span class="p">,</span>
            <span class="n">loss_layer</span><span class="o">=</span><span class="n">trax_layers</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="n">trax</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>
            <span class="n">n_steps_per_checkpoint</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training_task</span>
</pre></div>
</div>
</li>
<li><a id="orge338e45"></a>Evaluation Task<br>
<div class="outline-text-5" id="text-orge338e45">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">evaluation_task</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">training</span><span class="o">.</span><span class="n">EvalTask</span><span class="p">:</span>
    <span class="sd">"""The validation evaluation task"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluation_task</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_evaluation_task</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">EvalTask</span><span class="p">(</span>
            <span class="n">labeled_data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_generator</span><span class="p">,</span>
            <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">trax_layers</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
                     <span class="n">trax_layers</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">()],</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluation_task</span>
</pre></div>
</div>
</li>
<li><a id="org30d19b5"></a>Training Loop<br>
<div class="outline-text-5" id="text-org30d19b5">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">training</span><span class="o">.</span><span class="n">Loop</span><span class="p">:</span>
    <span class="sd">"""The thing to run the training"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training_loop</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training_loop</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">Loop</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">tasks</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_task</span><span class="p">,</span>
            <span class="n">eval_tasks</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_task</span><span class="p">,</span>
            <span class="n">output_dir</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_path</span><span class="p">)</span> 
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_training_loop</span>
</pre></div>
</div>
</li>
<li><a id="orgf2a119d"></a>Fitting the Model<br>
<div class="outline-text-5" id="text-orgf2a119d">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Runs the training loop"""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">training_loop</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training_loops</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orge8ee1b9">
<h3 id="orge8ee1b9">Practice In Making Predictions</h3>
<div class="outline-text-3" id="text-orge8ee1b9">
<p>Now that you have trained a model, you can access it as <code>training_loop.model</code> object. We will actually use <code>training_loop.eval_model</code> and in the next weeks you will learn why we sometimes use a different model for evaluation, e.g., one without dropout. For now, make predictions with your model.</p>
<p>Use the training data just to see how the prediction process works.</p>
<ul class="org-ul">
<li>Later, you will use validation data to evaluate your model's performance.</li>
</ul>
<p>Create a generator object.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_train_generator</span> <span class="o">=</span> <span class="n">train_generator</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
<p>Get one batch.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">tmp_train_generator</span><span class="p">)</span>
</pre></div>
<p>Position 0 has the model inputs (tweets as tensors). Position 1 has the targets (the actual labels).</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_inputs</span><span class="p">,</span> <span class="n">tmp_targets</span><span class="p">,</span> <span class="n">tmp_example_weights</span> <span class="o">=</span> <span class="n">tmp_batch</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The batch is a tuple of length </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">tmp_batch</span><span class="p">)</span><span class="si">}</span><span class="s2"> because position 0 contains the tweets, and position 1 contains the targets."</span><span class="p">)</span> 
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The shape of the tweet tensors is </span><span class="si">{</span><span class="n">tmp_inputs</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> (num of examples, length of tweet tensors)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The shape of the labels is </span><span class="si">{</span><span class="n">tmp_targets</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, which is the batch size."</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The shape of the example_weights is </span><span class="si">{</span><span class="n">tmp_example_weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, which is the same as inputs/targets size."</span><span class="p">)</span>
</pre></div>
<pre class="example">
The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.
The shape of the tweet tensors is (16, 14) (num of examples, length of tweet tensors)
The shape of the labels is (16,), which is the batch size.
The shape of the example_weights is (16,), which is the same as inputs/targets size.
</pre>
<p>Feed the tweet tensors into the model to get a prediction.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_pred</span> <span class="o">=</span> <span class="n">training_loop</span><span class="o">.</span><span class="n">eval_model</span><span class="p">(</span><span class="n">tmp_inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The prediction shape is </span><span class="si">{</span><span class="n">tmp_pred</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, num of tensor_tweets as rows"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Column 0 is the probability of a negative sentiment (class 0)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Column 1 is the probability of a positive sentiment (class 1)"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"View the prediction array"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_pred</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org0e6d39c">
The prediction shape is (16, 2), num of tensor_tweets as rows
Column 0 is the probability of a negative sentiment (class 0)
Column 1 is the probability of a positive sentiment (class 1)

View the prediction array
[[-1.2960873e+01 -2.3841858e-06]
 [-5.6474457e+00 -3.5326481e-03]
 [-5.3460855e+00 -4.7781467e-03]
 [-7.6736917e+00 -4.6515465e-04]
 [-5.2682662e+00 -5.1658154e-03]
 [-1.0566207e+01 -2.5749207e-05]
 [-5.6388092e+00 -3.5634041e-03]
 [-3.9540453e+00 -1.9363165e-02]
 [ 0.0000000e+00 -2.0700916e+01]
 [ 0.0000000e+00 -2.2949795e+01]
 [ 0.0000000e+00 -2.3168846e+01]
 [ 0.0000000e+00 -2.4553205e+01]
 [-9.5367432e-07 -1.3878939e+01]
 [ 0.0000000e+00 -1.6655178e+01]
 [ 0.0000000e+00 -1.5975946e+01]
 [ 0.0000000e+00 -2.0577690e+01]]
</pre>
<p>To turn these probabilities into categories (negative or positive sentiment prediction), for each row:</p>
<ul class="org-ul">
<li>Compare the probabilities in each column.</li>
<li>If column 1 has a value greater than column 0, classify that as a positive tweet.</li>
<li>Otherwise if column 1 is less than or equal to column 0, classify that example as a negative tweet.</li>
</ul>
<p>Turn probabilites into category predictions.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_is_positive</span> <span class="o">=</span> <span class="n">tmp_pred</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">tmp_pred</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tmp_is_positive</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Neg log prob </span><span class="si">{</span><span class="n">tmp_pred</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\t</span><span class="s2">Pos log prob </span><span class="si">{</span><span class="n">tmp_pred</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="se">\t</span><span class="s2"> is positive? </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="se">\t</span><span class="s2"> actual </span><span class="si">{</span><span class="n">tmp_targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org15fae13">
Neg log prob -12.9609   Pos log prob -0.0000     is positive? True       actual 1
Neg log prob -5.6474    Pos log prob -0.0035     is positive? True       actual 1
Neg log prob -5.3461    Pos log prob -0.0048     is positive? True       actual 1
Neg log prob -7.6737    Pos log prob -0.0005     is positive? True       actual 1
Neg log prob -5.2683    Pos log prob -0.0052     is positive? True       actual 1
Neg log prob -10.5662   Pos log prob -0.0000     is positive? True       actual 1
Neg log prob -5.6388    Pos log prob -0.0036     is positive? True       actual 1
Neg log prob -3.9540    Pos log prob -0.0194     is positive? True       actual 1
Neg log prob 0.0000     Pos log prob -20.7009    is positive? False      actual 0
Neg log prob 0.0000     Pos log prob -22.9498    is positive? False      actual 0
Neg log prob 0.0000     Pos log prob -23.1688    is positive? False      actual 0
Neg log prob 0.0000     Pos log prob -24.5532    is positive? False      actual 0
Neg log prob -0.0000    Pos log prob -13.8789    is positive? False      actual 0
Neg log prob 0.0000     Pos log prob -16.6552    is positive? False      actual 0
Neg log prob 0.0000     Pos log prob -15.9759    is positive? False      actual 0
Neg log prob 0.0000     Pos log prob -20.5777    is positive? False      actual 0
</pre>
<p>Notice that since you are making a prediction using a training batch, it's more likely that the model's predictions match the actual targets (labels).</p>
<ul class="org-ul">
<li>Every prediction that the tweet is positive is also matching the actual target of 1 (positive sentiment).</li>
<li>Similarly, all predictions that the sentiment is not positive matches the actual target of 0 (negative sentiment)</li>
</ul>
<p>One more useful thing to know is how to compare if the prediction is matching the actual target (label).</p>
<ul class="org-ul">
<li>The result of calculation <code>is_positive</code> is a boolean.</li>
<li>The target is a type trax.fastmath.numpy.int32</li>
<li>If you expect to be doing division, you may prefer to work with decimal numbers with the data type type trax.fastmath.numpy.int32</li>
</ul>
<p>View the array of booleans.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Array of booleans"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_is_positive</span><span class="p">)</span>
</pre></div>
<pre class="example">
Array of booleans
DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,
             False, False, False, False, False, False, False, False],            dtype=bool)
</pre>
<p>Convert booleans to type int32.</p>
<ul class="org-ul">
<li>True is converted to 1</li>
<li>False is converted to 0</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">tmp_is_positive_int</span> <span class="o">=</span> <span class="n">tmp_is_positive</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">trax</span><span class="o">.</span><span class="n">fastmath</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
<p>View the array of integers.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Array of integers"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_is_positive_int</span><span class="p">)</span>
</pre></div>
<pre class="example">
Array of integers
DeviceArray([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)
</pre>
<p>Convert boolean to type float32.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_is_positive_float</span> <span class="o">=</span> <span class="n">tmp_is_positive</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
<p>View the array of floats.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Array of floats"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_is_positive_float</span><span class="p">)</span>
</pre></div>
<pre class="example">
Array of floats
DeviceArray([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,
             0.], dtype=float32)
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tmp_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(16, 2)
</pre>
<p>Note that Python usually does type conversion for you when you compare a boolean to an integer.</p>
<ul class="org-ul">
<li>True compared to 1 is True, otherwise any other integer is False.</li>
<li>False compared to 0 is True, otherwise any ohter integer is False.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"True == 1: </span><span class="si">{</span><span class="kc">True</span> <span class="o">==</span> <span class="mi">1</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"True == 2: </span><span class="si">{</span><span class="kc">True</span> <span class="o">==</span> <span class="mi">2</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"False == 0: </span><span class="si">{</span><span class="kc">False</span> <span class="o">==</span> <span class="mi">0</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"False == 2: </span><span class="si">{</span><span class="kc">False</span> <span class="o">==</span> <span class="mi">2</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
True == 1: True
True == 2: False
False == 0: True
False == 2: False
</pre>
<p>However, we recommend that you keep track of the data type of your variables to avoid unexpected outcomes. So it helps to convert the booleans into integers.</p>
</div>
<div class="outline-4" id="outline-container-org773ca01">
<h4 id="org773ca01">Compare 1 to 1 rather than comparing True to 1.</h4>
<div class="outline-text-4" id="text-org773ca01">
<p>Hopefully you are now familiar with what kinds of inputs and outputs the model uses when making a prediction.</p>
<ul class="org-ul">
<li>This will help you implement a function that estimates the accuracy of the model's predictions.</li>
</ul>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org3da5d51">
<h3 id="org3da5d51">Evaluation</h3>
<div class="outline-text-3" id="text-org3da5d51"></div>
<div class="outline-4" id="outline-container-orgfbdd76e">
<h4 id="orgfbdd76e">5.1 Computing the accuracy of a batch</h4>
<div class="outline-text-4" id="text-orgfbdd76e">
<p>You will now write a function that evaluates your model on the validation set and returns the accuracy.</p>
<ul class="org-ul">
<li><code>preds</code> contains the predictions.</li>
<li>Its dimensions are <code>(batch_size, output_dim)</code>. <code>output_dim</code> is two in this case. Column 0 contains the probability that the tweet belongs to class 0 (negative sentiment). Column 1 contains probability that it belongs to class 1 (positive sentiment).</li>
<li>If the probability in column 1 is greater than the probability in column 0, then interpret this as the model's prediction that the example has label 1 (positive sentiment).</li>
<li>Otherwise, if the probabilities are equal or the probability in column 0 is higher, the model's prediction is 0 (negative sentiment).</li>
<li><code>y</code> contains the actual labels.</li>
<li><code>y_weights</code> contains the weights to give to predictions.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">y_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Compute a batch accuracy</span>

<span class="sd">    Args: </span>
<span class="sd">       preds: a tensor of shape (dim_batch, output_dim) </span>
<span class="sd">       y: a tensor of shape (dim_batch,) with the true labels</span>
<span class="sd">       y_weights: a n.ndarray with the a weight for each example</span>

<span class="sd">    Returns: </span>
<span class="sd">       accuracy: a float between 0-1 </span>
<span class="sd">       weighted_num_correct (np.float32): Sum of the weighted correct predictions</span>
<span class="sd">       sum_weights (np.float32): Sum of the weights</span>
<span class="sd">    """</span>
    <span class="c1"># Create an array of booleans, </span>
    <span class="c1"># True if the probability of positive sentiment is greater than</span>
    <span class="c1"># the probability of negative sentiment</span>
    <span class="c1"># else False</span>
    <span class="n">is_pos</span> <span class="o">=</span>  <span class="n">preds</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">preds</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="c1"># convert the array of booleans into an array of np.int32</span>
    <span class="n">is_pos_int</span> <span class="o">=</span> <span class="n">is_pos</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="c1"># compare the array of predictions (as int32) with the target (labels) of type int32</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">is_pos_int</span> <span class="o">==</span> <span class="n">y</span>

    <span class="c1"># Count the sum of the weights.</span>
    <span class="n">sum_weights</span> <span class="o">=</span> <span class="n">y_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># convert the array of correct predictions (boolean) into an arrayof np.float32</span>
    <span class="n">correct_float</span> <span class="o">=</span> <span class="n">correct</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Multiply each prediction with its corresponding weight.</span>
    <span class="n">weighted_correct_float</span> <span class="o">=</span> <span class="n">correct_float</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_weights</span><span class="p">)</span>

    <span class="c1"># Sum up the weighted correct predictions (of type np.float32), to go in the</span>
    <span class="c1"># denominator.</span>
    <span class="n">weighted_num_correct</span> <span class="o">=</span> <span class="n">weighted_correct_float</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Divide the number of weighted correct predictions by the sum of the</span>
    <span class="c1"># weights.</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">weighted_num_correct</span><span class="o">/</span><span class="n">sum_weights</span>

    <span class="k">return</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">weighted_num_correct</span><span class="p">,</span> <span class="n">sum_weights</span>
</pre></div>
<p>Get one batch.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_val_generator</span> <span class="o">=</span> <span class="n">valid_generator</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">tmp_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">tmp_val_generator</span><span class="p">)</span>
</pre></div>
<p>Position 0 has the model inputs (tweets as tensors) position 1 has the targets (the actual labels)</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_inputs</span><span class="p">,</span> <span class="n">tmp_targets</span><span class="p">,</span> <span class="n">tmp_example_weights</span> <span class="o">=</span> <span class="n">tmp_batch</span>
</pre></div>
<p>Feed the tweet tensors into the model to get a prediction.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_pred</span> <span class="o">=</span> <span class="n">training_loop</span><span class="o">.</span><span class="n">eval_model</span><span class="p">(</span><span class="n">tmp_inputs</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">tmp_acc</span><span class="p">,</span> <span class="n">tmp_num_correct</span><span class="p">,</span> <span class="n">tmp_num_predictions</span> <span class="o">=</span> <span class="n">compute_accuracy</span><span class="p">(</span><span class="n">preds</span><span class="o">=</span><span class="n">tmp_pred</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">tmp_targets</span><span class="p">,</span> <span class="n">y_weights</span><span class="o">=</span><span class="n">tmp_example_weights</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Model's prediction accuracy on a single training batch is: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">tmp_acc</span><span class="si">}</span><span class="s2">%"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Weighted number of correct predictions </span><span class="si">{</span><span class="n">tmp_num_correct</span><span class="si">}</span><span class="s2">; weighted number of total observations predicted </span><span class="si">{</span><span class="n">tmp_num_predictions</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Model's prediction accuracy on a single training batch is: 100.0%
Weighted number of correct predictions 64.0; weighted number of total observations predicted 64
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org80f15c0">
<h2 id="org80f15c0">End</h2>
<div class="outline-text-2" id="text-org80f15c0">
<p>Now that we have a trained model, in the <a href="posts/nlp/sentiment-analysis-testing-the-model/">next post</a> we'll test how well it did.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/sentiment-analysis-defining-the-model/">Sentiment Analysis: Defining the Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/sentiment-analysis-defining-the-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-23T15:46:13-08:00" itemprop="datePublished" title="2020-12-23 15:46">2020-12-23 15:46</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org643e5c0">Beginning</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org5a6e7e8">Imports</a></li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org8fa4fc4">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#orgdfb4612">Middle</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#orgdcf29d0">The Base Layer Class</a></li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org8587c34">The ReLU class</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org6bbd92a">Test It</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#orge797e46">The Dense class</a></li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org495b2f5">The Layers for the Trax-Based Model</a></li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org5c505c5">Dense</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org0d5a8f2">Online Documentation</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#org02c5701">The Classifier Function</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/#orga995614">Ending</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org643e5c0">
<h2 id="org643e5c0">Beginning</h2>
<div class="outline-text-2" id="text-org643e5c0">
<p>This continues a series on <a href="posts/nlp/sentiment-analysis-deep-learning-model/">sentiment analysis with deep learning</a>. In the <a href="posts/nlp/sentiment-analysis-pre-processing-the-data/">previous post</a> we loaded and processed our data set. In this post we'll see about actually defining the Neural Network.</p>
<p>In this part we will write your own library of layers. It will be very similar to the one used in Trax and also in Keras and PyTorch. The intention is that in writing our own small framework will help us understand how they all work and use them more effectively in the future.</p>
</div>
<div class="outline-3" id="outline-container-org5a6e7e8">
<h3 id="org5a6e7e8">Imports</h3>
<div class="outline-text-3" id="text-org5a6e7e8">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">expect</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">fastmath</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">trax</span>
<span class="kn">import</span> <span class="nn">trax.layers</span> <span class="k">as</span> <span class="nn">trax_layers</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.tensor_generator</span> <span class="kn">import</span> <span class="n">TensorBuilder</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8fa4fc4">
<h3 id="org8fa4fc4">Set Up</h3>
<div class="outline-text-3" id="text-org8fa4fc4">
<p>Some aliases to get closer to what the notebook has.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy_fastmath</span> <span class="o">=</span> <span class="n">fastmath</span><span class="o">.</span><span class="n">numpy</span>
<span class="n">random</span> <span class="o">=</span> <span class="n">fastmath</span><span class="o">.</span><span class="n">random</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgdfb4612">
<h2 id="orgdfb4612">Middle</h2>
<div class="outline-text-2" id="text-orgdfb4612"></div>
<div class="outline-3" id="outline-container-orgdcf29d0">
<h3 id="orgdcf29d0">The Base Layer Class</h3>
<div class="outline-text-3" id="text-orgdcf29d0">
<p>This will be the base class that the others will inherit from.</p>
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Layer</span><span class="p">:</span>
    <span class="sd">"""Base class for layers</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="sd">"""The forward propagation method</span>

<span class="sd">       Raises:</span>
<span class="sd">        NotImplementedError - method is called but child hasn't implemented it</span>
<span class="sd">       """</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">init_weights_and_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">,</span> <span class="n">random_key</span><span class="p">):</span>
        <span class="sd">"""method to initialize the weights</span>
<span class="sd">       based on the input signature and random key,</span>
<span class="sd">       be implemented by subclasses of this Layer class</span>
<span class="sd">       """</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">,</span> <span class="n">random_key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""initializes and returns the weights</span>

<span class="sd">       Note:</span>
<span class="sd">        This is just an alias for the ``init_weights_and_state``</span>
<span class="sd">       method for some reason</span>

<span class="sd">       Args: </span>
<span class="sd">        input_signature: who knows?</span>
<span class="sd">        random_key: once again, who knows?</span>

<span class="sd">       Returns:</span>
<span class="sd">        the weights</span>
<span class="sd">       """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights_and_state</span><span class="p">(</span><span class="n">input_signature</span><span class="p">,</span> <span class="n">random_key</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""This is an alias for the ``forward`` method</span>

<span class="sd">       Args:</span>
<span class="sd">        x: input array</span>

<span class="sd">       Returns:</span>
<span class="sd">        whatever the ``forward`` method does</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8587c34">
<h3 id="org8587c34">The ReLU class</h3>
<div class="outline-text-3" id="text-org8587c34">
<p>Here's the ReLU function:</p>
<p>\[ \mathrm{ReLU}(x) = \mathrm{max}(0,x) \]</p>
<p>We'll implement the ReLU activation function below. The function will take in a matrix or vector and it transform all the negative numbers into 0 while keeping all the positive numbers intact.</p>
<p>Please use numpy.maximum(A,k) to find the maximum between each element in A and a scalar k.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Relu</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""Relu activation function implementation"""</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">""""Performs the activation</span>

<span class="sd">       Args: </span>
<span class="sd">           - x: the input</span>

<span class="sd">       Returns:</span>
<span class="sd">           - activation: all positive or 0 version of x</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org6bbd92a">
<h4 id="org6bbd92a">Test It</h4>
<div class="outline-text-4" id="text-org6bbd92a">
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">relu_layer</span> <span class="o">=</span> <span class="n">Relu</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Test data is:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Output of Relu is:"</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">relu_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
Test data is:
[[-2. -1.  0.]
 [ 0.  1.  2.]]

Output of Relu is:
[[0. 0. 0.]
 [0. 1. 2.]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge797e46">
<h3 id="orge797e46">The Dense class</h3>
<div class="outline-text-3" id="text-orge797e46">
<p>Implement the forward function of the Dense class.</p>
<ul class="org-ul">
<li>The forward function multiplies the input to the layer (<code>x</code>) by the weight matrix (<code>W</code>).</li>
</ul>
<p>\[ \mathrm{forward}(\mathbf{x},\mathbf{W}) = \mathbf{xW} \]</p>
<ul class="org-ul">
<li>You can use <code>numpy.dot</code> to perform the matrix multiplication.</li>
</ul>
<p>Note that for more efficient code execution, you will use the trax version of <code>math</code>, which includes a trax version of <code>numpy</code> and also <code>random</code>.</p>
<p>Implement the weight initializer <code>new_weights</code> function</p>
<ul class="org-ul">
<li>Weights are initialized with a random key.</li>
<li>The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols)</li>
<li>The num of rows for weights should equal the number of columns in x, because for forward propagation, you will multiply x times weights.</li>
</ul>
<p>Please use <code>trax.fastmath.random.normal(key, shape, dtype=tf.float32)</code> to generate random values for the weight matrix. The key difference between this function and the standard <code>numpy</code> randomness is the explicit use of random keys, which need to be passed in. While it can look tedious at the first sight to pass the random key everywhere, you will learn in Course 4 why this is very helpful when implementing some advanced models.</p>
<ul class="org-ul">
<li><code>key</code> can be generated by calling <code>random.get_prng(seed)</code> and passing in a number for the <code>seed</code>.</li>
<li><code>shape</code> is a tuple with the desired shape of the weight matrix.
<ul class="org-ul">
<li>The number of rows in the weight matrix should equal the number of columns in the variable <code>x</code>. Since <code>x</code> may have 2 dimensions if it represents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x.</li>
<li>The number of columns in the weight matrix is the number of units chosen for that dense layer. Look at the <code>__init__</code> function to see which variable stores the number of units.</li>
</ul>
</li>
<li><code>dtype</code> is the data type of the values in the generated matrix; keep the default of <code>tf.float32</code>. In this case, don't explicitly set the dtype (just let it use the default value).</li>
</ul>
<p>Set the standard deviation of the random values to 0.1</p>
<ul class="org-ul">
<li>The values generated have a mean of 0 and standard deviation of 1.</li>
<li>Set the default standard deviation <code>stdev</code> to be 0.1 by multiplying the standard deviation to each of the values in the weight matrix.</li>
</ul>
<p>See how the fastmath.trax.random.normal function works.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">get_prng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"The random seed generated by random.get_prng"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_key</span><span class="p">)</span>
</pre></div>
<pre class="example">
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
The random seed generated by random.get_prng
DeviceArray([0, 1], dtype=uint32)
</pre>
<p>For some reason tensorflow can't find the GPU. Setting the log level to 0 like the message suggests shows that it gives up after trying to find a TPU, there's no indication that it's looking for the GPU.</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">())</span>
</pre></div>
<p>Hmmm. I'll have to troubleshoot that.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"choose a matrix with 2 rows and 3 columns"</span><span class="p">)</span>
<span class="n">tmp_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
choose a matrix with 2 rows and 3 columns
(2, 3)
</pre>
<p>Generate a weight matrix Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow Just avoid setting the dtype and allow it to use the default data type</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_weight</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">tmp_key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">tmp_shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Weight matrix generated with a normal distribution with mean 0 and stdev of 1"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_weight</span><span class="p">)</span>
</pre></div>
<pre class="example">
Weight matrix generated with a normal distribution with mean 0 and stdev of 1
DeviceArray([[ 0.957307  , -0.9699291 ,  1.0070664 ],
             [ 0.36619022,  0.17294823,  0.29092228]], dtype=float32)
</pre>
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    A dense (fully-connected) layer.</span>

<span class="sd">    Args:</span>
<span class="sd">     - n_units: the number of columns for our weight matrix</span>
<span class="sd">     - init_stdev: standard deviation for our initial weights</span>
<span class="sd">    """</span>
    <span class="n">n_units</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">init_stdev</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""The dot product of the input and the weights</span>

<span class="sd">       Args:</span>
<span class="sd">        x: input to multipyl</span>

<span class="sd">       Returns:</span>
<span class="sd">        product of x and weights</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_weights_and_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
                               <span class="n">random_key</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""initializes the weights</span>

<span class="sd">       Args:</span>
<span class="sd">        input_signature: tuple whose final dimension will be the number of rows</span>
<span class="sd">        random_ke: something to start the random normal generator with</span>
<span class="sd">       """</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_signature</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># to allow for more than two-dimensional matrices,</span>
        <span class="c1"># we use the last column of the input shape, rather than assuming it's</span>
        <span class="c1"># column 1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">random_key</span><span class="p">,</span>
                                      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_units</span><span class="p">))</span>
             <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_stdev</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">dense_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  <span class="c1">#sets  number of units in dense layer</span>
<span class="n">random_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">get_prng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># sets random seed</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">25.0</span><span class="p">]])</span> <span class="c1"># input array </span>

<span class="n">dense_layer</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">random_key</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Weights are</span><span class="se">\n</span><span class="s2"> "</span><span class="p">,</span><span class="n">dense_layer</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="c1">#Returns randomly generated weights</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">dense_layer</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Foward function output is "</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="c1"># Returns multiplied values of units and weights</span>

<span class="n">expected_weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.02837108</span><span class="p">,</span>  <span class="mf">0.09368162</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.10050076</span><span class="p">,</span>  <span class="mf">0.14165013</span><span class="p">,</span>  <span class="mf">0.10543301</span><span class="p">,</span>  <span class="mf">0.09108126</span><span class="p">,</span>
     <span class="o">-</span><span class="mf">0.04265672</span><span class="p">,</span>  <span class="mf">0.0986188</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.05575325</span><span class="p">,</span>  <span class="mf">0.00153249</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.20785688</span><span class="p">,</span>  <span class="mf">0.0554837</span><span class="p">,</span>   <span class="mf">0.09142365</span><span class="p">,</span>  <span class="mf">0.05744595</span><span class="p">,</span>  <span class="mf">0.07227863</span><span class="p">,</span>  <span class="mf">0.01210617</span><span class="p">,</span>
     <span class="o">-</span><span class="mf">0.03237354</span><span class="p">,</span>  <span class="mf">0.16234995</span><span class="p">,</span>  <span class="mf">0.02450038</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.13809784</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.06111237</span><span class="p">,</span>  <span class="mf">0.01403724</span><span class="p">,</span>  <span class="mf">0.08410042</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1094358</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.10775021</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11396459</span><span class="p">,</span>
     <span class="o">-</span><span class="mf">0.05933381</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01557652</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03832145</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11144515</span><span class="p">]])</span>

<span class="n">expected_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="o">-</span><span class="mf">3.0395496</span><span class="p">,</span>   <span class="mf">0.9266802</span><span class="p">,</span>   <span class="mf">2.5414743</span><span class="p">,</span>  <span class="o">-</span><span class="mf">2.050473</span><span class="p">,</span>   <span class="o">-</span><span class="mf">1.9769388</span><span class="p">,</span>  <span class="o">-</span><span class="mf">2.582209</span><span class="p">,</span>
      <span class="o">-</span><span class="mf">1.7952735</span><span class="p">,</span>   <span class="mf">0.94427425</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8980402</span><span class="p">,</span>  <span class="o">-</span><span class="mf">3.7497487</span><span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">dense_layer</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">expected_weights</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
Weights are
  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126
  -0.04265672  0.0986188  -0.05575325  0.00153249]
 [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617
  -0.03237354  0.16234995  0.02450038 -0.13809784]
 [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459
  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]
Foward function output is  [[-3.03954965  0.92668021  2.54147445 -2.05047299 -1.97693891 -2.58220917
  -1.79527355  0.94427423 -0.89804017 -3.74974866]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org495b2f5">
<h3 id="org495b2f5">The Layers for the Trax-Based Model</h3>
<div class="outline-text-3" id="text-org495b2f5">
<p>For the model implementation we will use the Trax layers library. Trax layers are very similar to the ones we implemented above, but in addition to trainable weights they also have a non-trainable state. This state is used in layers like batch normalization and for inference - we will learn more about it later on.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org5c505c5">
<h3 id="org5c505c5">Dense</h3>
<div class="outline-text-3" id="text-org5c505c5">
<p>First, look at the code of the Trax Dense layer and compare to the implementation above.</p>
<ul class="org-ul">
<li><a href="https://github.com/google/trax/blob/master/trax/layers/core.py#L29">Trax Dense layer implementation</a></li>
</ul>
<p>Another other important layer that we will use a lot is the <a href="https://github.com/google/trax/blob/master/trax/layers/combinators.py#L26">Serial</a> layer which allows us to execute one layer after another in sequence.</p>
<ul class="org-ul">
<li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas.</li>
<li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code></li>
</ul>
<p>The layer classes have pretty good docstrings, unlike the fastmath stuff, so it might be useful to look at it - but it's too long to include here.</p>
<p>We're also going to use an <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113">Embedding</a></p>
<ul class="org-ul">
<li><code>tl.Embedding(vocab_size, d_feature)</code>.</li>
<li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li>
<li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">tmp_embed</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">d_feature</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_embed</span><span class="p">)</span>
</pre></div>
<pre class="example">
Embedding_3_2
</pre>
<p>Another useful layer is the <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276">Mean</a> which calculates means across an axis. In this case, use axis = 1 (across rows) to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).</p>
<ul class="org-ul">
<li>For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements.</li>
</ul>
<p>Pretend the embedding matrix uses 2 elements for embedding the meaning of a word and has a vocabulary size of 3, so it has shape (2,3).</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_embed</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,],</span>
                         <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>
                         <span class="p">])</span>
</pre></div>
<p>First take the mean along axis 0, which creates a vector whose length equals the vocabulary size (the number of columns).</p>
<div class="highlight">
<pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp_embed</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
<pre class="example">
array([2.5, 3.5, 4.5])
</pre>
<p>If you take the mean along axis 1 it creates a vector whose length equals the number of elements in a word embedding (the rows).</p>
<div class="highlight">
<pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp_embed</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
<pre class="example">
array([2., 5.])
</pre>
<p>Finally, a <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242">LogSoftmax</a> layer gives you a log-softmax output.</p>
</div>
<div class="outline-4" id="outline-container-org0d5a8f2">
<h4 id="org0d5a8f2">Online Documentation</h4>
<div class="outline-text-4" id="text-org0d5a8f2">
<p>For completeness, here's some links to the Read the Docs documentation for these layers.</p>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">Dense</a></li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators">Serial</a></li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">Embedding</a></li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean">Mean</a></li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">LogSoftmax</a></li>
</ul>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org02c5701">
<h3 id="org02c5701">The Classifier Function</h3>
<div class="outline-text-3" id="text-org02c5701">
<div class="highlight">
<pre><span></span><span class="n">builder</span> <span class="o">=</span> <span class="n">TensorBuilder</span><span class="p">()</span>
<span class="n">size_of_vocabulary</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">classifier</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">size_of_vocabulary</span><span class="p">,</span>
               <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
               <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">"""Creates the classifier model</span>

<span class="sd">    Args:</span>
<span class="sd">     vocab_size: number of tokens in the training vocabulary</span>
<span class="sd">     embedding_dim: output dimension for the Embedding layer</span>
<span class="sd">     output_dim: dimension for the Dense layer</span>

<span class="sd">    Returns:</span>
<span class="sd">     the composed layer-model</span>
<span class="sd">    """</span>
    <span class="n">embed_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="c1"># Size of the vocabulary</span>
        <span class="n">d_feature</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>  <span class="c1"># Embedding dimension</span>

    <span class="n">mean_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">dense_output_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span> <span class="o">=</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="n">log_softmax_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
      <span class="n">embed_layer</span><span class="p">,</span>
      <span class="n">mean_layer</span><span class="p">,</span>
      <span class="n">dense_output_layer</span><span class="p">,</span>
      <span class="n">log_softmax_layer</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">tmp_model</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">tmp_model</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_model</span><span class="p">)</span>
</pre></div>
<pre class="example">
&lt;class 'trax.layers.combinators.Serial'&gt;
Serial[
  Embedding_9164_256
  Mean
  Dense_2
  LogSoftmax
]
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orga995614">
<h2 id="orga995614">Ending</h2>
<div class="outline-text-2" id="text-orga995614">
<p>Now that we have our Deep Learning model, we'll move on to <a href="posts/nlp/sentiment-analysis-training-the-model/">training it</a>.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/sentiment-analysis-pre-processing-the-data/">Sentiment Analysis: Pre-processing the Data</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/" rel="bookmark"><time class="published dt-published" datetime="2020-12-23T15:43:02-08:00" itemprop="datePublished" title="2020-12-23 15:43">2020-12-23 15:43</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#orgd67b414">Beginning</a>
<ul>
<li>
<ul>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#orga02ce4a">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#orgbff3410">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org0d260cb">Middle</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#orgf9aabc4">The NLTK Data</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org31ba059">Split It Up</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org53ed1c0">Split positive set into validation and training</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org8fd45df">Split negative set into validation and training</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org3776637">Combine the Data Sets</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org4c53d18">Building the vocabulary</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org89ed779">Converting a tweet to a tensor</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org7d66003">Creating a batch generator</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org64ee748">data_generator</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org955dd21">Test the train_generator</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org8d8e909">Bundle It Up</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#orgbcd7248">Imports</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org6a856fd">Defaults</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org29d9600">NLTK Settings</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org0310f6c">Special Tokens</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org099b9a1">The Builder</a></li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#orgc8e81ae">The Generator</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org4dcfa32">Test It Out</a></li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/#org0184f1c">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgd67b414">
<h2 id="orgd67b414">Beginning</h2>
<div class="outline-text-2" id="text-orgd67b414">
<p>This is the next in a series about building a Deep Learning model for sentiment analysis. The first post was <a href="posts/nlp/sentiment-analysis-deep-learning-model/">this one</a>.</p>
</div>
<div class="outline-4" id="outline-container-orga02ce4a">
<h4 id="orga02ce4a">Imports</h4>
<div class="outline-text-4" id="text-orga02ce4a">
<div class="highlight">
<pre><span></span><span class="c1"># from python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>

<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">contain_exactly</span><span class="p">,</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">twitter_samples</span>

<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.processor</span> <span class="kn">import</span> <span class="n">TwitterProcessor</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgbff3410">
<h3 id="orgbff3410">Set Up</h3>
<div class="outline-text-3" id="text-orgbff3410">
<p>The NLTK data has to be downloaded at least once.</p>
<div class="highlight">
<pre><span></span><span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">"twitter_samples"</span><span class="p">,</span> <span class="n">download_dir</span><span class="o">=</span><span class="s2">"~/data/datasets/nltk_data/"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0d260cb">
<h2 id="org0d260cb">Middle</h2>
<div class="outline-text-2" id="text-org0d260cb"></div>
<div class="outline-3" id="outline-container-orgf9aabc4">
<h3 id="orgf9aabc4">The NLTK Data</h3>
<div class="outline-text-3" id="text-orgf9aabc4">
<div class="highlight">
<pre><span></span><span class="n">positive</span> <span class="o">=</span> <span class="n">twitter_samples</span><span class="o">.</span><span class="n">strings</span><span class="p">(</span><span class="s1">'positive_tweets.json'</span><span class="p">)</span>
<span class="n">negative</span> <span class="o">=</span> <span class="n">twitter_samples</span><span class="o">.</span><span class="n">strings</span><span class="p">(</span><span class="s1">'negative_tweets.json'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Positive Tweets: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">positive</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Negative Tweets: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">negative</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Positive Tweets: 5,000
Negative Tweets: 5,000
</pre></div>
</div>
<div class="outline-3" id="outline-container-org31ba059">
<h3 id="org31ba059">Split It Up</h3>
<div class="outline-text-3" id="text-org31ba059">
<p>Instead of randomly splitting the data we're going to do a straight slice.</p>
<div class="highlight">
<pre><span></span><span class="n">SPLIT</span> <span class="o">=</span> <span class="mi">4000</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org53ed1c0">
<h4 id="org53ed1c0">Split positive set into validation and training</h4>
<div class="outline-text-4" id="text-org53ed1c0">
<div class="highlight">
<pre><span></span><span class="n">positive_validation</span>   <span class="o">=</span> <span class="n">positive</span><span class="p">[</span><span class="n">SPLIT</span><span class="p">:]</span>
<span class="n">positive_training</span>  <span class="o">=</span> <span class="n">positive</span><span class="p">[:</span><span class="n">SPLIT</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8fd45df">
<h4 id="org8fd45df">Split negative set into validation and training</h4>
<div class="outline-text-4" id="text-org8fd45df">
<div class="highlight">
<pre><span></span><span class="n">negative_validation</span> <span class="o">=</span> <span class="n">negative</span><span class="p">[</span><span class="n">SPLIT</span><span class="p">:]</span>
<span class="n">negative_training</span>  <span class="o">=</span> <span class="n">negative</span><span class="p">[:</span><span class="n">SPLIT</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3776637">
<h4 id="org3776637">Combine the Data Sets</h4>
<div class="outline-text-4" id="text-org3776637">
<p>The X data.</p>
<div class="highlight">
<pre><span></span><span class="n">train_x</span> <span class="o">=</span> <span class="n">positive_training</span> <span class="o">+</span> <span class="n">negative_training</span>
<span class="n">validation_x</span> <span class="o">=</span> <span class="n">positive_validation</span> <span class="o">+</span> <span class="n">negative_validation</span>
</pre></div>
<p>The labels (1 for positive, 0 for negative).</p>
<div class="highlight">
<pre><span></span><span class="n">train_y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">positive_training</span><span class="p">)),</span>
                       <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">negative_training</span><span class="p">)))</span>
<span class="n">validation_y</span>  <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">positive_validation</span><span class="p">)),</span>
                             <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">negative_validation</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"length of train_x </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"length of validation_x </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_x</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
length of train_x 8,000
length of validation_x 2,000
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org4c53d18">
<h3 id="org4c53d18">Building the vocabulary</h3>
<div class="outline-text-3" id="text-org4c53d18">
<p>Now build the vocabulary.</p>
<ul class="org-ul">
<li>Map each word in each tweet to an integer (an "index").</li>
<li>The following code does this for you, but please read it and understand what it's doing.</li>
<li>Note that you will build the vocabulary based on the training data.</li>
<li>To do so, you will assign an index to everyword by iterating over your training set.</li>
</ul>
<p>The vocabulary will also include some special tokens</p>
<ul class="org-ul">
<li><code>__PAD__</code>: padding</li>
<li><code>&lt;/e&gt;</code>: end of line</li>
<li><code>__UNK__</code>: a token representing any word that is not in the vocabulary.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">Tokens</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="s2">"__PAD__"</span><span class="p">,</span> <span class="n">ending</span><span class="o">=</span><span class="s2">"__&lt;/e&gt;__"</span><span class="p">,</span> <span class="n">unknown</span><span class="o">=</span><span class="s2">"__UNK__"</span><span class="p">)</span>
<span class="n">process</span> <span class="o">=</span> <span class="n">TwitterProcessor</span><span class="p">()</span>
<span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{</span><span class="n">Tokens</span><span class="o">.</span><span class="n">padding</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Tokens</span><span class="o">.</span><span class="n">ending</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Tokens</span><span class="o">.</span><span class="n">unknown</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">train_x</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">process</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
            <span class="n">vocabulary</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Words in the vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">count</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">token</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<pre class="example">
Words in the vocabulary: 9,164
0: __PAD__: 0
1: __&lt;/e&gt;__: 1
2: __UNK__: 2
3: followfriday: 3
4: top: 4
</pre></div>
</div>
<div class="outline-3" id="outline-container-org89ed779">
<h3 id="org89ed779">Converting a tweet to a tensor</h3>
<div class="outline-text-3" id="text-org89ed779">
<p>Now we'll write a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet).</p>
<ul class="org-ul">
<li>Note, the returned data type will be a <b>regular Python `list()`</b>
<ul class="org-ul">
<li>You won't use TensorFlow in this function</li>
<li>You also won't use a numpy array</li>
<li>You also won't use trax.fastmath.numpy array</li>
</ul>
</li>
<li>
<p>For words in the tweet that are not in the vocabulary, set them to the unique ID for the token `__UNK__`.</p>
<p>For example, given this string:</p>
</li>
</ul>
<pre class="example" id="orge180eee">
'@happypuppy, is Maria happy?'
</pre>
<p>You first tokenize it.</p>
<pre class="example" id="org2f08a8c">
['maria', 'happi']
</pre>
<p>Then convert each word into the index for it.</p>
<pre class="example" id="org9ee0461">
[2, 56]
</pre>
<p>Notice that the word "maria" is not in the vocabulary, so it is assigned the unique integer associated with the <code>__UNK__</code> token, because it is considered "unknown."</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">tweet_to_tensor</span><span class="p">(</span><span class="n">tweet</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">vocab_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                    <span class="n">unk_token</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s1">'__UNK__'</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">"""Convert a tweet to a list of indices</span>

<span class="sd">    Args: </span>
<span class="sd">       tweet - A string containing a tweet</span>
<span class="sd">       vocab_dict - The words dictionary</span>
<span class="sd">       unk_token - The special string for unknown tokens</span>
<span class="sd">       verbose - Print info during runtime</span>

<span class="sd">    Returns:</span>
<span class="sd">       tensor_l - A python list with indices for the tweet tokens</span>
<span class="sd">    """</span>
    <span class="c1"># Process the tweet into a list of words</span>
    <span class="c1"># where only important words are kept (stop words removed)</span>
    <span class="n">word_l</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"List of words from the processed tweet:"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">word_l</span><span class="p">)</span>

    <span class="c1"># Initialize the list that will contain the unique integer IDs of each word</span>
    <span class="n">tensor_l</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Get the unique integer ID of the __UNK__ token</span>
    <span class="n">unk_ID</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="p">[</span><span class="n">unk_token</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The unique integer ID for the unk_token is </span><span class="si">{</span><span class="n">unk_ID</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="c1"># for each word in the list:</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>

        <span class="c1"># Get the unique integer ID.</span>
        <span class="c1"># If the word doesn't exist in the vocab dictionary,</span>
        <span class="c1"># use the unique ID for __UNK__ instead.</span>
        <span class="n">word_ID</span> <span class="o">=</span> <span class="n">vocab_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">unk_ID</span><span class="p">)</span>

        <span class="c1"># Append the unique integer ID to the tensor list.</span>
        <span class="n">tensor_l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word_ID</span><span class="p">)</span> 

    <span class="k">return</span> <span class="n">tensor_l</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Actual tweet is</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">positive_validation</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Tensor of tweet:</span><span class="se">\n</span><span class="s2">"</span><span class="p">,</span> <span class="n">tweet_to_tensor</span><span class="p">(</span><span class="n">positive_validation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vocab_dict</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">))</span>
</pre></div>
<pre class="example">
Actual tweet is
 Bro:U wan cut hair anot,ur hair long Liao bo
Me:since ord liao,take it easy lor treat as save $ leave it longer :)
Bro:LOL Sibei xialan

Tensor of tweet:
 [1072, 96, 484, 2376, 750, 8220, 1132, 750, 53, 2, 2701, 796, 2, 2, 354, 606, 2, 3523, 1025, 602, 4599, 9, 1072, 158, 2, 2]
</pre>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_tweet_to_tensor</span><span class="p">():</span>
    <span class="n">test_cases</span> <span class="o">=</span> <span class="p">[</span>

        <span class="p">{</span>
            <span class="s2">"name"</span><span class="p">:</span><span class="s2">"simple_test_check"</span><span class="p">,</span>
            <span class="s2">"input"</span><span class="p">:</span> <span class="p">[</span><span class="n">positive_validation</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vocabulary</span><span class="p">],</span>
            <span class="s2">"expected"</span><span class="p">:[</span><span class="mi">444</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">304</span><span class="p">,</span> <span class="mi">567</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
            <span class="s2">"error"</span><span class="p">:</span><span class="s2">"The function gives bad output for val_pos[1]. Test failed"</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">"name"</span><span class="p">:</span><span class="s2">"datatype_check"</span><span class="p">,</span>
            <span class="s2">"input"</span><span class="p">:[</span><span class="n">positive_validation</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vocabulary</span><span class="p">],</span>
            <span class="s2">"expected"</span><span class="p">:</span><span class="nb">type</span><span class="p">([]),</span>
            <span class="s2">"error"</span><span class="p">:</span><span class="s2">"Datatype mismatch. Need only list not np.array"</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">"name"</span><span class="p">:</span><span class="s2">"without_unk_check"</span><span class="p">,</span>
            <span class="s2">"input"</span><span class="p">:[</span><span class="n">positive_validation</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vocabulary</span><span class="p">],</span>
            <span class="s2">"expected"</span><span class="p">:</span><span class="mi">6</span><span class="p">,</span>
            <span class="s2">"error"</span><span class="p">:</span><span class="s2">"Unk word check not done- Please check if you included mapping for unknown word"</span>
        <span class="p">}</span>
    <span class="p">]</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">test_case</span> <span class="ow">in</span> <span class="n">test_cases</span><span class="p">:</span>        
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">test_case</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"simple_test_check"</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">test_case</span><span class="p">[</span><span class="s2">"expected"</span><span class="p">]</span> <span class="o">==</span> <span class="n">tweet_to_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">test_case</span><span class="p">[</span><span class="s1">'input'</span><span class="p">])</span>
                <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">test_case</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"datatype_check"</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tweet_to_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">test_case</span><span class="p">[</span><span class="s1">'input'</span><span class="p">]),</span> <span class="n">test_case</span><span class="p">[</span><span class="s2">"expected"</span><span class="p">])</span>
                <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">test_case</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"without_unk_check"</span><span class="p">:</span>
                <span class="k">assert</span> <span class="kc">None</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tweet_to_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">test_case</span><span class="p">[</span><span class="s1">'input'</span><span class="p">])</span>
                <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">test_case</span><span class="p">[</span><span class="s1">'error'</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">count</span><span class="p">,</span><span class="s2">" Tests passed out of 3"</span><span class="p">)</span>
<span class="n">test_tweet_to_tensor</span><span class="p">()</span>            
</pre></div>
<pre class="example">
The function gives bad output for val_pos[1]. Test failed
2  Tests passed out of 3
</pre>
<p>Their tweet processor wipes out everything after the start of a URL, even if it isn't part of the URL, so they have fewer tokens, so the indices won't match exactly.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org7d66003">
<h3 id="org7d66003">Creating a batch generator</h3>
<div class="outline-text-3" id="text-org7d66003">
<p>Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets.</p>
<ul class="org-ul">
<li>If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model.</li>
<li>You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to treat some examples as more important to get right than others, but commonly this will all be 1.0).</li>
</ul>
<p>Once you create the generator, you could include it in a for loop:</p>
<pre class="example" id="orgb26a6f9">
for batch_inputs, batch_targets, batch_example_weights in data_generator:
</pre>
<p>You can also get a single batch like this:</p>
<pre class="example" id="org3bdfb75">
batch_inputs, batch_targets, batch_example_weights = next(data_generator)
</pre>
<p>The generator returns the next batch each time it's called.</p>
<ul class="org-ul">
<li>This generator returns the data in a format (tensors) that you could directly use in your model.</li>
<li>It returns a triple: the inputs, targets, and loss weights:</li>
</ul>
<p>– Inputs is a tensor that contains the batch of tweets we put into the model. – Targets is the corresponding batch of labels that we train to generate. – Loss weights here are just 1s with same shape as targets. Next week, you will use it to mask input padding.</p>
</div>
<div class="outline-4" id="outline-container-org64ee748">
<h4 id="org64ee748">data_generator</h4>
<div class="outline-text-4" id="text-org64ee748">
<p>A batch of spaghetti.</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED: Data generator</span>
<span class="k">def</span> <span class="nf">data_generator</span><span class="p">(</span><span class="n">data_pos</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">data_neg</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">loop</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">vocab_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">"""Generates batches of data</span>

<span class="sd">    Args: </span>
<span class="sd">       data_pos - Set of positive examples</span>
<span class="sd">       data_neg - Set of negative examples</span>
<span class="sd">       batch_size - number of samples per batch. Must be even</span>
<span class="sd">       loop - True or False</span>
<span class="sd">       vocab_dict - The words dictionary</span>
<span class="sd">       shuffle - Shuffle the data order</span>

<span class="sd">    Yield:</span>
<span class="sd">       inputs - Subset of positive and negative examples</span>
<span class="sd">       targets - The corresponding labels for the subset</span>
<span class="sd">       example_weights - An array specifying the importance of each example        </span>
<span class="sd">    """</span>
    <span class="c1"># make sure the batch size is an even number</span>
    <span class="c1"># to allow an equal number of positive and negative samples</span>
    <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># Number of positive examples in each batch is half of the batch size</span>
    <span class="c1"># same with number of negative examples in each batch</span>
    <span class="n">n_to_take</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="c1"># Use pos_index to walk through the data_pos array</span>
    <span class="c1"># same with neg_index and data_neg</span>
    <span class="n">pos_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">neg_index</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">len_data_pos</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_pos</span><span class="p">)</span>
    <span class="n">len_data_neg</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_neg</span><span class="p">)</span>

    <span class="c1"># Get and array with the data indexes</span>
    <span class="n">pos_index_lines</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">len_data_pos</span><span class="p">))</span>
    <span class="n">neg_index_lines</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">len_data_neg</span><span class="p">))</span>

    <span class="c1"># shuffle lines if shuffle is set to True</span>
    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">rnd</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">pos_index_lines</span><span class="p">)</span>
        <span class="n">rnd</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">neg_index_lines</span><span class="p">)</span>

    <span class="n">stop</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># Loop indefinitely</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">stop</span><span class="p">:</span>  

        <span class="c1"># create a batch with positive and negative examples</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># First part: Pack n_to_take positive examples</span>

        <span class="c1"># Start from pos_index and increment i up to n_to_take</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_to_take</span><span class="p">):</span>

            <span class="c1"># If the positive index goes past the positive dataset length,</span>
            <span class="k">if</span> <span class="n">pos_index</span> <span class="o">&gt;=</span> <span class="n">len_data_pos</span><span class="p">:</span> 

                <span class="c1"># If loop is set to False, break once we reach the end of the dataset</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">loop</span><span class="p">:</span>
                    <span class="n">stop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">;</span>
                    <span class="k">break</span><span class="p">;</span>

                <span class="c1"># If user wants to keep re-using the data, reset the index</span>
                <span class="n">pos_index</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
                    <span class="c1"># Shuffle the index of the positive sample</span>
                    <span class="n">rnd</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">pos_index_lines</span><span class="p">)</span>

            <span class="c1"># get the tweet as pos_index</span>
            <span class="n">tweet</span> <span class="o">=</span> <span class="n">data_pos</span><span class="p">[</span><span class="n">pos_index_lines</span><span class="p">[</span><span class="n">pos_index</span><span class="p">]]</span>

            <span class="c1"># convert the tweet into tensors of integers representing the processed words</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tweet_to_tensor</span><span class="p">(</span><span class="n">tweet</span><span class="p">,</span> <span class="n">vocab_dict</span><span class="p">)</span>

            <span class="c1"># append the tensor to the batch list</span>
            <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

            <span class="c1"># Increment pos_index by one</span>
            <span class="n">pos_index</span> <span class="o">=</span> <span class="n">pos_index</span> <span class="o">+</span> <span class="mi">1</span>


        <span class="c1"># Second part: Pack n_to_take negative examples</span>

        <span class="c1"># Using the same batch list, start from neg_index and increment i up to n_to_take</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">neg_index</span><span class="p">,</span> <span class="n">n_to_take</span><span class="p">):</span>

            <span class="c1"># If the negative index goes past the negative dataset length,</span>
            <span class="k">if</span> <span class="n">neg_index</span> <span class="o">&gt;</span> <span class="n">len_data_neg</span><span class="p">:</span>

                <span class="c1"># If loop is set to False, break once we reach the end of the dataset</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">loop</span><span class="p">:</span>
                    <span class="n">stop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">;</span>
                    <span class="k">break</span><span class="p">;</span>

                <span class="c1"># If user wants to keep re-using the data, reset the index</span>
                <span class="n">neg_index</span> <span class="o">=</span> <span class="mi">0</span>

                <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
                    <span class="c1"># Shuffle the index of the negative sample</span>
                    <span class="n">rnd</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">neg_index_lines</span><span class="p">)</span>
            <span class="c1"># get the tweet at neg_index</span>
            <span class="n">tweet</span> <span class="o">=</span> <span class="n">data_neg</span><span class="p">[</span><span class="n">neg_index_lines</span><span class="p">[</span><span class="n">neg_index</span><span class="p">]]</span>

            <span class="c1"># convert the tweet into tensors of integers representing the processed words</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tweet_to_tensor</span><span class="p">(</span><span class="n">tweet</span><span class="p">,</span> <span class="n">vocab_dict</span><span class="p">)</span>

            <span class="c1"># append the tensor to the batch list</span>
            <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

            <span class="c1"># Increment neg_index by one</span>
            <span class="n">neg_index</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">break</span><span class="p">;</span>

        <span class="c1"># Update the start index for positive data </span>
        <span class="c1"># so that it's n_to_take positions after the current pos_index</span>
        <span class="n">pos_index</span> <span class="o">+=</span> <span class="n">n_to_take</span>

        <span class="c1"># Update the start index for negative data </span>
        <span class="c1"># so that it's n_to_take positions after the current neg_index</span>
        <span class="n">neg_index</span> <span class="o">+=</span> <span class="n">n_to_take</span>

        <span class="c1"># Get the max tweet length (the length of the longest tweet) </span>
        <span class="c1"># (you will pad all shorter tweets to have this length)</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">])</span> 


        <span class="c1"># Initialize the input_l, which will </span>
        <span class="c1"># store the padded versions of the tensors</span>
        <span class="n">tensor_pad_l</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Pad shorter tweets with zeros</span>
        <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
            <span class="c1"># Get the number of positions to pad for this tensor so that it will be max_len long</span>
            <span class="n">n_pad</span> <span class="o">=</span> <span class="n">max_len</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

            <span class="c1"># Generate a list of zeros, with length n_pad</span>
            <span class="n">pad_l</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_pad</span>

            <span class="c1"># concatenate the tensor and the list of padded zeros</span>
            <span class="n">tensor_pad</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">+</span> <span class="n">pad_l</span>

            <span class="c1"># append the padded tensor to the list of padded tensors</span>
            <span class="n">tensor_pad_l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_pad</span><span class="p">)</span>

        <span class="c1"># convert the list of padded tensors to a numpy array</span>
        <span class="c1"># and store this as the model inputs</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tensor_pad_l</span><span class="p">)</span>

        <span class="c1"># Generate the list of targets for the positive examples (a list of ones)</span>
        <span class="c1"># The length is the number of positive examples in the batch</span>
        <span class="n">target_pos</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[:</span><span class="n">n_to_take</span><span class="p">])</span>

        <span class="c1"># Generate the list of targets for the negative examples (a list of zeros)</span>
        <span class="c1"># The length is the number of negative examples in the batch</span>
        <span class="n">target_neg</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="n">n_to_take</span><span class="p">:])</span>

        <span class="c1"># Concatenate the positve and negative targets</span>
        <span class="n">target_l</span> <span class="o">=</span> <span class="n">target_pos</span> <span class="o">+</span> <span class="n">target_neg</span>

        <span class="c1"># Convert the target list into a numpy array</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target_l</span><span class="p">)</span>

        <span class="c1"># Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()</span>
        <span class="n">example_weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>

        <span class="k">yield</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">example_weights</span>
</pre></div>
<p>Now you can use your data generator to create a data generator for the training data, and another data generator for the validation data.</p>
<p>We will create a third data generator that does not loop, for testing the final accuracy of the model.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Set the random number generator for the shuffle procedure</span>
<span class="n">rnd</span> <span class="o">=</span> <span class="n">random</span>
<span class="n">rnd</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span> 

<span class="c1"># Create the training data generator</span>
<span class="k">def</span> <span class="nf">train_generator</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">data_generator</span><span class="p">(</span><span class="n">positive_training</span><span class="p">,</span> <span class="n">negative_training</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">)</span>

<span class="c1"># Create the validation data generator</span>
<span class="k">def</span> <span class="nf">val_generator</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">data_generator</span><span class="p">(</span><span class="n">positive_validation</span><span class="p">,</span> <span class="n">negative_validation</span><span class="p">,</span>
                          <span class="n">batch_size</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">)</span>

<span class="c1"># Create the validation data generator</span>
<span class="k">def</span> <span class="nf">test_generator</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">data_generator</span><span class="p">(</span><span class="n">positive_validation</span><span class="p">,</span> <span class="n">negative_validation</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
                          <span class="kc">False</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">)</span>

<span class="c1"># Get a batch from the train_generator and inspect.</span>
<span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">example_weights</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">train_generator</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1"># this will print a list of 4 tensors padded with zeros</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Inputs: </span><span class="si">{</span><span class="n">inputs</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Targets: </span><span class="si">{</span><span class="n">targets</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Example Weights: </span><span class="si">{</span><span class="n">example_weights</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
Inputs: [[2030 4492 3231    9    0    0    0    0    0    0    0]
 [5009  571 2025 1475 5233 3532  142 3532  132  464    9]
 [3798  111   96  587 2960 4007    0    0    0    0    0]
 [ 256 3798    0    0    0    0    0    0    0    0    0]]
Targets: [1 1 0 0]
Example Weights: [1 1 1 1]
</pre></div>
</div>
<div class="outline-4" id="outline-container-org955dd21">
<h4 id="org955dd21">Test the train_generator</h4>
<div class="outline-text-4" id="text-org955dd21">
<p>Create a data generator for training data which produces batches of size 4 (for tensors and their respective targets).</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_data_gen</span> <span class="o">=</span> <span class="n">train_generator</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
<p>Call the data generator to get one batch and its targets.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_inputs</span><span class="p">,</span> <span class="n">tmp_targets</span><span class="p">,</span> <span class="n">tmp_example_weights</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">tmp_data_gen</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The inputs shape is </span><span class="si">{</span><span class="n">tmp_inputs</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The targets shape is </span><span class="si">{</span><span class="n">tmp_targets</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The example weights shape is </span><span class="si">{</span><span class="n">tmp_example_weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tmp_inputs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"input tensor: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">; target </span><span class="si">{</span><span class="n">tmp_targets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">; example weights </span><span class="si">{</span><span class="n">tmp_example_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
The inputs shape is (4, 14)
The targets shape is (4,)
The example weights shape is (4,)
input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1
input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1
input tensor: [5807 2931 3798    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1
input tensor: [ 865  261 3689 5808  313 4499  571 1248 2795  333 1220 3798    0    0]; target 0; example weights 1
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8d8e909">
<h3 id="org8d8e909">Bundle It Up</h3>
<div class="outline-text-3" id="text-org8d8e909"></div>
<div class="outline-4" id="outline-container-orgbcd7248">
<h4 id="orgbcd7248">Imports</h4>
<div class="outline-text-4" id="text-orgbcd7248">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">cycle</span>

<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">twitter_samples</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">.processor</span> <span class="kn">import</span> <span class="n">TwitterProcessor</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6a856fd">
<h4 id="org6a856fd">Defaults</h4>
<div class="outline-text-4" id="text-org6a856fd">
<div class="highlight">
<pre><span></span><span class="n">Defaults</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">split</span> <span class="o">=</span> <span class="mi">4000</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org29d9600">
<h4 id="org29d9600">NLTK Settings</h4>
<div class="outline-text-4" id="text-org29d9600">
<div class="highlight">
<pre><span></span><span class="n">NLTK</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">corpus</span><span class="o">=</span><span class="s2">"twitter_samples"</span><span class="p">,</span>
    <span class="n">negative</span> <span class="o">=</span> <span class="s2">"negative_tweets.json"</span><span class="p">,</span>
    <span class="n">positive</span><span class="o">=</span><span class="s2">"positive_tweets.json"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org0310f6c">
<h4 id="org0310f6c">Special Tokens</h4>
<div class="outline-text-4" id="text-org0310f6c">
<div class="highlight">
<pre><span></span><span class="n">SpecialTokens</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="s2">"__PAD__"</span><span class="p">,</span>
                          <span class="n">ending</span><span class="o">=</span><span class="s2">"__&lt;/e&gt;__"</span><span class="p">,</span>
                          <span class="n">unknown</span><span class="o">=</span><span class="s2">"__UNK__"</span><span class="p">)</span>

<span class="n">SpecialIDs</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">ending</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">unknown</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org099b9a1">
<h4 id="org099b9a1">The Builder</h4>
<div class="outline-text-4" id="text-org099b9a1">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorBuilder</span><span class="p">:</span>
    <span class="sd">"""converts tweets to tensors</span>

<span class="sd">    Args: </span>
<span class="sd">     - split: where to split the training and validation data</span>
<span class="sd">    """</span>
    <span class="n">split</span> <span class="o">=</span> <span class="n">Defaults</span><span class="o">.</span><span class="n">split</span>
    <span class="n">_positive</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_negative</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_positive_training</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_negative_training</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_positive_validation</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_negative_validation</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_process</span><span class="p">:</span> <span class="n">TwitterProcessor</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vocabulary</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_x_train</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgd22e4e4"></a>Positive Tweets<br>
<div class="outline-text-5" id="text-orgd22e4e4">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">positive</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The raw positive NLTK tweets"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positive</span> <span class="o">=</span> <span class="n">twitter_samples</span><span class="o">.</span><span class="n">strings</span><span class="p">(</span><span class="n">NLTK</span><span class="o">.</span><span class="n">positive</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive</span>
</pre></div>
</div>
</li>
<li><a id="org4508f77"></a>Negative Tweets<br>
<div class="outline-text-5" id="text-org4508f77">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">negative</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The raw negative NLTK tweets"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_negative</span> <span class="o">=</span> <span class="n">twitter_samples</span><span class="o">.</span><span class="n">strings</span><span class="p">(</span><span class="n">NLTK</span><span class="o">.</span><span class="n">negative</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative</span>
</pre></div>
</div>
</li>
<li><a id="orgf3c8905"></a>Positive Training<br>
<div class="outline-text-5" id="text-orgf3c8905">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">positive_training</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The positive training data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positive_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positive</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">split</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_training</span>
</pre></div>
</div>
</li>
<li><a id="org2014dfc"></a>Negative Training<br>
<div class="outline-text-5" id="text-org2014dfc">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">negative_training</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The negative training data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative_training</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_negative_training</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">split</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative_training</span>
</pre></div>
</div>
</li>
<li><a id="org04cb3fa"></a>Positive Validation<br>
<div class="outline-text-5" id="text-org04cb3fa">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">positive_validation</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The positive validation data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_validation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positive_validation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positive</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">split</span><span class="p">:]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_validation</span>
</pre></div>
</div>
</li>
<li><a id="org4e21767"></a>Negative Validation<br>
<div class="outline-text-5" id="text-org4e21767">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">negative_validation</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The negative validation data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative_validation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_negative_validation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">split</span><span class="p">:]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative_validation</span>
</pre></div>
</div>
</li>
<li><a id="orge87f382"></a>Twitter Processor<br>
<div class="outline-text-5" id="text-orge87f382">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TwitterProcessor</span><span class="p">:</span>
    <span class="sd">"""processor for tweets"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_process</span> <span class="o">=</span> <span class="n">TwitterProcessor</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process</span>
</pre></div>
</div>
</li>
<li><a id="org2891a57"></a>X Train<br>
<div class="outline-text-5" id="text-org2891a57">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">x_train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The unprocessed training data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_train</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positive_training</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative_training</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x_train</span>
</pre></div>
</div>
</li>
<li><a id="org575ccdb"></a>The Vocabulary<br>
<div class="outline-text-5" id="text-org575ccdb">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""A map of token to numeric id"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span> <span class="o">=</span> <span class="p">{</span><span class="n">SpecialTokens</span><span class="o">.</span><span class="n">padding</span><span class="p">:</span> <span class="n">SpecialIDs</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                            <span class="n">SpecialTokens</span><span class="o">.</span><span class="n">ending</span><span class="p">:</span> <span class="n">SpecialIDs</span><span class="o">.</span><span class="n">ending</span><span class="p">,</span>
                            <span class="n">SpecialTokens</span><span class="o">.</span><span class="n">unknown</span><span class="p">:</span> <span class="n">SpecialIDs</span><span class="o">.</span><span class="n">unknown</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">tweet</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">token</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span>
</pre></div>
</div>
</li>
<li><a id="org4e21234"></a>To Tensor<br>
<div class="outline-text-5" id="text-org4e21234">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tweet</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""Converts tweet to list of numeric identifiers</span>

<span class="sd">    Args:</span>
<span class="sd">     tweet: the string to convert</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of IDs for the tweet</span>
<span class="sd">    """</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">SpecialIDs</span><span class="o">.</span><span class="n">unknown</span><span class="p">)</span>
              <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">tweet</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">tensor</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgc8e81ae">
<h4 id="orgc8e81ae">The Generator</h4>
<div class="outline-text-4" id="text-orgc8e81ae">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TensorGenerator</span><span class="p">:</span>
    <span class="sd">"""Generates batches of vectorized-tweets</span>

<span class="sd">    Args:</span>
<span class="sd">     converter: TensorBuilder object</span>
<span class="sd">     positive_data: list of positive data</span>
<span class="sd">     negative_data: list of negative data</span>
<span class="sd">     batch_size: the size for each generated batch     </span>
<span class="sd">     shuffle: whether to shuffle the generated data</span>
<span class="sd">     infinite: whether to generate data forever</span>
<span class="sd">    """</span>
    <span class="n">converter</span><span class="p">:</span> <span class="n">TensorBuilder</span>
    <span class="n">positive_data</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">negative_data</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
    <span class="n">infinite</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">_positive_indices</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_negative_indices</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_positives</span><span class="p">:</span> <span class="nb">iter</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_negatives</span><span class="p">:</span> <span class="nb">iter</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgb1ba42e"></a>Positive Indices<br>
<div class="outline-text-5" id="text-orgb1ba42e">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">positive_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The indices to use to grab the positive tweets"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positive_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_positive_indices</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_positive_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positive_indices</span>
</pre></div>
</div>
</li>
<li><a id="orgf928c7f"></a>Negative Indices<br>
<div class="outline-text-5" id="text-orgf928c7f">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">negative_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""Indices for the negative tweets"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative_indices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">negative_data</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_negative_indices</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_negative_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negative_indices</span>
</pre></div>
</div>
</li>
<li><a id="org22c5e56"></a>Positives<br>
<div class="outline-text-5" id="text-org22c5e56">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">positives</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""The positive index generator"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positives</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_positives</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">positive_generator</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_positives</span>
</pre></div>
</div>
</li>
<li><a id="org34c67ec"></a>Negatives<br>
<div class="outline-text-5" id="text-org34c67ec">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">negatives</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""The negative index generator"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negatives</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_negatives</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative_generator</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_negatives</span>
</pre></div>
</div>
</li>
<li><a id="org857019f"></a>Positive Generator<br>
<div class="outline-text-5" id="text-org857019f">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">positive_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Generator of indices for positive tweets"""</span>
    <span class="n">stop</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positive_indices</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">positive_indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">infinite</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_positive_indices</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="org5e7b6a7"></a>Negative Generator<br>
<div class="outline-text-5" id="text-org5e7b6a7">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">negative_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""generator of indices for negative tweets"""</span>
    <span class="n">stop</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">negative_indices</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative_indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">stop</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">infinite</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_negative_indices</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="org91c0459"></a>The Iterator<br>
<div class="outline-text-5" id="text-org91c0459">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>
</div>
</li>
<li><a id="orgd9661c8"></a>The Next Method<br>
<div class="outline-text-5" id="text-orgd9661c8">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">half_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="c1"># get the indices</span>
    <span class="n">positives</span> <span class="o">=</span> <span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positives</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">half_batch</span><span class="p">))</span>
    <span class="n">negatives</span> <span class="o">=</span> <span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">negatives</span><span class="p">)</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">half_batch</span><span class="p">))</span>

    <span class="c1"># get the tweets</span>
    <span class="n">positives</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">positive_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">positives</span><span class="p">)</span>
    <span class="n">negatives</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">negative_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">negatives</span><span class="p">)</span>

    <span class="c1"># get the token ids</span>
    <span class="k">try</span><span class="p">:</span>    
        <span class="n">positives</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">converter</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span> <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">positives</span><span class="p">]</span>
        <span class="n">negatives</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">converter</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span> <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">negatives</span><span class="p">]</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
        <span class="c1"># the next(self.positives) in the first generator will raise a</span>
        <span class="c1"># RuntimeError if</span>
        <span class="c1"># we're not running this infinitely</span>
        <span class="k">raise</span> <span class="ne">StopIteration</span>

    <span class="n">batch</span> <span class="o">=</span> <span class="n">positives</span> <span class="o">+</span> <span class="n">negatives</span>

    <span class="n">longest</span> <span class="o">=</span> <span class="nb">max</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span> <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">))</span>

    <span class="n">paddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">longest</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">paddings</span> <span class="o">=</span> <span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">padding</span> <span class="k">for</span> <span class="n">padding</span> <span class="ow">in</span> <span class="n">paddings</span><span class="p">)</span>

    <span class="n">padded</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span> <span class="o">+</span> <span class="n">padding</span> <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">padding</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">padded</span><span class="p">)</span>

    <span class="c1"># the labels for the inputs</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">half_batch</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">half_batch</span><span class="p">)</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="c1"># default the weights to ones</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>    
    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">weights</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org4dcfa32">
<h3 id="org4dcfa32">Test It Out</h3>
<div class="outline-text-3" id="text-org4dcfa32">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.tensor_generator</span> <span class="kn">import</span> <span class="n">TensorBuilder</span><span class="p">,</span> <span class="n">TensorGenerator</span>

<span class="n">converter</span> <span class="o">=</span> <span class="n">TensorBuilder</span><span class="p">()</span>
<span class="n">expect</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">converter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">tweet</span> <span class="o">=</span> <span class="n">positive_validation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1072</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">484</span><span class="p">,</span> <span class="mi">2376</span><span class="p">,</span> <span class="mi">750</span><span class="p">,</span> <span class="mi">8220</span><span class="p">,</span> <span class="mi">1132</span><span class="p">,</span> <span class="mi">750</span><span class="p">,</span> <span class="mi">53</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2701</span><span class="p">,</span> <span class="mi">796</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span>
            <span class="mi">354</span><span class="p">,</span> <span class="mi">606</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3523</span><span class="p">,</span> <span class="mi">1025</span><span class="p">,</span> <span class="mi">602</span><span class="p">,</span> <span class="mi">4599</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">1072</span><span class="p">,</span> <span class="mi">158</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">contain_exactly</span><span class="p">(</span><span class="o">*</span><span class="n">expected</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">TensorGenerator</span><span class="p">(</span><span class="n">converter</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">generator</span><span class="p">))</span>
</pre></div>
<pre class="example">
(array([[ 749, 1019,  313, 1020,   75],
       [1009,    9,    0,    0,    0],
       [3540, 6030, 6031, 3798,    0],
       [  50,   96, 3798,    0,    0]]), array([1, 1, 0, 0]), array([1, 1, 1, 1]))
</pre>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">count</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">generator</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">break</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">generator</span><span class="p">))</span>
</pre></div>
<pre class="example" id="org69a6d5f">
[[  22 1228  434  354  227 2371    9]
 [ 267  160   89    0    0    0    0]
 [ 315 1008 8480 3798 2108  371 3233]
 [8232 8233  791 3798    0    0    0]]

[[1173 1061  586    9  896  729 1264  345 1062 1063]
 [3387  558  991 2166 3388 3231  558  238  120    0]
 [ 198 5997 3798    0    0    0    0    0    0    0]
 [ 223  310 3798    0    0    0    0    0    0    0]]

[[4015 4015 4015 4016  231 2117   57  422    9 4017 4018 4019   86   86]
 [2554   57  102  358   75    0    0    0    0    0    0    0    0    0]
 [  50   38  881 3798    0    0    0    0    0    0    0    0    0    0]
 [6729 6730 6731  382 3798    0    0    0    0    0    0    0    0    0]]

[[3479   75    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [4636 4637  233 4299  111  237 2626    9    0    0    0    0    0    0
     0    0    0]
 [  73  381  463 4321  142   96 7390 7391   92   85 1394 7392 5895 7393
    45 3798 7394]
 [8863 2844  991  127 5818    0    0    0    0    0    0    0    0    0
     0    0    0]]

[[ 226  615   22   75    0    0]
 [2135  703  237  435 3124    9]
 [2379 6264 3798    0    0    0]
 [6504 1912 2380 3798    0    0]]

[[5623  120    0    0    0    0    0    0    0    0]
 [ 133   54  102   63 1300   56    9   50   92 3181]
 [2094  383   73  464 3798    0    0    0    0    0]
 [ 223  101 8754  383 2085 5818 8755    0    0    0]]

(array([[ 374,   44, 2981,  435,  132,  111, 1040, 1382,    9,    0,    0,
           0],
       [ 369,  398,  283,    9, 2671, 1411,  136,  184,  769, 1262, 2061,
        3460],
       [1094, 9024,  315,  381, 3798,    0,    0,    0,    0,    0,    0,
           0],
       [9036, 3798,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0]]), array([1, 1, 0, 0]), array([1, 1, 1, 1]))
</pre>
<p>Ladies and gentlemen, we have ourselves a generator.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0184f1c">
<h2 id="org0184f1c">End</h2>
<div class="outline-text-2" id="text-org0184f1c">
<p>Now that we have our data, the next step will be to <a href="posts/nlp/sentiment-analysis-defining-the-model/">define the model</a>.</p>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="index-18.html" rel="prev">Newer posts</a></li>
<li class="next"><a href="index-16.html" rel="next">Older posts</a></li>
</ul>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script><!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
