<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="A denoising autoencoder." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Denoising Autoencoder | In Too Deep</title>
<link href="../../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/posts/nano/autoencoders/denoising-autoencoder/" rel="canonical"><!--[if lt IE 9]><script src="../../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../../site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="../../../assets/javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
<meta content="Cloistered Monkey" name="author">
<link href="/posts/nano/autoencoders/convolutional-autoencoder/" rel="prev" title="Convolutional Autoencoder" type="text/html">
<link href="/posts/nano/style-transfer/style-transfer/" rel="next" title="Style Transfer" type="text/html">
<meta content="In Too Deep" property="og:site_name">
<meta content="Denoising Autoencoder" property="og:title">
<meta content="https://necromuralist.github.io/In-Too-Deep/posts/nano/autoencoders/denoising-autoencoder/" property="og:url">
<meta content="A denoising autoencoder." property="og:description">
<meta content="article" property="og:type">
<meta content="2018-12-21T18:07:29-08:00" property="article:published_time">
<meta content="autoencoder" property="article:tag">
<meta content="cnn" property="article:tag">
<meta content="exercise" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="/posts/nano/autoencoders/denoising-autoencoder/index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="/posts/nano/autoencoders/denoising-autoencoder/">Denoising Autoencoder</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/autoencoders/denoising-autoencoder/" rel="bookmark"><time class="published dt-published" datetime="2018-12-21T18:07:29-08:00" itemprop="datePublished" title="2018-12-21 18:07">2018-12-21 18:07</time></a></p>
<p class="sourceline"><a class="sourcelink" href="/posts/nano/autoencoders/denoising-autoencoder/index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/autoencoders/denoising-autoencoder/#org9ce129c">Set Up</a></li>
<li><a href="/posts/nano/autoencoders/denoising-autoencoder/#org3b844d0">Visualize the Data</a></li>
<li><a href="/posts/nano/autoencoders/denoising-autoencoder/#org12a3dcb">Denoising</a></li>
<li><a href="/posts/nano/autoencoders/denoising-autoencoder/#orgbfbd035">Define the NN Architecture</a></li>
<li><a href="/posts/nano/autoencoders/denoising-autoencoder/#org0c92fe0">Initialize The NN</a></li>
<li><a href="/posts/nano/autoencoders/denoising-autoencoder/#org4e748cc">Training</a></li>
<li><a href="/posts/nano/autoencoders/denoising-autoencoder/#org5a8330d">Checking out the results</a></li>
</ul>
</div>
</div>
<p>Sticking with the MNIST dataset, let's add noise to our data and see if we can define and train an autoencoder to <i>de</i>-noise the images.</p>
<div class="outline-2" id="outline-container-org9ce129c">
<h2 id="org9ce129c">Set Up</h2>
<div class="outline-text-2" id="text-org9ce129c"></div>
<div class="outline-3" id="outline-container-orgcb554a9">
<h3 id="orgcb554a9">Imports</h3>
<div class="outline-text-3" id="text-orgcb554a9"></div>
<div class="outline-4" id="outline-container-orgae31d8a">
<h4 id="orgae31d8a">Python</h4>
<div class="outline-text-4" id="text-orgae31d8a">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from collections import namedtuple
from datetime import datetime
from pathlib import Path
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgcdf7578">
<h4 id="orgcdf7578">PyPi</h4>
<div class="outline-text-4" id="text-orgcdf7578">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from torchvision import datasets
from graphviz import Graph
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org889b177">
<h3 id="org889b177">The Plotting</h3>
<div class="outline-text-3" id="text-org889b177">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=3)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgac7bbca">
<h3 id="orgac7bbca">The Data</h3>
<div class="outline-text-3" id="text-orgac7bbca"></div>
<div class="outline-4" id="outline-container-org264dc4d">
<h4 id="org264dc4d">The Transform</h4>
<div class="outline-text-4" id="text-org264dc4d">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>transform = transforms.ToTensor()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org576c12c">
<h4 id="org576c12c">Load the Training and Test Datasets</h4>
<div class="outline-text-4" id="text-org576c12c">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>path = Path("~/datasets/MNIST/").expanduser()
print(path.is_dir())
</pre></div>
<pre class="example">
True

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_data = datasets.MNIST(root=path, train=True,
                            download=True, transform=transform)
test_data = datasets.MNIST(root=path, train=False,
                           download=True, transform=transform)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8d0bcac">
<h4 id="org8d0bcac">Create training and test dataloaders</h4>
<div class="outline-text-4" id="text-org8d0bcac">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>NUM_WORKERS = 0
BATCH_SIZE = 20
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
                                           num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,
                                          num_workers=NUM_WORKERS)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org8382696">
<h3 id="org8382696">Test for <a href="http://pytorch.org/docs/stable/cuda.html">CUDA</a></h3>
<div class="outline-text-3" id="text-org8382696">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using: {}".format(device))
</pre></div>
<pre class="example">
Using: cuda:0

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org3b844d0">
<h2 id="org3b844d0">Visualize the Data</h2>
<div class="outline-text-2" id="text-org3b844d0"></div>
<div class="outline-3" id="outline-container-org2f6eec2">
<h3 id="org2f6eec2">Obtain One Batch of Training Images</h3>
<div class="outline-text-3" id="text-org2f6eec2">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2656f06">
<h3 id="org2656f06">Get One Image From the Batch</h3>
<div class="outline-text-3" id="text-org2656f06">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>img = numpy.squeeze(images[0])
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2dd0cf8">
<h3 id="org2dd0cf8">Plot</h3>
<div class="outline-text-3" id="text-org2dd0cf8">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
figure.suptitle("Sample Image", weight="bold")
image = axe.imshow(img, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="first_image.png" src="/posts/nano/autoencoders/denoising-autoencoder/first_image.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org12a3dcb">
<h2 id="org12a3dcb">Denoising</h2>
<div class="outline-text-2" id="text-org12a3dcb">
<p>As I've mentioned before, autoencoders like the ones you've built so far aren't too useful in practive. However, they can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1.</p>
<p><b>We'll use noisy images as input and the original, clean images as targets.</b></p>
<p>Since this is a harder problem for the network, we'll want to use <i>deeper</i> convolutional layers here; layers with more feature maps. You might also consider adding additional layers. I suggest starting with a depth of 32 for the convolutional layers in the encoder, and the same depths going backward through the decoder.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgbfbd035">
<h2 id="orgbfbd035">Define the NN Architecture</h2>
<div class="outline-text-2" id="text-orgbfbd035">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>graph = Graph(format="png")

# Input layer
graph.node("a", "28x28x1 Input")

# the Encoder
graph.node("b", "28x28x32 Convolution")
graph.node("c", "14x14x32 MaxPool")
graph.node("d", "14x14x16 Convolution")
graph.node("e", "7x7x16 MaxPool")
graph.node("f", "7x7x8 Convolution")
graph.node("g", "3x3x8 MaxPool")

# The Decoder
graph.node("h", "7x7x8 Transpose Convolution")
graph.node("i", "14x14x16 Transpose Convolution")
graph.node("j", "28x28x32 Transpose Convolution")
graph.node("k", "28x28x1 Convolution")

# The Output
graph.node("l", "28x28x1 Output")

edges = "abcdefghijkl"
graph.edges([edges[edge] + edges[edge+1] for edge in range(len(edges) - 1)])

graph.render("graphs/network.dot")
graph
</pre></div>
<div class="figure">
<p><img alt="network.dot.png" src="/posts/nano/autoencoders/denoising-autoencoder/network.dot.png"></p>
</div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>Layer = namedtuple("Layer", "kernel stride in_depth out_depth padding".split())
Layer.__new__.__defaults__= (0,)
def output_size(input_size: int, layer: Layer, expected: int) -&gt; int:
    """Calculates the output size of the layer

    Args:
     input_size: the size of the input to the layer
     layer: named tuple with values for the layer
     expected: the value you are expecting

    Returns:
     the size of the output

    Raises:
     AssertionError: the calculated value wasn't the expected one
    """
    size = 1 + int(
        (input_size - layer.kernel + 2 * layer.padding)/layer.stride)
    print(layer)
    print("Layer Output: {0} x {0} x {1}".format(size, layer.out_depth))
    assert size == expected, size
    return size
</pre></div>
</div>
<div class="outline-3" id="outline-container-org62745d2">
<h3 id="org62745d2">The Encoder Layers</h3>
<div class="outline-text-3" id="text-org62745d2"></div>
<div class="outline-4" id="outline-container-orge0a78f1">
<h4 id="orge0a78f1">Layer One</h4>
<div class="outline-text-4" id="text-orge0a78f1">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> INPUT_DEPTH = 1
 convolution_one = Layer(kernel = 3,
                         padding = 1,
                         stride = 1,
                         in_depth=INPUT_DEPTH,
                         out_depth = 32)
 INPUT_ONE = 28
 OUTPUT_ONE = output_size(INPUT_ONE, convolution_one, INPUT_ONE)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=1, out_depth=32, padding=1)
Layer Output: 28 x 28 x 32

</pre></div>
</div>
<div class="outline-4" id="outline-container-org9a12ba8">
<h4 id="org9a12ba8">Layer Two</h4>
<div class="outline-text-4" id="text-org9a12ba8">
<p>The second layer is a MaxPool layer that will keep the depth of thirty-two but will halve the size to fourteen. According to the <a href="https://cs231n.github.io/convolutional-networks/">CS 231 n</a> page on Convolutional Networks, there are only two values for the kernel size that are usually used - 2 and 3, and the stride is usually just 2, with a kernel size of 2 being more common, and as it turns out, a kernel size of 2 and a stride of 2 will reduce our input dimensions by a half, which is what we want.</p>
\begin{align} W &amp;= \frac{28 - 2}{2} + 1\\ &amp;= 14\\ \end{align} /home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> max_pool_one = Layer(kernel=2, stride=2,
                      in_depth=convolution_one.out_depth,
                      out_depth=convolution_one.out_depth)
 OUTPUT_TWO = output_size(OUTPUT_ONE, max_pool_one, 14)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=32, out_depth=32, padding=0)
Layer Output: 14 x 14 x 32

</pre></div>
</div>
<div class="outline-4" id="outline-container-org20e7478">
<h4 id="org20e7478">Layer Three</h4>
<div class="outline-text-4" id="text-org20e7478">
<p>Our third layer is another convolutional layer that preserves the input width and height but this time the output will have a depth of 16.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>convolution_two = Layer(kernel=3, stride=1, in_depth=max_pool_one.out_depth,
                        out_depth=16, padding=1)
OUTPUT_THREE = output_size(OUTPUT_TWO, convolution_two, OUTPUT_TWO)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=32, out_depth=16, padding=1)
Layer Output: 14 x 14 x 16

</pre></div>
</div>
<div class="outline-4" id="outline-container-org43a4bc2">
<h4 id="org43a4bc2">Layer Four</h4>
<div class="outline-text-4" id="text-org43a4bc2">
<p>The fourth layer is another max-pool layer that will halve the dimensions.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>max_pool_two = Layer(kernel=2, stride=2, in_depth=convolution_two.out_depth,
                        out_depth=convolution_two.out_depth)
OUTPUT_FOUR = output_size(OUTPUT_THREE, max_pool_two, 7)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=16, out_depth=16, padding=0)
Layer Output: 7 x 7 x 16

</pre></div>
</div>
<div class="outline-4" id="outline-container-org7f235e1">
<h4 id="org7f235e1">Layer Five</h4>
<div class="outline-text-4" id="text-org7f235e1">
<p>The fifth layer is another convolutional layer that will reduce the depth to eight.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>convolution_three = Layer(kernel=3, stride=1,
                          in_depth=max_pool_two.out_depth, out_depth=8,
                          padding=1)
OUTPUT_FIVE = output_size(OUTPUT_FOUR, convolution_three, 7)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=16, out_depth=8, padding=1)
Layer Output: 7 x 7 x 8

</pre></div>
</div>
<div class="outline-4" id="outline-container-org2ac1b50">
<h4 id="org2ac1b50">Layer Six</h4>
<div class="outline-text-4" id="text-org2ac1b50">
<p>The last layer in the encoder is a max pool layer that reduces the previous layer by half (to dimensions of 3) while preserving the depth.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>max_pool_three = Layer(kernel=2, stride=2,
                       in_depth=convolution_three.out_depth,
                       out_depth=convolution_three.out_depth)
OUTPUT_SIX = output_size(OUTPUT_FIVE, max_pool_three, 3)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=8, out_depth=8, padding=0)
Layer Output: 3 x 3 x 8

</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgeed9121">
<h3 id="orgeed9121">Decoders</h3>
<div class="outline-text-3" id="text-orgeed9121"></div>
<div class="outline-4" id="outline-container-orgd6f074b">
<h4 id="orgd6f074b">Layer Six</h4>
<div class="outline-text-4" id="text-orgd6f074b">
<p>This is a transpose convolution layer to (more than) double the size of the image. The image put out by the encoder is 3x3, but we want a 7x7 output, not a 6x6, so the kernel has to be upped to 3.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>transpose_one = Layer(kernel=3, stride=2, out_depth=8,
                      in_depth=max_pool_three.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgeaea8e7">
<h4 id="orgeaea8e7">Layer Seven</h4>
<div class="outline-text-4" id="text-orgeaea8e7">
<p>This will double the size again (to 14x14) and increase the depth to 16.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>transpose_two = Layer(kernel=2, stride=2, out_depth=16,
                      in_depth=transpose_one.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf410cde">
<h4 id="orgf410cde">Layer Eight</h4>
<div class="outline-text-4" id="text-orgf410cde">
<p>This will double the size to 28x28 and up the depth back again to 32, the size of our original encoding convolution.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>transpose_three = Layer(kernel=2, stride=2, out_depth=32,
                        in_depth=transpose_two.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org238d808">
<h4 id="org238d808">Layer Nine</h4>
<div class="outline-text-4" id="text-org238d808">
<p>This is a convolution layer to bring the depth back to one.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>convolution_out = Layer(kernel=3, stride=1, in_depth=transpose_three.out_depth,
                        out_depth=1, padding=1)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org63c69d4">
<h3 id="org63c69d4">The Implementation</h3>
<div class="outline-text-3" id="text-org63c69d4">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class ConvDenoiser(nn.Module):
    def __init__(self):
        super().__init__()
        ## encoder layers ##
        self.convolution_1 =  nn.Conv2d(in_channels=convolution_one.in_depth,
                                       out_channels=convolution_one.out_depth,
                                       kernel_size=convolution_one.kernel,
                                       padding=convolution_one.padding)

        self.convolution_2 = nn.Conv2d(in_channels=convolution_two.in_depth,
                                       out_channels=convolution_two.out_depth,
                                       kernel_size=convolution_two.kernel,
                                       padding=convolution_two.padding)

        self.convolution_3 = nn.Conv2d(in_channels=convolution_three.in_depth,
                                       out_channels=convolution_three.out_depth,
                                       kernel_size=convolution_three.kernel,
                                       padding=convolution_three.padding)

        self.max_pool = nn.MaxPool2d(kernel_size=max_pool_one.kernel,
                                     stride=max_pool_one.stride)

        ## decoder layers ##
        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2
        self.transpose_convolution_1 = nn.ConvTranspose2d(
            in_channels=transpose_one.in_depth,
            out_channels=transpose_one.out_depth,
            kernel_size=transpose_one.kernel,
            stride=transpose_one.stride)

        self.transpose_convolution_2 = nn.ConvTranspose2d(
            in_channels=transpose_two.in_depth, 
            out_channels=transpose_two.out_depth,
            kernel_size=transpose_two.kernel,
            stride=transpose_two.stride)

        self.transpose_convolution_3 = nn.ConvTranspose2d(
            in_channels=transpose_three.in_depth,
            out_channels=transpose_three.out_depth,
            kernel_size=transpose_three.kernel,
            stride=transpose_three.stride)

        self.convolution_out = nn.Conv2d(in_channels=convolution_out.in_depth,
                                         out_channels=convolution_out.out_depth,
                                         kernel_size=convolution_out.kernel,
                                         padding=convolution_out.padding)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        return


    def forward(self, x):
        ## encode ##
        x = self.max_pool(self.relu(self.convolution_1(x)))
        x = self.max_pool(self.relu(self.convolution_2(x)))
        x = self.max_pool(self.relu(self.convolution_3(x)))

        ## decode ##
        x = self.relu(self.transpose_convolution_1(x))
        x = self.relu(self.transpose_convolution_2(x))
        x = self.relu(self.transpose_convolution_3(x))
        return self.sigmoid(self.convolution_out(x))
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0c92fe0">
<h2 id="org0c92fe0">Initialize The NN</h2>
<div class="outline-text-2" id="text-org0c92fe0">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model = ConvDenoiser()
print(model)
</pre></div>
<pre class="example">
ConvDenoiser(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (t_conv1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2))
  (t_conv2): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))
  (t_conv3): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))
  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>test = ConvDenoiser()
dataiter = iter(train_loader)
images, labels = dataiter.next()
x = test.convolution_1(images)
assert x.shape == torch.Size([BATCH_SIZE, 32, 28, 28])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 32, 14, 14])
print(x.shape)

x = test.convolution_2(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 14, 14])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 7, 7])
print(x.shape)

x = test.convolution_3(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 7, 7])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 3, 3]), x.shape

x = test.transpose_convolution_1(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 7, 7]), x.shape
print(x.shape)

x = test.transpose_convolution_2(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 14, 14])
print(x.shape)

x = test.transpose_convolution_3(x)
assert x.shape == torch.Size([BATCH_SIZE, 32, 28, 28])
print(x.shape)

x = test.convolution_out(x)
assert x.shape == torch.Size([BATCH_SIZE, 1, 28, 28])
print(x.shape)
</pre></div>
<pre class="example">
torch.Size([20, 32, 28, 28])
torch.Size([20, 32, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 7, 7])
torch.Size([20, 8, 7, 7])
torch.Size([20, 8, 7, 7])
torch.Size([20, 16, 14, 14])
torch.Size([20, 32, 28, 28])
torch.Size([20, 1, 28, 28])

</pre></div>
</div>
<div class="outline-2" id="outline-container-org4e748cc">
<h2 id="org4e748cc">Training</h2>
<div class="outline-text-2" id="text-org4e748cc">
<p>We are only concerned with the training images, which we can get from the <code>train_loader</code>.</p>
<p>In this case, we are actually <b>adding some noise</b> to these images and we'll feed these <code>noisy_imgs</code> to our model. The model will produce reconstructed images based on the noisy input. But, we want it to produce <span class="underline">normal</span> un-noisy images, and so, when we calculate the loss, we will still compare the reconstructed outputs to the original images!</p>
<p>Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use <code>MSELoss</code>. And compare output images and input images as follows:</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
</pre></div>
<p><b>Warning:</b> I spent an unreasonable amount of time trying to de-bug this thing because I was passing in the model's parameters to the optimizer before passing it to the GPU. I don't know why it didn't throw an error, but it didn't, it just never learned and gave me really high losses. I think it's because the style of these notebooks is to create the parts all over the place so there might have been another 'model' variable in the namespace. In any case, move away from this style and start putting everything into functions and classes - especially the stuff that comes from udacity.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Trainer:
    """Trains our model

    Args:
     data: data-iterator for training
     epochs: number of times to train on the data
     noise: factor for the amount of noise to add
     learning_rate: rate for the optimizer
    """
    def __init__(self, data: torch.utils.data.DataLoader, epochs: int=30,
                 noise:float=0.5,
                 learning_rate:float=0.001) -&gt; None:
        self.data = data
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.noise = noise
        self._criterion = None
        self._model = None
        self._device = None
        self._optimizer = None
        return

    @property
    def device(self) -&gt; torch.device:
        """CUDA or CPU"""
        if self._device is None:
            self._device = torch.device(
                "cuda:0" if torch.cuda.is_available() else "cpu")
        return self._device

    @property
    def criterion(self) -&gt; nn.MSELoss:
        """Loss-calculator"""
        if self._criterion is None:
            self._criterion = nn.MSELoss()
        return self._criterion

    @property
    def model(self) -&gt; ConvDenoiser:
        """Our model"""
        if self._model is None:
            self._model = ConvDenoiser()
            self.model.to(self.device)
        return self._model

    @property
    def optimizer(self) -&gt; torch.optim.Adam:
        """The gradient descent optimizer"""
        if self._optimizer is None:
            self._optimizer = torch.optim.Adam(self.model.parameters(),
                                               lr=self.learning_rate)
        return self._optimizer

    def __call__(self) -&gt; None:
        """Trains the model on the data"""
        self.model.train()
        started = datetime.now()
        for epoch in range(1, self.epochs + 1):
            train_loss = 0.0
            for batch in self.data:
                images, _ = batch
                images = images.to(self.device)
                ## add random noise to the input images
                noisy_imgs = (images
                              + self.noise
                              * torch.randn(*images.shape).to(self.device))
                # Clip the images to be between 0 and 1
                noisy_imgs = numpy.clip(noisy_imgs, 0., 1.).to(self.device)

                # clear the gradients of all optimized variables
                self.optimizer.zero_grad()
                ## forward pass: compute predicted outputs by passing *noisy* images to the model
                outputs = self.model(noisy_imgs)
                # calculate the loss
                # the "target" is still the original, not-noisy images
                loss = self.criterion(outputs, images)
                # backward pass: compute gradient of the loss with respect to model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                self.optimizer.step()
                # update running training loss
                train_loss += loss.item() * images.size(0)

            # print avg training statistics 
            train_loss = train_loss/len(train_loader)
            print('Epoch: {} \tTraining Loss: {:.6f}'.format(
                epoch, 
                train_loss
                ))
        ended = datetime.now()
        print("Ended: {}".format(ended))
        print("Elapsed: {}".format(ended - started))
        return
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_the_model = Trainer(train_loader)
train_the_model()
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 0.952294
Epoch: 2        Training Loss: 0.686571
Epoch: 3        Training Loss: 0.647284
Epoch: 4        Training Loss: 0.628790
Epoch: 5        Training Loss: 0.615522
Epoch: 6        Training Loss: 0.604566
Epoch: 7        Training Loss: 0.595838
Epoch: 8        Training Loss: 0.585816
Epoch: 9        Training Loss: 0.578257
Epoch: 10       Training Loss: 0.572502
Epoch: 11       Training Loss: 0.566983
Epoch: 12       Training Loss: 0.562720
Epoch: 13       Training Loss: 0.558449
Epoch: 14       Training Loss: 0.554410
Epoch: 15       Training Loss: 0.550995
Epoch: 16       Training Loss: 0.546916
Epoch: 17       Training Loss: 0.543798
Epoch: 18       Training Loss: 0.541859
Epoch: 19       Training Loss: 0.539242
Epoch: 20       Training Loss: 0.536748
Epoch: 21       Training Loss: 0.534675
Epoch: 22       Training Loss: 0.532690
Epoch: 23       Training Loss: 0.531692
Epoch: 24       Training Loss: 0.529910
Epoch: 25       Training Loss: 0.528826
Epoch: 26       Training Loss: 0.526354
Epoch: 27       Training Loss: 0.526260
Epoch: 28       Training Loss: 0.525294
Epoch: 29       Training Loss: 0.524029
Epoch: 30       Training Loss: 0.523341
Epoch: 31       Training Loss: 0.522387
Epoch: 32       Training Loss: 0.521689
Ended: 2018-12-22 14:10:08.869789
Elapsed: 0:14:14.036518
</pre></div>
</div>
<div class="outline-2" id="outline-container-org5a8330d">
<h2 id="org5a8330d">Checking out the results</h2>
<div class="outline-text-2" id="text-org5a8330d">
<p>Here I'm adding noise to the test images and passing them through the autoencoder. It does a suprising great job of removing the noise, even though it's sometimes difficult to tell what the original number is.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span># obtain one batch of test images
dataiter = iter(test_loader)
images, labels = dataiter.next()

# add noise to the test images
noisy_imgs = images + noise_factor * torch.randn(*images.shape)
noisy_imgs = numpy.clip(noisy_imgs, 0., 1.)

# get sample outputs
noisy_imgs = noisy_imgs.to(train_the_model.device)
output = train_the_model.model(noisy_imgs)
# prep images for display
noisy_imgs = noisy_imgs.cpu().numpy()

# output is resized into a batch of iages
output = output.view(BATCH_SIZE, 1, 28, 28)
# use detach when it's an output that requires_grad
output = output.detach().cpu().numpy()
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span># plot the first ten input images and then reconstructed images
fig, axes = pyplot.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))

# input images on top row, reconstructions on bottom
for noisy_imgs, row in zip([noisy_imgs, output], axes):
    for img, ax in zip(noisy_imgs, row):
        ax.imshow(numpy.squeeze(img), cmap='gray')
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
</pre></div>
<div class="figure">
<p><img alt="de-noised.png" src="/posts/nano/autoencoders/denoising-autoencoder/de-noised.png"></p>
</div>
<p>That did surprisingly well.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="/categories/autoencoder/" rel="tag">autoencoder</a></li>
<li><a class="tag p-category" href="/categories/cnn/" rel="tag">cnn</a></li>
<li><a class="tag p-category" href="/categories/exercise/" rel="tag">exercise</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="/posts/nano/autoencoders/convolutional-autoencoder/" rel="prev" title="Convolutional Autoencoder">Previous post</a></li>
<li class="next"><a href="/posts/nano/style-transfer/style-transfer/" rel="next" title="Style Transfer">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Contents © 2019 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
