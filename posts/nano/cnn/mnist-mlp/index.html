<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="MNIST Digit Recognition with a Multi-Layer Perceptron (again)." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>MNIST MLP | In Too Deep</title>
<link href="../../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/mnist-mlp/" rel="canonical"><!--[if lt IE 9]><script src="../../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../../site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="../../../assets/javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
<meta content="Cloistered Monkey" name="author">
<link href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/" rel="prev" title="Dog Classification Project Overview" type="text/html">
<link href="/posts/nano/dog-breed-classifier/dog-breed-classification/" rel="next" title="Dog Breed Classification" type="text/html">
<meta content="In Too Deep" property="og:site_name">
<meta content="MNIST MLP" property="og:title">
<meta content="https://necromuralist.github.io/In-Too-Deep/posts/nano/cnn/mnist-mlp/" property="og:url">
<meta content="MNIST Digit Recognition with a Multi-Layer Perceptron (again)." property="og:description">
<meta content="article" property="og:type">
<meta content="2018-11-25T17:29:13-08:00" property="article:published_time">
<meta content="classification" property="article:tag">
<meta content="cnn" property="article:tag">
<meta content="exercise" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="/posts/nano/cnn/mnist-mlp/index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="/posts/nano/cnn/mnist-mlp/">MNIST MLP</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/cnn/mnist-mlp/" rel="bookmark"><time class="published dt-published" datetime="2018-11-25T17:29:13-08:00" itemprop="datePublished" title="2018-11-25 17:29">2018-11-25 17:29</time></a></p>
<p class="sourceline"><a class="sourcelink" href="/posts/nano/cnn/mnist-mlp/index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/cnn/mnist-mlp/#org08b84c9">Introduction</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org6e03897">Imports</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org4411e4e">Setup the Plotting</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#orgba21831">The Data</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org643b77c">Visualize a Batch of Training Data</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org533c33e">Define the Network Architecture</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org6e62061">Specify the Loss Function and Optimizer</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org0966693">Train the Network</a></li>
<li><a href="/posts/nano/cnn/mnist-mlp/#org8ce9bb0">Test the Trained Network</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org08b84c9">
<h2 id="org08b84c9">Introduction</h2>
<div class="outline-text-2" id="text-org08b84c9">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>We are going to train a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multi-Layer Perceptron</a> to classify images from the <a href="http://yann.lecun.com/exdb/mnist/">MNIST database</a> of hand-written digits.</p>
<p>We're going to do it using the following steps.</p>
<ol class="org-ol">
<li>Load and visualize the data</li>
<li>Define a neural network</li>
<li>Train the model</li>
<li>Evaluate the performance of our trained model on a test dataset</li>
</ol>
</div>
</div>
<div class="outline-2" id="outline-container-org6e03897">
<h2 id="org6e03897">Imports</h2>
<div class="outline-text-2" id="text-org6e03897"></div>
<div class="outline-3" id="outline-container-org4dde658">
<h3 id="org4dde658">From Python</h3>
<div class="outline-text-3" id="text-org4dde658">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from datetime import datetime
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org6a7c425">
<h3 id="org6a7c425">From PyPi</h3>
<div class="outline-text-3" id="text-org6a7c425">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from dotenv import load_dotenv
from torchvision import datasets
import matplotlib.pyplot as pyplot
import seaborn
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torch
import numpy
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb6e4036">
<h3 id="orgb6e4036">This Project</h3>
<div class="outline-text-3" id="text-orgb6e4036">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4411e4e">
<h2 id="org4411e4e">Setup the Plotting</h2>
<div class="outline-text-2" id="text-org4411e4e">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=3)
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgba21831">
<h2 id="orgba21831">The Data</h2>
<div class="outline-text-2" id="text-orgba21831"></div>
<div class="outline-3" id="outline-container-org7df55a7">
<h3 id="org7df55a7">The Path To the Data</h3>
<div class="outline-text-3" id="text-org7df55a7">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>load_dotenv()
path = DataPathTwo(folder_key="MNIST")
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print(path.folder)
print(path.folder.exists())
</pre></div>
<pre class="example">
/home/hades/datasets/MNIST
True

</pre></div>
</div>
<div class="outline-3" id="outline-container-orge4ee6a9">
<h3 id="orge4ee6a9">Some Settings</h3>
<div class="outline-text-3" id="text-orge4ee6a9">
<p>Since I downloaded the data earlier for some other exercise forking sub-processes is probably unnecessary, and for the training and testing we'll use a relatively small batch-size of 20.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>WORKERS = 0
BATCH_SIZE = 20
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org421879e">
<h3 id="org421879e">A Transform</h3>
<div class="outline-text-3" id="text-org421879e">
<p>We're just going to convert the images to tensors.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>transform = transforms.ToTensor()
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org91bc4a8">
<h3 id="org91bc4a8">Split Up the Training and Testing Data</h3>
<div class="outline-text-3" id="text-org91bc4a8">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_data = datasets.MNIST(root=path.folder, train=True,
                            download=True, transform=transform)
test_data = datasets.MNIST(root=path.folder, train=False,
                           download=True, transform=transform)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb33ee98">
<h3 id="orgb33ee98">Create the Batch Loaders</h3>
<div class="outline-text-3" id="text-orgb33ee98">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_batches = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
                                            num_workers=WORKERS)
test_batches = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, 
                                           num_workers=WORKERS)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org643b77c">
<h2 id="org643b77c">Visualize a Batch of Training Data</h2>
<div class="outline-text-2" id="text-org643b77c">
<p>The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data.</p>
</div>
<div class="outline-3" id="outline-container-org0b0d02e">
<h3 id="org0b0d02e">Grab a batch</h3>
<div class="outline-text-3" id="text-org0b0d02e">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>images, labels = iter(train_batches).next()
images = images.numpy()
</pre></div>
<p>Now that we have a batch we're going to plot the images in the batch, along with the corresponding labels.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure = pyplot.figure(figsize=(25, 4))
figure.suptitle("First Batch", weight="bold")
for index in numpy.arange(BATCH_SIZE):
    ax = figure.add_subplot(2, BATCH_SIZE/2, index+1, xticks=[], yticks=[])
    ax.imshow(numpy.squeeze(images[index]), cmap='gray')
    # print out the correct label for each image
    # .item() gets the value contained in a Tensor
    ax.set_title(str(labels[index].item()))
</pre></div>
<div class="figure">
<p><img alt="batch.png" src="/posts/nano/cnn/mnist-mlp/batch.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org8814740">
<h3 id="org8814740">View a Single Image</h3>
<div class="outline-text-3" id="text-org8814740">
<p>Now we're going to take a closer look at the second image in the batch.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>image = numpy.squeeze(images[1])

figure = pyplot.figure(figsize = (12,12)) 
ax = figure.add_subplot(111)
ax.imshow(image, cmap='gray')
width, height = image.shape
threshold = image.max()/2.5
for x in range(width):
    for y in range(height):
        val = round(image[x][y],2) if image[x][y] !=0 else 0
        ax.annotate(str(val), xy=(y,x),
                    horizontalalignment='center',
                    verticalalignment='center',
                    color='white' if image[x][y]&lt;threshold else 'black')
</pre></div>
<div class="figure">
<p><img alt="image.png" src="/posts/nano/cnn/mnist-mlp/image.png"></p>
</div>
<p>We're looking at a single image with the normalized values for each pixel superimposed on it. It looks like black is 0 and white is 1, although for this image most of the 'white' pixels are just a little less than one.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org533c33e">
<h2 id="org533c33e">Define the Network <a href="http://pytorch.org/docs/stable/nn.html">Architecture</a></h2>
<div class="outline-text-2" id="text-org533c33e">
<p>The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting.</p>
<p>These values are based on the <a href="https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py">keras</a> example implementation.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>INPUT_NODES = 28 * 28
HIDDEN_NODES = 512
DROPOUT = 0.2
CLASSES = 10
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Net(nn.Module):
    def __init__(self):
        super().__init__()        
        self.fully_connected_layer_1 = nn.Linear(INPUT_NODES, HIDDEN_NODES)
        self.fully_connected_layer_2 = nn.Linear(HIDDEN_NODES, HIDDEN_NODES)
        self.output = nn.Linear(HIDDEN_NODES, CLASSES)
        self.dropout = nn.Dropout(p=DROPOUT)
        return

    def forward(self, x):
        # flatten image input
        x = x.view(-1, 28 * 28)
        # add hidden layer, with relu activation function
        x = self.dropout(F.relu(self.fully_connected_layer_1(x)))
        x = self.dropout(F.relu(self.fully_connected_layer_2(x)))        
        return self.output(x)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org468ce76">
<h3 id="org468ce76">Initialize the NN</h3>
<div class="outline-text-3" id="text-org468ce76">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model = Net()
print(model)
</pre></div>
<pre class="example">
Net(
  (fully_connected_layer_1): Linear(in_features=784, out_features=512, bias=True)
  (fully_connected_layer_2): Linear(in_features=512, out_features=512, bias=True)
  (output): Linear(in_features=512, out_features=10, bias=True)
  (dropout): Dropout(p=0.2)
)

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgd593509">
<h3 id="orgd593509">A Little CUDA</h3>
<div class="outline-text-3" id="text-orgd593509">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org6e62061">
<h2 id="org6e62061">Specify the <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">Loss Function</a> and <a href="http://pytorch.org/docs/stable/optim.html">Optimizer</a></h2>
<div class="outline-text-2" id="text-org6e62061">
<p>It's recommended that you use <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">cross-entropy loss</a> for classification. If you look at the documentation you can see that PyTorch's cross entropy function applies a softmax function to the output layer <b>and</b> then calculates the log loss (so you don't want to do softmax as part of the model output).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org0966693">
<h2 id="org0966693">Train the Network</h2>
<div class="outline-text-2" id="text-org0966693">
<p>The steps for training/learning from a batch of data are:</p>
<ol class="org-ol">
<li>Clear the gradients of all optimized variables</li>
<li>Forward pass: compute predicted outputs by passing inputs to the model</li>
<li>Calculate the loss</li>
<li>Backward pass: compute gradient of the loss with respect to model parameters</li>
<li>Perform a single optimization step (parameter update)</li>
<li>Update average training loss</li>
</ol>
<p>The following loop trains for 30 epochs; feel free to change this number. For now, we suggest somewhere between 20-50 epochs. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>EPOCHS = 30
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>start = datetime.now()
model.train() # prep model for training

for epoch in range(EPOCHS):
    # monitor training loss
    train_loss = 0.0
    train_losses = []
    # train the model
    for data, target in train_batches:
        # move it to the GPU or CPU
        data, target = data.to(device), target.to(device)
        # clear the gradients of all optimized variables
        optimizer.zero_grad()
        # forward pass: compute predicted outputs by passing inputs to the model
        output = model(data)
        # calculate the loss
        loss = criterion(output, target)
        # backward pass: compute gradient of the loss with respect to model parameters
        loss.backward()
        # perform a single optimization step (parameter update)
        optimizer.step()
        # update running training loss
        train_loss += loss.item() * data.size(0)

        # print training statistics 
        # calculate average loss over an epoch
    train_loss = train_loss/len(train_batches.dataset)
    train_losses.append(train_loss)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(
        epoch+1, 
        train_loss
        ))
print("Training Time: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 0.826836
Epoch: 2        Training Loss: 0.324859
Epoch: 3        Training Loss: 0.251608
Epoch: 4        Training Loss: 0.202294
Epoch: 5        Training Loss: 0.170231
Epoch: 6        Training Loss: 0.146775
Epoch: 7        Training Loss: 0.127352
Epoch: 8        Training Loss: 0.115026
Epoch: 9        Training Loss: 0.104332
Epoch: 10       Training Loss: 0.093575
Epoch: 11       Training Loss: 0.084913
Epoch: 12       Training Loss: 0.077826
Epoch: 13       Training Loss: 0.071506
Epoch: 14       Training Loss: 0.067273
Epoch: 15       Training Loss: 0.063749
Epoch: 16       Training Loss: 0.058150
Epoch: 17       Training Loss: 0.054770
Epoch: 18       Training Loss: 0.051584
Epoch: 19       Training Loss: 0.047762
Epoch: 20       Training Loss: 0.045219
Epoch: 21       Training Loss: 0.041732
Epoch: 22       Training Loss: 0.040526
Epoch: 23       Training Loss: 0.038247
Epoch: 24       Training Loss: 0.035713
Epoch: 25       Training Loss: 0.033801
Epoch: 26       Training Loss: 0.031963
Epoch: 27       Training Loss: 0.031082
Epoch: 28       Training Loss: 0.028971
Epoch: 29       Training Loss: 0.027500
Epoch: 30       Training Loss: 0.026876
Training Time: 0:05:59.808071
</pre></div>
</div>
<div class="outline-2" id="outline-container-org8ce9bb0">
<h2 id="org8ce9bb0">Test the Trained Network</h2>
<div class="outline-text-2" id="text-org8ce9bb0">
<p>Finally, we test our best model on previously unseen <b>test data</b> and evaluate it's performance. Testing on unseen data is a good way to check that our model generalizes well. It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its overall loss and accuracy.</p>
</div>
<div class="outline-3" id="outline-container-org70de0ec">
<h3 id="org70de0ec"><code>model.eval()</code></h3>
<div class="outline-text-3" id="text-org70de0ec">
<p><code>model.eval(</code>) will set all the layers in your model to evaluation mode. This affects layers like dropout layers that turn "off" nodes during training with some probability, but should allow every node to be "on" for evaluation!</p>
</div>
</div>
<div class="outline-3" id="outline-container-org5833fd8">
<h3 id="org5833fd8">Set Up the Testing</h3>
<div class="outline-text-3" id="text-org5833fd8">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">class_correct</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mf">0.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">class_total</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mf">0.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_batches</span><span class="p">:</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># forward pass: compute predicted outputs by passing inputs to the model</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="c1"># calculate the loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="c1"># update test loss </span>
    <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># convert output probabilities to predicted class</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># compare predictions to true label</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">prediction</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">prediction</span><span class="p">)))</span>
    <span class="c1"># calculate test accuracy for each object class</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">class_correct</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="n">correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">class_total</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">print</span><span class="p">(</span><span class="s2">"Test Time: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<pre class="example">
Test Time: 0:00:01.860151

</pre></div>
</div>
<div class="outline-3" id="outline-container-org7282665">
<h3 id="org7282665">Calculate and Print Average Test Loss</h3>
<div class="outline-text-3" id="text-org7282665">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">test_loss</span> <span class="o">=</span> <span class="n">test_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_batches</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Test Loss: {:.6f}</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Test Accuracy of Batch {}: {:.2f} ({}/{})'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">class_correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_correct</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Test Accuracy of {}: N/A (no training examples)'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="k">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">Test Accuracy (Overall): </span><span class="si">%2d%%</span><span class="s1"> (</span><span class="si">%2d</span><span class="s1">/</span><span class="si">%2d</span><span class="s1">)'</span> <span class="o">%</span> <span class="p">(</span>
    <span class="mf">100.</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_total</span><span class="p">),</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_correct</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_total</span><span class="p">)))</span>
</pre></div>
<pre class="example">
Test Loss: 0.056054

Test Accuracy of Batch 0: 99.18 (972.0/980.0)
Test Accuracy of Batch 1: 99.21 (1126.0/1135.0)
Test Accuracy of Batch 2: 98.16 (1013.0/1032.0)
Test Accuracy of Batch 3: 98.02 (990.0/1010.0)
Test Accuracy of Batch 4: 98.47 (967.0/982.0)
Test Accuracy of Batch 5: 98.43 (878.0/892.0)
Test Accuracy of Batch 6: 98.12 (940.0/958.0)
Test Accuracy of Batch 7: 97.47 (1002.0/1028.0)
Test Accuracy of Batch 8: 97.13 (946.0/974.0)
Test Accuracy of Batch 9: 98.12 (990.0/1009.0)

Test Accuracy (Overall): 98% (9824/10000)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org519870e">
<h3 id="org519870e">Visualize Sample Test Results</h3>
<div class="outline-text-3" id="text-org519870e">
<p>This cell displays test images and their labels in this format: <code>predicted (ground-truth)</code>. The text will be green for accurately classified examples and red for incorrect predictions.</p>
</div>
<div class="outline-4" id="outline-container-orgf68c0e2">
<h4 id="orgf68c0e2">Obtain One Batch of Test Images</h4>
<div class="outline-text-4" id="text-orgf68c0e2">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_batches</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

<span class="c1"># get sample outputs</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="c1"># convert output probabilities to predicted class</span>
<span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># prep images for display</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="c1"># plot the images in the batch, along with predicted and true labels</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">idx</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"{} ({})"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span> <span class="nb">str</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())),</span>
                 <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="s2">"green"</span> <span class="k">if</span> <span class="n">preds</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">==</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">else</span> <span class="s2">"red"</span><span class="p">))</span>
</pre></div>
<div class="figure">
<p><img alt="test.png" src="/posts/nano/cnn/mnist-mlp/test.png"></p>
</div>
<p>This model is surprisingly accurate. I say surprising, even though we created a very accurate model previously, because in my original implementation I used <code>RMSprop</code> as the optimizer, because that's what the Keras implementation used, but then I only got 11%. I'm guessing that there's some extra tuning you need to do to the parameters for <code>RMSprop</code> but I just naively used the defaults. In any case, it semms that SGD is still the champ.</p>
</div>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="/categories/classification/" rel="tag">classification</a></li>
<li><a class="tag p-category" href="/categories/cnn/" rel="tag">cnn</a></li>
<li><a class="tag p-category" href="/categories/exercise/" rel="tag">exercise</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="/posts/nano/dog-breed-classifier/dog-classification-project-overview/" rel="prev" title="Dog Classification Project Overview">Previous post</a></li>
<li class="next"><a href="/posts/nano/dog-breed-classifier/dog-breed-classification/" rel="next" title="Dog Breed Classification">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Contents © 2019 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
