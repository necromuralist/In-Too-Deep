<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="The Maximum Likelihood prediction method." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Maximum Likelihood | Neurotic Networking</title>
<link href="../../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nano/introduction-to-neural-networks/maximum-likelihood/" rel="canonical"><!--[if lt IE 9]><script src="../../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../one-hot-encoding/" rel="prev" title="One-Hot Encoding" type="text/html">
<link href="../../../grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/" rel="next" title="Okay, but what about this deep-learning stuff?" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Maximum Likelihood" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nano/introduction-to-neural-networks/maximum-likelihood/" property="og:url">
<meta content="The Maximum Likelihood prediction method." property="og:description">
<meta content="article" property="og:type">
<meta content="2018-10-23T21:29:52-07:00" property="article:published_time">
<meta content="lecture" property="article:tag">
<meta content="neural networks" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Maximum Likelihood</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2018-10-23T21:29:52-07:00" itemprop="datePublished" title="2018-10-23 21:29">2018-10-23 21:29</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org66e3f70">What is this about?</a></li>
<li><a href="#orgfbc89ec">Yeah, okay, but how do you do this?</a></li>
<li><a href="#orgff70f7f">The Problem With Products</a></li>
<li><a href="#orge66f597">Cross Entropy</a></li>
<li><a href="#orgd7335e4">Okay, but how do we implement this?</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org66e3f70">
<h2 id="org66e3f70">What is this about?</h2>
<div class="outline-text-2" id="text-org66e3f70">
<p>We want a way to train our neural network based on the data we have - how do we do this? One way is to use <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood</a> where we give weights based on the past occurrences for each score.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgfbc89ec">
<h2 id="orgfbc89ec">Yeah, okay, but how do you do this?</h2>
<div class="outline-text-2" id="text-orgfbc89ec">
<p>First, remember that our probability for any point being 0 or 1 is based on the sigmoid.</p>
<p>\[ \hat{y} = \sigma(Wx+b) \]</p>
<p>Where \(\hat{y}\) is the probability that a point is non-negative.</p>
<p>So we can take the product of the sigmoid of all the points in our data set and find the probability that any point is a 1. If we were to find a model that maximized this probability, we would have a model that separated our categories - this is the Maximum Likelihood Model.</p>
<p>To be more specific, we calculate \(\hat{y}\) for all of our training set points and multiply them to get the total probability (multiplication is an AND operation - \(p(a) \land p(b) \land p(c) = p(a) \times p(b) \time p(c)\)) then we adjust our moder to maximize this probability.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgff70f7f">
<h2 id="orgff70f7f">The Problem With Products</h2>
<div class="outline-text-2" id="text-orgff70f7f">
<p>Each of our probablilities is less than 1, so the more of them you have, the smaller their product will become. What we want to do is use addition - which is where the logarithm comes in.</p>
<p>\[ p(a) * p(b) * p(c) = \log(a) + \log p(b) + \log p(4) \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-orge66f597">
<h2 id="orge66f597">Cross Entropy</h2>
<div class="outline-text-2" id="text-orge66f597">
<p>Our logarithms give us the values that we want to maximize, but another way to look at is as "we want to minimize the error". We can do this by using the negatives of the logarithms to find the error and trying to minimize their sums.</p>
<p>\[ \textit{cross entropy} = -\log p(a) - \log p(b) - \log p(4) \]</p>
<p>More generally:</p>
<p>\[ \textit{Cross Entropy} = -\sum_{i=1}^m y_i \ln(p_i) + (1 - y_i)\ln(1-p_i) \]</p>
<p>Where <i>y</i> is vector of 1's and 0's. When <i>y</i> is 0, the left term is 0 and when <i>y</i> is 1 the right term is 0 so it works as sort of a conditional to choose which term to use.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgd7335e4">
<h2 id="orgd7335e4">Okay, but how do we implement this?</h2>
<div class="outline-text-2" id="text-orgd7335e4">
<p>Write a function that takes as input two lists Y, P, and returns the float corresponding to their cross-entropy.</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">P</span><span class="p">):</span>
    <span class="sd">"""calculates the cross entropy of two lists</span>

<span class="sd">    Args:</span>
<span class="sd">     Y: lists of 1s and 0s</span>
<span class="sd">     P: lists of probabilities that Y is 1</span>
<span class="sd">    Returns:</span>
<span class="sd">     cross-entropy: the cross entropy of the two lists</span>
<span class="sd">    """</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">not_Y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span>    
    <span class="n">P</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
    <span class="n">not_P</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">P</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">Y</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">)</span> <span class="o">+</span> <span class="n">not_Y</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">not_P</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">Y</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> 
<span class="n">P</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="mf">4.8283137373</span>
<span class="n">entropy</span> <span class="o">=</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">P</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">entropy</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">entropy</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="o">**</span><span class="mi">5</span>
</pre></div>
<pre class="example">
4.828313737302301
</pre></div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../../categories/lecture/" rel="tag">lecture</a></li>
<li><a class="tag p-category" href="../../../../categories/neural-networks/" rel="tag">neural networks</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../one-hot-encoding/" rel="prev" title="One-Hot Encoding">Previous post</a></li>
<li class="next"><a href="../../../grokking/03_forward_propagation/okay-but-what-about-this-deep-learning-stuff/" rel="next" title="Okay, but what about this deep-learning stuff?">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
