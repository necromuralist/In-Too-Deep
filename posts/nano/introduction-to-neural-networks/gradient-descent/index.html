<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Gradient Descent for neural networks." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Gradient Descent | In Too Deep</title>
<link href="../../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/posts/nano/introduction-to-neural-networks/gradient-descent/" rel="canonical"><!--[if lt IE 9]><script src="../../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../../site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="../../../assets/javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
<meta content="Cloistered Monkey" name="author">
<link href="/posts/grokking/04_gradient_descent/compare-and-learn/" rel="prev" title="Compare and Learn" type="text/html">
<link href="/posts/nano/introduction-to-neural-networks/gradient-descent-practice/" rel="next" title="Gradient Descent Practice" type="text/html">
<meta content="In Too Deep" property="og:site_name">
<meta content="Gradient Descent" property="og:title">
<meta content="https://necromuralist.github.io/In-Too-Deep/posts/nano/introduction-to-neural-networks/gradient-descent/" property="og:url">
<meta content="Gradient Descent for neural networks." property="og:description">
<meta content="article" property="og:type">
<meta content="2018-10-26T18:30:34-07:00" property="article:published_time">
<meta content="gradient descent" property="article:tag">
<meta content="lecture" property="article:tag">
<meta content="neural networks" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="/posts/nano/introduction-to-neural-networks/gradient-descent/index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="/posts/nano/introduction-to-neural-networks/gradient-descent/">Gradient Descent</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/nano/introduction-to-neural-networks/gradient-descent/" rel="bookmark"><time class="published dt-published" datetime="2018-10-26T18:30:34-07:00" itemprop="datePublished" title="2018-10-26 18:30">2018-10-26 18:30</time></a></p>
<p class="sourceline"><a class="sourcelink" href="/posts/nano/introduction-to-neural-networks/gradient-descent/index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/nano/introduction-to-neural-networks/gradient-descent/#orga68a4ac">What is this about?</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/gradient-descent/#org5119f91">How does Gradient Descent Work?</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/gradient-descent/#orgc04d052">Okay, but what again?</a></li>
<li><a href="/posts/nano/introduction-to-neural-networks/gradient-descent/#org51b76d6">So, how do you put it all together?</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orga68a4ac">
<h2 id="orga68a4ac">What is this about?</h2>
<div class="outline-text-2" id="text-orga68a4ac">
<p>We have an initial network and we make a prediction using the inputs. USing the output we can calculate the error. Now that we have the error we need to update our weights - how do we do this? With <a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a>, a method that tries to pursue a downward trajectory using the slope of our errors.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org5119f91">
<h2 id="org5119f91">How does Gradient Descent Work?</h2>
<div class="outline-text-2" id="text-org5119f91">
<p>We start by making an initial prediction.</p>
<p>\[ \hat{y} = \sigma(Wx+b) \]</p>
<p>which it turns out is not accurate. We then subtract the gradient of the error (a partial derivative \(\frac{\delta E}{\delta W}\)) multiplied by some learning rate \(\alpha\) that governs how much we are willing to change at each step down the hill.</p>
<p>\[ w'_i \gets w_i - \alpha \frac{\delta E}{\delta W_i}\\ b' \gets b -\alpha \frac{\delta E}{\delta b}\\ \hat{y'} = \sigma(W'x t b') \]</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgc04d052">
<h2 id="orgc04d052">Okay, but what again?</h2>
<div class="outline-text-2" id="text-orgc04d052">
<p>To find our gradient we need to take some derivatives. Let's start with the derivative of our sigmoid.</p>
<p>\[ \sigma' = \sigma(x) (1 - \sigma(x)) \] The lecturer shows the derivation, but take my word for it, this is what it is.</p>
<p>Our error is:</p>
<p>\[ E = -y \ln(\hat{y}) - (1 - y)\ln(1 - \hat{y}) \]</p>
<p>and the derivative of this error:</p>
<p>\[ \frac{\delta}{\delta_{wj}}\hat{y} = \hat{y}(1 - \hat{y}) \dot x_j \]</p>
<p>Trust me, this is the derivation. And the derivation of our error becomes:</p>
<p>\[ \frac{\delta}{\delta w_j} = -(y - \hat{y})x_j \]</p>
<p>and for the bias term we get:</p>
<p>\[ \frac{\delta}{\delta b} = -(y - \hat{y}) \]</p>
<p>And our overall gradient can be written as:</p>
<p>\[ \Delta E = -(y - \hat{y})(x_1, \ldots, x_n, 1) \]</p>
<p>So our gradient is the coordinates of the points times the error. This means that the closer our prediction is to the true value, the smaller the gradient will be, and vice-versa, much like the <i>Perceptron Trick</i> we learned earlier.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org51b76d6">
<h2 id="org51b76d6">So, how do you put it all together?</h2>
<div class="outline-text-2" id="text-org51b76d6">
<p>Okay, this is how you update your weights. First, scale \(\alpha\) to match your data set (divide by the number of rows). \[ \alpha = \frac{1}{m}\alpha \]</p>
<p>Now calculate your new weights.</p>
<p>\[ w_i' \gets w_i + \alpha(y - \hat{y})x_i \]</p>
<p>And the new bias.</p>
<p>\[ b' \gets b \alpha(y - \hat{y}) \]</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="/categories/gradient-descent/" rel="tag">gradient descent</a></li>
<li><a class="tag p-category" href="/categories/lecture/" rel="tag">lecture</a></li>
<li><a class="tag p-category" href="/categories/neural-networks/" rel="tag">neural networks</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="/posts/grokking/04_gradient_descent/compare-and-learn/" rel="prev" title="Compare and Learn">Previous post</a></li>
<li class="next"><a href="/posts/nano/introduction-to-neural-networks/gradient-descent-practice/" rel="next" title="Gradient Descent Practice">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Contents Â© 2019 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
