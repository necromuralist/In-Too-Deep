#+BEGIN_COMMENT
.. title: Multi-Layer Perceptrons
.. slug: multi-layer-perceptrons
.. date: 2018-11-17 18:51:40 UTC-08:00
.. tags: lecture,perceptron
.. category: Lecture
.. link: 
.. description: Another look at multi-layer perceptrons.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 1
This is basically like the previous gradient-descent post but with more layers.
* Set Up
** Imports
*** From PyPi
#+BEGIN_SRC python :session perceptron :results none
from graphviz import Graph
import numpy
#+END_SRC

* The Activation Function
#+BEGIN_SRC python :session perceptron :results none
def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1 + numpy.exp(-x))
#+END_SRC
* Defining Our Network
  These variables will define our network size.

#+BEGIN_SRC python :session perceptron :results none
N_input = 4
N_hidden = 3
N_output = 2
#+END_SRC

Which produces a network like this.

#+BEGIN_SRC python :session perceptron :results raw drawer :ipyfile ../../../files/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/network.dot.png
graph = Graph(format="png")

# input layer
graph.node("a", "x1")
graph.node("b", "x2")
graph.node("c", "x3")
graph.node("d", "x4")

# hidden layer
graph.node("e", "h1")
graph.node("f", "h2")
xograph.node("g", "h3")

# output layer
graph.node("h", "")
graph.node("i", "")

graph.edges(["ae", "af", "ag", "be", "bf", "bg", "ce", "cf", "cg", "de", "df", "dg"])
graph.edges(["eh", "ei", "fh", "fi", "gh", "gi"])

graph.render("graphs/network.dot")
graph
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[27]:
[[file:../../../files/posts/nano/introduction-to-neural-networks/multi-layer-perceptrons/network.dot.png]]
:END:

[[file:network.dot.png]]

Next, set the random seed.

#+BEGIN_SRC python :session perceptron :results none
numpy.random.seed(42)
#+END_SRC

Some fake data to train on.

#+BEGIN_SRC python :session perceptron :results none
X = numpy.random.randn(4)
#+END_SRC

#+BEGIN_SRC python :session perceptron :results output :exports both
print(X.shape)
#+END_SRC

#+RESULTS:
: (4,)

Now initialize our weights.

#+BEGIN_SRC python :session perceptron :results none
weights_input_to_hidden = numpy.random.normal(0, scale=0.1, size=(N_input, N_hidden))
weights_hidden_to_output = numpy.random.normal(0, scale=0.1, size=(N_hidden, N_output))
#+END_SRC

#+BEGIN_SRC python :session perceptron :results output :exports both
print(weights_input_to_hidden.shape)
print(weights_hidden_to_output.shape)
#+END_SRC

#+RESULTS:
: (4, 3)
: (3, 2)

* Forward Pass
  This is one forward pass through our network.

#+BEGIN_SRC python :session perceptron :results none
hidden_layer_in = X.dot(weights_input_to_hidden)
hidden_layer_out = sigmoid(hidden_layer_in)
#+END_SRC

#+BEGIN_SRC python :session perceptron :results output :exports both
print('Hidden-layer Output:')
print(hidden_layer_out)
#+END_SRC

#+RESULTS:
: Hidden-layer Output:
: [0.5609517  0.4810582  0.44218495]

Now our output.

#+BEGIN_SRC python :session perceptron :results none
output_layer_in = hidden_layer_out.dot(weights_hidden_to_output)
output_layer_out = sigmoid(output_layer_in)
#+END_SRC

#+BEGIN_SRC python :session perceptron :results output :exports both
print('Output-layer Output:')
print(output_layer_out)
#+END_SRC

#+RESULTS:
: Output-layer Output:
: [0.49936449 0.46156347]
