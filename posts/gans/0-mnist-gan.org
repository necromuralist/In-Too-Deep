#+BEGIN_COMMENT
.. title: MNIST GAN
.. slug: mnist-gan
.. date: 2021-04-06 17:48:17 UTC-07:00
.. tags: gans
.. category: GANs
.. link: 
.. description: An MNIST GAN with pytorch.
.. type: text
.. has_math: True
#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-193f97fd-cdd7-456a-86d6-1597061029ee-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
%config InlineBackend.figure_format 'retina'
#+END_SRC

**Note:** The current version of pytorch (1.8.1) causes a Segmentation Fault in my nvidia-docker container (running CUDA 11, python 3.9, and Ubuntu 20.04). The fault comes at different points in the code depending on what I do - sometimes it's the backward's propagation, sometimes it's the pytorch binary that causes it, sometimes it's the libcuda binary... trying to debug it is probably beyond me so to get this working I had to go to the previous version of pytorch (1.7.1).

**Update:** The previous error happened when I was using pip. I switched to using conda and that seems to have fixed it. One of the downsides to using conda is that it seems to download all the binary dependencies and it takes much longer to install everything than it does with pip, maybe because I don't have a really fat internet pipe. Oh, well.

* Beginning
** Imports
#+begin_src python :results none
# python
from collections import namedtuple
from functools import partial

# from pypi
from torch import nn
from torchvision import transforms
from torchvision.datasets import MNIST
from torchvision.utils import make_grid
from torch.utils.data import DataLoader

import hvplot.pandas
import matplotlib.pyplot as pyplot
import pandas
import torch

# local code
from graeae import EmbedHoloviews, Timer
#+end_src
** Some Setup
   First we'll set the manual seed to make this reproducible.
#+begin_src python :results none
torch.manual_seed(0)
#+end_src

This is a convenience object to time the training.
#+begin_src python :results none
TIMER = Timer()
#+end_src

This is for plotting.

#+begin_src python :results none
slug = "mnist-gan"

Embed = partial(EmbedHoloviews, folder_path=f"files/posts/gans/{slug}")

Plot = namedtuple("Plot", ["width", "height", "fontscale", "tan", "blue", "red"])
PLOT = Plot(
    width=900,
    height=750,
    fontscale=2,
    tan="#ddb377",
    blue="#4687b7",
    red="#ce7b6d",
 )
#+end_src
* Middle
** The MNIST Dataset
The training images we will be using are from a dataset called [[http://yann.lecun.com/exdb/mnist/][MNIST]]. The dataset contains 60,000 images of handwritten digits, from 0 to 9.

The images are 28 pixels x 28 pixels in size. The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or "color channel", is needed to represent them. Pytorch has a [[https://pytorch.org/vision/0.8/datasets.html#mnist][version of it]] ready-made for their system so we'll use theirs.

** The Generator
The first step is to build the generator component.

We'll start by creating a function to make a single layer/block for the generator's neural network. Each block should include a [[https://pytorch.org/docs/stable/generated/torch.nn.Linear.html][linear transformation]] (\(y=xA^T + b\)) to the input to another shape, [[https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html][batch normalization]] for stabilization, and finally a non-linear activation function ([[https://pytorch.org/docs/master/generated/torch.nn.ReLU.html][ReLU]] in this case).

#+begin_src python :results none
def generator_block(input_features: int, output_features: int) -> nn.Sequential:
    """
    Creates a block of the generator's neural network

    Args:
      input_features: the dimension of the input vector
      output_features: the dimension of the output vector

    Returns:
        a generator neural network layer, with a linear transformation 
          followed by a batch normalization and then a relu activation
    """
    return nn.Sequential(
        nn.Linear(input_features, output_features),
        nn.BatchNorm1d(output_features),
        nn.ReLU(inplace=True),
    )
#+end_src

*** Verify the generator block function
#+begin_src python :results none
def test_gen_block(in_features: int, out_features: int,
                   test_rows: int=1000) -> None:
    """Test the generator block creator

    Args:
     in_features: number of features for the block input
     out_features: the final number of features for it to output
     test_rows: how many rows to put in the test Tensor

    Raises:
     AssertionError: something isn't right
    """
    block = generator_block(in_features, out_features)

    # Check the three parts
    assert len(block) == 3
    assert type(block[0]) == nn.Linear
    assert type(block[1]) == nn.BatchNorm1d
    assert type(block[2]) == nn.ReLU
    
    # Check the output shape
    test_output = block(torch.randn(test_rows, in_features))
    assert tuple(test_output.shape) == (test_rows, out_features)

    # check the normalization
    assert 0.65 > test_output.std() > 0.55
    return

test_gen_block(25, 12)
test_gen_block(15, 28)
#+end_src
*** Building the Generator Class
    Now that we have the block-builder we can define our Generator network. It's going to contain a sequence of blocks output by our block-building function and a final two layers that use the linear transformation again, but don't apply normalization and use a [[https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html][Sigmoid Function]] instead of the ReLU. Each block will have an output double that of the previous one.

#+begin_src plantuml :results none :exports results :file ../../files/posts/gans/mnist-gan/generator.png
class nn.Module #white;line:lightslategray
Generator -|> nn.Module
class Generator #white;line:lightslategray {
 int: input_dimension
 int: image_dimension
 int: hidden_dimension
 nn.Sequential: generator

 torch.Tensor: forward(torch.Tensor: noise)
}
#+end_src

[[file:generator.png]]

#+begin_src python :results none
class Generator(nn.Module):
    """Generator Class

    Args:
      input_dimension: the dimension of the noise vector
      image_dimension: the dimension of the images, fitted for the dataset used
         (MNIST images are 28 x 28 = 784 so that is the default)
      hidden_dimension: the initial hidden-layer dimension
    """
    def __init__(self, input_dimension: int=10, image_dimension: int=784,
                 hidden_dimension: int=128):
        super().__init__()

        self.generator = nn.Sequential(
            get_generator_block(input_dimension, hidden_dimension),
            get_generator_block(hidden_dimension, hidden_dimension * 2),
            get_generator_block(hidden_dimension * 2, hidden_dimension * 4),
            get_generator_block(hidden_dimension * 4, hidden_dimension * 8),
            nn.Linear(hidden_dimension * 8, image_dimension),
            nn.Sigmoid()
        )
        return

    def forward(self, noise: torch.Tensor) -> torch.Tensor:
        """
        Method for a forward pass of the generator

        Args:
         noise: a noise tensor with dimensions (n_samples, z_dim)

        Returns: 
         generated images.
        """
        return self.generator(noise)
#+end_src

*** Verify the Generator Class

#+begin_src python :results none
def test_generator(z_dim: int, im_dim: int, hidden_dim: int, 
                   num_test: int=10000) -> None:
    """Test the Generator Class

    Args:
     z_dim: the size of the input
     im_dim: the size of the image
     hidden_dim: the size of the initial hidden layer

    Raises:
     AssertionError: something is wrong
    """
    gen = Generator(z_dim, im_dim, hidden_dim).generator
    
    # Check there are six modules in the sequential part
    assert len(gen) == 6
    test_input = torch.randn(num_test, z_dim)
    test_output = gen(test_input)

    # Check that the output shape is correct
    assert tuple(test_output.shape) == (num_test, im_dim)

    # Chechk the output
    assert 0 < test_output.max() < 1, "Make sure to use a sigmoid"
    assert test_output.min() < 0.5, "Don't use a block in your solution"
    assert 0.15 > test_output.std() > 0.05, "Don't use batchnorm here"
    return

test_generator(5, 10, 20)
test_generator(20, 8, 24)
#+end_src

** Noise
To be able to use the generator, we will need to be able to create noise vectors. The noise vector =z= has the important role of making sure the images generated from the same class don't all look the same -- think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.

 Note that whenever you create a new tensor using torch.ones, torch.zeros, or [[https://pytorch.org/docs/master/generated/torch.randn.html][torch.randn]], you either need to create it on the target device, e.g. =torch.ones(3, 3, device=device)=, or move it onto the target device using =torch.ones(3, 3).to(device)=. You do not need to do this if you're creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as =torch.ones_like=. In general, use =torch.ones_like= and =torch.zeros_like= instead of =torch.ones= or =torch.zeros= where possible.

#+begin_src python :results none
def get_noise(n_samples: int, z_dim: int, device='cuda') -> torch.Tensor:
    """create noise vectors

    Args:
        n_samples: the number of samples to generate, a scalar
        z_dim: the dimension of the noise vector, a scalar
        device: the device type
    """
    return torch.randn(n_samples, z_dim, device=device)
#+end_src

*** Verify the noise vector function
#+begin_src python :results none
def test_get_noise(n_samples, z_dim, device='cpu'):
    noise = get_noise(n_samples, z_dim, device)
    
    # Make sure a normal distribution was used
    assert tuple(noise.shape) == (n_samples, z_dim)
    assert torch.abs(noise.std() - torch.tensor(1.0)) < 0.01
    assert str(noise.device).startswith(device)

test_get_noise(1000, 32)
#+end_src

** The Discriminator
The second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.

*Note: You use [[https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html][leaky ReLUs]] to prevent the "dying ReLU" problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient.* 

#+begin_src python :results none
def get_discriminator_block(input_dim: int, output_dim: int,
                            negative_slope: float=0.2) -> nn.Sequential:
    """Create the Discriminator block

    Args:
      input_dim: the dimension of the input vector, a scalar
      output_dim: the dimension of the output vector, a scalar
      negative_slope: angle for the negative slope

    Returns:
        a discriminator neural network layer, with a linear transformation 
          followed by an nn.LeakyReLU activation with negative slope of 0.2 
    """
    return nn.Sequential(
        nn.Linear(input_dim, output_dim),
        nn.LeakyReLU(negative_slope=0.2)
    )
#+end_src

*** Verify the discriminator block function

#+begin_src python :results none
def test_disc_block(in_features, out_features, num_test=10000):
    block = get_discriminator_block(in_features, out_features)

    # Check there are two parts
    assert len(block) == 2
    test_input = torch.randn(num_test, in_features)
    test_output = block(test_input)

    # Check that the shape is right
    assert tuple(test_output.shape) == (num_test, out_features)
    
    # Check that the LeakyReLU slope is about 0.2
    assert -test_output.min() / test_output.max() > 0.1
    assert -test_output.min() / test_output.max() < 0.3
    assert test_output.std() > 0.3
    assert test_output.std() < 0.5

test_disc_block(25, 12)
test_disc_block(15, 28)
#+end_src


*** The Discriminator Class
The discriminator class holds 2 values:

 - The image dimension
 - The hidden dimension

 The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator's neural network you are given a forward pass function that takes in an image tensor to be classified.

#+begin_src python :results none
class Discriminator(nn.Module):
    """The Discriminator Class

    Args:
        im_dim: the dimension of the images, fitted for the dataset used, a scalar
            (MNIST images are 28x28 = 784 so that is your default)
        hidden_dim: the inner dimension, a scalar
    """
    def __init__(self, im_dim: int=784, hidden_dim: int=128):
        super().__init__()
        self.disc = nn.Sequential(
            get_discriminator_block(im_dim, hidden_dim * 4),
            get_discriminator_block(hidden_dim * 4, hidden_dim * 2),
            get_discriminator_block(hidden_dim * 2, hidden_dim),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """forward pass of the discriminator

        Args:
            image: a flattened image tensor with dimension (im_dim)
        
        Returns a 1-dimension tensor representing fake/real.
        """
        return self.disc(image)
#+end_src

**** Verify the discriminator class
#+begin_src python :results none
def test_discriminator(z_dim, hidden_dim, num_test=100):
    
    disc = Discriminator(z_dim, hidden_dim).disc

    # Check there are three parts
    assert len(disc) == 4

    # Check the linear layer is correct
    test_input = torch.randn(num_test, z_dim)
    test_output = disc(test_input)
    assert tuple(test_output.shape) == (num_test, 1)
    
    # Don't use a block
    assert not isinstance(disc[-1], nn.Sequential)

test_discriminator(5, 10)
test_discriminator(20, 8)
#+end_src

** Training
First, you will set your parameters:
   -   criterion: the loss function ([[https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html?highlight=bcewithlogitsloss][BCEWithLogitsLoss]]
   -   n_epochs: the number of times you iterate through the entire dataset when training
   -   z_dim: the dimension of the noise vector
   -   display_step: how often to display/visualize the images
   -   batch_size: the number of images per forward/backward pass
   -   lr: the learning rate
   -   device: the device type, here using a GPU (which runs CUDA), not CPU

 Next, you will load the MNIST dataset as tensors using a dataloader.


*** Set your parameters
#+begin_src python :results none
criterion = nn.BCEWithLogitsLoss()
z_dim = 64
batch_size = 128
lr = 0.00001
#+end_src

*** Load MNIST dataset as tensors

#+begin_src python :results none
dataloader = DataLoader(
    MNIST('.', download=True, transform=transforms.ToTensor()),
    batch_size=batch_size,
    shuffle=True)
#+end_src

 Now, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.

#+begin_src python :results none
device = "cuda"
gen = Generator(z_dim).to(device)
gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
disc = Discriminator().to(device) 
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)
#+end_src

Before you train your GAN, you will need to create functions to calculate the discriminator's loss and the generator's loss. This is how the discriminator and generator will know how they are doing and improve themselves. Since the generator is needed when calculating the discriminator's loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!

 Remember that you have already defined a loss function earlier (=criterion=) and you are encouraged to use [[https://pytorch.org/docs/master/generated/torch.ones_like.html?highlight=ones_like#torch.ones_like][=torch.ones_like=]] and [[https://pytorch.org/docs/master/generated/torch.zeros_like.html?highlight=zeros_like#torch.zeros_like][=torch.zeros_like=]] instead of =torch.ones= or =torch.zeros=. If you use =torch.ones= or =torch.zeros=, you'll need to pass =device=device= to them.

#+begin_src python :results none
def get_disc_loss(gen: Generator, disc: Discriminator,
                  criterion: nn.BCEWithLogitsLoss,
                  real: torch.Tensor,
                  num_images: int, z_dim: int, 
                  device: str="cuda"):
    """
    Get the loss of the discriminator given inputs.

    Args:
        gen: the generator model, which returns an image given z-dimensional noise
        disc: the discriminator model, which returns a single-dimensional prediction of real/fake
        criterion: the loss function, which should be used to compare 
               the discriminator's predictions to the ground truth reality of the images 
               (e.g. fake = 0, real = 1)
        real: a batch of real images
        num_images: the number of images the generator should produce, 
                which is also the length of the real images
        z_dim: the dimension of the noise vector, a scalar
        device: the device type

    Returns:
        disc_loss: a torch scalar loss value for the current batch
    """
    noise = torch.randn(num_images, z_dim, device=device)
    fakes = gen(noise).detach()

    fake_prediction = disc(fakes)
    fake_loss = criterion(fake_prediction, torch.zeros_like(fake_prediction))

    real_prediction = disc(real)
    real_loss = criterion(real_prediction, torch.ones_like(real_prediction))
    disc_loss = (fake_loss + real_loss)/2
    return disc_loss
#+end_src

#+begin_src python :results none
def test_disc_reasonable(num_images=10):
    # Don't use explicit casts to cuda - use the device argument
    import inspect, re
    lines = inspect.getsource(get_disc_loss)
    assert (re.search(r"to\(.cuda.\)", lines)) is None
    assert (re.search(r"\.cuda\(\)", lines)) is None
    
    z_dim = 64
    gen = torch.zeros_like
    disc = lambda x: x.mean(1)[:, None]
    criterion = torch.mul # Multiply
    real = torch.ones(num_images, z_dim)
    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')
    assert torch.all(torch.abs(disc_loss.mean() - 0.5) < 1e-5)
    
    gen = torch.ones_like
    criterion = torch.mul # Multiply
    real = torch.zeros(num_images, z_dim)
    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu')) < 1e-5)
    
    gen = lambda x: torch.ones(num_images, 10)
    disc = lambda x: x.mean(1)[:, None] + 10
    criterion = torch.mul # Multiply
    real = torch.zeros(num_images, 10)
    assert torch.all(torch.abs(get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu').mean() - 5) < 1e-5)

    gen = torch.ones_like
    disc = nn.Linear(64, 1, bias=False)
    real = torch.ones(num_images, 64) * 0.5
    disc.weight.data = torch.ones_like(disc.weight.data) * 0.5
    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)
    criterion = lambda x, y: torch.sum(x) + torch.sum(y)
    disc_loss = get_disc_loss(gen, disc, criterion, real, num_images, z_dim, 'cpu').mean()
    disc_loss.backward()
    assert torch.isclose(torch.abs(disc.weight.grad.mean() - 11.25), torch.tensor(3.75))
    return

test_disc_reasonable()
#+end_src
    
#+begin_src python :results none
def test_disc_loss(max_tests = 10):
    z_dim = 64
    gen = Generator(z_dim).to(device)
    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
    disc = Discriminator().to(device) 
    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)
    num_steps = 0
    for real, _ in dataloader:
        cur_batch_size = len(real)
        real = real.view(cur_batch_size, -1).to(device)

        ### Update discriminator ###
        # Zero out the gradient before backpropagation
        disc_opt.zero_grad()

        # Calculate discriminator loss
        disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)
        assert (disc_loss - 0.68).abs() < 0.05, disc_loss

        # Update gradients
        disc_loss.backward(retain_graph=True)

        # Check that they detached correctly
        assert gen.gen[0][0].weight.grad is None

        # Update optimizer
        old_weight = disc.disc[0][0].weight.data.clone()
        disc_opt.step()
        new_weight = disc.disc[0][0].weight.data
        
        # Check that some discriminator weights changed
        assert not torch.all(torch.eq(old_weight, new_weight))
        num_steps += 1
        if num_steps >= max_tests:
            break

test_disc_loss()
#+end_src
*** Generator Loss
#+begin_src python :results none
def get_gen_loss(gen: Generator,
                 disc: Discriminator,
                 criterion: nn.BCEWithLogitsLoss,
                 num_images: int,
                 z_dim: int, device: str="cuda") -> torch.Tensor:
    """Calculates the loss for the generator

    Args:
        gen: the generator model, which returns an image given z-dimensional noise
        disc: the discriminator model, which returns a single-dimensional prediction of real/fake
        criterion: the loss function, which should be used to compare 
               the discriminator's predictions to the ground truth reality of the images 
               (e.g. fake = 0, real = 1)
        num_images: the number of images the generator should produce, 
                which is also the length of the real images
        z_dim: the dimension of the noise vector, a scalar
        device: the device type
    Returns:
        gen_loss: a torch scalar loss value for the current batch
    """
    noise = torch.randn(num_images, z_dim, device=device)
    fakes = gen(noise)
    fake_prediction = disc(fakes)
    gen_loss = criterion(fake_prediction, torch.ones_like(fake_prediction))
    return gen_loss
#+end_src

#+begin_src python :results none
def test_gen_reasonable(num_images=10):
    # Don't use explicit casts to cuda - use the device argument
    import inspect, re
    lines = inspect.getsource(get_gen_loss)
    assert (re.search(r"to\(.cuda.\)", lines)) is None
    assert (re.search(r"\.cuda\(\)", lines)) is None
    
    z_dim = 64
    gen = torch.zeros_like
    disc = nn.Identity()
    criterion = torch.mul # Multiply
    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')
    assert torch.all(torch.abs(gen_loss_tensor) < 1e-5)
    #Verify shape. Related to gen_noise parametrization
    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)

    gen = torch.ones_like
    disc = nn.Identity()
    criterion = torch.mul # Multiply
    real = torch.zeros(num_images, 1)
    gen_loss_tensor = get_gen_loss(gen, disc, criterion, num_images, z_dim, 'cpu')
    assert torch.all(torch.abs(gen_loss_tensor - 1) < 1e-5)
    #Verify shape. Related to gen_noise parametrization
    assert tuple(gen_loss_tensor.shape) == (num_images, z_dim)
    return
test_gen_reasonable(10)
#+end_src

#+begin_src python :results none
def test_gen_loss(num_images):
    z_dim = 64
    gen = Generator(z_dim).to(device)
    gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
    disc = Discriminator().to(device) 
    disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)
    
    gen_loss = get_gen_loss(gen, disc, criterion, num_images, z_dim, device)
    
    # Check that the loss is reasonable
    assert (gen_loss - 0.7).abs() < 0.1
    gen_loss.backward()
    old_weight = gen.gen[0][0].weight.clone()
    gen_opt.step()
    new_weight = gen.gen[0][0].weight
    assert not torch.all(torch.eq(old_weight, new_weight))
test_gen_loss(18)
#+end_src
*** All Together
For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess. 
 
 It’s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It's important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.

 After you've submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.

#+begin_src python :results output :exports both
cur_step = 0
mean_generator_loss = 0
mean_discriminator_loss = 0
test_generator = True # Whether the generator should be tested
gen_loss = False
error = False
n_epochs = 2000
display_step = 4100
generator_losses = []
discriminator_losses = []
steps = []

with TIMER:
    for epoch in range(n_epochs):
      
        # Dataloader returns the batches
        for real, _ in dataloader:
            cur_batch_size = len(real)
    
            # Flatten the batch of real images from the dataset
            real = real.view(cur_batch_size, -1).to(device)
    
            ### Update discriminator ###
            # Zero out the gradients before backpropagation
            disc_opt.zero_grad()
    
            # Calculate discriminator loss
            disc_loss = get_disc_loss(gen, disc, criterion, real, cur_batch_size, z_dim, device)
    
            # Update gradients
            disc_loss.backward(retain_graph=True)
    
            # Update optimizer
            disc_opt.step()
    
            # For testing purposes, to keep track of the generator weights
            if test_generator:
                old_generator_weights = gen.gen[0][0].weight.detach().clone()
    
            ### Update generator ###
            gen_opt.zero_grad()
            gen_loss = get_gen_loss(gen, disc, criterion, cur_batch_size, z_dim, device)
            gen_loss.backward(retain_graph=True)
            gen_opt.step()

            # For testing purposes, to check that your code changes the generator weights
            if test_generator:
                try:
                    assert lr > 0.0000002 or (gen.gen[0][0].weight.grad.abs().max() < 0.0005 and epoch == 0)
                    assert torch.any(gen.gen[0][0].weight.detach().clone() != old_generator_weights)
                except:
                    error = True
                    print("Runtime tests have failed")
    
            # Keep track of the average discriminator loss
            mean_discriminator_loss += disc_loss.item() / display_step
    
            # Keep track of the average generator loss
            mean_generator_loss += gen_loss.item() / display_step
    
            if cur_step % display_step == 0 and cur_step > 0:
                print(f"Epoch {epoch}, step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}")
                steps.append(cur_step)
                generator_losses.append(mean_generator_loss)
                discriminator_losses.append(mean_discriminator_loss)
            cur_step += 1
#+end_src

#+RESULTS:
#+begin_example
Started: 2021-04-08 19:11:09.461117
Epoch 5, step 2500: Generator loss: 1.706548154211052, discriminator loss: 0.2566282903790473
Epoch 10, step 5000: Generator loss: 4.417493268251426, discriminator loss: 0.37590573319792847
Epoch 15, step 7500: Generator loss: 8.217398338270204, discriminator loss: 0.4420946755893531
Epoch 21, step 10000: Generator loss: 12.379702310991277, discriminator loss: 0.49946175085604244
Epoch 26, step 12500: Generator loss: 16.363392679834355, discriminator loss: 0.577481897354875
Epoch 31, step 15000: Generator loss: 20.321313246965392, discriminator loss: 0.6705450104258994
Epoch 37, step 17500: Generator loss: 23.881395485830232, discriminator loss: 0.7909670361138917
Epoch 42, step 20000: Generator loss: 27.36178849205961, discriminator loss: 0.9245524749659035
Epoch 47, step 22500: Generator loss: 30.756254529428357, discriminator loss: 1.0683966985411961
Epoch 53, step 25000: Generator loss: 33.873566954183424, discriminator loss: 1.2374154507704083
Epoch 58, step 27500: Generator loss: 36.76653855376236, discriminator loss: 1.4335610504932743
Epoch 63, step 30000: Generator loss: 39.610195555067065, discriminator loss: 1.6299822613395802
Epoch 69, step 32500: Generator loss: 42.27110341444029, discriminator loss: 1.8545818868704136
Epoch 74, step 35000: Generator loss: 44.86730858569149, discriminator loss: 2.081926002264768
Epoch 79, step 37500: Generator loss: 47.34035383772865, discriminator loss: 2.3272732418782995
Epoch 85, step 40000: Generator loss: 49.69807465667742, discriminator loss: 2.5900223485894514
Epoch 90, step 42500: Generator loss: 51.95912191028614, discriminator loss: 2.856632189888516
Epoch 95, step 45000: Generator loss: 54.13774062051793, discriminator loss: 3.1388706683166423
Epoch 101, step 47500: Generator loss: 56.25892917881031, discriminator loss: 3.435482066709555
Epoch 106, step 50000: Generator loss: 58.2940666561604, discriminator loss: 3.742299897164866
Epoch 111, step 52500: Generator loss: 60.34588112130169, discriminator loss: 4.039923962772651
Epoch 117, step 55000: Generator loss: 62.3250578921796, discriminator loss: 4.3601536815710755
Epoch 122, step 57500: Generator loss: 64.21707550911917, discriminator loss: 4.693669865030843
Epoch 127, step 60000: Generator loss: 66.14931350994115, discriminator loss: 5.012754998887372
Epoch 133, step 62500: Generator loss: 68.01088003492343, discriminator loss: 5.350926510263262
Epoch 138, step 65000: Generator loss: 69.7833545449736, discriminator loss: 5.705678011608883
Epoch 143, step 67500: Generator loss: 71.56750503945366, discriminator loss: 6.058190715546184
Epoch 149, step 70000: Generator loss: 73.28055478563336, discriminator loss: 6.422111075831223
Epoch 154, step 72500: Generator loss: 74.93712217669513, discriminator loss: 6.801459926683468
Epoch 159, step 75000: Generator loss: 76.57140328321462, discriminator loss: 7.186894027460396
Epoch 165, step 77500: Generator loss: 78.11976942777646, discriminator loss: 7.59347033443528
Epoch 170, step 80000: Generator loss: 79.70259762425445, discriminator loss: 7.990671021056967
Epoch 175, step 82500: Generator loss: 81.29402320809406, discriminator loss: 8.38323437005357
Epoch 181, step 85000: Generator loss: 82.81459570746449, discriminator loss: 8.79464628630952
Epoch 186, step 87500: Generator loss: 84.37445686025625, discriminator loss: 9.187008984566525
Epoch 191, step 90000: Generator loss: 85.85529090266233, discriminator loss: 9.62971806451164
Epoch 197, step 92500: Generator loss: 87.28264569795147, discriminator loss: 10.0601266036578
Epoch 202, step 95000: Generator loss: 88.77136517236256, discriminator loss: 10.470760706252658
Epoch 207, step 97500: Generator loss: 90.20359932258185, discriminator loss: 10.896903645534154
Epoch 213, step 100000: Generator loss: 91.64949153683249, discriminator loss: 11.314317919439938
Epoch 218, step 102500: Generator loss: 93.04353729379224, discriminator loss: 11.754701970989366
Epoch 223, step 105000: Generator loss: 94.48434179880661, discriminator loss: 12.17579472559178
Epoch 229, step 107500: Generator loss: 95.90043624894685, discriminator loss: 12.609691903144922
Epoch 234, step 110000: Generator loss: 97.22790038921927, discriminator loss: 13.06728125928124
Epoch 239, step 112500: Generator loss: 98.54396488256565, discriminator loss: 13.530596924526252
Epoch 245, step 115000: Generator loss: 99.77238301303473, discriminator loss: 14.021221584183708
Epoch 250, step 117500: Generator loss: 100.95588274421799, discriminator loss: 14.52276291984987
Epoch 255, step 120000: Generator loss: 102.16950242385977, discriminator loss: 15.009217036432748
Epoch 261, step 122500: Generator loss: 103.34768993141779, discriminator loss: 15.510240219909676
Epoch 266, step 125000: Generator loss: 104.49850498325966, discriminator loss: 16.02100355718803
Epoch 271, step 127500: Generator loss: 105.63248440983429, discriminator loss: 16.529967280096464
Epoch 277, step 130000: Generator loss: 106.77215565025928, discriminator loss: 17.040578575879902
Epoch 282, step 132500: Generator loss: 107.84948109347918, discriminator loss: 17.577629218131907
Epoch 287, step 135000: Generator loss: 108.89120032596693, discriminator loss: 18.1230313348835
Epoch 293, step 137500: Generator loss: 109.94543989174456, discriminator loss: 18.66669545603442
Epoch 298, step 140000: Generator loss: 111.05784844493985, discriminator loss: 19.186080698353518
Epoch 303, step 142500: Generator loss: 112.11954127087729, discriminator loss: 19.72143556161576
Epoch 309, step 145000: Generator loss: 113.1505551135316, discriminator loss: 20.266378742129064
Epoch 314, step 147500: Generator loss: 114.14504244511286, discriminator loss: 20.82540130450163
Epoch 319, step 150000: Generator loss: 115.14619642219694, discriminator loss: 21.377254920214682
Epoch 325, step 152500: Generator loss: 116.18683508672831, discriminator loss: 21.91325990107692
Epoch 330, step 155000: Generator loss: 117.20365596852427, discriminator loss: 22.464628956920198
Epoch 335, step 157500: Generator loss: 118.18476380837096, discriminator loss: 23.02610586987158
Epoch 341, step 160000: Generator loss: 119.1733443738712, discriminator loss: 23.57863494028462
Epoch 346, step 162500: Generator loss: 120.0683158794178, discriminator loss: 24.17668376757525
Epoch 351, step 165000: Generator loss: 121.00822785911677, discriminator loss: 24.759683359057014
Epoch 357, step 167500: Generator loss: 122.01716691696792, discriminator loss: 25.306333132869433
Epoch 362, step 170000: Generator loss: 122.95615759954634, discriminator loss: 25.882448339409144
Epoch 367, step 172500: Generator loss: 123.8601865704076, discriminator loss: 26.472507165831818
Epoch 373, step 175000: Generator loss: 124.77679342136544, discriminator loss: 27.06009588730988
Epoch 378, step 177500: Generator loss: 125.68742787552021, discriminator loss: 27.650220775634565
Epoch 383, step 180000: Generator loss: 126.58067360401337, discriminator loss: 28.24680029824429
Epoch 389, step 182500: Generator loss: 127.47687033252896, discriminator loss: 28.841687268430586
Epoch 394, step 185000: Generator loss: 128.33945010419103, discriminator loss: 29.470305139536162
Epoch 399, step 187500: Generator loss: 129.2645899116776, discriminator loss: 30.056467109007148
Epoch 405, step 190000: Generator loss: 130.17483343897044, discriminator loss: 30.65010625776692
Epoch 410, step 192500: Generator loss: 131.07306051247323, discriminator loss: 31.24635483381194
Epoch 415, step 195000: Generator loss: 131.98270811326725, discriminator loss: 31.83840948063775
Epoch 421, step 197500: Generator loss: 132.89332210309777, discriminator loss: 32.427245197707826
Epoch 426, step 200000: Generator loss: 133.85536700406317, discriminator loss: 32.99699493227643
Epoch 431, step 202500: Generator loss: 134.78184310503255, discriminator loss: 33.582932585127054
Epoch 437, step 205000: Generator loss: 135.67147595069721, discriminator loss: 34.18765857823589
Epoch 442, step 207500: Generator loss: 136.59887173002065, discriminator loss: 34.770955285043314
Epoch 447, step 210000: Generator loss: 137.53319753001017, discriminator loss: 35.35211720700919
Epoch 453, step 212500: Generator loss: 138.4486942873986, discriminator loss: 35.94530614820166
Epoch 458, step 215000: Generator loss: 139.33902888264944, discriminator loss: 36.54999590321232
Epoch 463, step 217500: Generator loss: 140.27206793022475, discriminator loss: 37.13930196701909
Epoch 469, step 220000: Generator loss: 140.99687212999171, discriminator loss: 37.93871315395262
Epoch 474, step 222500: Generator loss: 141.7673044035706, discriminator loss: 38.59500126797581
Epoch 479, step 225000: Generator loss: 142.5946069137365, discriminator loss: 39.21644382466705
Epoch 485, step 227500: Generator loss: 143.47817903824173, discriminator loss: 39.82380570806908
Epoch 490, step 230000: Generator loss: 144.42988614442692, discriminator loss: 40.41459694806965
Epoch 495, step 232500: Generator loss: 145.41308410630532, discriminator loss: 40.99819621782919
Epoch 501, step 235000: Generator loss: 146.35154331310105, discriminator loss: 41.60116203337314
Epoch 506, step 237500: Generator loss: 147.3414293385067, discriminator loss: 42.182913084965904
Epoch 511, step 240000: Generator loss: 148.16208219452346, discriminator loss: 42.84799684844597
Epoch 517, step 242500: Generator loss: 149.01690332217666, discriminator loss: 43.4645494998036
Epoch 522, step 245000: Generator loss: 149.90361780090743, discriminator loss: 44.07395624421216
Epoch 527, step 247500: Generator loss: 150.81761934249764, discriminator loss: 44.67337528136382
Epoch 533, step 250000: Generator loss: 151.760788434009, discriminator loss: 45.27149211621905
Epoch 538, step 252500: Generator loss: 152.72171067755622, discriminator loss: 45.862570312196205
Epoch 543, step 255000: Generator loss: 153.70058994908774, discriminator loss: 46.446673588067306
Epoch 549, step 257500: Generator loss: 154.6540338594482, discriminator loss: 47.04409442761572
Epoch 554, step 260000: Generator loss: 155.61953176954265, discriminator loss: 47.64129806395185
Epoch 559, step 262500: Generator loss: 156.57851000073427, discriminator loss: 48.23621501955407
Epoch 565, step 265000: Generator loss: 157.57013796937912, discriminator loss: 48.81841204716583
Epoch 570, step 267500: Generator loss: 158.58745419824487, discriminator loss: 49.39728153505935
Epoch 575, step 270000: Generator loss: 159.61897925506034, discriminator loss: 49.97088020893944
Epoch 581, step 272500: Generator loss: 160.62414082653933, discriminator loss: 50.55845826935191
Epoch 586, step 275000: Generator loss: 161.6360176999136, discriminator loss: 51.13836716422452
Epoch 591, step 277500: Generator loss: 162.6937001745749, discriminator loss: 51.703352506631745
Epoch 597, step 280000: Generator loss: 163.713712522297, discriminator loss: 52.28744219620823
Epoch 602, step 282500: Generator loss: 164.74417761338188, discriminator loss: 52.86771041123264
Epoch 607, step 285000: Generator loss: 165.76970245681252, discriminator loss: 53.44563473485114
Epoch 613, step 287500: Generator loss: 166.811878925066, discriminator loss: 54.01528675569889
Epoch 618, step 290000: Generator loss: 167.8307581976232, discriminator loss: 54.590264578408025
Epoch 623, step 292500: Generator loss: 168.891059790879, discriminator loss: 55.156935418433086
Epoch 628, step 295000: Generator loss: 169.9474142856893, discriminator loss: 55.71990455088043
Epoch 634, step 297500: Generator loss: 171.0481772922807, discriminator loss: 56.27077199867391
Epoch 639, step 300000: Generator loss: 172.11923127556335, discriminator loss: 56.83505055757194
Epoch 644, step 302500: Generator loss: 173.16847663276675, discriminator loss: 57.40616466661116
Epoch 650, step 305000: Generator loss: 174.07904141102364, discriminator loss: 58.13510520734215
Epoch 655, step 307500: Generator loss: 174.9117115099017, discriminator loss: 58.74583771123328
Epoch 660, step 310000: Generator loss: 175.81728609905886, discriminator loss: 59.34154992217456
Epoch 666, step 312500: Generator loss: 176.7578212393589, discriminator loss: 59.92914648076939
Epoch 671, step 315000: Generator loss: 177.6959055232357, discriminator loss: 60.52539835767199
Epoch 676, step 317500: Generator loss: 178.65148061598035, discriminator loss: 61.114570335239605
Epoch 682, step 320000: Generator loss: 179.61467326215066, discriminator loss: 61.701995908111826
Epoch 687, step 322500: Generator loss: 180.57620892596987, discriminator loss: 62.314891455287345
Epoch 692, step 325000: Generator loss: 181.45410679765476, discriminator loss: 62.936326666588116
Epoch 698, step 327500: Generator loss: 182.3509983194426, discriminator loss: 63.536605342692496
Epoch 703, step 330000: Generator loss: 183.2767872462343, discriminator loss: 64.13359040188224
Epoch 708, step 332500: Generator loss: 184.24614562447758, discriminator loss: 64.71242399436842
Epoch 714, step 335000: Generator loss: 185.2339295223548, discriminator loss: 65.29031219341155
Epoch 719, step 337500: Generator loss: 186.22102292967574, discriminator loss: 65.87176483958355
Epoch 724, step 340000: Generator loss: 187.16576725860415, discriminator loss: 66.46805416325945
Epoch 730, step 342500: Generator loss: 188.17708793676715, discriminator loss: 67.02993150789138
Epoch 735, step 345000: Generator loss: 189.2133280149301, discriminator loss: 67.59322807443746
Epoch 740, step 347500: Generator loss: 190.23208963008815, discriminator loss: 68.16751613625887
Epoch 746, step 350000: Generator loss: 191.25920752848035, discriminator loss: 68.7402887957158
Epoch 751, step 352500: Generator loss: 192.28080943237188, discriminator loss: 69.31388918965479
Epoch 756, step 355000: Generator loss: 193.3113937983357, discriminator loss: 69.88720360422755
Epoch 762, step 357500: Generator loss: 194.36098039241693, discriminator loss: 70.46414442196532
Epoch 767, step 360000: Generator loss: 195.2757543816169, discriminator loss: 71.13062453675919
Epoch 772, step 362500: Generator loss: 196.20393474234035, discriminator loss: 71.72337642310346
Epoch 778, step 365000: Generator loss: 197.17895898321507, discriminator loss: 72.2954984176345
Epoch 783, step 367500: Generator loss: 198.16447116800182, discriminator loss: 72.87443887000731
Epoch 788, step 370000: Generator loss: 199.0904656322084, discriminator loss: 73.4706785914252
Epoch 794, step 372500: Generator loss: 200.0776066501466, discriminator loss: 74.04545851565092
Epoch 799, step 375000: Generator loss: 201.02695143798252, discriminator loss: 74.6362730719872
Epoch 804, step 377500: Generator loss: 201.990468873819, discriminator loss: 75.22039345236489
Epoch 810, step 380000: Generator loss: 203.01058207302907, discriminator loss: 75.78494052414334
Epoch 815, step 382500: Generator loss: 204.05225880001382, discriminator loss: 76.35463489610616
Epoch 820, step 385000: Generator loss: 205.01460667336755, discriminator loss: 76.94180617091138
Epoch 826, step 387500: Generator loss: 206.05344676898304, discriminator loss: 77.51117122195475
Epoch 831, step 390000: Generator loss: 207.07375686963422, discriminator loss: 78.09562644241576
Epoch 836, step 392500: Generator loss: 208.0923817031217, discriminator loss: 78.6823844633292
Epoch 842, step 395000: Generator loss: 209.13077087057394, discriminator loss: 79.25322038175547
Epoch 847, step 397500: Generator loss: 210.17313802006993, discriminator loss: 79.82443133078309
Epoch 852, step 400000: Generator loss: 211.24034579869058, discriminator loss: 80.39010535690288
Epoch 858, step 402500: Generator loss: 212.29597073660645, discriminator loss: 80.95657187560273
Epoch 863, step 405000: Generator loss: 213.36997850652477, discriminator loss: 81.5198167693678
Epoch 868, step 407500: Generator loss: 214.34861638153268, discriminator loss: 82.11812956745074
Epoch 874, step 410000: Generator loss: 215.34710292697525, discriminator loss: 82.68937682878448
Epoch 879, step 412500: Generator loss: 216.41345989072963, discriminator loss: 83.24355317315504
Epoch 884, step 415000: Generator loss: 217.28458412296192, discriminator loss: 83.92686493608393
Epoch 890, step 417500: Generator loss: 218.29466441413823, discriminator loss: 84.48695280879178
Epoch 895, step 420000: Generator loss: 219.3093764458604, discriminator loss: 85.05175313127683
Epoch 900, step 422500: Generator loss: 220.34153808440644, discriminator loss: 85.61656451819609
Epoch 906, step 425000: Generator loss: 221.36335393692892, discriminator loss: 86.20032007145346
Epoch 911, step 427500: Generator loss: 222.4004497556632, discriminator loss: 86.77878700938892
Epoch 916, step 430000: Generator loss: 223.45292786750198, discriminator loss: 87.35415507568788
Epoch 922, step 432500: Generator loss: 224.49835159925806, discriminator loss: 87.92856502636128
Epoch 927, step 435000: Generator loss: 225.5315329488691, discriminator loss: 88.51101210754575
Epoch 932, step 437500: Generator loss: 226.5669734054742, discriminator loss: 89.09301339096481
Epoch 938, step 440000: Generator loss: 227.48296755303724, discriminator loss: 89.70019514662671
Epoch 943, step 442500: Generator loss: 228.48196144422903, discriminator loss: 90.27654089993891
Epoch 948, step 445000: Generator loss: 229.51780844437448, discriminator loss: 90.83335400045588
Epoch 954, step 447500: Generator loss: 230.60036065892555, discriminator loss: 91.38414017357304
Epoch 959, step 450000: Generator loss: 231.67713544449177, discriminator loss: 91.94271953728851
Epoch 964, step 452500: Generator loss: 232.64530618559718, discriminator loss: 92.56010472605873
Epoch 970, step 455000: Generator loss: 233.68903298704024, discriminator loss: 93.11562871940767
Epoch 975, step 457500: Generator loss: 234.72853977193282, discriminator loss: 93.68664950763602
Epoch 980, step 460000: Generator loss: 235.69478953835426, discriminator loss: 94.26967884075088
Epoch 986, step 462500: Generator loss: 236.70701852056465, discriminator loss: 94.835702462167
Epoch 991, step 465000: Generator loss: 237.76209552497326, discriminator loss: 95.39685135828884
Epoch 996, step 467500: Generator loss: 238.79892911128448, discriminator loss: 95.97513031529836
Ended: 2021-04-08 20:55:49.865623
Elapsed: 1:44:40.404506
#+end_example
** Looking at the Final model.
#+begin_src python :results none
def plot_image(image: torch.Tensor,
                filename: str,
                title: str,
                num_images: int=25,
                size: tuple=(1, 28, 28),
                folder: str="files/posts/gans/mnist-gan/") -> None:
    """Plot the image and save it

    Args:
     image: the tensor with the image to plot
     filename: name for the final image file
     title: title to put on top of the image
     num_images: how many images to put in the composite image
     size: the size for the image
     folder: sub-folder to save the file in
    """
    unflattened_image = image.detach().cpu().view(-1, *size)
    image_grid = make_grid(unflattened_image[:num_images], nrow=5)

    pyplot.title(title)
    pyplot.grid(False)
    pyplot.imshow(image_grid.permute(1, 2, 0).squeeze())

    pyplot.tick_params(bottom=False, top=False, labelbottom=False,
                       right=False, left=False, labelleft=False)
    pyplot.savefig(folder + filename)
    print(f"[[file:{filename}]]")
    return
#+end_src
#+begin_src python :results output :exports both
fake_noise = get_noise(cur_batch_size, z_dim, device=device)
fake = gen(fake_noise)
plot_image(image=fake, filename="fake_digits.png", title="Fake Digits")
#+end_src

 [[file:fake_digits.png]]


#+begin_src python :results output :exports both
plot_image(real, filename="real_digits.png", title="Real Digits")
#+end_src

[[file:real_digits.png]]

#+begin_src python :results none
plotting = pandas.DataFrame.from_dict({
    "Step": steps,
    "Generator Loss": generator_losses,
    "Discriminator Loss": discriminator_losses
})

gen_plot = plotting.hvplot(x="Step", y="Generator Loss", color=PLOT.blue)
disc_plot = plotting.hvplot(x="Step", y="Discriminator Loss", color=PLOT.red)

plot = (gen_plot * disc_plot).opts(title="Training Losses",
                                   height=PLOT.height,
                                   width=PLOT.width,
                                   ylabel="Loss",
                                   fontscale=PLOT.fontscale)
output = Embed(plot=plot, file_name="losses")()
#+end_src

#+begin_src python :results output html :exports output
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="losses.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

I thought something was wrong with the losses, at first, since they seem to go up over time, but the loss is based on the Generator and the Discriminator being able to do their job, so as they get better, the loss goes up. The main one for us to note is the Discriminator loss, since this is how much it gets fooled by the Generator. Since it's still going up this likely means that the Generator can still improve.

* End

