<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="The Triplet Loss for the Siamese Network." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Siamese Networks: Hard Negative Mining | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/siamese-networks-hard-negative-mining/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../siamese-networks-defining-the-model/" rel="prev" title="Siamese Networks: Defining the Model" type="text/html">
<link href="../siamese-networks-training-the-model/" rel="next" title="Siamese Networks: Training the Model" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Siamese Networks: Hard Negative Mining" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/siamese-networks-hard-negative-mining/" property="og:url">
<meta content="The Triplet Loss for the Siamese Network." property="og:description">
<meta content="article" property="og:type">
<meta content="2021-01-25T19:37:28-08:00" property="article:published_time">
<meta content="nlp" property="article:tag">
<meta content="siamese networks" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Siamese Networks: Hard Negative Mining</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2021-01-25T19:37:28-08:00" itemprop="datePublished" title="2021-01-25 19:37">2021-01-25 19:37</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgc9d5976">Hard Negative Mining</a>
<ul>
<li><a href="#org3888bfa">Imports</a></li>
</ul>
</li>
<li><a href="#org2420079">Implementation</a>
<ul>
<li><a href="#org3de16f3">More Detailed Instructions</a>
<ul>
<li><a href="#org580bf9a">Cosine Similarity</a></li>
<li><a href="#org9227c80">Closest Negative</a></li>
<li><a href="#orgefefc9b">Mean Negative</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orga20ed25">Bundle It Up</a>
<ul>
<li><a href="#org7871263">Imports</a></li>
<li><a href="#orgf5a7499">Triplet Loss</a></li>
<li><a href="#org60382a0">Triplet Loss Layer</a></li>
<li><a href="#orga1a80f5">Check It Out</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgc9d5976">
<h2 id="orgc9d5976">Hard Negative Mining</h2>
<div class="outline-text-2" id="text-orgc9d5976">
<p>Now we will now implement the <code>TripletLoss</code>. Loss is composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the <b>closest negative</b>. Our loss expression is then:</p>
\begin{align} \mathcal{Loss_1(A,P,N)} &amp;=\max \left( -cos(A,P) + mean_{neg} +\alpha, 0\right) \\ \mathcal{Loss_2(A,P,N)} &amp;=\max \left( -cos(A,P) + closest_{neg} +\alpha, 0\right) \\ \mathcal{Loss(A,P,N)} &amp;= mean(Loss_1 + Loss_2) \\ \end{align}
<p>Here is a list of things we have to do:</p>
<ul class="org-ul">
<li>As this will be run inside trax, use <code>fastnp.xyz</code> when using any <code>xyz</code> numpy function</li>
<li>Use <code>fastnp.dot</code> to calculate the similarity matrix \(v_1v_2^T\) of dimension <code>batch_size</code> x <code>batch_size</code></li>
<li>Take the score of the duplicates on the diagonal <code>fastnp.diagonal</code></li>
<li>Use the <code>trax</code> functions <code>fastnp.eye</code> and <code>fastnp.maximum</code> for the identity matrix and the maximum.</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org3888bfa">
<h3 id="org3888bfa">Imports</h3>
<div class="outline-text-3" id="text-org3888bfa">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">trax.fastmath</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">fastnp</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org2420079">
<h2 id="org2420079">Implementation</h2>
<div class="outline-text-2" id="text-org2420079"></div>
<div class="outline-3" id="outline-container-org3de16f3">
<h3 id="org3de16f3">More Detailed Instructions</h3>
<div class="outline-text-3" id="text-org3de16f3">
<p>We'll describe the algorithm using a detailed example. Below, V1, V2 are the output of the normalization blocks in our model. Here we will use a batch_size of 4 and a d_model of 3. The inputs, Q1, Q2 are arranged so that corresponding inputs are duplicates while non-corresponding entries are not. The outputs will have the same pattern.</p>
<p>This testcase arranges the outputs, v1,v2, to highlight different scenarios. Here, the first two outputs V1[0], V2[0] match exactly - so the model is generating the same vector for Q1[0] and Q2[0] inputs. The second outputs differ, circled in orange, we set, V2[1] is set to match V2[**2**], simulating a model which is generating very poor results. V1[3] and V2[3] match exactly again while V1[4] and V2[4] are set to be exactly wrong - 180 degrees from each other, circled in blue.</p>
</div>
<div class="outline-4" id="outline-container-org580bf9a">
<h4 id="org580bf9a">Cosine Similarity</h4>
<div class="outline-text-4" id="text-org580bf9a">
<p>The first step is to compute the cosine similarity matrix or <code>score</code> in the code. This is \(V_1 V_2^T\) which is generated with <code>fastnp.dot</code>.</p>
<p>The clever arrangement of inputs creates the data needed for positive <b>and</b> negative examples without having to run all pair-wise combinations. Because Q1[n] is a duplicate of only Q2[n], other combinations are explicitly created negative examples or <b>Hard Negative</b> examples. The matrix multiplication efficiently produces the cosine similarity of all positive/negative combinations as shown above on the left side of the diagram. 'Positive' are the results of duplicate examples and 'negative' are the results of explicitly created negative examples. The results for our test case are as expected, V1[0]V2[0] match producing '1' while our other 'positive' cases (in green) don't match well, as was arranged. The V2[2] was set to match V1[3] producing a poor match at <code>score[2,2]</code> and an undesired 'negative' case of a '1' shown in grey.</p>
<p>With the similarity matrix (<code>score</code>) we can begin to implement the loss equations. First, we can extract \(\cos(A,P)\) by utilizing <code>fastnp.diagonal</code>. The goal is to grab all the green entries in the diagram above. This is <code>positive</code> in the code.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org9227c80">
<h4 id="org9227c80">Closest Negative</h4>
<div class="outline-text-4" id="text-org9227c80">
<p>Next, we will create the <b>closest_negative</b>. This is the nonduplicate entry in V2 that is closest (has largest cosine similarity) to an entry in V1. Each row, n, of <code>score</code> represents all comparisons of the results of Q1[n] vs Q2[x] within a batch. A specific example in our testcase is row <code>score[2,:]</code>. It has the cosine similarity of V1[2] and V2[x]. The <b>closest_negative</b>, as was arranged, is V2[2] which has a score of 1. This is the maximum value of the 'negative' entries (blue entries in the diagram).</p>
<p>To implement this, we need to pick the maximum entry on a row of <code>score</code>, ignoring the 'positive'/green entries. To avoid selecting the 'positive'/green entries, we can make them larger negative numbers. Multiply <code>fastnp.eye(batch_size)</code> with 2.0 and subtract it out of <code>scores</code>. The result is <code>negative_without_positive</code>. Now we can use <code>fastnp.max</code>, row by row (axis=1), to select the maximum which is <code>closest_negative</code>.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgefefc9b">
<h4 id="orgefefc9b">Mean Negative</h4>
<div class="outline-text-4" id="text-orgefefc9b">
<p>Next, we'll create <b>mean_negative</b>. As the name suggests, this is the mean of all the 'negative'/blue values in <code>score</code> on a row by row basis. We can use <code>fastnp.eye(batch_size)</code> and a constant, this time to create a mask with zeros on the diagonal. Element-wise multiply this with <code>score</code> to get just the 'negative values. This is <code>negative_zero_on_duplicate</code> in the code. Compute the mean by using <code>fastnp.sum</code> on <code>negative_zero_on_duplicate</code> for <code>axis=1</code> and divide it by <code>(batch_size - 1)</code> . This is <code>mean_negative</code>.</p>
<p>Now, we can compute loss using the two equations above and <code>fastnp.maximum</code>. This will form <code>triplet_loss1</code> and <code>triplet_loss2</code>.</p>
<p><code>triple_loss</code> is the <code>fastnp.mean</code> of the sum of the two individual losses.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">TripletLossFn</span><span class="p">(</span><span class="n">v1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">v2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                  <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">interpreters</span><span class="o">.</span><span class="n">xla</span><span class="o">.</span><span class="n">DeviceArray</span><span class="p">:</span>
    <span class="sd">"""Custom Loss function.</span>

<span class="sd">    Args:</span>
<span class="sd">       v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.</span>
<span class="sd">       v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.</span>
<span class="sd">       margin (float, optional): Desired margin. Defaults to 0.25.</span>

<span class="sd">    Returns:</span>
<span class="sd">       jax.interpreters.xla.DeviceArray: Triplet Loss.</span>
<span class="sd">    """</span>
    <span class="c1"># use fastnp to take the dot product of the two batches (don't forget to transpose the second argument)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">fastnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="c1"># calculate new batch size</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="c1"># use fastnp to grab all postive =diagonal= entries in =scores=</span>
    <span class="n">positive</span> <span class="o">=</span> <span class="n">fastnp</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>  <span class="c1"># the positive ones (duplicates)</span>
    <span class="c1"># multiply =fastnp.eye(batch_size)= with 2.0 and subtract it out of =scores=</span>
    <span class="n">negative_without_positive</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">-</span> <span class="p">(</span><span class="n">fastnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="c1"># take the row by row =max= of =negative_without_positive=. </span>
    <span class="c1"># Hint: negative_without_positive.max(axis = [?])  </span>
    <span class="n">closest_negative</span> <span class="o">=</span> <span class="n">fastnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">negative_without_positive</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># subtract =fastnp.eye(batch_size)= out of 1.0 and do element-wise multiplication with =scores=</span>
    <span class="n">negative_zero_on_duplicate</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">fastnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span> <span class="o">*</span> <span class="n">scores</span>
    <span class="c1"># use =fastnp.sum= on =negative_zero_on_duplicate= for =axis=1= and divide it by =(batch_size - 1)= </span>
    <span class="n">mean_negative</span> <span class="o">=</span> <span class="n">fastnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">negative_zero_on_duplicate</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># compute =fastnp.maximum= among 0.0 and =A=</span>
    <span class="c1"># A = subtract =positive= from =margin= and add =closest_negative= </span>
    <span class="n">triplet_loss1</span> <span class="o">=</span> <span class="n">fastnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">margin</span> <span class="o">-</span> <span class="n">positive</span> <span class="o">+</span> <span class="n">closest_negative</span><span class="p">)</span>
    <span class="c1"># compute =fastnp.maximum= among 0.0 and =B=</span>
    <span class="c1"># B = subtract =positive= from =margin= and add =mean_negative=</span>
    <span class="n">triplet_loss2</span> <span class="o">=</span> <span class="n">fastnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">margin</span> <span class="o">-</span> <span class="n">positive</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean_negative</span><span class="p">)</span>
    <span class="c1"># add the two losses together and take the =fastnp.mean= of it</span>
    <span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">fastnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">triplet_loss1</span> <span class="o">+</span> <span class="n">triplet_loss2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">triplet_loss</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">v1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.26726124</span><span class="p">,</span> <span class="mf">0.53452248</span><span class="p">,</span> <span class="mf">0.80178373</span><span class="p">],[</span><span class="mf">0.5178918</span> <span class="p">,</span> <span class="mf">0.57543534</span><span class="p">,</span> <span class="mf">0.63297887</span><span class="p">]])</span>
<span class="n">v2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.26726124</span><span class="p">,</span>  <span class="mf">0.53452248</span><span class="p">,</span>  <span class="mf">0.80178373</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.5178918</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.57543534</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.63297887</span><span class="p">]])</span>
<span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">TripletLossFn</span><span class="p">(</span><span class="n">v2</span><span class="p">,</span> <span class="n">v1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Triplet Loss: </span><span class="si">{</span><span class="n">triplet_loss</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">triplet_loss</span> <span class="o">==</span> <span class="mf">0.5</span>
</pre></div>
<pre class="example">
Triplet Loss: 0.5
</pre>
<p>To make a layer out of a function with no trainable variables, use <code>tl.Fn</code>.</p>
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="k">def</span> <span class="nf">TripletLoss</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mf">0.25</span><span class="p">):</span>
    <span class="n">triplet_loss_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">TripletLossFn</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">(</span><span class="s1">'TripletLoss'</span><span class="p">,</span> <span class="n">triplet_loss_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga20ed25">
<h2 id="orga20ed25">Bundle It Up</h2>
<div class="outline-text-2" id="text-orga20ed25">
<p>Unfortunately trax does some kind of weirdness where it counts the arguments of the things you use as layers, so class-based stuff won't work (because it counts the <code>self</code> argument, giving it too many to expect). There might be a way to work around this, but it doesn't appear to be documented so this has to be done with only functions. That's not bad, it's just unexpected (and not well documented).</p>
</div>
<div class="outline-3" id="outline-container-org7871263">
<h3 id="org7871263">Imports</h3>
<div class="outline-text-3" id="text-org7871263">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">trax.fastmath</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">fastmath_numpy</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">trax</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf5a7499">
<h3 id="orgf5a7499">Triplet Loss</h3>
<div class="outline-text-3" id="text-orgf5a7499">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">triplet_loss</span><span class="p">(</span><span class="n">v1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">v2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span><span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">interpreters</span><span class="o">.</span><span class="n">xla</span><span class="o">.</span><span class="n">DeviceArray</span><span class="p">:</span>
    <span class="sd">"""Calculates the triplet loss</span>

<span class="sd">    Args:</span>
<span class="sd">     v1: normalized batch for question 1</span>
<span class="sd">     v2: normalized batch for question 2</span>

<span class="sd">    Returns:</span>
<span class="sd">     triplet loss</span>
<span class="sd">    """</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="n">positive</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
    <span class="n">negative_without_positive</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">-</span> <span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="n">closest_negative</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">negative_without_positive</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">negative_zero_on_duplicate</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span> <span class="o">*</span> <span class="n">scores</span>
    <span class="n">mean_negative</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">negative_zero_on_duplicate</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">triplet_loss1</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">margin</span> <span class="o">-</span> <span class="n">positive</span> <span class="o">+</span> <span class="n">closest_negative</span><span class="p">)</span>
    <span class="n">triplet_loss2</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">margin</span> <span class="o">-</span> <span class="n">positive</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean_negative</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">triplet_loss1</span> <span class="o">+</span> <span class="n">triplet_loss2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org60382a0">
<h3 id="org60382a0">Triplet Loss Layer</h3>
<div class="outline-text-3" id="text-org60382a0">
<p>Another not well documented limitation is that the function you create the layer from isn't allowed to take have default values, so if we want to allow the <code>margin</code> to have a default, we have to use <code>partial</code> to set the value before creating the layer…</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">triplet_loss_layer</span><span class="p">(</span><span class="n">margin</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">:</span>
    <span class="sd">"""Converts the triplet_loss function to a trax layer"""</span>
    <span class="n">with_margin</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">triplet_loss</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">(</span><span class="s2">"TripletLoss"</span><span class="p">,</span> <span class="n">with_margin</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga1a80f5">
<h3 id="orga1a80f5">Check It Out</h3>
<div class="outline-text-3" id="text-orga1a80f5">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.siamese_networks</span> <span class="kn">import</span> <span class="n">triplet_loss_layer</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">triplet_loss_layer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">layer</span><span class="p">))</span>
</pre></div>
<pre class="example">
&lt;class 'trax.layers.base.PureLayer'&gt;
</pre></div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/siamese-networks/" rel="tag">siamese networks</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../siamese-networks-defining-the-model/" rel="prev" title="Siamese Networks: Defining the Model">Previous post</a></li>
<li class="next"><a href="../siamese-networks-training-the-model/" rel="next" title="Siamese Networks: Training the Model">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
