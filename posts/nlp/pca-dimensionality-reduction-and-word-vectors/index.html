<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Applying the PCA for dimensionality reduction." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>PCA Dimensionality Reduction and Word Vectors | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/pca-dimensionality-reduction-and-word-vectors/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../pca-exploration/" rel="prev" title="PCA Exploration" type="text/html">
<link href="../more-matrix-math-in-python/" rel="next" title="More Matrix Math in Python" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="PCA Dimensionality Reduction and Word Vectors" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/pca-dimensionality-reduction-and-word-vectors/" property="og:url">
<meta content="Applying the PCA for dimensionality reduction." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-10-03T19:48:52-07:00" property="article:published_time">
<meta content="dimensionality reduction" property="article:tag">
<meta content="nlp" property="article:tag">
<meta content="pca" property="article:tag">
<meta content="visualization" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">PCA Dimensionality Reduction and Word Vectors</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-10-03T19:48:52-07:00" itemprop="datePublished" title="2020-10-03 19:48">2020-10-03 19:48</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0271170">Beginning</a>
<ul>
<li><a href="#org0e62574">Imports</a></li>
<li><a href="#org85cad36">Set Up</a></li>
</ul>
</li>
<li><a href="#org0ce11f9">Middle</a>
<ul>
<li><a href="#orgb16ca70">Predicting Relationships Among Words</a></li>
<li><a href="#org2a66e18">Plotting With PCA</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org0271170">
<h2 id="org0271170">Beginning</h2>
<div class="outline-text-2" id="text-org0271170">
<p>This is an extension of the previous two posts about <a href="../word-embeddings/">Word Embeddings</a> and <a href="../pca-exploration/">Principal Component Analysis</a>. Once again we're going to start with pre-trained word embeddings rather than train our own and then take the embeddings and explore them to better understand them.</p>
</div>
<div class="outline-3" id="outline-container-org0e62574">
<h3 id="org0e62574">Imports</h3>
<div class="outline-text-3" id="text-org0e62574">
<div class="highlight">
<pre><span></span><span class="c1"># from python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">be_true</span><span class="p">,</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">default_rng</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org85cad36">
<h3 id="org85cad36">Set Up</h3>
<div class="outline-text-3" id="text-org85cad36"></div>
<div class="outline-4" id="outline-container-org91b022c">
<h4 id="org91b022c">The Timer</h4>
<div class="outline-text-4" id="text-org91b022c">
<p>Just something to tell how long some processes take.</p>
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org634fc1d">
<h4 id="org634fc1d">Plotting</h4>
<div class="outline-text-4" id="text-org634fc1d">
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"pca-dimensionality-reduction-and-word-vectors"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span>
                <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
    <span class="n">color_cycle</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">Cycle</span><span class="p">([</span><span class="s2">"#4687b7"</span><span class="p">,</span> <span class="s2">"#ce7b6d"</span><span class="p">])</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org28e0a3a">
<h4 id="org28e0a3a">Randomness</h4>
<div class="outline-text-4" id="text-org28e0a3a">
<div class="highlight">
<pre><span></span><span class="n">numpy_random</span> <span class="o">=</span> <span class="n">default_rng</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge616293">
<h4 id="orge616293">The Environment</h4>
<div class="outline-text-4" id="text-orge616293">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org39e2b52">
<h4 id="org39e2b52">The Embeddings</h4>
<div class="outline-text-4" id="text-org39e2b52">
<p>These are the same embeddings as in the <a href="../word-embeddings/">Word Embeddings</a> exploration. They're loaded a dictionary of arrays (vectors). The original source is the Google News pre-trained data set available from the <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a> archive, but it is 3.64 gigabytes so Coursera extracted a subset of it to work with.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"WORD_EMBEDDINGS"</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">path</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>

<span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">==</span> <span class="mi">243</span>
</pre></div>
<p>The instructors also provide some code to show you how to create a different subset and I'm assuming that what they're showing is the actual way that they built this dataset. For future reference, this is the code given.</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">'./GoogleNews-vectors-negative300.bin'</span><span class="p">,</span> <span class="n">binary</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">'capitals.txt'</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">set_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
<span class="n">select_words</span> <span class="o">=</span> <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'king'</span><span class="p">,</span> <span class="s1">'queen'</span><span class="p">,</span> <span class="s1">'oil'</span><span class="p">,</span> <span class="s1">'gas'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'sad'</span><span class="p">,</span> <span class="s1">'city'</span><span class="p">,</span> <span class="s1">'town'</span><span class="p">,</span> <span class="s1">'village'</span><span class="p">,</span> <span class="s1">'country'</span><span class="p">,</span> <span class="s1">'continent'</span><span class="p">,</span> <span class="s1">'petroleum'</span><span class="p">,</span> <span class="s1">'joyful'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">select_words</span><span class="p">:</span>
    <span class="n">set_words</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_word_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">):</span>

    <span class="n">word_embeddings</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">set_words</span><span class="p">:</span>
            <span class="n">word_embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">word_embeddings</span>

<span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">get_word_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb247946">
<h4 id="orgb247946">The Data</h4>
<div class="outline-text-4" id="text-orgb247946">
<p>The data set is a space-separated-values file with no header.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"CAPITALS"</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">path</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">" "</span><span class="p">,</span>
                       <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s2">"City 1"</span><span class="p">,</span> <span class="s2">"Country 1"</span><span class="p">,</span> <span class="s2">"City 2"</span><span class="p">,</span> <span class="s2">"Country 2"</span><span class="p">])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
<pre class="example">
   City 1 Country 1   City 2    Country 2
0  Athens    Greece  Baghdad         Iraq
1  Athens    Greece  Bangkok     Thailand
2  Athens    Greece  Beijing        China
3  Athens    Greece   Berlin      Germany
4  Athens    Greece     Bern  Switzerland
</pre>
<p>It looks odd because this is actually an evaluation set. The first three columns are used to predict the fourth (e.g. <i>Athens, Greece,</i> and <i>Baghdad</i> are used to predict that <i>Baghdad</i> is the capital of <i>Iraq</i>).</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0ce11f9">
<h2 id="org0ce11f9">Middle</h2>
<div class="outline-text-2" id="text-org0ce11f9"></div>
<div class="outline-3" id="outline-container-orgb16ca70">
<h3 id="orgb16ca70">Predicting Relationships Among Words</h3>
<div class="outline-text-3" id="text-orgb16ca70">
<p>This part is about writing a function that will use the word embeddings to predict relationships among words.</p>
</div>
<div class="outline-4" id="outline-container-org8ea791f">
<h4 id="org8ea791f">Requirements</h4>
<div class="outline-text-4" id="text-org8ea791f">
<ul class="org-ul">
<li>The arguments will be three words</li>
<li>The first two will be considered related to each other somehow</li>
<li>The function will then predict a fourth word that is related to the third word in a way that is similar to the relationship between the first two words.</li>
</ul>
<p>Another way to look at is it that if you are given three words - <i>Athens, Greece,</i> and <i>Bangkok</i> then the function will fill in the blank for "Athens is to Greece as Bangkok is to __".</p>
<p>Because of our input data set what the function will end up doing is finding the capital of a country. But first we need a distance function.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org514deb4">
<h4 id="org514deb4">Cosine Similarity</h4>
<div class="outline-text-4" id="text-org514deb4">\begin{align} \cos (\theta) &amp;=\frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}\\ &amp;= \frac{\sum_{i=1}^{n} A_{i} B_{i}}{\sqrt{\sum_{i=1}^{n} A_{i}^{2}} \sqrt{\sum_{i=1}^{n} B_{i}^{2}}}\\ \end{align}
<ul class="org-ul">
<li><i>A</i> and <i>B</i> are the word vectors and \(A_i\) or \(B_i\) is the <i>ith</i> item of that vector</li>
<li>If the output is 0 then they are opposites and if the output is 1 then they are the same</li>
<li>If the number is between 0 and 1 then it is a similarity score</li>
<li>If the number is between 0 and -1 then it is a dissimilarity score</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">'''Calculates the cosine similarity between two arrays</span>

<span class="sd">    Args:</span>
<span class="sd">       A: a numpy array which corresponds to a word vector</span>
<span class="sd">       B: A numpy array which corresponds to a word vector</span>
<span class="sd">    Return:</span>
<span class="sd">       cos: numerical number representing the cosine similarity between A and B.</span>
<span class="sd">    '''</span>
    <span class="n">dot_product</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="n">norm_of_A</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
    <span class="n">norm_of_B</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">dot_product</span><span class="o">/</span><span class="p">(</span><span class="n">norm_of_A</span> <span class="o">*</span> <span class="n">norm_of_B</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cos</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">king</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"king"</span><span class="p">]</span>
<span class="n">queen</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"queen"</span><span class="p">]</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">king</span><span class="p">,</span> <span class="n">queen</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The Cosine Similarity between 'king' and 'queen': </span><span class="si">{</span><span class="n">similarity</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">0.6510956</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">similarity</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
The Cosine Similarity between 'king' and 'queen': 0.65.
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgdb78612">
<h4 id="orgdb78612">Euclidean Distance</h4>
<div class="outline-text-4" id="text-orgdb78612">
<p>In addition to the Cosine Similarity we can use the (probably better known) Euclidean Distance.</p>
\begin{aligned} d(\mathbf{A}, \mathbf{B})=d(\mathbf{B}, \mathbf{A}) &amp;=\sqrt{\left(A_{1}-B_{1}\right)^{2}+\left(A_{2}-B_{2}\right)^{2}+\cdots+\left(A_{n}-B_{n}\right)^{2}} \\ &amp;=\sqrt{\sum_{i=1}^{n}\left(A_{i}-B_{i}\right)^{2}} \end{aligned}
<ul class="org-ul">
<li><i>n</i> is the number of elements in the vector</li>
<li><i>A</i> and <i>B</i> are the corresponding word vectors.</li>
<li>The more similar the words, the more likely the Euclidean distance will be close to 0 (and zero means they are the same).</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">euclidean</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Calculate the euclidean distance between two vectors</span>

<span class="sd">    Args:</span>
<span class="sd">       A: a numpy array which corresponds to a word vector</span>
<span class="sd">       B: A numpy array which corresponds to a word vector</span>
<span class="sd">    Return:</span>
<span class="sd">       d: numerical number representing the Euclidean distance between A and B.</span>
<span class="sd">    """</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(((</span><span class="n">A</span> <span class="o">-</span> <span class="n">B</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">d</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">euclidean</span><span class="p">(</span><span class="n">king</span><span class="p">,</span> <span class="n">queen</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">2.4796925</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The Euclidean Distance between 'king' and 'queen' is </span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
The Euclidean Distance between 'king' and 'queen' is 2.48.
</pre></div>
</div>
<div class="outline-4" id="outline-container-org24e90cd">
<h4 id="org24e90cd">The Predictor</h4>
<div class="outline-text-4" id="text-org24e90cd">
<p>Here's whdere we make the function that tries to predict the Country for a given Capital City. This will use the cosine similarity. This first version will use brute-force.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_country</span><span class="p">(</span><span class="n">city1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">country1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">city2</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Find the country that has a particular capital city</span>

<span class="sd">    Args:</span>
<span class="sd">       city1: a string (the capital city of country1)</span>
<span class="sd">       country1: a string (the country of capital1)</span>
<span class="sd">       city2: a string (the capital city of country2)</span>
<span class="sd">       embeddings: a dictionary where the keys are words and values are their embeddings</span>
<span class="sd">    Return:</span>
<span class="sd">       countries: most likely country, similarity score</span>
<span class="sd">    """</span>
    <span class="n">group</span> <span class="o">=</span> <span class="nb">set</span><span class="p">((</span><span class="n">city1</span><span class="p">,</span> <span class="n">country1</span><span class="p">,</span> <span class="n">city2</span><span class="p">))</span>

    <span class="n">city1_emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">city1</span><span class="p">]</span>

    <span class="n">country1_emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">country1</span><span class="p">]</span>

    <span class="n">city2_emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">city2</span><span class="p">]</span>

    <span class="n">vec</span> <span class="o">=</span> <span class="n">country1_emb</span> <span class="o">-</span> <span class="n">city1_emb</span>  <span class="o">+</span> <span class="n">city2_emb</span>

    <span class="c1"># Initialize the similarity to -1 (it will be replaced by a similarities that are closer to +1)</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="c1"># initialize country to an empty string</span>
    <span class="n">country</span> <span class="o">=</span> <span class="s1">''</span>

    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">group</span><span class="p">:</span>
            <span class="n">word_emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
            <span class="c1"># calculate cosine similarity between embedding of country 2 and the word in the embeddings dictionary</span>
            <span class="n">cur_similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">word_emb</span><span class="p">)</span>

            <span class="c1"># if the cosine similarity is more similar than the previously best similarity...</span>
            <span class="k">if</span> <span class="n">cur_similarity</span> <span class="o">&gt;</span> <span class="n">similarity</span><span class="p">:</span>

                <span class="c1"># update the similarity to the new, better similarity</span>
                <span class="n">similarity</span> <span class="o">=</span> <span class="n">cur_similarity</span>

                <span class="c1"># store the country as a tuple, which contains the word and the similarity</span>
                <span class="n">country</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">similarity</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">country</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">actual_country</span><span class="p">,</span> <span class="n">actual_similarity</span> <span class="o">=</span> <span class="n">get_country</span><span class="p">(</span><span class="s2">"Athens"</span><span class="p">,</span> <span class="s2">"Greece"</span><span class="p">,</span> <span class="s2">"Cairo"</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Cairo is the capital of </span><span class="si">{</span><span class="n">actual_country</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>

<span class="n">expected_country</span><span class="p">,</span> <span class="n">expected_similarity</span> <span class="o">=</span> <span class="s2">"Egypt"</span><span class="p">,</span> <span class="mf">0.7626821</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual_country</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected_country</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">actual_similarity</span><span class="p">,</span> <span class="n">expected_similarity</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
Cairo is the capital of Egypt.
</pre></div>
</div>
<div class="outline-4" id="outline-container-org6186c9d">
<h4 id="org6186c9d">Checking the Model Accuracy</h4>
<div class="outline-text-4" id="text-org6186c9d">
<p>\[ \text{Accuracy}=\frac{\text{Correct # of predictions}}{\text{Total # of predictions}} \]</p>
<div class="highlight">
<pre><span></span><span class="n">country_getter</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">get_country</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_accuracy</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">'''Calculate the fraction of correct capitals</span>

<span class="sd">    Args:</span>
<span class="sd">       embeddings: a dictionary where the key is a word and the value is its embedding</span>

<span class="sd">    Return:</span>
<span class="sd">       accuracy: the accuracy of the model</span>
<span class="sd">    '''</span>
    <span class="n">num_correct</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># loop through the rows of the dataframe</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>

        <span class="c1"># get city1</span>
        <span class="n">city1</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">"City 1"</span><span class="p">]</span>

        <span class="c1"># get country1</span>
        <span class="n">country1</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">"Country 1"</span><span class="p">]</span>

        <span class="c1"># get city2</span>
        <span class="n">city2</span> <span class="o">=</span>  <span class="n">row</span><span class="p">[</span><span class="s2">"City 2"</span><span class="p">]</span>

        <span class="c1"># get country2</span>
        <span class="n">country2</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s2">"Country 2"</span><span class="p">]</span>

        <span class="c1"># use get_country to find the predicted country2</span>
        <span class="n">predicted_country2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">country_getter</span><span class="p">(</span><span class="n">city1</span><span class="o">=</span><span class="n">city1</span><span class="p">,</span> <span class="n">country1</span><span class="o">=</span><span class="n">country1</span><span class="p">,</span> <span class="n">city2</span><span class="o">=</span><span class="n">city2</span><span class="p">)</span>

        <span class="c1"># if the predicted country2 is the same as the actual country2...</span>
        <span class="k">if</span> <span class="n">predicted_country2</span> <span class="o">==</span> <span class="n">country2</span><span class="p">:</span>
            <span class="c1"># increment the number of correct by 1</span>
            <span class="n">num_correct</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># get the number of rows in the data dataframe (length of dataframe)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

    <span class="c1"># calculate the accuracy by dividing the number correct by m</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">num_correct</span><span class="o">/</span><span class="n">m</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">get_accuracy</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">0.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">accuracy</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
2020-10-07 17:50:28,897 graeae.timers.timer start: Started: 2020-10-07 17:50:28.897165
2020-10-07 17:50:50,755 graeae.timers.timer end: Ended: 2020-10-07 17:50:50.755424
2020-10-07 17:50:50,756 graeae.timers.timer end: Elapsed: 0:00:21.858259
Accuracy: 0.92
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2a66e18">
<h3 id="org2a66e18">Plotting With PCA</h3>
<div class="outline-text-3" id="text-org2a66e18"></div>
<div class="outline-4" id="outline-container-org115c862">
<h4 id="org115c862">Computing the PCA</h4>
<div class="outline-text-4" id="text-org115c862">
<p>Now we'll write a function to do the Principal Component Analysis for our embeddings.</p>
<ul class="org-ul">
<li>The word vectors are of dimension 300.</li>
<li>Use PCA to change the 300 dimensions to <code>n_components</code> dimensions.</li>
<li>The new matrix should be of dimension <code>m, n_components</code> (<code>m</code> being the number of rows).</li>
<li></li>
<li>First de-mean the data</li>
<li>Get the eigenvalues using `linalg.eigh`. Use `eigh` rather than `eig` since R is symmetric. The performance gain when using `eigh` instead of `eig` is substantial.</li>
<li>Sort the eigenvectors and eigenvalues by decreasing order of the eigenvalues.</li>
<li>Get a subset of the eigenvectors (choose how many principle components you want to use using `n_components`).</li>
<li>Return the new transformation of the data by multiplying the eigenvectors with the original data.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_pca</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n_components</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate the principal components for X</span>

<span class="sd">    Args:</span>
<span class="sd">       X: of dimension (m,n) where each row corresponds to a word vector</span>
<span class="sd">       n_components: Number of components you want to keep.</span>

<span class="sd">    Return:</span>
<span class="sd">       X_reduced: data transformed in 2 dims/columns + regenerated original data</span>
<span class="sd">    """</span>
    <span class="c1"># you need to set axis to 0 or it will calculate the mean of the entire matrix instead of one per row</span>
    <span class="n">X_demeaned</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate the covariance matrix</span>
    <span class="c1"># the default numpy.cov assumes the rows are variables, not columns so set rowvar to False</span>
    <span class="n">covariance_matrix</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X_demeaned</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># calculate eigenvectors & eigenvalues of the covariance matrix</span>
    <span class="n">eigen_vals</span><span class="p">,</span> <span class="n">eigen_vecs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">covariance_matrix</span><span class="p">)</span>

    <span class="c1"># sort eigenvalue in increasing order (get the indices from the sort)</span>
    <span class="n">idx_sorted</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigen_vals</span><span class="p">)</span>

    <span class="c1"># reverse the order so that it's from highest to lowest.</span>
    <span class="n">idx_sorted_decreasing</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">idx_sorted</span><span class="p">))</span>

    <span class="c1"># sort the eigen values by idx_sorted_decreasing</span>
    <span class="n">eigen_vals_sorted</span> <span class="o">=</span> <span class="n">eigen_vals</span><span class="p">[</span><span class="n">idx_sorted_decreasing</span><span class="p">]</span>

    <span class="c1"># sort eigenvectors using the idx_sorted_decreasing indices</span>
    <span class="c1"># We're only sorting the columns so remember to get all the rows in the slice</span>
    <span class="n">eigen_vecs_sorted</span> <span class="o">=</span> <span class="n">eigen_vecs</span><span class="p">[:,</span> <span class="n">idx_sorted_decreasing</span><span class="p">]</span>

    <span class="c1"># select the first n eigenvectors (n is desired dimension</span>
    <span class="c1"># of rescaled data array, or dims_rescaled_data)</span>
    <span class="c1"># once again, make sure to get all the rows and only slice the columns</span>
    <span class="n">eigen_vecs_subset</span> <span class="o">=</span> <span class="n">eigen_vecs_sorted</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_components</span><span class="p">]</span>

    <span class="c1"># transform the data by multiplying the transpose of the eigenvectors </span>
    <span class="c1"># with the transpose of the de-meaned data</span>
    <span class="c1"># Then take the transpose of that product.</span>
    <span class="n">X_reduced</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">eigen_vecs_subset</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_demeaned</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span> <span class="n">X_reduced</span>
</pre></div>
<p>I was getting the wrong values because for some reason so I decided to take out the call to random (since the seed was being set the values were always the same anyway) and just declare the test input array.</p>
<div class="highlight">
<pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">4.17022005e-01</span><span class="p">,</span> <span class="mf">7.20324493e-01</span><span class="p">,</span> <span class="mf">1.14374817e-04</span><span class="p">,</span> <span class="mf">3.02332573e-01</span><span class="p">,</span>
                  <span class="mf">1.46755891e-01</span><span class="p">,</span> <span class="mf">9.23385948e-02</span><span class="p">,</span> <span class="mf">1.86260211e-01</span><span class="p">,</span> <span class="mf">3.45560727e-01</span><span class="p">,</span>
                  <span class="mf">3.96767474e-01</span><span class="p">,</span> <span class="mf">5.38816734e-01</span><span class="p">],</span>
                 <span class="p">[</span><span class="mf">4.19194514e-01</span><span class="p">,</span> <span class="mf">6.85219500e-01</span><span class="p">,</span> <span class="mf">2.04452250e-01</span><span class="p">,</span> <span class="mf">8.78117436e-01</span><span class="p">,</span>
                  <span class="mf">2.73875932e-02</span><span class="p">,</span> <span class="mf">6.70467510e-01</span><span class="p">,</span> <span class="mf">4.17304802e-01</span><span class="p">,</span> <span class="mf">5.58689828e-01</span><span class="p">,</span>
                  <span class="mf">1.40386939e-01</span><span class="p">,</span> <span class="mf">1.98101489e-01</span><span class="p">],</span>
                 <span class="p">[</span><span class="mf">8.00744569e-01</span><span class="p">,</span> <span class="mf">9.68261576e-01</span><span class="p">,</span> <span class="mf">3.13424178e-01</span><span class="p">,</span> <span class="mf">6.92322616e-01</span><span class="p">,</span>
                  <span class="mf">8.76389152e-01</span><span class="p">,</span> <span class="mf">8.94606664e-01</span><span class="p">,</span> <span class="mf">8.50442114e-02</span><span class="p">,</span> <span class="mf">3.90547832e-02</span><span class="p">,</span>
                  <span class="mf">1.69830420e-01</span><span class="p">,</span> <span class="mf">8.78142503e-01</span><span class="p">]])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">X_reduced</span> <span class="o">=</span> <span class="n">compute_pca</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># eigen_vecs, eigen_subset, X_demeaned = compute_pca(X, n_components=2)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Your original matrix was "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="s2">" and it became:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_reduced</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
 <span class="p">[</span><span class="mf">0.43437323</span><span class="p">,</span> <span class="mf">0.49820384</span><span class="p">],</span>
 <span class="p">[</span><span class="mf">0.42077249</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.50351448</span><span class="p">],</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.85514571</span><span class="p">,</span> <span class="mf">0.00531064</span><span class="p">],</span>
<span class="p">])</span>

<span class="n">numpy</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_almost_equal</span><span class="p">(</span><span class="n">X_reduced</span><span class="p">,</span> <span class="n">expected</span><span class="p">)</span>
</pre></div>
<pre class="example">
Your original matrix was (3, 10) and it became:
[[ 0.43437323  0.49820384]
 [ 0.42077249 -0.50351448]
 [-0.85514571  0.00531064]]
</pre></div>
</div>
<div class="outline-4" id="outline-container-org47c4d7c">
<h4 id="org47c4d7c">Plot It</h4>
<div class="outline-text-4" id="text-org47c4d7c">
<p>We'll use most of the non-country words to create a plot to see how well the PCA does.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'oil'</span><span class="p">,</span> <span class="s1">'gas'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'sad'</span><span class="p">,</span> <span class="s1">'city'</span><span class="p">,</span> <span class="s1">'town'</span><span class="p">,</span>
         <span class="s1">'village'</span><span class="p">,</span> <span class="s1">'country'</span><span class="p">,</span> <span class="s1">'continent'</span><span class="p">,</span> <span class="s1">'petroleum'</span><span class="p">,</span> <span class="s1">'joyful'</span><span class="p">]</span>
<span class="n">subset</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">compute_pca</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reduced</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">"X Y"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="n">reduced</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">,</span> <span class="n">text_baseline</span><span class="o">=</span><span class="s2">"top"</span><span class="p">)</span>

<span class="n">points</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span> <span class="o">*</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"PCA of Words"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"pca_words"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="pca_words.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>It appears to have worked fairly well.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgeeec538">
<h4 id="orgeeec538">Sklearn Comparison</h4>
<div class="outline-text-4" id="text-orgeeec538">
<p>As a comparison here's what SKlearn's PCA does.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">subset</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reduced</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s2">"X Y"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="n">reduced</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">,</span> <span class="n">text_baseline</span><span class="o">=</span><span class="s2">"top"</span><span class="p">)</span>

<span class="n">points</span> <span class="o">=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span> <span class="o">*</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"PCA of Words (SKLearn)"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"sklearn_pca_words"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="sklearn_pca_words.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>They look fairly comparable, I'll conclude that they are close (or close enough).</p>
</div>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/dimensionality-reduction/" rel="tag">dimensionality reduction</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/pca/" rel="tag">pca</a></li>
<li><a class="tag p-category" href="../../../categories/visualization/" rel="tag">visualization</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../pca-exploration/" rel="prev" title="PCA Exploration">Previous post</a></li>
<li class="next"><a href="../more-matrix-math-in-python/" rel="next" title="More Matrix Math in Python">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
