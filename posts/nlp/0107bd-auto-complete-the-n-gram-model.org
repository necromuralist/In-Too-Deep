#+BEGIN_COMMENT
.. title: Auto-Complete: the N-Gram Model
.. slug: auto-complete-the-n-gram-model
.. date: 2020-12-04 15:17:18 UTC-08:00
.. tags: nlp,auto-complete,n-gram
.. category: NLP
.. link: 
.. description: Implementing the N-Gram Language model for auto-complete.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-628b96bb-9fe2-4219-af43-264f81238d87-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  We'll continue on from the {{% lancelot title="previous post" %}}auto-complete-pre-process-the-data-ii{{% /lancelot %}} in which we finished pre-processing the data to build our {{% doc %}}auto-complete{{% /doc %}} system.
* End
  Now that we have the N-Gram model we'll move on to checking its {{% lancelot title="Perplexity" %}}auto-complete-perplexity{{% /lancelot %}}.
* Raw
#+begin_example
# ## Part 2: Develop n-gram based language models
# 
# In this section, you will develop the n-grams language model.
# - Assume the probability of the next word depends only on the previous n-gram.
# - The previous n-gram is the series of the previous 'n' words.
# 
# The conditional probability for the word at position 't' in the sentence, given that the words preceding it are $w_{t-1}, w_{t-2} \cdots w_{t-n}$ is:
# 
# $$ P(w_t | w_{t-1}\dots w_{t-n}) \tag{1}$$
# 
# You can estimate this probability  by counting the occurrences of these series of words in the training data.
# - The probability can be estimated as a ratio, where
# - The numerator is the number of times word 't' appears after words t-1 through t-n appear in the training data.
# - The denominator is the number of times word t-1 through t-n appears in the training data.
# 
# $$ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2} $$
# 
# - The function $C(\cdots)$ denotes the number of occurence of the given sequence. 
# - $\hat{P}$ means the estimation of $P$. 
# - Notice that denominator of the equation (2) is the number of occurence of the previous $n$ words, and the numerator is the same sequence followed by the word $w_t$.
# 
# Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.
# 
# The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).

# <a name='ex-08'></a>
# ### Exercise 08
# Next, you will implement a function that computes the counts of n-grams for an arbitrary number $n$.
# 
# When computing the counts for n-grams, prepare the sentence beforehand by prepending $n-1$ starting markers "<s\>" to indicate the beginning of the sentence.  
# - For example, in the bi-gram model (N=2), a sequence with two start tokens "<s\><s\>" should predict the first word of a sentence.
# - So, if the sentence is "I like food", modify it to be "<s\><s\> I like food".
# - Also prepare the sentence for counting by appending an end token "<e\>" so that the model can predict when to finish a sentence.
# 
# Technical note: In this implementation, you will store the counts as a dictionary.
# - The key of each key-value pair in the dictionary is a **tuple** of n words (and not a list)
# - The value in the key-value pair is the number of occurrences.  
# - The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created).  A tuple is "immutable", so it cannot be altered after it is first created.  This makes a tuple suitable as a data type for the key in a dictionary.

# <details>    
# <summary>
#     <font size="3" color="darkgreen"><b>Hints</b></font>
# </summary>
# <p>
# <ul>
#     <li> To prepend or append, you can create lists and concatenate them using the + operator </li>
#     <li> To create a list of a repeated value, you can follow this syntax: <code>['a'] * 3</code> to get <code>['a','a','a']</code> </li>
#     <li>To set the range for index 'i', think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token).  So the index positions are <code>[0,1,2,3,4]</code>.  The largest index 'i' where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram. </li>
#     <li>Remember that the <code>range()</code> function excludes the value that is used for the maximum of the range.  <code> range(3) </code> produces (0,1,2) but excludes 3. </li>
# </ul>
# </p>
# 

# In[ ]:


# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
### GRADED FUNCTION: count_n_grams ###
def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):
    """
    Count all n-grams in the data
    
    Args:
        data: List of lists of words
        n: number of words in a sequence
    
    Returns:
        A dictionary that maps a tuple of n-words to its frequency
    """
    
    # Initialize dictionary of n-grams and their counts
    n_grams = {}

    ### START CODE HERE (Replace instances of 'None' with your code) ###
    
    # Go through each sentence in the data
    for sentence in None: # complete this line
        
        # prepend start token n times, and  append <e> one time
        sentence = None
        
        # convert list to tuple
        # So that the sequence of words can be used as
        # a key in the dictionary
        sentence = None
        
        # Use 'i' to indicate the start of the n-gram
        # from index 0
        # to the last index where the end of the n-gram
        # is within the sentence.
        
        for i in range(None): # complete this line

            # Get the n-gram from i to i+n
            n_gram = None

            # check if the n-gram is in the dictionary
            if n_gram in None: # complete this line
            
                # Increment the count for this n-gram
                n_grams[n_gram] += None
            else:
                # Initialize this n-gram count to 1
                n_grams[n_gram] = None
    
            ### END CODE HERE ###
    return n_grams


# In[ ]:


# test your code
# CODE REVIEW COMMENT: Outcome does not match expected outcome
sentences = [['i', 'like', 'a', 'cat'],
             ['this', 'dog', 'is', 'like', 'a', 'cat']]
print("Uni-gram:")
print(count_n_grams(sentences, 1))
print("Bi-gram:")
print(count_n_grams(sentences, 2))


# Expected outcome:
# 
# ```CPP
# Uni-gram:
# {('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}
# Bi-gram:
# {('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}
# ```

# <a name='ex-09'></a>
# ### Exercise 09
# 
# Next, estimate the probability of a word given the prior 'n' words using the n-gram counts.
# 
# $$ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2} $$
# 
# This formula doesn't work when a count of an n-gram is zero..
# - Suppose we encounter an n-gram that did not occur in the training data.  
# - Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).
# 
# A way to handle zero counts is to add k-smoothing.  
# - K-smoothing adds a positive constant $k$ to each numerator and $k \times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary.
# 
# $$ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n) + k}{C(w_{t-1}\dots w_{t-n}) + k|V|} \tag{3} $$
# 
# 
# For n-grams that have a zero count, the equation (3) becomes $\frac{1}{|V|}$.
# - This means that any n-gram with zero count has the same probability of $\frac{1}{|V|}$.
# 
# Define a function that computes the probability estimate (3) from n-gram counts and a constant $k$.
# 
# - The function takes in a dictionary 'n_gram_counts', where the key is the n-gram and the value is the count of that n-gram.
# - The function also takes another dictionary n_plus1_gram_counts, which you'll use to find the count for the previous n-gram plus the current word.

# <details>    
# <summary>
#     <font size="3" color="darkgreen"><b>Hints</b></font>
# </summary>
# <p>
# <ul>
#     <li>To define a tuple containing a single value, add a comma after that value.  For example: <code>('apple',)</code> is a tuple containing a single string 'apple' </li>
#     <li>To concatenate two tuples, use the '+' operator</li>
#     <li><a href="" > words </a> </li>
# </ul>
# </p>
# 

# In[ ]:


# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
### GRADED FUNCTION: estimate_probability ###
def estimate_probability(word, previous_n_gram, 
                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):
    """
    Estimate the probabilities of a next word using the n-gram counts with k-smoothing
    
    Args:
        word: next word
        previous_n_gram: A sequence of words of length n
        n_gram_counts: Dictionary of counts of n-grams
        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams
        vocabulary_size: number of words in the vocabulary
        k: positive constant, smoothing parameter
    
    Returns:
        A probability
    """
    # convert list to tuple to use it as a dictionary key
    previous_n_gram = tuple(previous_n_gram)
    
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    
    # Set the denominator
    # If the previous n-gram exists in the dictionary of n-gram counts,
    # Get its count.  Otherwise set the count to zero
    # Use the dictionary that has counts for n-grams
    previous_n_gram_count = None
        
    # Calculate the denominator using the count of the previous n gram
    # and apply k-smoothing
    denominator = None

    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple
    n_plus1_gram = None
  
    # Set the count to the count in the dictionary,
    # otherwise 0 if not in the dictionary
    # use the dictionary that has counts for the n-gram plus current word
    n_plus1_gram_count = None
        
    # Define the numerator use the count of the n-gram plus current word,
    # and apply smoothing
    numerator = None

    # Calculate the probability as the numerator divided by denominator
    probability = None
    
    ### END CODE HERE ###
    
    return probability


# In[ ]:


# test your code
sentences = [['i', 'like', 'a', 'cat'],
             ['this', 'dog', 'is', 'like', 'a', 'cat']]
unique_words = list(set(sentences[0] + sentences[1]))

unigram_counts = count_n_grams(sentences, 1)
bigram_counts = count_n_grams(sentences, 2)
tmp_prob = estimate_probability("cat", "a", unigram_counts, bigram_counts, len(unique_words), k=1)

print(f"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}")


# ##### Expected output
# 
# ```CPP
# The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333
# ```

# ### Estimate probabilities for all words
# 
# The function defined below loops over all words in vocabulary to calculate probabilities for all possible words.
# - This function is provided for you.

# In[ ]:


def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):
    """
    Estimate the probabilities of next words using the n-gram counts with k-smoothing
    
    Args:
        previous_n_gram: A sequence of words of length n
        n_gram_counts: Dictionary of counts of (n+1)-grams
        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams
        vocabulary: List of words
        k: positive constant, smoothing parameter
    
    Returns:
        A dictionary mapping from next words to the probability.
    """
    
    # convert list to tuple to use it as a dictionary key
    previous_n_gram = tuple(previous_n_gram)
    
    # add <e> <unk> to the vocabulary
    # <s> is not needed since it should not appear as the next word
    vocabulary = vocabulary + ["<e>", "<unk>"]
    vocabulary_size = len(vocabulary)
    
    probabilities = {}
    for word in vocabulary:
        probability = estimate_probability(word, previous_n_gram, 
                                           n_gram_counts, n_plus1_gram_counts, 
                                           vocabulary_size, k=k)
        probabilities[word] = probability

    return probabilities


# In[ ]:


# test your code
sentences = [['i', 'like', 'a', 'cat'],
             ['this', 'dog', 'is', 'like', 'a', 'cat']]
unique_words = list(set(sentences[0] + sentences[1]))
unigram_counts = count_n_grams(sentences, 1)
bigram_counts = count_n_grams(sentences, 2)
estimate_probabilities("a", unigram_counts, bigram_counts, unique_words, k=1)


# ##### Expected output
# 
# ```CPP
# {'cat': 0.2727272727272727,
#  'i': 0.09090909090909091,
#  'this': 0.09090909090909091,
#  'a': 0.09090909090909091,
#  'is': 0.09090909090909091,
#  'like': 0.09090909090909091,
#  'dog': 0.09090909090909091,
#  '<e>': 0.09090909090909091,
#  '<unk>': 0.09090909090909091}
# ```

# In[ ]:


# Additional test
trigram_counts = count_n_grams(sentences, 3)
estimate_probabilities(["<s>", "<s>"], bigram_counts, trigram_counts, unique_words, k=1)


# ##### Expected output
# 
# ```CPP
# {'cat': 0.09090909090909091,
#  'i': 0.18181818181818182,
#  'this': 0.18181818181818182,
#  'a': 0.09090909090909091,
#  'is': 0.09090909090909091,
#  'like': 0.09090909090909091,
#  'dog': 0.09090909090909091,
#  '<e>': 0.09090909090909091,
#  '<unk>': 0.09090909090909091}
# ```

# ### Count and probability matrices
# 
# As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.  
# - It can be more intuitive to present them as count or probability matrices.
# - The functions defined in the next cells return count or probability matrices.
# - This function is provided for you.

# In[ ]:


def make_count_matrix(n_plus1_gram_counts, vocabulary):
    # add <e> <unk> to the vocabulary
    # <s> is omitted since it should not appear as the next word
    vocabulary = vocabulary + ["<e>", "<unk>"]
    
    # obtain unique n-grams
    n_grams = []
    for n_plus1_gram in n_plus1_gram_counts.keys():
        n_gram = n_plus1_gram[0:-1]
        n_grams.append(n_gram)
    n_grams = list(set(n_grams))
    
    # mapping from n-gram to row
    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}
    # mapping from next word to column
    col_index = {word:j for j, word in enumerate(vocabulary)}
    
    nrow = len(n_grams)
    ncol = len(vocabulary)
    count_matrix = np.zeros((nrow, ncol))
    for n_plus1_gram, count in n_plus1_gram_counts.items():
        n_gram = n_plus1_gram[0:-1]
        word = n_plus1_gram[-1]
        if word not in vocabulary:
            continue
        i = row_index[n_gram]
        j = col_index[word]
        count_matrix[i, j] = count
    
    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)
    return count_matrix


# In[ ]:


sentences = [['i', 'like', 'a', 'cat'],
                 ['this', 'dog', 'is', 'like', 'a', 'cat']]
unique_words = list(set(sentences[0] + sentences[1]))
bigram_counts = count_n_grams(sentences, 2)

print('bigram counts')
display(make_count_matrix(bigram_counts, unique_words))


# ##### Expected output
# 
# ```CPP
# bigram counts
#           cat    i   this   a  is   like  dog  <e>   <unk>
# (<s>,)    0.0   1.0  1.0  0.0  0.0  0.0   0.0  0.0    0.0
# (a,)      2.0   0.0  0.0  0.0  0.0  0.0   0.0  0.0    0.0
# (this,)   0.0   0.0  0.0  0.0  0.0  0.0   1.0  0.0    0.0
# (like,)   0.0   0.0  0.0  2.0  0.0  0.0   0.0  0.0    0.0
# (dog,)    0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0    0.0
# (cat,)    0.0   0.0  0.0  0.0  0.0  0.0   0.0  2.0    0.0
# (is,)     0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0
# (i,)      0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0
# ```

# In[ ]:


# Show trigram counts
print('\ntrigram counts')
trigram_counts = count_n_grams(sentences, 3)
display(make_count_matrix(trigram_counts, unique_words))


# ##### Expected output
# 
# ```CPP
# trigram counts
#               cat    i   this   a  is   like  dog  <e>   <unk>
# (dog, is)     0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0
# (this, dog)   0.0   0.0  0.0  0.0  1.0  0.0   0.0  0.0    0.0
# (a, cat)      0.0   0.0  0.0  0.0  0.0  0.0   0.0  2.0    0.0
# (like, a)     2.0   0.0  0.0  0.0  0.0  0.0   0.0  0.0    0.0
# (is, like)    0.0   0.0  0.0  1.0  0.0  0.0   0.0  0.0    0.0
# (<s>, i)      0.0   0.0  0.0  0.0  0.0  1.0   0.0  0.0    0.0
# (i, like)     0.0   0.0  0.0  1.0  0.0  0.0   0.0  0.0    0.0
# (<s>, <s>)    0.0   1.0  1.0  0.0  0.0  0.0   0.0  0.0    0.0
# (<s>, this)   0.0   0.0  0.0  0.0  0.0  0.0   1.0  0.0    0.0
# ```

# The following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form.
# - This function is provided for you.

# In[ ]:


def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):
    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)
    count_matrix += k
    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)
    return prob_matrix


# In[ ]:


sentences = [['i', 'like', 'a', 'cat'],
                 ['this', 'dog', 'is', 'like', 'a', 'cat']]
unique_words = list(set(sentences[0] + sentences[1]))
bigram_counts = count_n_grams(sentences, 2)
print("bigram probabilities")
display(make_probability_matrix(bigram_counts, unique_words, k=1))


# In[ ]:


print("trigram probabilities")
trigram_counts = count_n_grams(sentences, 3)
display(make_probability_matrix(trigram_counts, unique_words, k=1))


# Confirm that you obtain the same results as for the `estimate_probabilities` function that you implemented.
#+end_example
