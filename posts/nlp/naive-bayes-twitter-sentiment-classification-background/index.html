<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Some background to the Naive Bayes Twitter sentiment classification model." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Using Naive Bayes to Classify Tweets by Sentiment | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/naive-bayes-twitter-sentiment-classification-background/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../../bibliographies/text-data-management-and-analysis/" rel="prev" title="Text Data Management and Analysis" type="text/html">
<link href="../naive-bayes-twitter-sentiment-classification/" rel="next" title="Implementing a Naive Bayes Twitter Sentiment Classifier" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Using Naive Bayes to Classify Tweets by Sentiment" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/naive-bayes-twitter-sentiment-classification-background/" property="og:url">
<meta content="Some background to the Naive Bayes Twitter sentiment classification model." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-08-28T09:25:00-07:00" property="article:published_time">
<meta content="naive bayes" property="article:tag">
<meta content="nlp" property="article:tag">
<meta content="sentiment analysis" property="article:tag">
<meta content="twitter" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/Neurotic-Networking/"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Using Naive Bayes to Classify Tweets by Sentiment</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-08-28T09:25:00-07:00" itemprop="datePublished" title="2020-08-28 09:25">2020-08-28 09:25</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org567fdce">Beginning</a></li>
<li><a href="#org825f8a4">Middle</a></li>
<li><a href="#org240e8e2">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org567fdce">
<h2 id="org567fdce">Beginning</h2>
<div class="outline-text-2" id="text-org567fdce">
<p>In a <a href="../implementing-twitter-logistic-regression/">previous post</a> I implemented a Logistic Regression model to classify twitter tweets as having a positive or negative sentiment. This time I'll be using the same data set (from <a href="http://www.nltk.org/">NLTK</a>) but implementing it with a <a href="https://www.wikiwand.com/en/Naive_Bayes_classifier">Naive Bayes</a> model. This post will look at some of the math behind it and the next one will translate the math into code.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org825f8a4">
<h2 id="org825f8a4">Middle</h2>
<div class="outline-text-2" id="text-org825f8a4"></div>
<div class="outline-4" id="outline-container-orgb7158bb">
<h4 id="orgb7158bb">Bayesian Inference</h4>
<div class="outline-text-4" id="text-orgb7158bb">
<p>What we want is to take a document (<i>D</i>) - which is a tweet in this case - and guess its classification \(\hat{c}\). We do this by calculating the probability for both of our classifications (<i>positive</i> and <i>negative</i>) using Bayes' Rule and then choosing the classification with the higher probability.</p>
\begin{align} \hat{c} &amp;= \underset{c \in C}{\mathrm{argmax}} P(c|d)\\ &amp;= \underset{c \in C}{\mathrm{argmax}} P(D|c)P(c)\\ \end{align}
<p>So our guess as to what class the document belongs to is the classification with the highest probability given the document - and "the probability of the classification given the document", when translated using Bayes' Rule, becomes the probability of the document given the classification (the <i>likelihood</i> of the document) times the prior probability of any document belonging to the class. But then you might wonder - if there's only one of each document then won't the probability always be \(\frac{1}{c}\)? It would, so we use the words within the document to calculate the probability for the document. How? Well, I mentioned earlier that we make two assumptions - that the documents can be represented as a bag of words and that they are independent. The independent assumption allows us to figure out the total probability using the Multiplication Rule:</p>
<p>\[ P(A \cap B) = P(A)P(B) \]</p>
<p>The probability of A and B is the product of their probabilities. In this case we are calculating the probability of the document as the product of the conditional probabilities of the words given the class:</p>
<p>\[ P(D|c) = \prod_{i=1}^{n}P(w_i | c) \]</p>
<p>Where the <i>n</i> refers to the number of words in the document. Given this we could re-write the previous equation like this.</p>
\begin{align} \hat{c} &amp;= \underset{c \in C}{\textrm{argmax}} P(c) \prod_{i}^{n} P(w_i | c)\\ \end{align}
<p>But it turns out this form isn't really ideal. Among other things you're multiplying values that range from 0 to 1, with most values being less than 1, so the more classes you have, the smaller this number will get and you could end up with really small numbers leading to <a href="https://www.wikiwand.com/en/Arithmetic_underflow">underflow</a>. So we're going to do a log transform of the equation which will also simplify the computation a little (although nowadays I don't know that that's so much of a consideration).</p>
<p>\[ \hat{c} = \underset{c \in C}{\textrm{argmax}} \log{P(c)} + \sum_{i=1}^n \log{P(w_i|c)} \]</p>
<p>This is what we'll use to classify tweets after training the model by building up the probabilities.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgab446b0">
<h4 id="orgab446b0">Ratios</h4>
<div class="outline-text-4" id="text-orgab446b0">
<p>While I wrote out the general case where you take the class with the highest probability, in this case we only have two classes, <i>positive</i> and <i>negative</i> so we can take advantage of this and make our classification using the ratio of the conditional probabilities for each class (the log <a href="https://www.wikiwand.com/en/Odds_ratio">odds ratio</a>). We're going to use the ratio of positive to negative.</p>
<p>\[ \log{\frac{P(positive|D)}{P(negative | D)}} = \log{\frac{P(positive)}{P(negative)}} + \sum_{i=1}^n \log{\frac{P(w_i|positive)}{P(w_i|negative)}} \]</p>
<p>Since <i>positive</i> is the numerator, and the log of values less than one are negative, this ratio will be positive when the review is likely positive and negative otherwise, so we can use the sign of this ratio to classify tweets.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgfac174e">
<h4 id="orgfac174e">Priors and Log Priors</h4>
<div class="outline-text-4" id="text-orgfac174e">
<p>Now we can start picking apart our ratio. The prior probabilities are just the fraction of our training set that matches a variable. So the prior probabilities of the document classifications can be described like this:</p>
\begin{align} P(D_{positive}) &amp;= \frac{\textit{number of positive tweets}}{\textit{total number of tweets}}\\ &amp;= \frac{D_{pos}}{D}\\ \end{align} \begin{align} P(D_{negative}) &amp;= \frac{\textit{number of negative tweets}}{\textit{total number of tweets}}\\ &amp;= \frac{D_{neg}}{D}\\ \end{align}
<p>But as I noted above we are going to use the ratio of the prior probabilities \(\frac{P(D_{pos})}{P(D_{neg})}\) and if you look at them, they have the same denominator (<i>D</i>) so taking the ratio of the probabilities means the denominator cancels out and we end up with the ratio of the positive to negative documents.</p>
\begin{align} \frac{P(D_{pos})}{P(D_{neg})} &amp;= \frac{\frac{D_{pos}}{D}}{\frac{D_{neg}}{D}}\\ &amp;= \frac{\left( \frac{D_{pos}}{\cancel{D}}\right) \left(\frac{\cancel{D}}{D_{neg}}\right) }{ \cancel{\left(\frac{D_{neg}}{D}\right)} \cancel{\left(\frac{D}{D_{neg}}\right)} }\\ &amp;= \frac{D_{pos}}{D_{neg}}\\ \end{align}
<p>And as I noted above, we'll be using a log transform so our ratio (which will be called <i>logprior</i>) needs to be transformed as well.</p>
\begin{align} \text{logprior} &amp;= log \left( \frac{P(D_{pos})}{P(D_{neg})} \right) \\ &amp;= log \left( \frac{D_{pos}}{D_{neg}} \right)\\ \end{align}
<p>Note that \(log(\frac{A}{B})\) is the same as \(log(A) - log(B)\). So the logprior can also be calculated as the difference between two logs:</p>
\begin{align} \text{logprior} &amp;= \log (P(D_{pos})) - \log (P(D_{neg})) \\ &amp;= \log (D_{pos}) - \log (D_{neg})\\ \end{align}
<p>I don't know that this helps any with computation, but it makes it clearer (to me) that the ratio will be positive when the tweet's sentiment is positive and negative when the sentiment is negative.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgc700cb6">
<h4 id="orgc700cb6">Positive and Negative Word Probabilities</h4>
<div class="outline-text-4" id="text-orgc700cb6">
<p>Now for the second part of our equation. To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:</p>
<ul class="org-ul">
<li>\(freq_{pos} =\) the number of times the word is counted in a document with a label of 1</li>
<li>\(freq_{neg} =\) the number of times the word is counted in a document with a label of 0</li>
<li>\(N_{pos} = \) the number of words in all the positive documents</li>
<li>\(N_{neg} = \) the number of words in all the negative documents</li>
<li><i>V</i> is the number of unique words in the entire set of documents</li>
<li><i>W</i> is a word in a document</li>
</ul>
<p>So now we can re-write our numerator and denominator for the second term.</p>
\begin{align} P(W|positive) &amp;= P(W_{pos})\\ &amp;= \frac{freq_{pos}}{N_{pos}}\\ \end{align} \begin{align} P(W | negative ) &amp;= P(W_{neg})\\ &amp;= \frac{freq_{neg}}{N_{neg}}\\ \end{align}
<p>Meaning that the likelihood of the word given the class is the number of times the word shows up in documents of that class divided by a count of all the unique words in the corpus. One thing to notice, though, is that our numerators have the count for a word within documents labeled with the classification, but it's not guaranteed that all of the words will show up in both classes (the word "horrible" might only show up in the negative tweets, for instance) so if a word shows up in one class but not the other, we might end up with a zero in the numerator or denominator and not only is division by zero not defined, but neither is the logarithm of zero. The solution is to add 1 to the numerator and the size of the vocabulary to the denominator (adding 1 for each word). Besides fixing our arithmetic problem there's some other more mathy reasons for doing this that are explained in this <a href="https://en.wikipedia.org/wiki/Additive_smoothing">wikipedia article</a>.</p>
<p>With those changes we now have:</p>
\begin{align} P(W_{pos}) &amp;= \frac{freq_{pos} + 1}{N_{pos} + V}\\ \end{align} \begin{align} P(W_{neg}) &amp;= \frac{freq_{neg} + 1}{N_{neg} + V}\\ \end{align}
<p>And the log-likelihood term becomes:</p>
\begin{align} \text{loglikelihood} &amp;= \log \left(\frac{P(W_{pos})}{P(W_{neg})} \right)\\ &amp;= \log P(W_{pos}) - \log P(W_{neg})\\ &amp;= \log \frac{freq_{pos} + 1}{N_{pos} + V} - \log \frac{freq_{neg} + 1}{N_{neg} + V} \end{align}</div>
</div>
</div>
<div class="outline-2" id="outline-container-org240e8e2">
<h2 id="org240e8e2">End</h2>
<div class="outline-text-2" id="text-org240e8e2">
<p>Now that we have the math I'm going to implement the model using python in <a href="../naive-bayes-twitter-sentiment-classification/">this post</a>.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/naive-bayes/" rel="tag">naive bayes</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/sentiment-analysis/" rel="tag">sentiment analysis</a></li>
<li><a class="tag p-category" href="../../../categories/twitter/" rel="tag">twitter</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../../bibliographies/text-data-management-and-analysis/" rel="prev" title="Text Data Management and Analysis">Previous post</a></li>
<li class="next"><a href="../naive-bayes-twitter-sentiment-classification/" rel="next" title="Implementing a Naive Bayes Twitter Sentiment Classifier">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
