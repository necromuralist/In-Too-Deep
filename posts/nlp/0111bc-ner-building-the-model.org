#+BEGIN_COMMENT
.. title: NER: Building the Model
.. slug: ner-building-the-model
.. date: 2021-01-13 15:01:26 UTC-08:00
.. tags: lstm,rnn,nlp,ner
.. category: NLP
.. link: 
.. description: Building the NER model with Trax.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-9974ba11-9b71-4b8e-8dc9-4b5779900b41-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  - {{% lancelot title="The First Post" %}}named-entity-recognition{{% /lancelot %}}
  - {{% lancelot title="The Previous Post" %}}ner-data{{% /lancelot %}}
  - {{% lancelot title="The Next Post" %}}ner-training-the-model{{% /lancelot %}}
* Raw
#+begin_example python
# Concretely: 
# 
# * Use the input tensors you built in your data generator
# * Feed it into an Embedding layer, to produce more semantic entries
# * Feed it into an LSTM layer
# * Run the output through a linear layer
# * Run the result through a log softmax layer to get the predicted class for each word.
# 
# Good news! We won't make you implement the LSTM unit drawn above. However, we will ask you to build the model. 
# 
# <a name="ex02"></a>
# ### Exercise 02
# 
# **Instructions:** Implement the initialization step and the forward function of your Named Entity Recognition system.  
# Please utilize help function e.g. `help(tl.Dense)` for more information on a layer
#    
# - [tl.Serial](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26): Combinator that applies layers serially (by function composition).
#     - You can pass in the layers as arguments to `Serial`, separated by commas. 
#     - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))` 
# 
# 
# -  [tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113): Initializes the embedding. In this case it is the dimension of the model by the size of the vocabulary. 
#     - `tl.Embedding(vocab_size, d_feature)`.
#     - `vocab_size` is the number of unique words in the given vocabulary.
#     - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).
#     
# 
# -  [tl.LSTM](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87):`Trax` LSTM layer of size d_model. 
#     - `LSTM(n_units)` Builds an LSTM layer of n_cells.
# 
# 
# 
# -  [tl.Dense](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L28):  A dense layer.
#     - `tl.Dense(n_units)`: The parameter `n_units` is the number of units chosen for this dense layer.  
# 
# 
# - [tl.LogSoftmax](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242): Log of the output probabilities.
#     - Here, you don't need to set any parameters for `LogSoftMax()`.
#  
# 
# **Online documentation**
# 
# - [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators)
# 
# - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)
# 
# -  [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM)
# 
# -  [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)
# 
# - [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax)    

# In[ ]:


# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: NER
def NER(vocab_size=35181, d_model=50, tags=tag_map):
    '''
      Input: 
        vocab_size - integer containing the size of the vocabulary
        d_model - integer describing the embedding size
      Output:
        model - a trax serial model
    '''
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    model = tl.Serial(
      None, # Embedding layer
      None, # LSTM layer
      None, # Dense layer with len(tags) units
      None  # LogSoftmax layer
      )
      ### END CODE HERE ###
    return model


# In[ ]:


# initializing your model
model = NER()
# display your model
print(model)


# **Expected output:**  
# ```
# Serial[
#   Embedding_35181_50
#   LSTM_50
#   Dense_17
#   LogSoftmax
# ]
# ```  
# 


#+end_example
