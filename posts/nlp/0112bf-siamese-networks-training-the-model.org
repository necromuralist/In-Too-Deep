#+BEGIN_COMMENT
.. title: Siamese Networks: Training the Model
.. slug: siamese-networks-training-the-model
.. date: 2021-01-25 19:38:08 UTC-08:00
.. tags: nlp,siamese networks
.. category: NLP
.. link: 
.. description: Training the Siamese Network Model.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-470af90d-2b2e-49aa-aa8b-423a58bb3cea-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
 Now we are going to train the Siamese Network Model model. As usual, we have to define the cost function and the optimizer. We also have to feed in the built model. Before, going into the training, we will use a special data set up. We will define the inputs using the data generator we built above. The lambda function acts as a seed to remember the last batch that was given. Run the cell below to get the question pairs inputs. 
** Imports
#+begin_src python :results none
# pypi
import trax

# this project
from neurotic.nlp.siamese_networks import DataGenerator, DataLoader, TOKENS
#+end_src
** Set Up
*** The Data
#+begin_src python :results none
loader = DataLoader()

data = loader.data
#+end_src
*** The Data generator
#+begin_src python :results output :exports both
batch_size = 256
train_generator = DataGenerator(data.train.question_one, data.train.question_two,
                                batch_size=batch_size)
validation_generator = DataGenerator(data.validate.question_one,
                                     data.validate.question_two,
                                     batch_size=batch_size)
print(f"training question 1 rows: {len(data.train.question_one):,}")
print(f"validation question 1 rows: {len(data.validate.question_one):,}")
#+end_src    

#+RESULTS:
: training question 1 rows: 89,179
: validation question 1 rows: 22,295

* Middle
** Training the Model

We will now write a function that takes in the model and trains it. To train the model we have to decide how many times to iterate over the entire data set; each iteration is defined as an =epoch=. For each epoch, you have to go over all the data, using the training iterator.

 - Create =TrainTask= and =EvalTask=
 - Create the training loop =trax.supervised.training.Loop=
 - Pass in the following depending on the context (train_task or eval_task):
     - =labeled_data=generator=
     - =metrics=[TripletLoss()]=,
     - =loss_layer=TripletLoss()=
     - =optimizer=trax.optimizers.Adam= with learning rate of 0.01
     - =lr_schedule=lr_schedule=,
     - =output_dir=output_dir=
 
 
We will be using the triplet loss function with Adam optimizer. Please read the [[https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=adam#trax.optimizers.adam.Adam][trax Adam]] documentation to get a full understanding. 

This function should return a =training.Loop= object. To read more about this check the [[https://trax-ml.readthedocs.io/en/latest/trax.supervised.html?highlight=loop#trax.supervised.training.Loop][training.Loop]] documentation.

#+begin_src python :results none
lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)
#+end_src

#+begin_src python :results none
def train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir='model/'):
    """Training the Siamese Model

    Args:
        Siamese (function): Function that returns the Siamese model.
        TripletLoss (function): Function that defines the TripletLoss loss function.
        lr_schedule (function): Trax multifactor schedule function.
        train_generator (generator, optional): Training generator. Defaults to train_generator.
        val_generator (generator, optional): Validation generator. Defaults to val_generator.
        output_dir (str, optional): Path to save model to. Defaults to 'model/'.

    Returns:
        trax.supervised.training.Loop: Training loop for the model.
    """
    output_dir = os.path.expanduser(output_dir)

    ### START CODE HERE (Replace instances of 'None' with your code) ###

    train_task = training.TrainTask(
        labeled_data=None,       # Use generator (train)
        loss_layer=None,         # Use triplet loss. Don't forget to instantiate this object
        optimizer=None,          # Don't forget to add the learning rate parameter
        lr_schedule=lr_schedule, # Use Trax multifactor schedule function
    )

    eval_task = training.EvalTask(
        labeled_data=None,       # Use generator (val)
        metrics=[None],          # Use triplet loss. Don't forget to instantiate this object
    )
    
    ### END CODE HERE ###

    training_loop = training.Loop(Siamese(),
                                  train_task,
                                  eval_task=eval_task,
                                  output_dir=output_dir)

    return training_loop
#+end_src

#+begin_src python :results output :exports both
train_steps = 5
training_loop = train_model(Siamese, TripletLoss, lr_schedule)
training_loop.run(train_steps)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: NameError                                 Traceback (most recent call last)
: <ipython-input-15-b8ba7e43cd69> in <module>
:       1 train_steps = 5
: ----> 2 training_loop = train_model(Siamese, TripletLoss, lr_schedule)
:       3 training_loop.run(train_steps)
: 
: NameError: name 'train_model' is not defined
:END:


# The model was only trained for 5 steps due to the constraints of this environment. For the rest of the assignment you will be using a pretrained model but now you should understand how the training can be done using Trax.

# <a name='4'></a>
# 
