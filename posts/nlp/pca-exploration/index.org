#+BEGIN_COMMENT
.. title: PCA Exploration
.. slug: pca-exploration
.. date: 2020-10-01 17:53:17 UTC-07:00
.. tags: pca,nlp,visualization
.. category: Visualization
.. link: 
.. description: Visualizing word vectors with PCA.
.. type: text
.. has_math: True

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-b4d8ee67-7cb7-4dec-9d79-b543e6f5204b-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
** Set Up
*** Imports
#+begin_src python :results none
# python
from argparse import Namespace
from functools import partial

import math
import random

# pypi
from numpy.random import default_rng
from sklearn.decomposition import PCA

import holoviews
import hvplot.pandas
import numpy
import pandas

# my stuff
from graeae import EmbedHoloviews
#+end_src
*** Plotting
#+begin_src python :results none
SLUG = "pca-exploration"
Embed = partial(EmbedHoloviews,
                folder_path=f"files/posts/nlp/{SLUG}")

Plot = Namespace(
    width=990,
    height=780,
    fontscale=2,
    tan="#ddb377",
    blue="#4687b7",
    red="#ce7b6d",
    color_cycle = holoviews.Cycle(["#4687b7", "#ce7b6d"])
)
#+end_src
*** Randomness
#+begin_src python :results none
numpy_random = default_rng()
#+end_src
* Middle
** Some Random Uniform Data
   We're going to create a data set generated by numpy's [[https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html][uniform]] function, which takes three arguments - =low=, =high=, and =size=.

#+begin_src python :results none
correlation = 1
x = numpy_random.uniform(1, 2, 1000)
y = x.copy()
#+end_src
*** Center It
    We're going to center it around the mean.
#+begin_src python :results none
x -= numpy.mean(x)
y -= numpy.mean(y)
#+end_src

*** Pandas and a Plot
#+begin_src python :results none
data = pandas.DataFrame(dict(x=x, y=y))
#+end_src

    We're going to use [[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html][sklearn's PCA]] for Principal Component Analysis. The =n_components= argument is the number of components it will keep -we'll keep 2.
#+begin_src python :results none
pca = PCA(n_components=2)
#+end_src

Now fit it to the data.

#+begin_src python :results none
transformation_model = pca.fit(data)
#+end_src

And then transform it (note that there's a [[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform][fit_transform]] method to do it in one step).

#+begin_src python :results none
pca_data = pandas.DataFrame(
    transformation_model.transform(data),
    columns=["Principal Component 1", "Principal Component 2"])
#+end_src

Now Plot it.

#+begin_src python :results none
original = data.hvplot.scatter(x="x", y="y", color=Plot.blue)
transformed = pca_data.hvplot.scatter(x="Principal Component 1", y="Principal Component 2", color=Plot.red)
plot = (original * transformed).opts(
    title="Correlated and PCA Data",
    height=Plot.height,
    width=Plot.width,
    fontscale=Plot.fontscale,
)

outcome = Embed(plot=plot, file_name="correlated_data")()
#+end_src

#+begin_src python :results output html :exports both
print(outcome)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="correlated_data.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

Exciting. So we've taken a 2-D line and rotated it.
*** Understanding the Model
The Eigenvectors (principal component).

#+begin_src python :results output :exports both
print(transformation_model.components_)
#+end_src

#+RESULTS:
: [[ 0.70710678  0.70710678]
:  [-0.70710678  0.70710678]]

The \(cos(45^o)\) is =0.7071= which is what you see as values for the Eigenvectors. So you can interpret this [[https://www.wikiwand.com/en/Rotation_matrix][rotation matrix]] as:

\[
R = \begin{bmatrix} cos(45^o) & sin(45^o)\\
-sin(45^o) & cos(45^o)\\
\end{bmatrix}
\]

Why \(45^o\)? If you look at the blue line in the plot (or think about the fact that \(x=y\)), our original line sits at a 45 degree angle so applying this transformation rotates it flat.

The Eigenvalues (explained variance).

#+begin_src python :results output :exports both
print(transformation_model.explained_variance_)
#+end_src

#+RESULTS:
: [1.59912782e-01 7.31437644e-33]

The equation for [[https://www.dummies.com/education/math/business-statistics/how-to-calculate-the-variance-and-standard-deviation-in-the-uniform-distribution/][variance or a uniform distribution]] is:

\[
Var = \frac{(b - a)^2}{12}
\]

When we called the =uniform= function we set =b= to 2, and =a= to 1, so we get.

#+begin_src python :results output :exports both
print((2 - 1)**2/12)
#+end_src

#+RESULTS:
: 0.08333333333333333

If you look at the Eigenvalues we got, the second term is \(7 \times 10^{-33}\) which is pretty much zero, and the second term is about =0.16=, so what we have here is.

\begin{align}
Var &= \langle Var(x) + Var(y), 0\rangle\\
    &= \langle 0.083 + 0.083, 0 \rangle\\
    &= \langle 0.16, 0 \rangle\\
\end{align}

It rounds more to 0.167, but close enough, the point is that the first component contributed all the variance and the second didn't contribute any.

** Normal Random Data
   I'm going to use numpy's random [[https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html][normal]] function to generate the data. The three arguments it takes are =loc= (the mean), =scale= (the standard deviation), and =size= (the number of numbers to generate).

#+begin_src python :results none
standard_deviation_1 = 1
standard_deviation_2 = 0.333
points = 10**3

x = numpy_random.normal(0, standard_deviation_1, points)
y = numpy_random.normal(0, standard_deviation_2, points)
#+end_src

Even though we specify that the mean is 0, because it is random it isn't exactly zero so we should center it.

#+begin_src python :results output :exports both
print(f"x mean start: {x.mean()}")
print(f"y mean start: {y.mean()}")
x = x - x.mean()
y = y - y.mean()

print(f"\nx mean: {x.mean()}")
print(f"y mean: {y.mean()}")
#+end_src

#+RESULTS:
: x mean start: -0.009698529455667203
: y mean start: 0.013492715633367562
: 
: x mean: 1.7763568394002505e-17
: y mean: 3.552713678800501e-18

#+begin_src python :results none
data = pandas.DataFrame(dict(x=x, y=y))
plot = data.hvplot.scatter(x="x", y="y").opts(
    title="Random Normal Data",
    height=Plot.height,
    width=Plot.width,
    fontscale=Plot.fontscale,
    color=Plot.blue,
)
outcome = Embed(plot=plot, file_name="random_normal_data")()
#+end_src

#+begin_src python :results output html :exports both
print(outcome)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="random_normal_data.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

Now we're going to put the =x= and =y= data into a matrix and rotate it.

#+begin_src python :results output :exports both
covariance = 1
angle = numpy.arctan(1/covariance)
print(f"angle: {angle * 180/math.pi}\n")

rotation_matrix = numpy.array([[numpy.cos(angle), numpy.sin(angle)],
                               [-numpy.sin(angle), numpy.cos(angle)]])
print(rotation_matrix)
#+end_src

#+RESULTS:
: angle: 45.0
: 
: [[ 0.70710678  0.70710678]
:  [-0.70710678  0.70710678]]

You might notice that this is the same rotation matrix that we had before with the sklearn eigenvectors, so we could have used that, but this is how you would roll your own.

Now we can apply the rotation by taking the dot-product between the data array and the rotatiot-matrix.

#+begin_src python :results none
rotated = data.dot(rotation_matrix)
rotated.columns = ["x", "y"]
plot = rotated.hvplot.scatter(x="x", y="y").opts(
    title="Rotated Normal Data",
    width=Plot.width,
    height=Plot.height,
    fontscale=Plot.fontscale,
    color=Plot.blue,
)
outcome = Embed(plot=plot, file_name="rotated_normal_data")()
#+end_src

#+begin_src python :results output html :exports both
print(outcome)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="rotated_normal_data.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

Now we can apply the PCA.

#+begin_src python :results none
pca = PCA(n_components=2)
fitted = pca.fit(rotated)
#+end_src

Once again, the Eigenvectors (the transformation matirix).
#+begin_src python :results output :exports both
print(fitted.components_)
#+end_src

#+RESULTS:
: [[-0.71060471 -0.70359146]
:  [-0.70359146  0.71060471]]

And then the Eigenvalues (the variance).

#+begin_src python :results output :exports both
variance = fitted.explained_variance_
print(variance)
#+end_src

#+RESULTS:
: [0.98287571 0.10862302]

Now we apply the PCA and plot it.

#+begin_src python :results none
pca_data = fitted.transform(rotated)
pca_data = pandas.DataFrame(pca_data, columns="x y".split())
#+end_src

#+begin_src python :results none
FIRST_ROW, SECOND_ROW = 0, 1
FIRST_COLUMN, SECOND_COLUMN = 0, 1
ORIGIN = 0
FIRST_SCALAR, SECOND_SCALAR = (standard_deviation_1 * 3,
                               standard_deviation_2 * 3)
COLUMNS = "x y".split()

first_axis = pandas.DataFrame([
    [ORIGIN, ORIGIN],
    rotation_matrix[FIRST_ROW][:]],
                              columns=COLUMNS)
first_axis *= FIRST_SCALAR


second_axis = pandas.DataFrame([
    [ORIGIN, ORIGIN],
    rotation_matrix[SECOND_ROW][:]],
                               columns=COLUMNS)
second_axis *= SECOND_SCALAR
#+end_src

#+begin_src python :results none
transformed = pca_data.hvplot.scatter(x="x", y="y", color=Plot.red, fill_alpha=0)
rotated_plot = rotated.hvplot.scatter(x="x", y="y", color=Plot.blue, fill_alpha=0)
first_axis_plot = first_axis.hvplot(x="x", y="y", color="red")
second_axis_plot = second_axis.hvplot(x="x", y="y", color="orange")

plot = (transformed * rotated_plot * first_axis_plot * second_axis_plot).opts(
    title="PCA of Random Normal Data",
    width=Plot.width,
    height=Plot.height,
    fontscale=Plot.fontscale
)

outcome = Embed(plot=plot, file_name="pca_random_normal")()
#+end_src

#+begin_src python :results output html :exports both
print(outcome)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="pca_random_normal.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

 - The rotation matrix took the original uncorrelated variables and transformed them into correllated variables (the red circles).
 - Fitting the PCA to our correlated finds the rotation matrix that was used to create the blue points.
 - Applying the PCA transformation undoes the rotation

The Explained Variance is roughly our original standard deviations squared.

#+begin_src python :results output :exports both
print(numpy.sqrt(variance))
#+end_src

#+RESULTS:
: [0.99140088 0.32958007]
** Dimensionality Reduction
   The previous sections were meant to understand what PCA is doing, but to use the PCA for visualization we will use it to reduce the number of dimensions of a data set so that it can be plotted. First let's look at our rotated data with the dimensions separated.

#+begin_src python :results none
first_component = rotated.copy()
first_component["y"] = 0
second_component = rotated.copy()
second_component["x"] = 0

original = rotated.hvplot.scatter(x="x", y="y", color=Plot.tan,
                                  fill_alpha=0)
first = first_component.hvplot.scatter(x="x", y="y",
                                       color=Plot.blue, fill_alpha=0)
second = second_component.hvplot.scatter(x="x", y="y",
                                         color=Plot.red, fill_alpha=0)

plot = (original * first * second).opts(
    title="Data Decomposition",
    width=Plot.width,
    height=Plot.height,
    fontscale=Plot.fontscale
)
outcome = Embed(plot=plot, file_name="data_decomposed")()
#+end_src

#+begin_src python :results output html :exports both
print(outcome)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="data_decomposed.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

This is only a teaser to doing an actual dimensionality reduction. 
