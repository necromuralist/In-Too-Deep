#+BEGIN_COMMENT
.. title: Twitter Logistic Regression Visualization
.. slug: twitter-logistic-regression
.. date: 2020-07-10 23:08:03 UTC-07:00
.. tags: nlp,twitter,logistic regression,sentiment analysis
.. category: NLP
.. link: 
.. description: Creating a Logistic Regression Model to predict tweet sentiment.
.. type: text
.. updated: 2020-07-23 23:08:03 UTC-07:00
#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-8f00099f-5408-4a3c-aefe-d889b96e7923.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  Having created our logistic regression model for tweet sentiment we're going to visualize what it's doing.
** Set Up
*** Imports
#+begin_src python :results none
# from python
from argparse import Namespace
from functools import partial
from pathlib import Path

import os

# from pypi
from bokeh.models.tools import HoverTool
from dotenv import load_dotenv
from nltk.corpus import twitter_samples 

import holoviews
import hvplot.pandas
import pandas

# Some helper code
from graeae import EmbedHoloviews
#+end_src
*** Plotting
    Some constants for the plotting.
#+begin_src python :results none
SLUG = "twitter-logistic-regression"
Embed = partial(EmbedHoloviews,
                folder_path=f"files/posts/nlp/{SLUG}")

Plot = Namespace(
    width=990,
    height=780,
    tan="#ddb377",
    blue="#4687b7",
    red="#ce7b6d",
    font_scale=2,
    color_cycle = holoviews.Cycle(["#4687b7", "#ce7b6d"])
)
#+end_src
** The Tweet Vectors
   In an earlier post we built a dictionary-like set to count the number of times each token was in a positive tweet and in a negative tweet. To represent a tweet as a vector for training the model you then sum the total counts for the tokens in the tweet when they are positive and when they are positive. 

Come again?

Lets say you have a tweet ="a b c"= which tokenizes to =a, b, c=. Look up the positive and negative tweet counts for each token and then add them:

| Token | Positive | Negative |
|-------+----------+----------|
| a     |        1 |        4 |
| b     |        2 |        5 |
| c     |        3 |        6 |
|-------+----------+----------|
| Total |        6 | 15       |

So to represent this tweet you would create a vector of the form:

\begin{align}
\hat{v} &= \langle bias, positive, negative \rangle\\
&= \langle 1, 6, 15\rangle\\
\end{align}

**Note:** The bias is always one (it just is).

We're skipping the step where we actually create the vectors and just loading a prepared set.

#+begin_src python :results output :exports both
load_dotenv("posts/nlp/.env")
FEATURES = Path(os.environ["TWITTER_REGRESSION_DATA"]).expanduser()
data = pandas.read_csv(FEATURES)
print(data.iloc[0])
#+end_src

#+RESULTS:
: bias            1
: positive     3117
: negative       67
: sentiment       1
: Name: 0, dtype: int64

This first row is the vector representation for a tweet that I was talking about (plus a label column). The =bias= is always one, the =positive= value is the sum of the positive tweet counts for each token in the tweet and the =negative= values is the sum of the negative tweet counts for each of the tokens.

Since we're using this for plotting I'm going to make the =sentiment= into a string column to make it easier to interpret.

#+begin_src python :results output :exports both
sentiment = {
    0: "Negative",
    1:"Positive"
}
data.loc[:, "sentiment"] = data.sentiment.map(sentiment)

print(data.sentiment.value_counts())

#+end_src

#+RESULTS:
: Positive    4000
: Negative    4000
: Name: sentiment, dtype: int64

If you followed the previous post you can probably figure out that this is the training set.

*** The Weights
    Since I saved the weights for our Logistic Regression model we can load it now

#+begin_src python :results output :exports both
weights_path = Path(os.environ["TWITTER_REGRESSION_WEIGHTS"]).expanduser()
assert weights_path.is_file()
theta = pandas.read_csv(weights_path)
print(theta)

# 'weights' is just something to make it easier to remember which column is which
Weights = Namespace(
    bias=0,
    positive=1,
    negative=2
)
#+end_src

#+RESULTS:
:            bias  positive  negative
: 0  6.369479e-08  0.000537 -0.000558

* Middle
** Plot The Vectors
   We can plot the positive vs negative counts for each tweet to see how correlated they seem to be.

#+begin_src python :results none
hover = HoverTool(
    tooltips = [
        ("Positive", "@positive{0,0}"),
        ("Negative", "@negative{0,0}"),
        ("Sentiment", "@sentiment"),
    ]
)

plot = data.hvplot.scatter(x="positive", y="negative", by="sentiment",
                           color=Plot.color_cycle, tools=[hover]).opts(
                               height=Plot.height,
                               width=Plot.width,
                               fontscale=Plot.font_scale,
                               title="Positive vs Negative",
                           )

output = Embed(plot=plot, file_name="positive_negative_scatter")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="positive_negative_scatter.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

Looking at the plot you can see that representing the tweets this way seems to have created a fairly separable dataset (although there's some mixing when the counts are low).
*** Add the Model
    Since we've been given the model's weights we can plot its output when fed the vectors to see how it separates the data. To get the equation for the separation line we need to solve for the positive or negative terms when the product of the weights and the vector is 0 ($\theta \times x = 0$, where /x/ is our vector $\langle bias, positive, negative \rangle$).

Get ready for some algebra.

\begin{align}
\theta \times x &= 0\\
\theta \times \langle bias, positive, negative \rangle &= 0\\
\theta \times \langle 1, positive, negative \rangle &= 0\\
\theta_0 + \theta_1 \times positive + \theta_2 \times negative &= 0\\
\theta_2 \times negative &= -\theta_0 - \theta_1 \times positive\\
negative &= \frac{-\theta_0 - \theta_1 \times positive}{\theta_2}\\
\end{align}

This is the equation for our separation line (on our plot =positive= is the /x-axis/ and =negative= is the /y-axis/, which we can translate to a function to apply to our data.

#+begin_src python :results none
def negative(theta: list, positive: float) -> float:
    """Calculate the negative value

    This calculates the value for the separation line

    Args:
     theta: list of weights for the logistic regression
     positive: count of positive tweets matching tweet

    Returns:
     the calculated negative value for the separation line
    """
    return (-theta.bias
            - positive * theta.positive)/theta.negative

negative_ = partial(negative, theta=theta)
#+end_src
*** Plot Again
    So now we can plot the separation live with our data

#+begin_src python :results none
data["regression negative"] = data.positive.apply(lambda positive: negative_(positive=positive))
scatter = data.hvplot.scatter(x="positive", y="negative", by="sentiment", color=Plot.color_cycle)

most_positive = data.positive.max()
line = data.hvplot(x="positive", y="regression negative", color="gray")
plot = (scatter * line).opts(
    height=Plot.height,
    width=Plot.width,
    fontscale=Plot.font_scale,
    title="Positive vs Negative",
)
output = Embed(plot=plot, file_name="positive_negative_separated")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="positive_negative_separated.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

So, the model basically creates a diagonal line that separates the positive and negative tweets.
* End
And that's it, not a lot here, just an intuitive look at the model and a demonstration of how this representation of the tweets makes them easily separable.
