#+BEGIN_COMMENT
.. title: Parts-of-Speech Tagging: Hidden Markov Model
.. slug: parts-of-speech-tagging-hidden-markov-model
.. date: 2020-11-19 17:26:36 UTC-08:00
.. tags: nlp,pos tagging,hidden markov model
.. category: NLP
.. link: 
.. description: Implementing the Hidden Markov Model for Part-of-Speech tagging.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-028649ef-067e-46f6-951a-51d4f5ecb339-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Raw
#+begin_example
# # Part 2: Hidden Markov Models for POS
# 
# Now you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder
# - The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization. 
# - In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc. 
# - By completing this part of the assignment you will get a 95% accuracy on the same dataset you used in Part 1.
# 
# The Markov Model contains a number of states and the probability of transition between those states. 
# - In this case, the states are the parts-of-speech. 
# - A Markov Model utilizes a transition matrix, `A`. 
# - A Hidden Markov Model adds an observation or emission matrix `B` which describes the probability of a visible observation when we are in a particular state. 
# - In this case, the emissions are the words in the corpus
# - The state, which is hidden, is the POS tag of that word.

# <a name='2.1'></a>
# ## Part 2.1 Generating Matrices
# 
# ### Creating the 'A' transition probabilities matrix
# Now that you have your `emission_counts`, `transition_counts`, and `tag_counts`, you will start implementing the Hidden Markov Model. 
# 
# This will allow you to quickly construct the 
# - `A` transition probabilities matrix.
# - and the `B` emission probabilities matrix. 
# 
# You will also use some smoothing when computing these matrices. 
# 
# Here is an example of what the `A` transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):
# 
# 
# |**A**  |...|         RBS  |          RP  |         SYM  |      TO  |          UH|...
# | --- ||---:-------------| ------------ | ------------ | -------- | ---------- |----
# |**RBS**  |...|2.217069e-06  |2.217069e-06  |2.217069e-06  |0.008870  |2.217069e-06|...
# |**RP**   |...|3.756509e-07  |7.516775e-04  |3.756509e-07  |0.051089  |3.756509e-07|...
# |**SYM**  |...|1.722772e-05  |1.722772e-05  |1.722772e-05  |0.000017  |1.722772e-05|...
# |**TO**   |...|4.477336e-05  |4.472863e-08  |4.472863e-08  |0.000090  |4.477336e-05|...
# |**UH**  |...|1.030439e-05  |1.030439e-05  |1.030439e-05  |0.061837  |3.092348e-02|...
# | ... |...| ...          | ...          | ...          | ...      | ...        | ...
# 
# Note that the matrix above was computed with smoothing. 
# 
# Each cell gives you the probability to go from one part of speech to another. 
# - In other words, there is a 4.47e-8 chance of going from parts-of-speech `TO` to `RP`. 
# - The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.
# 
# The smoothing was done as follows: 
# 
# $$ P(t_i | t_{i-1}) = \frac{C(t_{i-1}, t_{i}) + \alpha }{C(t_{i-1}) +\alpha * N}\tag{3}$$
# 
# - $N$ is the total number of tags
# - $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in `transition_counts` dictionary.
# - $C(t_{i-1})$ is the count of the previous POS in the `tag_counts` dictionary.
# - $\alpha$ is a smoothing parameter.

# <a name='ex-03'></a>
# ### Exercise 03
# 
# **Instructions:** Implement the `create_transition_matrix` below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix `A`. 

# In[ ]:


# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: create_transition_matrix
def create_transition_matrix(alpha, tag_counts, transition_counts):
    ''' 
    Input: 
        alpha: number used for smoothing
        tag_counts: a dictionary mapping each tag to its respective count
        transition_counts: transition count for the previous word and tag
    Output:
        A: matrix of dimension (num_tags,num_tags)
    '''
    # Get a sorted list of unique POS tags
    all_tags = sorted(tag_counts.keys())
    
    # Count the number of unique POS tags
    num_tags = len(all_tags)
    
    # Initialize the transition matrix 'A'
    A = np.zeros((num_tags,num_tags))
    
    # Get the unique transition tuples (previous POS, current POS)
    trans_keys = set(transition_counts.keys())
    
    ### START CODE HERE (Replace instances of 'None' with your code) ### 
    
    # Go through each row of the transition matrix A
    for i in range(num_tags):
        
        # Go through each column of the transition matrix A
        for j in range(num_tags):

            # Initialize the count of the (prev POS, current POS) to zero
            count = 0
        
            # Define the tuple (prev POS, current POS)
            # Get the tag at position i and tag at position j (from the all_tags list)
            key = None

            # Check if the (prev POS, current POS) tuple 
            # exists in the transition counts dictionary
            if None: #complete this line
                
                # Get count from the transition_counts dictionary 
                # for the (prev POS, current POS) tuple
                count = None
                
            # Get the count of the previous tag (index position i) from tag_counts
            count_prev_tag = None
            
            # Apply smoothing using count of the tuple, alpha, 
            # count of previous tag, alpha, and total number of tags
            A[i,j] = None

    ### END CODE HERE ###
    
    return A


# In[ ]:


alpha = 0.001
A = create_transition_matrix(alpha, tag_counts, transition_counts)
# Testing your function
print(f"A at row 0, col 0: {A[0,0]:.9f}")
print(f"A at row 3, col 1: {A[3,1]:.4f}")

print("View a subset of transition matrix A")
A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )
print(A_sub)


# ##### Expected Output
# ```CPP
# A at row 0, col 0: 0.000007040
# A at row 3, col 1: 0.1691
# View a subset of transition matrix A
#               RBS            RP           SYM        TO            UH
# RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06
# RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07
# SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05
# TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05
# UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02
# ```

# ### Create the 'B' emission probabilities matrix
# 
# Now you will create the `B` transition matrix which computes the emission probability. 
# 
# You will use smoothing as defined below: 
# 
# $$P(w_i | t_i) = \frac{C(t_i, word_i)+ \alpha}{C(t_{i}) +\alpha * N}\tag{4}$$
# 
# - $C(t_i, word_i)$ is the number of times $word_i$ was associated with $tag_i$ in the training data (stored in `emission_counts` dictionary).
# - $C(t_i)$ is the number of times $tag_i$ was in the training data (stored in `tag_counts` dictionary).
# - $N$ is the number of words in the vocabulary
# - $\alpha$ is a smoothing parameter. 
# 
# The matrix `B` is of dimension (num_tags, N), where num_tags is the number of possible parts-of-speech tags. 
# 
# Here is an example of the matrix, only a subset of tags and words are shown: 
# <p style='text-align: center;'> <b>B Emissions Probability Matrix (subset)</b>  </p>
# 
# |**B**| ...|          725 |     adroitly |    engineers |     promoted |      synergy| ...|
# |----|----|--------------|--------------|--------------|--------------|-------------|----|
# |**CD**  | ...| **8.201296e-05** | 2.732854e-08 | 2.732854e-08 | 2.732854e-08 | 2.732854e-08| ...|
# |**NN**  | ...| 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | 7.521128e-09 | **2.257091e-05**| ...|
# |**NNS** | ...| 1.670013e-08 | 1.670013e-08 |**4.676203e-04** | 1.670013e-08 | 1.670013e-08| ...|
# |**VB**  | ...| 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08 | 3.779036e-08| ...|
# |**RB**  | ...| 3.226454e-08 | **6.456135e-05** | 3.226454e-08 | 3.226454e-08 | 3.226454e-08| ...|
# |**RP**  | ...| 3.723317e-07 | 3.723317e-07 | 3.723317e-07 | **3.723317e-07** | 3.723317e-07| ...|
# | ...    | ...|     ...      |     ...      |     ...      |     ...      |     ...      | ...|
# 
# 

# <a name='ex-04'></a>
# ### Exercise 04
# **Instructions:** Implement the `create_emission_matrix` below that computes the `B` emission probabilities matrix. Your function takes in $\alpha$, the smoothing parameter, `tag_counts`, which is a dictionary mapping each tag to its respective count, the `emission_counts` dictionary where the keys are (tag, word) and the values are the counts. Your task is to output a matrix that computes equation 4 for each cell in matrix `B`. 

# In[ ]:


# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: create_emission_matrix

def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):
    '''
    Input: 
        alpha: tuning parameter used in smoothing 
        tag_counts: a dictionary mapping each tag to its respective count
        emission_counts: a dictionary where the keys are (tag, word) and the values are the counts
        vocab: a dictionary where keys are words in vocabulary and value is an index.
               within the function it'll be treated as a list
    Output:
        B: a matrix of dimension (num_tags, len(vocab))
    '''
    
    # get the number of POS tag
    num_tags = len(tag_counts)
    
    # Get a list of all POS tags
    all_tags = sorted(tag_counts.keys())
    
    # Get the total number of unique words in the vocabulary
    num_words = len(vocab)
    
    # Initialize the emission matrix B with places for
    # tags in the rows and words in the columns
    B = np.zeros((num_tags, num_words))
    
    # Get a set of all (POS, word) tuples 
    # from the keys of the emission_counts dictionary
    emis_keys = set(list(emission_counts.keys()))
    
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    
    # Go through each row (POS tags)
    for i in None: # complete this line
        
        # Go through each column (words)
        for j in None: # complete this line

            # Initialize the emission count for the (POS tag, word) to zero
            count = 0
                    
            # Define the (POS tag, word) tuple for this row and column
            key =  None

            # check if the (POS tag, word) tuple exists as a key in emission counts
            if None: # complete this line
        
                # Get the count of (POS tag, word) from the emission_counts d
                count = None
                
            # Get the count of the POS tag
            count_tag = None
                
            # Apply smoothing and store the smoothed value 
            # into the emission matrix B for this row and column
            B[i,j] = None

    ### END CODE HERE ###
    return B


# In[ ]:


# creating your emission probability matrix. this takes a few minutes to run. 
B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))

print(f"View Matrix position at row 0, column 0: {B[0,0]:.9f}")
print(f"View Matrix position at row 3, column 1: {B[3,1]:.9f}")

# Try viewing emissions for a few words in a sample dataframe
cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']

# Get the integer ID for each word
cols = [vocab[a] for a in cidx]

# Choose POS tags to show in a sample dataframe
rvals =['CD','NN','NNS', 'VB','RB','RP']

# For each POS tag, get the row number from the 'states' list
rows = [states.index(a) for a in rvals]

# Get the emissions for the sample of words, and the sample of POS tags
B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )
print(B_sub)


# ##### Expected Output
# 
# ```CPP
# View Matrix position at row 0, column 0: 0.000006032
# View Matrix position at row 3, column 1: 0.000000720
#               725      adroitly     engineers      promoted       synergy
# CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08
# NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05
# NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08
# VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08
# RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08
# RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07
# ```
#+end_example
