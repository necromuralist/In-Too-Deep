#+BEGIN_COMMENT
.. title: Twitter Preprocessing With NLTK
.. slug: twitter-preprocessing-with-nltk
.. date: 2020-07-03 21:23:48 UTC-07:00
.. tags: nlp,nltk,twitter,preprocessing
.. category: Data Preprocessing
.. link: 
.. description: Preprocessing twitter tweets with NLTK.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-5ad9c362-4446-4c87-8aed-8772124bdb58.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
 This is a look at taking a corpus of [[https://twitter.com/explore][Twitter]] data gathered as part of the [[https://www.nltk.org/][Natural Language Toolkit (NLTK)]] and creating a preprocessor for a [[https://www.wikiwand.com/en/Sentiment_analysis][Sentiment Analysis]] pipeline. This dataset has entries whose sentiment was categorized by hand so it's a convenient source for training models.

The [[https://www.nltk.org/howto/corpus.html][NLTK Corpus How To]] has a brief description of the Twitter dataset and they also have [[https://www.nltk.org/howto/twitter.html][some documentation]] about how to gather new data using the Twitter API yourself.

** Set Up
*** Imports
#+begin_src python :results none
# from python
from argparse import Namespace
from functools import partial
from pathlib import Path
from pprint import pprint

import random
import re
import string

# from pypi
from nltk.corpus import stopwords
from nltk.corpus import twitter_samples
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer

import hvplot.pandas
import nltk
import pandas

# my stuff
from graeae import CountPercentage, EmbedHoloviews
#+end_src
*** Data
    The first thing to do is download the dataset using the [[https://www.nltk.org/data.html][download]] function. If you don't pass an argument to it a dialog will open and you can choose to download any or all of their datasets, but for this exercise we'll just download the Twitter samples. Note that if you run this function and the samples were already downloaded then it won't re-download them so it's safe to call it in any case.

#+begin_src python :results none
nltk.download('twitter_samples')
#+end_src

The data is contained in three files. You can see the file names using the =twitter_samples.fileids= function.

#+begin_src python :results output :exports both
for name in twitter_samples.fileids():
    print(f" - {name}")
#+end_src

#+RESULTS:
:  - negative_tweets.json
:  - positive_tweets.json
:  - tweets.20150430-223406.json

As you can see (or maybe guess) two of the files contain tweets that have been categorized as negative or positive. The third file has another 20,000 tweets that aren't classified.

If you want to work with the files directly without using the NLTK interface (or move or delete them) you can use the =twitter_samples.abspaths= function to see where they are.

#+begin_src python :results output :exports both
pprint(twitter_samples.abspaths())
#+end_src

#+RESULTS:
: [FileSystemPathPointer('/home/athena/nltk_data/corpora/twitter_samples/negative_tweets.json'),
:  FileSystemPathPointer('/home/athena/nltk_data/corpora/twitter_samples/positive_tweets.json'),
:  FileSystemPathPointer('/home/athena/nltk_data/corpora/twitter_samples/tweets.20150430-223406.json')]

The dataset contains the JSON for each tweet, including some metadata, which you can access through the =twitter_samples.docs= function. Here's a sample.

#+begin_src python :results output :exports both
pprint(twitter_samples.docs()[0])
#+end_src

#+RESULTS:
#+begin_example
{'contributors': None,
 'coordinates': None,
 'created_at': 'Fri Jul 24 10:42:49 +0000 2015',
 'entities': {'hashtags': [], 'symbols': [], 'urls': [], 'user_mentions': []},
 'favorite_count': 0,
 'favorited': False,
 'geo': None,
 'id': 624530164626534400,
 'id_str': '624530164626534400',
 'in_reply_to_screen_name': None,
 'in_reply_to_status_id': None,
 'in_reply_to_status_id_str': None,
 'in_reply_to_user_id': None,
 'in_reply_to_user_id_str': None,
 'is_quote_status': False,
 'lang': 'en',
 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},
 'place': None,
 'retweet_count': 0,
 'retweeted': False,
 'source': '<a href="https://mobile.twitter.com" rel="nofollow">Mobile Web '
           '(M2)</a>',
 'text': 'hopeless for tmr :(',
 'truncated': False,
 'user': {'contributors_enabled': False,
          'created_at': 'Sun Mar 08 05:43:40 +0000 2015',
          'default_profile': False,
          'default_profile_image': False,
          'description': '⇨ [V] TravelGency █ 2/4 Goddest from Girls Day █ 92L '
                         '█ sucrp',
          'entities': {'description': {'urls': []}},
          'favourites_count': 196,
          'follow_request_sent': False,
          'followers_count': 1281,
          'following': False,
          'friends_count': 1264,
          'geo_enabled': True,
          'has_extended_profile': False,
          'id': 3078803375,
          'id_str': '3078803375',
          'is_translation_enabled': False,
          'is_translator': False,
          'lang': 'id',
          'listed_count': 3,
          'location': 'wearegsd;favor;pucukfams;barbx',
          'name': 'yuwra ✈ ',
          'notifications': False,
          'profile_background_color': '000000',
          'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/585476378365014016/j1mvQu3c.png',
          'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/585476378365014016/j1mvQu3c.png',
          'profile_background_tile': True,
          'profile_banner_url': 'https://pbs.twimg.com/profile_banners/3078803375/1433287528',
          'profile_image_url': 'http://pbs.twimg.com/profile_images/622631732399898624/kmYsX_k1_normal.jpg',
          'profile_image_url_https': 'https://pbs.twimg.com/profile_images/622631732399898624/kmYsX_k1_normal.jpg',
          'profile_link_color': '000000',
          'profile_sidebar_border_color': '000000',
          'profile_sidebar_fill_color': '000000',
          'profile_text_color': '000000',
          'profile_use_background_image': True,
          'protected': False,
          'screen_name': 'yuwraxkim',
          'statuses_count': 19710,
          'time_zone': 'Jakarta',
          'url': None,
          'utc_offset': 25200,
          'verified': False}}
#+end_example

There's some potentially useful data here - like if the tweet was re-tweeted, but for what we're doing we'll just use the tweet itself.

To get just the text of the tweets you use the =twitter_samples.strings= function.

#+begin_src python :results output :exports both
help(twitter_samples.strings)
#+end_src

#+RESULTS:
: Help on method strings in module nltk.corpus.reader.twitter:
: 
: strings(fileids=None) method of nltk.corpus.reader.twitter.TwitterCorpusReader instance
:     Returns only the text content of Tweets in the file(s)
:     
:     :return: the given file(s) as a list of Tweets.
:     :rtype: list(str)
: 

Note that it says that it returns only the given file(s) as a list of tweets but it also makes the =fileids= argument optional. If you don't pass in any argument you end up with the tweets from all the files in the same list, which you probably don't want.

#+begin_src python :results none
positive_tweets = twitter_samples.strings('positive_tweets.json')
negative_tweets = twitter_samples.strings('negative_tweets.json')
all_tweets = twitter_samples.strings("tweets.20150430-223406.json")
#+end_src

Now I'll download the stopwords for our pre-processing and setup the english stopwords for use later.

#+begin_src python :results none
nltk.download('stopwords')
english_stopwords = stopwords.words("english")
#+end_src
*** The Random Seed
    This just sets the random seed so that we get the same values if we re-run this later on (although this is a little tricky with the notebook, since you can call the same code multiple times).

#+begin_src python :results none
random.seed(20200704)
#+end_src

*** Plotting
    I won't be doing a lot of plotting here, but this is a setup for the little that I do.

#+begin_src python :results none
SLUG = "twitter-preprocessing-with-nltk"
Embed = partial(EmbedHoloviews,
                folder_path=f"../../files/posts/nlp/{SLUG}",
                create_folder=False)
#+end_src

* Middle
It can be more convenient to use a [[https://pandas.pydata.org/pandas-docs/stable/reference/series.html][Pandas Series]] for some checks of the tweets so I'll convert the all-tweets list to one.

#+begin_src python :results none
all_tweets = pandas.Series(all_tweets)
#+end_src
** Explore the Data
   Let's start by looking at the number of tweets we got and confirming that the =strings= function gave us back a list of strings like the docstring said it would.

#+begin_src python :results output :exports both
print(f"Number of tweets: {len(all_tweets):,}")
print(f'Number of positive tweets: {len(positive_tweets):,}')
print(f'Number of negative tweets: {len(negative_tweets):,}')

for thing in (positive_tweets, negative_tweets):
    assert type(thing) is list
    assert type(random.choice(thing)) is str
#+end_src

#+RESULTS:
: Number of tweets: 20,000
: Number of positive tweets: 5,000
: Number of negative tweets: 5,000


We can see that the data for each file is made up of strings stored in a list and there were 20,000 tweets in total but only half as much were categorized.

*** Looking At Some Examples
#+begin_src python :results output :exports both
print(f"Random Positive Tweet: {random.choice(positive_tweets)}")
print(f"Random Negative Tweet: {random.choice(negative_tweets)}")
#+end_src

#+RESULTS:
: Random Positive Tweet: @Aaliyan_ Lucky me :))
: Random Negative Tweet: @NotRedbutBlue awww :(
: at least u never got called luis manzano tho

Sometimes the tweets look more like text message replies than micro-blog posts. One thing the original exercise noted is that there are [[https://www.wikiwand.com/en/Emoji][Emoticons]] in the dataset that need to be handled.

*** The First Token
    Later on we're going to remove the "RT" (re-tweet) token at the start of the strings. Let's look at how significant this is.

#+begin_src python :results output :exports both
first_tokens = tweets.str.split(expand=True)[0]
top_ten = CountPercentage(first_tokens, stop=10, value_label="First Token")
top_ten()
#+end_src

| First Token   |   Count |   Percent (%) |
|---------------+---------+---------------|
| RT            |   13287 |         92.92 |
| I             |     160 |          1.12 |
| Farage        |     141 |          0.99 |
| The           |     134 |          0.94 |
| VIDEO:        |     132 |          0.92 |
| Nigel         |     117 |          0.82 |
| Ed            |     116 |          0.81 |
| Miliband      |      77 |          0.54 |
| SNP           |      69 |          0.48 |
| @UKIP         |      67 |          0.47 |

#+begin_src python :results none
plot = top_ten.table.hvplot.bar(y="Percent (%)", x="First Token").opts(
    title="Top Ten Tweet First Tokens", 
    width=900,
    height=800)
output = Embed(plot=plot, file_name="top_ten", create_folder=False)
#+end_src

#+begin_src python :results output html :exports output
print(output())
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="top_ten.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

So, about 93 % of the unclassified tweets start with =RT=, making it perhaps not so informative a token. Or maybe it is... what does a re-tweet tell us? Let's look at if the re-tweeted show up as duplicates and if so, how many times they show up.

#+begin_src python :results output :exports both
retweeted = tweets[tweets.str.startswith("RT")].value_counts().iloc[:10]
for item in retweeted.values:
    print(f" - {item}")
#+end_src

  - 491
  - 430
  - 131
  - 131
  - 117
  - 103
  - 82
  - 73
  - 69
  - 68

Some of the entries are the same tweet repeated hundreds of times. Does each one count as an additional entry? I don't show it here because the tweets are kind of long, but the top five are all about British politics, so there might have been some kind of bias in the way the tweets were gathered.

** Processing the Data
   There are four basic steps for NLP pre-processing:
   - [[https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html][Tokenization]]
   - Lower-casing
   - Removing [[https://www.wikiwand.com/en/Stop_words][stop words]] and punctuation
   - [[https://www.wikiwand.com/en/Stemming][Stemming]]

We're going to start by taking one tweet and seeing how it is transformed by this process.

#+begin_src python :results output :exports both
THE_CHOSEN = positive_tweets[2277]
print(THE_CHOSEN)
#+end_src

#+RESULTS:
: My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i
*** Cleaning Up Twitter-Specific Markup
    Although I listed four steps in the beginning, there's often another step where we remove things that are common or not useful but known in advance. In this case we want to remove re-tweet tags (=RT=), hyperlinks, and hashtags. We're going to do that with python's built in [[https://docs.python.org/3/library/re.html][regular expression]] module. We're also going to do it one tweet at a time, although you could perhapse more efficiently do it in bulk using pandas.

#+begin_src python :results none
START_OF_LINE = r"^"
OPTIONAL = "?"
ANYTHING = "."
ZERO_OR_MORE = "*"
ONE_OR_MORE = "+"

SPACE = "\s"
SPACES = SPACE + ONE_OR_MORE
EVERYTHING_OR_NOTHING = ANYTHING + ZERO_OR_MORE

ERASE = ""
FORWARD_SLASH = "\/"
NEWLINES = r"[\r\n]"
#+end_src
**** Re-Tweets
     None of the positive or negative samples have this tag so I'm going to pull an example from the complete set just to show it working.

#+begin_src python :results output :exports both
RE_TWEET = START_OF_LINE + "RT" + SPACES

tweet = all_tweets[0]
print(tweet)
tweet = re.sub(RE_TWEET, ERASE, tweet)
print(tweet)
#+end_src

#+RESULTS:
: RT @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain £170 billion per year! #BetterOffOut #UKIP
: @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain £170 billion per year! #BetterOffOut #UKIP
**** Hyperlinks
#+begin_src python :results output :exports both
HYPERLINKS = ("http" + "s" + OPTIONAL + ":" + FORWARD_SLASH + FORWARD_SLASH
              + EVERYTHING_OR_NOTHING + NEWLINES + ZERO_OR_MORE)

print(THE_CHOSEN)
re_chosen = re.sub(HYPERLINKS, ERASE, THE_CHOSEN)
print(re_chosen)
#+end_src

#+RESULTS:
: My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i
: My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… 


Note that the way the regular expression is written, it eats everything after the =http=.
**** HashTags
     We aren't removing the actual hash-tags, just the hash-marks (=#=).

#+begin_src python :results output :exports both
HASH = "#"
re_chosen = re.sub(HASH, ERASE, re_chosen)
print(re_chosen)
#+end_src
#+RESULTS:
: My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… 
*** Tokenize
    NLTK has a tokenizer specially built for tweets. The =twitter_samples= module actually has a =tokenizer= function that breaks the tweets up, but since we are using regular expressions to clean up the strings a little first, it makes more sense to tokenize the strings afterwards. Also note that one of the steps in the pipeline is to lower-case the letters, which the =TweetTokenizer= will do for us if we set the =preserve_case= argument to =False=.

#+begin_src python :results output :exports both
print(help(TweetTokenizer))
#+end_src

#+RESULTS:
#+begin_example
Help on class TweetTokenizer in module nltk.tokenize.casual:

class TweetTokenizer(builtins.object)
 |  TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False)
 |  
 |  Tokenizer for tweets.
 |  
 |      >>> from nltk.tokenize import TweetTokenizer
 |      >>> tknzr = TweetTokenizer()
 |      >>> s0 = "This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--"
 |      >>> tknzr.tokenize(s0)
 |      ['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']
 |  
 |  Examples using `strip_handles` and `reduce_len parameters`:
 |  
 |      >>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)
 |      >>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'
 |      >>> tknzr.tokenize(s1)
 |      [':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']
 |  
 |  Methods defined here:
 |  
 |  __init__(self, preserve_case=True, reduce_len=False, strip_handles=False)
 |      Initialize self.  See help(type(self)) for accurate signature.
 |  
 |  tokenize(self, text)
 |      :param text: str
 |      :rtype: list(str)
 |      :return: a tokenized list of strings; concatenating this list returns        the original string if `preserve_case=False`
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)

None
#+end_example

#+begin_src python :results none
tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                           reduce_len=True)
#+end_src

As I mentioned, =preserve_case= lower-cases the letters. The other two arguments are =strip_handles= which removes the twitter-handles and =reduce_len= which limits the number of times a character can be repeated to three - so =zzzzz= will be changed to =zzz=. Now we can tokenize our partly cleaned token.

#+begin_src python :results output :exports both
print(re_chosen)
tokens = tokenizer.tokenize(re_chosen)
print(tokens)
#+end_src

#+RESULTS:
: My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… 
: ['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']
*** Remove Stop Words and Punctuation
#+begin_src python :results output :exports both
print(english_stopwords)
print(string.punctuation)
#+end_src

#+RESULTS:
: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]
: !"#$%&'()*+,-./:;<=>?@[\]^_`{|}~

#+begin_src python :results output :exports both
cleaned = [word for word in tokens if (word not in english_stopwords and
                                       word not in string.punctuation)]
print(cleaned)
#+end_src

#+RESULTS:
: ['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']


*** Stemming
    We're going to use the [[https://www.nltk.org/_modules/nltk/stem/porter.html][Porter Stemmer]] from NLTK ([[https://tartarus.org/martin/PorterStemmer/][this]] is the official Porter Stemmer algorithm page) to stem the words.

#+begin_src python :results none
stemmer = PorterStemmer()
#+end_src

#+begin_src python :results output :exports both
stemmed = [stemmer.stem(word) for word in cleaned]
print(stemmed)
#+end_src

#+RESULTS:
: ['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']

* End
  So now we've seen the basic steps that we're going to need to preprocess our tweets for [[https://www.wikiwand.com/en/Sentiment_analysis][Sentiment Analysis]].

**Note:** This is a re-write of an exercise taken from Coursera's [[https://www.deeplearning.ai/natural-language-processing-specialization/][Natural Language Processing]] specialization.

  The rest of this is outside the scope of the exercise, it's just to get it all into one place. 
** Tests
   I'm going to use [[https://github.com/pytest-dev/pytest-bdd][pytest-bdd]] to run the tests for the pre-processor but I'm also going to take advantage of org-babel and keep the scenario definitions and the test functions grouped by what they do, even though they will exist in two different files (=tweet_preprocessing.feature= and =test_preprocessing.py=) when tangled out of this file.
*** The Tangles
#+begin_src feature :tangle ../../tests/features/twitter/tweet_preprocessing.feature
Feature: Tweet pre-processor

<<stock-processing>>

<<re-tweet-processing>>

<<hyperlink-processing>>

<<hash-processing>>

<<tokenization-preprocessing>>

<<stop-word-preprocessing>>

<<stem-preprocessing>>

<<whole-shebang-preprocessing>>
#+end_src

#+begin_src python :tangle ../../tests/functional/twitter/fixtures.py
# from pypi
import pytest

# software under test
from neurotic.nlp.twitter.processor import TwitterProcessor

class Katamari:
    """Something to stick values into"""

@pytest.fixture
def katamari():
    return Katamari()


@pytest.fixture
def processor():
    return TwitterProcessor()
#+end_src

#+begin_src python :tangle ../../tests/functional/twitter/test_preprocessing.py
# from python
import random
import string

# from pypi
from expects import (
    contain_exactly,
    equal,
    expect
)
from pytest_bdd import (
    given,
    scenarios,
    then,
    when,
)

And = when


# fixtures
from fixtures import katamari, processor

scenarios("../../features/twitter/tweet_preprocessing.feature")


<<test-stock-symbol>>


<<test-re-tweet>>


<<test-hyperlinks>>


<<test-hashtags>>


<<test-tokenization>>


<<test-unstopping>>


<<test-stem>>


<<test-call>>
#+end_src

Now on to the sections that go into the tangles.
*** Stock Symbols
    Twitter has a special symbol for stocks which is a dollar sign followed by the stock ticker name (e.g. =$HOG= for Harley Davidson) that I'll remove. This is going to assume anything with a dollar sign immediately followed by a letter, number, or underscore is a stock symbol.
#+begin_src feature :noweb-ref stock-processing
Scenario: A tweet with a stock symbol is cleaned
  Given a tweet with a stock symbol in it
  When the tweet is cleaned
  Then it has the text removed
#+end_src

#+begin_src python :noweb-ref test-stock-symbol
#Scenario: A tweet with a stock symbol is cleaned


@given("a tweet with a stock symbol in it")
def setup_stock_symbol(katamari, faker):
    symbol = "".join(random.choices(string.ascii_uppercase, k=4))
    head, tail = faker.sentence(), faker.sentence()
    katamari.to_clean = (f"{head} ${symbol} "
                         f"{tail}")

    # the cleaner ignores spaces so there's going to be two spaces between
    # the head and tail after the symbol is removed
    katamari.expected = f"{head}  {tail}"
    return

#   When the tweet is cleaned
#   Then it has the text removed
#+end_src
*** The Re-tweets
    This tests that we can remove the RT tag.
#+begin_src feature :noweb-ref re-tweet-processing
Scenario: A re-tweet is cleaned.

  Given a tweet that has been re-tweeted
  When the tweet is cleaned
  Then it has the text removed
#+end_src

#+begin_src python :noweb-ref test-re-tweet
# Scenario: A re-tweet is cleaned.

@given("a tweet that has been re-tweeted")
def setup_re_tweet(katamari, faker):
    katamari.expected = faker.sentence()
    spaces = " " * random.randrange(1, 10)
    katamari.to_clean = f"RT{spaces}{katamari.expected}"
    return


@when("the tweet is cleaned")
def process_tweet(katamari, processor):
    katamari.actual = processor.clean(katamari.to_clean)
    return


@then("it has the text removed")
def check_cleaned_text(katamari):
    expect(katamari.expected).to(equal(katamari.actual))
    return
#+end_src

*** Hyperlinks
    Now test that we can remove hyperlinks.
#+begin_src feature :noweb-ref hyperlink-processing
Scenario: The tweet has a hyperlink
  Given a tweet with a hyperlink
  When the tweet is cleaned
  Then it has the text removed
#+end_src

#+begin_src python :noweb-ref test-hyperlinks
# Scenario: The tweet has a hyperlink

@given("a tweet with a hyperlink")
def setup_hyperlink(katamari, faker):
    base = faker.sentence()
    katamari.expected = base
    katamari.to_clean = base + faker.uri() + "\n" * random.randrange(5)
    return
#+end_src
*** Hash Symbols
    Test that we can remove the pound symbol.

#+begin_src feature :noweb-ref hash-processing
Scenario: A tweet has hash symbols in it.
  Given a tweet with hash symbols
  When the tweet is cleaned
  Then it has the text removed
#+end_src

#+begin_src python :noweb-ref test-hashtags
@given("a tweet with hash symbols")
def setup_hash_symbols(katamari, faker):
    expected = faker.sentence()
    tokens = expected.split()
    expected_tokens = expected.split()

    for count in range(random.randrange(1, 10)):
        index = random.randrange(len(tokens))
        word = faker.word()
        tokens = tokens[:index] + [f"#{word}"] + tokens[index:]
        expected_tokens = expected_tokens[:index] + [word] + expected_tokens[index:]
    katamari.to_clean = " ".join(tokens)
    katamari.expected = " ".join(expected_tokens)
    return
#+end_src
*** Tokenization
    This is being done by NLTK, so it might not really make sense to test it, but I figured adding a test would make it more likely that I'd slow down enough to understand what it's doing.

#+begin_src feature :noweb-ref tokenization-preprocessing
Scenario: The text is tokenized
  Given a string of text
  When the text is tokenized
  Then it is the expected list of strings
#+end_src

#+begin_src python :noweb-ref test-tokenization
# Scenario: The text is tokenized


@given("a string of text")
def setup_text(katamari):
    katamari.text = "Time flies like an Arrow, fruit flies like a BANANAAAA!"
    katamari.expected = ("time flies like an arrow , "
                         "fruit flies like a bananaaa !").split()
    return


@when("the text is tokenized")
def tokenize(katamari, processor):
    katamari.actual = processor.tokenizer.tokenize(katamari.text)
    return


@then("it is the expected list of strings")
def check_tokens(katamari):
    expect(katamari.actual).to(contain_exactly(*katamari.expected))
    return
#+end_src
*** Stop Word Removal
    Check that we're removing stop-words and punctuation.

#+begin_src feature :noweb-ref stop-word-preprocessing
Scenario: The user removes stop words and punctuation
  Given a tokenized string
  When the string is un-stopped
  Then it is the expected list of strings
#+end_src

#+begin_src python :noweb-ref test-unstopping
#Scenario: The user removes stop words and punctuation


@given("a tokenized string")
def setup_tokenized_string(katamari):
    katamari.source = ("now is the winter of our discontent , "
                       "made glorious summer by this son of york ;").split()
    katamari.expected = ("winter discontent made glorious "
                         "summer son york".split())
    return


@when("the string is un-stopped")
def un_stop(katamari, processor):
    katamari.actual = processor.remove_useless_tokens(katamari.source)
    return
#  Then it is the expected list of strings
#+end_src
*** Stemming
    This is kind of a fake test. I guessed incorrectly what the stemming would do the first time so I had to go back and match the test values to what it output. I don't think I'll take the time to learn how the stemming is working, though, so it'll have to do.
#+begin_src feature :noweb-ref stem-preprocessing
Scenario: The user stems the tokens
  Given a tokenized string
  When the string is un-stopped
  And tokens are stemmed
  Then it is the expected list of strings
#+end_src

#+begin_src python :noweb-ref test-stem
# Scenario: The user stems the tokens
#  Given a tokenized string
#  When the string is un-stopped
 

@And("tokens are stemmed")
def stem_tokens(katamari, processor):
    katamari.actual = processor.stem(katamari.actual)
    katamari.expected = "winter discont made gloriou summer son york".split()
    return


#  Then it is the expected list of strings
#+end_src
*** The Whole Shebang
    I made some of the steps separate just for illustration and testing, but I'll make the processor callable so they don't have to be done separately.
#+begin_src feature :noweb-ref whole-shebang-preprocessing
Scenario: The user calls the processor
  Given a tweet
  When the processor is called with the tweet
  Then it returns the cleaned, tokenized, and stemmed list
#+end_src

#+begin_src python :noweb-ref test-call
# Scenario: The user calls the processor


@given("a tweet")
def setup_tweet(katamari, faker):
    katamari.words = "How now, brown cow? Whither and dither the smelly laulau. Boooooow Wooooow!"
    katamari.tweet = f"RT {katamari.words}  #bocceballs {faker.uri()}"
    katamari.expected = "brown cow whither dither smelli laulau booow wooow boccebal".split()
    return


@when("the processor is called with the tweet")
def process_tweet(katamari, processor):
    katamari.actual = processor(katamari.tweet)
    return


@then("it returns the cleaned, tokenized, and stemmed list")
def check_processed_tweet(katamari):
    expect(katamari.actual).to(contain_exactly(*katamari.expected))
    return
#+end_src

** Implementation
   I'm going to implement it as a class rather than a function just so that all this stuff that's floating around in the notebook as global variables is collected in one place.

#+begin_src python :tangle ../../neurotic/nlp/twitter/processor.py
# python
import re
import string

# pypi
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer

import attr
import nltk

<<regular-expressions>>


@attr.s
class TwitterProcessor:
    """processor for tweets"""
    _tokenizer = attr.ib(default=None)
    _stopwords = attr.ib(default=None)
    _stemmer = attr.ib(default=None)

    <<processor-clean>>

    <<processor-tokenizer>>

    <<processor-un-stop>>

    <<processor-stopwords>>

    <<processor-stemmer>>

    <<processor-stem>>

    <<processor-call>>
#+end_src
*** A Regular Expression Helper
#+begin_src python :noweb-ref regular-expressions
class WheatBran:
    """This is a holder for the regular expressions"""
    START_OF_LINE = r"^"
    OPTIONAL = "{}?"
    ANYTHING = "."
    ZERO_OR_MORE = "{}*"
    ONE_OR_MORE = "{}+"
    ONE_OF_THESE = "[{}]"

    SPACE = r"\s"
    SPACES = ONE_OR_MORE.format(SPACE)
    PART_OF_A_WORD = r"\w"
    EVERYTHING_OR_NOTHING = ZERO_OR_MORE.format(ANYTHING)

    ERASE = ""
    FORWARD_SLASHES = r"\/\/"
    NEWLINES = ONE_OF_THESE.format(r"\r\n")
    # a dollar is a special regular expression character meaning end of line
    # so escape it
    DOLLAR_SIGN = r"\$"

    # to remove
    STOCK_SYMBOL = DOLLAR_SIGN + ZERO_OR_MORE.format(PART_OF_A_WORD)
    RE_TWEET = START_OF_LINE + "RT" + SPACES
    HYPERLINKS = ("http" + OPTIONAL.format("s") + ":" + FORWARD_SLASHES
                  + EVERYTHING_OR_NOTHING + ZERO_OR_MORE.format(NEWLINES))
    HASH = "#"

    remove = [STOCK_SYMBOL, RE_TWEET, HYPERLINKS, HASH]
#+end_src
*** The Clean Method
#+begin_src python :noweb-ref processor-clean
def clean(self, tweet: str) -> str:
    """Removes sub-strings from the tweet

    Args:
     tweet: string tweet

    Returns:
     tweet with certain sub-strings removed
    """
    for expression in WheatBran.remove:
        tweet = re.sub(expression, WheatBran.ERASE, tweet)
    return tweet
#+end_src
*** The Tokenizer
#+begin_src python :noweb-ref processor-tokenizer
@property
def tokenizer(self) -> TweetTokenizer:
    """The NLTK Tweet Tokenizer

    It will:
     - tokenize a string
     - remove twitter handles
     - remove repeated characters after the first three
    """
    if self._tokenizer is None:
        self._tokenizer = TweetTokenizer(preserve_case=False,
                                         strip_handles=True,
                                         reduce_len=True)
    return self._tokenizer
#+end_src
*** Stopwords
    This might make more sense to be done at the module level, but I'll see how it goes.

#+begin_src python :noweb-ref processor-stopwords
@property
def stopwords(self) -> list:
    """NLTK English stopwords
    
    Warning:
     if the stopwords haven't been downloaded this also tries too download them
    """
    if self._stopwords is None:
        nltk.download('stopwords')
        self._stopwords =  stopwords.words("english")
    return self._stopwords
#+end_src
*** Un-Stop the Tokens
#+begin_src python :noweb-ref processor-un-stop
def remove_useless_tokens(self, tokens: list) -> list:
    """Remove stopwords and punctuation

    Args:
     tokens: list of strings

    Returns:
     tokens with unuseful tokens removed
    """    
    return [word for word in tokens if (word not in self.stopwords and
                                        word not in string.punctuation)]
#+end_src
*** Stem the Tokens
#+begin_src python :noweb-ref processor-stemmer
@property
def stemmer(self) -> PorterStemmer:
    """Porter Stemmer for the tokens"""
    if self._stemmer is None:
        self._stemmer = PorterStemmer()
    return self._stemmer
#+end_src
#+begin_src python :noweb-ref processor-stem
def stem(self, tokens: list) -> list:
    return [self.stemmer.stem(word) for word in tokens]
#+end_src
*** Call Me

#+begin_src python :noweb-ref processor-call
def __call__(self, tweet: str) -> list:
    """does all the processing in one step

    Args:
     tweet: string to process
    """
    cleaned = self.clean(tweet)    
    cleaned = self.tokenizer.tokenize(cleaned)
    cleaned = self.stem(cleaned)
    cleaned = self.remove_useless_tokens(cleaned)
    return cleaned
#+end_src
