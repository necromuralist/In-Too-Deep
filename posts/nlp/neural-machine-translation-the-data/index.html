<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="The data for our machine translation model." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neural Machine Translation: The Data | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/neural-machine-translation-the-data/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../neural-machine-translation/" rel="prev" title="Neural Machine Translation" type="text/html">
<link href="../neural-machine-translation-the-attention-model/" rel="next" title="Neural Machine Translation: The Attention Model" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Neural Machine Translation: The Data" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/neural-machine-translation-the-data/" property="og:url">
<meta content="The data for our machine translation model." property="og:description">
<meta content="article" property="og:type">
<meta content="2021-02-14T14:53:32-08:00" property="article:published_time">
<meta content="machine translation" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Neural Machine Translation: The Data</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2021-02-14T14:53:32-08:00" itemprop="datePublished" title="2021-02-14 14:53">2021-02-14 14:53</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orge2c24f4">The Data</a>
<ul>
<li><a href="#org5fac1cc">Imports</a></li>
</ul>
</li>
<li><a href="#org15ff595">Middle</a>
<ul>
<li><a href="#org94c1ae2">Loading the Data</a>
<ul>
<li><a href="#org97a21dd">The Training Data</a></li>
<li><a href="#orgcee823a">The Evaluation Data</a></li>
</ul>
</li>
<li><a href="#org801a11d">Tokenization and Formatting</a></li>
<li><a href="#org535fb56">Integer assigned as end-of-sentence (EOS)</a>
<ul>
<li><a href="#org118083f">Filter long sentences</a></li>
</ul>
</li>
<li><a href="#org1764a38">tokenize & detokenize helper functions</a></li>
<li><a href="#org895f730">Bucketing</a>
<ul>
<li><a href="#orge1fbae7">Bucketing to create streams of batches.</a></li>
</ul>
</li>
<li><a href="#org73fda4f">Exploring the data</a></li>
</ul>
</li>
<li><a href="#org7516a44">Bundle it Up</a>
<ul>
<li><a href="#orgcb57f09">Imports</a></li>
<li><a href="#orgfce077b">Constants</a></li>
<li><a href="#org88bc5e5">Tokenizer/Detokenizer</a>
<ul>
<li><a href="#org6e0071c">Tokenizer</a></li>
<li><a href="#org85ca6e9">Detokenizer</a></li>
</ul>
</li>
<li><a href="#org2f4c821">Data Generator</a>
<ul>
<li><a href="#org4edeed6">Append End of Sentence</a></li>
<li><a href="#orgf640abd">Generator Function</a></li>
<li><a href="#orga634fb3">Batch Stream</a></li>
</ul>
</li>
<li><a href="#org7092d91">Try It Out</a></li>
</ul>
</li>
<li><a href="#orga61fbcf">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orge2c24f4">
<h2 id="orge2c24f4">The Data</h2>
<div class="outline-text-2" id="text-orge2c24f4">
<p>This is the first post in a series that will look at creating a Long-Short-Term-Memory (LSTM) model with attention for Machine Learning. The <a href="../neural-machine-translation/">previous post</a> was an overview that holds the links to all the posts in the series.</p>
</div>
<div class="outline-3" id="outline-container-org5fac1cc">
<h3 id="org5fac1cc">Imports</h3>
<div class="outline-text-3" id="text-org5fac1cc">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">termcolor</span> <span class="kn">import</span> <span class="n">colored</span>

<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">trax</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org15ff595">
<h2 id="org15ff595">Middle</h2>
<div class="outline-text-2" id="text-org15ff595"></div>
<div class="outline-3" id="outline-container-org94c1ae2">
<h3 id="org94c1ae2">Loading the Data</h3>
<div class="outline-text-3" id="text-org94c1ae2">
<p>Next, we will import the dataset we will use to train the model. If you are running out of space, you can just use a small dataset from <a href="http://opus.nlpl.eu/">Opus</a>, a growing collection of translated texts from the web. Particularly, we will get an English to German translation subset specified as <code>opus/medical</code> which has medical related texts. If storage is not an issue, you can opt to get a larger corpus such as the English to German translation dataset from <a href="https://paracrawl.eu/">ParaCrawl</a>, a large multi-lingual translation dataset created by the European Union. Both of these datasets are available via <a href="https://www.tensorflow.org/datasets">Tensorflow Datasets (TFDS)</a> and you can browse through the other available datasets <a href="https://www.tensorflow.org/datasets/catalog/overview">here</a>. As you'll see below, you can easily access this dataset from TFDS with <code>trax.data.TFDS</code>. The result is a python generator function yielding tuples. Use the <code>keys</code> argument to select what appears at which position in the tuple. For example, <code>keys=('en', 'de')</code> below will return pairs as (English sentence, German sentence).</p>
<p>The <a href="https://www.tensorflow.org/datasets/catalog/para_crawl#para_crawlende"><code>para_crawl/ende</code></a> dataset is 4.04 GiB while the <a href="https://www.tensorflow.org/datasets/catalog/opus#opusmedical_default_config"><code>opus/medical</code></a> dataset is 188.85 MiB.</p>
<p><b>Note:</b> Trying to download the ParaCrawl dataset using trax creates an out of resource error. You can try downloading the source from:</p>
<p><a href="https://s3.amazonaws.com/web-language-models/paracrawl/release4/en-de.bicleaner07.txt.gz">https://s3.amazonaws.com/web-language-models/paracrawl/release4/en-de.bicleaner07.txt.gz</a></p>
<p>Although I haven't figured out how to get it into the trax data yet so I'm sticking with the smaller data set.</p>
</div>
<div class="outline-4" id="outline-container-org97a21dd">
<h4 id="org97a21dd">The Training Data</h4>
<div class="outline-text-4" id="text-org97a21dd">
<p>The first time you run this it will download the dataset, after that it will just load it from the file.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/data/tensorflow/translation/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>

<span class="n">data_set</span> <span class="o">=</span> <span class="s2">"opus/medical"</span>
<span class="c1"># data_set = "para_crawl/ende"</span>

<span class="n">train_stream_fn</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFDS</span><span class="p">(</span><span class="n">data_set</span><span class="p">,</span>
                                 <span class="n">data_dir</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
                                 <span class="n">keys</span><span class="o">=</span><span class="p">(</span><span class="s1">'en'</span><span class="p">,</span> <span class="s1">'de'</span><span class="p">),</span>
                                 <span class="n">eval_holdout_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                 <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org1f44cfc">
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-6-fb62d04026f5&gt; in &lt;module&gt;
      4 # data_set = "para_crawl/ende"
      5 
----&gt; 6 train_stream_fn = trax.data.TFDS(data_set,
      7                                  data_dir=path,
      8                                  keys=('en', 'de'),

/usr/local/lib/python3.8/dist-packages/gin/config.py in gin_wrapper(*args, **kwargs)
   1067       scope_info = " in scope '{}'".format(scope_str) if scope_str else ''
   1068       err_str = err_str.format(name, fn_or_cls, scope_info)
-&gt; 1069       utils.augment_exception_message_and_reraise(e, err_str)
   1070 
   1071   return gin_wrapper

/usr/local/lib/python3.8/dist-packages/gin/utils.py in augment_exception_message_and_reraise(exception, message)
     39   proxy = ExceptionProxy()
     40   ExceptionProxy.__qualname__ = type(exception).__qualname__
---&gt; 41   raise proxy.with_traceback(exception.__traceback__) from None
     42 
     43 

/usr/local/lib/python3.8/dist-packages/gin/config.py in gin_wrapper(*args, **kwargs)
   1044 
   1045     try:
-&gt; 1046       return fn(*new_args, **new_kwargs)
   1047     except Exception as e:  # pylint: disable=broad-except
   1048       err_str = ''

/usr/local/lib/python3.8/dist-packages/gin/config.py in gin_wrapper(*args, **kwargs)
   1067       scope_info = " in scope '{}'".format(scope_str) if scope_str else ''
   1068       err_str = err_str.format(name, fn_or_cls, scope_info)
-&gt; 1069       utils.augment_exception_message_and_reraise(e, err_str)
   1070 
   1071   return gin_wrapper

/usr/local/lib/python3.8/dist-packages/gin/utils.py in augment_exception_message_and_reraise(exception, message)
     39   proxy = ExceptionProxy()
     40   ExceptionProxy.__qualname__ = type(exception).__qualname__
---&gt; 41   raise proxy.with_traceback(exception.__traceback__) from None
     42 
     43 

/usr/local/lib/python3.8/dist-packages/gin/config.py in gin_wrapper(*args, **kwargs)
   1044 
   1045     try:
-&gt; 1046       return fn(*new_args, **new_kwargs)
   1047     except Exception as e:  # pylint: disable=broad-except
   1048       err_str = ''

~/trax/trax/data/tf_inputs.py in TFDS(dataset_name, data_dir, tfds_preprocess_fn, keys, train, shuffle_train, host_id, n_hosts, eval_holdout_size)
    279   else:
    280     subsplit = None
--&gt; 281   (train_data, eval_data, _) = _train_and_eval_dataset(
    282       dataset_name, data_dir, eval_holdout_size,
    283       train_shuffle_files=shuffle_train, subsplit=subsplit)

~/trax/trax/data/tf_inputs.py in _train_and_eval_dataset(dataset_name, data_dir, eval_holdout_size, train_shuffle_files, eval_shuffle_files, subsplit)
    224   if eval_holdout_examples &gt; 0 or subsplit is not None:
    225     n_train = train_examples - eval_holdout_examples
--&gt; 226     train_start = int(n_train * subsplit[0])
    227     train_end = int(n_train * subsplit[1])
    228     if train_end - train_start &lt; 1:

TypeError: 'NoneType' object is not subscriptable
  In call to configurable 'TFDS' (&lt;function TFDS at 0x7f960c527280&gt;)
  In call to configurable 'TFDS' (&lt;function TFDS at 0x7f960c526f70&gt;)
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgcee823a">
<h4 id="orgcee823a">The Evaluation Data</h4>
<div class="outline-text-4" id="text-orgcee823a">
<p>Since we already downloaded the data in the previous code-block, this will just load the evaluation set from the downloaded data.</p>
<div class="highlight">
<pre><span></span><span class="n">eval_stream_fn</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFDS</span><span class="p">(</span><span class="s1">'opus/medical'</span><span class="p">,</span>
                                <span class="n">data_dir</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
                                <span class="n">keys</span><span class="o">=</span><span class="p">(</span><span class="s1">'en'</span><span class="p">,</span> <span class="s1">'de'</span><span class="p">),</span>
                                <span class="n">eval_holdout_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                                <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
<p>Notice that TFDS returns a generator <b>function</b>, not a generator. This is because in Python, you cannot reset generators so you cannot go back to a previously yielded value. During deep learning training, you use Stochastic Gradient Descent and don't actually need to go back – but it is sometimes good to be able to do that, and that's where the functions come in. Let's print a a sample pair from our train and eval data. Notice that the raw output is represented in bytes (denoted by the <code>b'</code> prefix) and these will be converted to strings internally in the next steps.</p>
<div class="highlight">
<pre><span></span><span class="n">train_stream</span> <span class="o">=</span> <span class="n">train_stream_fn</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'train data (en, de) tuple:'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="nb">next</span><span class="p">(</span><span class="n">train_stream</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
<pre class="example">
[31mtrain data (en, de) tuple:[0m (b'Tel: +421 2 57 103 777\n', b'Tel: +421 2 57 103 777\n')

</pre>
<div class="highlight">
<pre><span></span><span class="n">eval_stream</span> <span class="o">=</span> <span class="n">eval_stream_fn</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'eval data (en, de) tuple:'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="nb">next</span><span class="p">(</span><span class="n">eval_stream</span><span class="p">))</span>
</pre></div>
<pre class="example">
[31meval data (en, de) tuple:[0m (b'Lutropin alfa Subcutaneous use.\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\n')
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org801a11d">
<h3 id="org801a11d">Tokenization and Formatting</h3>
<div class="outline-text-3" id="text-org801a11d">
<p>Now that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:</p>
<p><b>Tokenizing the sentences using subword representations:</b> We want to represent each sentence as an array of integers instead of strings. For our application, we will use <b>subword</b> representations to tokenize our sentences. This is a common technique to avoid out-of-vocabulary words by allowing parts of words to be represented separately. For example, instead of having separate entries in your vocabulary for –"fear", "fearless", "fearsome", "some", and "less"–, you can simply store –"fear", "some", and "less"– then allow your tokenizer to combine these subwords when needed. This allows it to be more flexible so you won't have to save uncommon words explicitly in your vocabulary (e.g. <b>stylebender</b>, <b>nonce</b>, etc). Tokenizing is done with the `trax.data.Tokenize()` command and we have provided you the combined subword vocabulary for English and German (i.e. `ende_32k.subword`) retrieved from <a href="https://storage.googleapis.com/trax-ml/vocabs/ende_32k.subword">https://storage.googleapis.com/trax-ml/vocabs/ende_32k.subword</a> (I'm using the web-interface, but you could also just download it and put it in a directory).</p>
<div class="highlight">
<pre><span></span><span class="n">VOCAB_FILE</span> <span class="o">=</span> <span class="s1">'ende_32k.subword'</span>
<span class="n">VOCAB_DIR</span> <span class="o">=</span> <span class="s2">"gs://trax-ml/vocabs/"</span> <span class="c1"># google storage</span>

<span class="c1"># Tokenize the dataset.</span>
<span class="n">tokenized_train_stream</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Tokenize</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">)(</span><span class="n">train_stream</span><span class="p">)</span>
<span class="n">tokenized_eval_stream</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Tokenize</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">)(</span><span class="n">eval_stream</span><span class="p">)</span>
</pre></div>
<p><b>Append an end-of-sentence token to each sentence:</b> We will assign a token (i.e. in this case <code>1</code>) to mark the end of a sentence. This will be useful in inference/prediction so we'll know that the model has completed the translation.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org535fb56">
<h3 id="org535fb56">Integer assigned as end-of-sentence (EOS)</h3>
<div class="outline-text-3" id="text-org535fb56">
<div class="highlight">
<pre><span></span><span class="n">EOS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">append_eos</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
    <span class="sd">"""helper to add end of sentence token to sentences in the stream</span>

<span class="sd">    Yields:</span>
<span class="sd">     next tuple of numpy arrays with EOS token added (inputs, targets)</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
        <span class="n">inputs_with_eos</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS</span><span class="p">]</span>
        <span class="n">targets_with_eos</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS</span><span class="p">]</span>
        <span class="k">yield</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs_with_eos</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">targets_with_eos</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">tokenized_train_stream</span> <span class="o">=</span> <span class="n">append_eos</span><span class="p">(</span><span class="n">tokenized_train_stream</span><span class="p">)</span>
<span class="n">tokenized_eval_stream</span> <span class="o">=</span> <span class="n">append_eos</span><span class="p">(</span><span class="n">tokenized_eval_stream</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org118083f">
<h4 id="org118083f">Filter long sentences</h4>
<div class="outline-text-4" id="text-org118083f">
<p>We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the <code>trax.data.FilterByLength()</code> method and you can see its syntax below.</p>
<p>Filter too long sentences to not run out of memory. length_keys=[0, 1] means we filter both English and German sentences, so both must not be longer that 256 tokens for training and 512 tokens for evaluation.</p>
<div class="highlight">
<pre><span></span><span class="n">filtered_train_stream</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">FilterByLength</span><span class="p">(</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">length_keys</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])(</span><span class="n">tokenized_train_stream</span><span class="p">)</span>
<span class="n">filtered_eval_stream</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">FilterByLength</span><span class="p">(</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">length_keys</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])(</span><span class="n">tokenized_eval_stream</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">filtered_train_stream</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Single tokenized example input:'</span><span class="p">,</span> <span class="s1">'red'</span> <span class="p">),</span> <span class="n">train_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Single tokenized example target:'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">train_target</span><span class="p">)</span>
</pre></div>
<pre class="example">
[31mSingle tokenized example input:[0m [ 2538  2248    30 12114 23184 16889     5     2 20852  6456 20592  5812
  3932    96  5178  3851    30  7891  3550 30650  4729   992     1]
[31mSingle tokenized example target:[0m [ 1872    11  3544    39  7019 17877 30432    23  6845    10 14222    47
  4004    18 21674     5 27467  9513   920   188 10630    18  3550 30650
  4729   992     1]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1764a38">
<h3 id="org1764a38">tokenize & detokenize helper functions</h3>
<div class="outline-text-3" id="text-org1764a38">
<p>Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your trax models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following:</p>
<ul class="org-ul">
<li>word2Ind: a dictionary mapping the word to its index.</li>
<li>ind2Word: a dictionary mapping the index to its word.</li>
<li>word2Count: a dictionary mapping the word to the number of times it appears.</li>
<li>num_words: total number of words that have appeared.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">input_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
             <span class="n">vocab_file</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">EOS</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">EOS</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Encodes a string to an array of integers</span>

<span class="sd">    Args:</span>
<span class="sd">       input_str: human-readable string to encode</span>
<span class="sd">       vocab_file: filename of the vocabulary text file</span>
<span class="sd">       vocab_dir: path to the vocabulary file</span>

<span class="sd">    Returns:</span>
<span class="sd">       tokenized version of the input string</span>
<span class="sd">    """</span>
    <span class="c1"># Use the trax.data.tokenize method. It takes streams and returns streams,</span>
    <span class="c1"># we get around it by making a 1-element stream with `iter`.</span>
    <span class="n">inputs</span> <span class="o">=</span>  <span class="nb">next</span><span class="p">(</span><span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="nb">iter</span><span class="p">([</span><span class="n">input_str</span><span class="p">]),</span>
                                      <span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">,</span>
                                      <span class="n">vocab_dir</span><span class="o">=</span><span class="n">vocab_dir</span><span class="p">))</span>

    <span class="c1"># Mark the end of the sentence with EOS</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS</span><span class="p">]</span>

    <span class="c1"># Adding the batch dimension to the front of the shape</span>
    <span class="n">batch_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">batch_inputs</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">detokenize</span><span class="p">(</span><span class="n">integers</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
               <span class="n">vocab_file</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">vocab_dir</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">EOS</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">EOS</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""Decodes an array of integers to a human readable string</span>

<span class="sd">    Args:</span>
<span class="sd">       integers: array of integers to decode</span>
<span class="sd">       vocab_file: filename of the vocabulary text file</span>
<span class="sd">       vocab_dir: path to the vocabulary file</span>

<span class="sd">    Returns:</span>
<span class="sd">       str: the decoded sentence.</span>
<span class="sd">    """</span>
    <span class="c1"># Remove the dimensions of size 1</span>
    <span class="n">integers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">integers</span><span class="p">))</span>

    <span class="c1"># Remove the EOS to decode only the original tokens</span>
    <span class="k">if</span> <span class="n">EOS</span> <span class="ow">in</span> <span class="n">integers</span><span class="p">:</span>
        <span class="n">integers</span> <span class="o">=</span> <span class="n">integers</span><span class="p">[:</span><span class="n">integers</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">EOS</span><span class="p">)]</span> 

    <span class="k">return</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">integers</span><span class="p">,</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">vocab_dir</span><span class="p">)</span>
</pre></div>
<p>Let's see how we might use these functions:</p>
<p>Detokenize an input-target pair of tokenized sentences</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Single detokenized example input:'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">detokenize</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Single detokenized example target:'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">detokenize</span><span class="p">(</span><span class="n">train_target</span><span class="p">,</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
</pre></div>
<pre class="example">
[31mSingle detokenized example input:[0m During treatment with olanzapine, adolescents gained significantly more weight compared with adults.

[31mSingle detokenized example target:[0m Während der Behandlung mit Olanzapin nahmen die Jugendlichen im Vergleich zu Erwachsenen signifikant mehr Gewicht zu.

</pre>
<p>Tokenize and detokenize a word that is not explicitly saved in the vocabulary file. See how it combines the subwords – 'hell' and 'o'– to form the word 'hello'.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s2">"tokenize('hello'): "</span><span class="p">,</span> <span class="s1">'green'</span><span class="p">),</span> <span class="n">tokenize</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">,</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s2">"detokenize([17332, 140, 1]): "</span><span class="p">,</span> <span class="s1">'green'</span><span class="p">),</span> <span class="n">detokenize</span><span class="p">([</span><span class="mi">17332</span><span class="p">,</span> <span class="mi">140</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">))</span>
</pre></div>
<pre class="example">
[32mtokenize('hello'): [0m [[17332   140     1]]
[32mdetokenize([17332, 140, 1]): [0m hello
</pre></div>
</div>
<div class="outline-3" id="outline-container-org895f730">
<h3 id="org895f730">Bucketing</h3>
<div class="outline-text-3" id="text-org895f730">
<p>Bucketing the tokenized sentences is an important technique used to speed up training in NLP. Here is a <a href="https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976">nice article describing it in detail</a> but the gist is very simple. Our inputs have variable lengths and you want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to some wasted computation though. For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket.</p>
<p>We batch the sentences with similar length together and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows us to waste less computation when processing padded sequences.</p>
<p>In Trax, it is implemented in the <a href="https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378">bucket_by_length</a> function.</p>
</div>
<div class="outline-4" id="outline-container-orge1fbae7">
<h4 id="orge1fbae7">Bucketing to create streams of batches.</h4>
<div class="outline-text-4" id="text-orge1fbae7">
<p>Buckets are defined in terms of boundaries and batch sizes. Batch_sizes[i] determines the batch size for items with length &lt; boundaries[i]. So below, we'll take a batch of 256 sentences of length &lt; 8, 128 if length is between 8 and 16, and so on – and only 2 if length is over 512. We'll do the bucketing using <a href="https://trax-ml.readthedocs.io/en/latest/trax.data.html?highlight=bucket_by_length#trax.data.inputs.bucket_by_length">bucket_by_length</a>.</p>
<div class="highlight">
<pre><span></span><span class="n">boundaries</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">power_of_two</span> <span class="k">for</span> <span class="n">power_of_two</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
<span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">power_of_two</span> <span class="k">for</span> <span class="n">power_of_two</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
</pre></div>
<p>Create the generators.</p>
<div class="highlight">
<pre><span></span><span class="n">train_batch_stream</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">BucketByLength</span><span class="p">(</span>
    <span class="n">boundaries</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="p">,</span>
    <span class="n">length_keys</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># As before: count inputs and targets to length.</span>
<span class="p">)(</span><span class="n">filtered_train_stream</span><span class="p">)</span>

<span class="n">eval_batch_stream</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">BucketByLength</span><span class="p">(</span>
    <span class="n">boundaries</span><span class="p">,</span> <span class="n">batch_sizes</span><span class="p">,</span>
    <span class="n">length_keys</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">)(</span><span class="n">filtered_eval_stream</span><span class="p">)</span>
</pre></div>
<p>Add masking for the padding (0s) using <a href="https://trax-ml.readthedocs.io/en/latest/trax.data.html">add_loss_weights</a> (we're using <code>AddLossWeights</code> but the documentation for that just says "see add_loss_weights"). I can't find any documentation for it, but I think the 0's are what BucketByLength uses for padding.</p>
<div class="highlight">
<pre><span></span><span class="n">train_batch_stream</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AddLossWeights</span><span class="p">(</span><span class="n">id_to_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">train_batch_stream</span><span class="p">)</span>
<span class="n">eval_batch_stream</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AddLossWeights</span><span class="p">(</span><span class="n">id_to_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">eval_batch_stream</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org73fda4f">
<h3 id="org73fda4f">Exploring the data</h3>
<div class="outline-text-3" id="text-org73fda4f">
<p>We will now be displaying some of our data. You will see that the functions defined above (i.e. <code>tokenize()</code> and <code>detokenize()</code>) do the same things you have been doing again and again throughout the specialization. We gave these so you can focus more on building the model from scratch. Let us first get the data generator and get one batch of the data.</p>
<div class="highlight">
<pre><span></span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">mask_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">train_batch_stream</span><span class="p">)</span>
</pre></div>
<p>Let's see the data type of a batch.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"input_batch data type: "</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">input_batch</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"target_batch data type: "</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">target_batch</span><span class="p">))</span>
</pre></div>
<pre class="example">
input_batch data type:  &lt;class 'numpy.ndarray'&gt;
target_batch data type:  &lt;class 'numpy.ndarray'&gt;
</pre>
<p>Let's see the shape of this particular batch (batch length, sentence length).</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"input_batch shape: "</span><span class="p">,</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"target_batch shape: "</span><span class="p">,</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
input_batch shape:  (32, 64)
target_batch shape:  (32, 64)
</pre>
<p>The <code>input_batch</code> and <code>target_batch</code> are Numpy arrays consisting of tokenized English sentences and German sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix). The number of sentences in each batch is usually a power of 2 for optimal computer memory usage.</p>
<p>We can now visually inspect some of the data. You can run the cell below several times to shuffle through the sentences. Just to note, while this is a standard data set that is used widely, it does have some known wrong translations. With that, let's pick a random sentence and print its tokenized representation.</p>
<p>Pick a random index less than the batch size.</p>
<div class="highlight">
<pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_batch</span><span class="p">))</span>
</pre></div>
<p>Use the index to grab an entry from the input and target batch.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'THIS IS THE ENGLISH SENTENCE: </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">detokenize</span><span class="p">(</span><span class="n">input_batch</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">),</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: </span><span class="se">\n</span><span class="s1"> '</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">input_batch</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'THIS IS THE GERMAN TRANSLATION: </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">detokenize</span><span class="p">(</span><span class="n">target_batch</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">),</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">target_batch</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orgc8a8f7b">
[31mTHIS IS THE ENGLISH SENTENCE: 
[0m Kidneys and urinary tract (no effects were found to be common); uncommon: blood in the urine, proteins in the urine, sugar in the urine; rare: urge to pass urine, kidney pain, passing urine frequently.
 

[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: 
 [0m [ 5381 17607  3093     8  8670  6086   105 19166     5    50   154  1743
   152  1103     9    32   568  8076 19124  6847    64  6196     6     4
  8670   510     2 13355   823     6     4  8670   510     2  4968     6
     4  8670   510   115  7227    64  7628     9  2685  8670   510     2
 12220  5509 12095     2 19632  8670   510  7326  3550 30650  4729   992
     1     0     0     0] 

[31mTHIS IS THE GERMAN TRANSLATION: 
[0m Harndrang, Nierenschmerzen, häufiges Wasserlassen.
 

[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: 
[0m [ 5135 14970  2920     2  6262  4594 27552    28     2 20052    33  3736
   530  3550 30650  4729   992     1     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0] 
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org7516a44">
<h2 id="org7516a44">Bundle it Up</h2>
<div class="outline-text-2" id="text-org7516a44"></div>
<div class="outline-3" id="outline-container-orgcb57f09">
<h3 id="orgcb57f09">Imports</h3>
<div class="outline-text-3" id="text-orgcb57f09">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">trax</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfce077b">
<h3 id="orgfce077b">Constants</h3>
<div class="outline-text-3" id="text-orgfce077b">
<div class="highlight">
<pre><span></span><span class="n">DataDefaults</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"DataDefaults"</span><span class="p">,</span>
                          <span class="p">[</span><span class="s2">"path"</span><span class="p">,</span>
                           <span class="s2">"dataset"</span><span class="p">,</span>
                           <span class="s2">"keys"</span><span class="p">,</span>
                           <span class="s2">"evaluation_size"</span><span class="p">,</span>
                           <span class="s2">"end_of_sentence"</span><span class="p">,</span>
                           <span class="s2">"vocabulary_file"</span><span class="p">,</span>
                           <span class="s2">"vocabulary_path"</span><span class="p">,</span>
                           <span class="s2">"length_keys"</span><span class="p">,</span>
                           <span class="s2">"boundaries"</span><span class="p">,</span>
                           <span class="s2">"batch_sizes"</span><span class="p">,</span>
                           <span class="s2">"padding_token"</span><span class="p">])</span>

<span class="n">DEFAULTS</span> <span class="o">=</span> <span class="n">DataDefaults</span><span class="p">(</span>
    <span class="n">path</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="s2">"~/data/tensorflow/translation/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(),</span>
    <span class="n">dataset</span><span class="o">=</span><span class="s2">"opus/medical"</span><span class="p">,</span>
    <span class="n">keys</span><span class="o">=</span><span class="p">(</span><span class="s2">"en"</span><span class="p">,</span> <span class="s2">"de"</span><span class="p">),</span>
    <span class="n">evaluation_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">end_of_sentence</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">vocabulary_file</span><span class="o">=</span><span class="s2">"ende_32k.subword"</span><span class="p">,</span>
    <span class="n">vocabulary_path</span><span class="o">=</span><span class="s2">"gs://trax-ml/vocabs/"</span><span class="p">,</span>
    <span class="n">length_keys</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">boundaries</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">power_of_two</span> <span class="k">for</span> <span class="n">power_of_two</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)],</span>
    <span class="n">batch_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="o">**</span><span class="n">power_of_two</span> <span class="k">for</span> <span class="n">power_of_two</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span>
    <span class="n">padding_token</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">MaxLength</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"MaxLength"</span><span class="p">,</span> <span class="s2">"train evaluate"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="n">MaxLength</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">evaluate</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
<span class="n">END_OF_SENTENCE</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org88bc5e5">
<h3 id="org88bc5e5">Tokenizer/Detokenizer</h3>
<div class="outline-text-3" id="text-org88bc5e5"></div>
<div class="outline-4" id="outline-container-org6e0071c">
<h4 id="org6e0071c">Tokenizer</h4>
<div class="outline-text-4" id="text-org6e0071c">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">input_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
             <span class="n">vocab_file</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">end_of_sentence</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">end_of_sentence</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Encodes a string to an array of integers</span>

<span class="sd">    Args:</span>
<span class="sd">       input_str: human-readable string to encode</span>
<span class="sd">       vocab_file: filename of the vocabulary text file</span>
<span class="sd">       vocab_dir: path to the vocabulary file</span>
<span class="sd">       end_of_sentence: token for the end of sentence</span>
<span class="sd">    Returns:</span>
<span class="sd">       tokenized version of the input string</span>
<span class="sd">    """</span>
    <span class="c1"># The trax.data.tokenize method takes streams and returns streams,</span>
    <span class="c1"># we get around it by making a 1-element stream with `iter`.</span>
    <span class="n">inputs</span> <span class="o">=</span>  <span class="nb">next</span><span class="p">(</span><span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="nb">iter</span><span class="p">([</span><span class="n">input_str</span><span class="p">]),</span>
                                      <span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">,</span>
                                      <span class="n">vocab_dir</span><span class="o">=</span><span class="n">vocab_dir</span><span class="p">))</span>

    <span class="c1"># Mark the end of the sentence with EOS</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">end_of_sentence</span><span class="p">]</span>

    <span class="c1"># Adding the batch dimension to the front of the shape</span>
    <span class="n">batch_inputs</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch_inputs</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org85ca6e9">
<h4 id="org85ca6e9">Detokenizer</h4>
<div class="outline-text-4" id="text-org85ca6e9">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">detokenize</span><span class="p">(</span><span class="n">integers</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
               <span class="n">vocab_file</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">vocab_dir</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">end_of_sentence</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">end_of_sentence</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""Decodes an array of integers to a human readable string</span>

<span class="sd">    Args:</span>
<span class="sd">       integers: array of integers to decode</span>
<span class="sd">       vocab_file: filename of the vocabulary text file</span>
<span class="sd">       vocab_dir: path to the vocabulary file</span>
<span class="sd">       end_of_sentence: token to mark the end of a sentence</span>
<span class="sd">    Returns:</span>
<span class="sd">       str: the decoded sentence.</span>
<span class="sd">    """</span>
    <span class="c1"># Remove the dimensions of size 1</span>
    <span class="n">integers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">integers</span><span class="p">))</span>

    <span class="c1"># Remove the EOS to decode only the original tokens</span>
    <span class="k">if</span> <span class="n">end_of_sentence</span> <span class="ow">in</span> <span class="n">integers</span><span class="p">:</span>
        <span class="n">integers</span> <span class="o">=</span> <span class="n">integers</span><span class="p">[:</span><span class="n">integers</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">end_of_sentence</span><span class="p">)]</span> 

    <span class="k">return</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">integers</span><span class="p">,</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">vocab_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org2f4c821">
<h3 id="org2f4c821">Data Generator</h3>
<div class="outline-text-3" id="text-org2f4c821">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">DataGenerator</span><span class="p">:</span>
    <span class="sd">"""Generates the streams of data</span>

<span class="sd">    Args:</span>
<span class="sd">     training: whether this generates training data or not</span>
<span class="sd">     path: path to the data set</span>
<span class="sd">     data_set: name of the data set (from tensorflow datasets)</span>
<span class="sd">     keys: the names of the data</span>
<span class="sd">     max_length: longest allowed set of tokens</span>
<span class="sd">     evaluation_fraction: how much of the data is saved for evaluation</span>
<span class="sd">     length_keys: keys (indexes) to use when setting length</span>
<span class="sd">     boundaries: upper limits for batch sizes</span>
<span class="sd">     batch_sizes: batch_size for each boundary</span>
<span class="sd">     padding_token: which token is used for padding</span>
<span class="sd">     vocabulary_file: name of the sub-words vocabulary file</span>
<span class="sd">     vocabulary_path: where to find the vocabulary file</span>
<span class="sd">     end_of_sentence: token to indicate the end of a sentence</span>
<span class="sd">    """</span>
    <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
    <span class="n">path</span><span class="p">:</span> <span class="n">Path</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">path</span>
    <span class="n">data_set</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">dataset</span>
    <span class="n">keys</span><span class="p">:</span> <span class="nb">tuple</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">keys</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="o">.</span><span class="n">train</span>
    <span class="n">length_keys</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">length_keys</span>
    <span class="n">boundaries</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">boundaries</span>
    <span class="n">batch_sizes</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">batch_sizes</span>
    <span class="n">evaluation_fraction</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">evaluation_size</span>
    <span class="n">vocabulary_file</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">vocabulary_file</span>
    <span class="n">vocabulary_path</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">vocabulary_path</span>
    <span class="n">padding_token</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">padding_token</span>
    <span class="n">end_of_sentence</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">DEFAULTS</span><span class="o">.</span><span class="n">end_of_sentence</span>
    <span class="n">_generator_function</span><span class="p">:</span> <span class="nb">type</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_batch_generator</span><span class="p">:</span> <span class="nb">type</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org4edeed6">
<h4 id="org4edeed6">Append End of Sentence</h4>
<div class="outline-text-4" id="text-org4edeed6">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">end_of_sentence_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original</span><span class="p">):</span>
    <span class="sd">"""Generator that adds end of sentence tokens</span>

<span class="sd">    Args:</span>
<span class="sd">     original: generator to add the end of sentence tokens to</span>

<span class="sd">    Yields:</span>
<span class="sd">     next tuple of arrays with EOS token added</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">original</span><span class="p">:</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">end_of_sentence</span><span class="p">]</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">end_of_sentence</span><span class="p">]</span>
        <span class="k">yield</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> 
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf640abd">
<h4 id="orgf640abd">Generator Function</h4>
<div class="outline-text-4" id="text-orgf640abd">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">generator_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Function to create the data generator"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generator_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_generator_function</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFDS</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_set</span><span class="p">,</span>
                                                  <span class="n">data_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">path</span><span class="p">,</span>
                                                  <span class="n">keys</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">,</span>
                                                  <span class="n">eval_holdout_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_fraction</span><span class="p">,</span>
                                                  <span class="n">train</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generator_function</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga634fb3">
<h4 id="orga634fb3">Batch Stream</h4>
<div class="outline-text-4" id="text-orga634fb3">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">batch_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""batch data generator"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_generator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">generator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_function</span><span class="p">()</span>
        <span class="n">generator</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Tokenize</span><span class="p">(</span>
            <span class="n">vocab_file</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_file</span><span class="p">,</span>
            <span class="n">vocab_dir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_path</span><span class="p">)(</span><span class="n">generator</span><span class="p">)</span>
        <span class="n">generator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_of_sentence_generator</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span>
        <span class="n">generator</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">FilterByLength</span><span class="p">(</span>
            <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
            <span class="n">length_keys</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">length_keys</span><span class="p">)(</span><span class="n">generator</span><span class="p">)</span>
        <span class="n">generator</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">BucketByLength</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">boundaries</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_sizes</span><span class="p">,</span>
            <span class="n">length_keys</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">length_keys</span>
        <span class="p">)(</span><span class="n">generator</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_generator</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">AddLossWeights</span><span class="p">(</span>
            <span class="n">id_to_mask</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_token</span><span class="p">)(</span><span class="n">generator</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_generator</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7092d91">
<h3 id="org7092d91">Try It Out</h3>
<div class="outline-text-3" id="text-org7092d91">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.machine_translation</span> <span class="kn">import</span> <span class="n">DataGenerator</span><span class="p">,</span> <span class="n">detokenize</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">DataGenerator</span><span class="p">()</span><span class="o">.</span><span class="n">batch_generator</span>
<span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">mask_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>


<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'THIS IS THE ENGLISH SENTENCE: </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">detokenize</span><span class="p">(</span><span class="n">input_batch</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">),</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: </span><span class="se">\n</span><span class="s1"> '</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">input_batch</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'THIS IS THE GERMAN TRANSLATION: </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">detokenize</span><span class="p">(</span><span class="n">target_batch</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">vocab_file</span><span class="o">=</span><span class="n">VOCAB_FILE</span><span class="p">,</span> <span class="n">vocab_dir</span><span class="o">=</span><span class="n">VOCAB_DIR</span><span class="p">),</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">colored</span><span class="p">(</span><span class="s1">'THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: </span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="s1">'red'</span><span class="p">),</span> <span class="n">target_batch</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orgf57d507">
[31mTHIS IS THE ENGLISH SENTENCE: 
[0m Signs of hypersensitivity reactions include hives, generalised urticaria, tightness of the chest, wheezing, hypotension and anaphylaxis.
 

[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: 
 [0m [10495    14     7 10224 19366 10991  1020  3481  2486     2  9547  7417
   103  4572 11927  9371     2 13197  1496     7     4 24489    62     2
 16402 24010   211     2  4814 23010 12122    22     8  4867 19606  6457
  5175    14  3550 30650  4729   992     1     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0] 

[31mTHIS IS THE GERMAN TRANSLATION: 
[0m Überempfindlichkeitsreaktionen können sich durch Anzeichen wie Nesselausschlag, generalisierte Urtikaria, Engegefühl im Brustkorb, Pfeifatmung, Blutdruckabfall und Anaphylaxie äußern.
 

[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: 
[0m [ 3916 29551 13504  5020  4094 13522   119    51   121  8602    93 31508
  6050 30327  6978     2  9547  7417  2446  5618  4581  5530  1384     2
 26006  7831 13651     5    47  8584  4076  5262   868     2 25389  8898
 28268     2  9208 29697 17944    83    12  9925 19606  6457 16384     5
 11790  3550 30650  4729   992     1     0     0     0     0     0     0
     0     0     0     0] 
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orga61fbcf">
<h2 id="orga61fbcf">End</h2>
<div class="outline-text-2" id="text-orga61fbcf">
<p>Now that we have our data prepared it's time to move on to <a href="../neural-machine-translation-the-attention-model/">defining the Attention Model</a>.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/machine-translation/" rel="tag">machine translation</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../neural-machine-translation/" rel="prev" title="Neural Machine Translation">Previous post</a></li>
<li class="next"><a href="../neural-machine-translation-the-attention-model/" rel="next" title="Neural Machine Translation: The Attention Model">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
