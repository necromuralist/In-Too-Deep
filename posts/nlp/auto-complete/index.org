#+BEGIN_COMMENT
.. title: Auto-Complete
.. slug: auto-complete
.. date: 2020-12-03 18:15:23 UTC-08:00
.. tags: nlp,n-grams,auto-complete
.. category: NLP
.. link: 
.. description: Using n-grams to make an auto-completer.
.. type: text

#+END_COMMENT
* The Parts
  - {{% lancelot title="Pre-Processing I" %}}auto-complete-pre-process-the-data-i{{% /lancelot %}}
  - {{% lancelot title="Pre-Processing II" %}}auto-complete-pre-process-the-data-ii{{% /lancelot %}}
  - {{% lancelot title="The N-Gram Model" %}}auto-complete-the-n-gram-model{{% /lancelot %}}
  - {{% lancelot title="Perplexity" %}}auto-complete-perplexity{{% /lancelot %}}
  - {{% lancelot title="The Complete System" %}}auto-complete-building-the-auto-complete-system{{{% /lancelot %}}
* Raw
#+begin_example
# # Language Models: Auto-Complete
# 
# In this assignment, you will build an auto-complete system.  Auto-complete system is something you may see every day
# - When you google something, you often have suggestions to help you complete your search. 
# - When you are writing an email, you get suggestions telling you possible endings to your sentence.  
# 
# By the end of this assignment, you will develop a prototype of such a system.
# 
# <img src = "stanford.png" style="width:700px;height:300px;"/>

# ## Outline
# - [1 Load and Preprocess Data](#1)
# - [1.1: Load the data](#1.1)
# - [1.2 Pre-process the data](#1.2)
#     - [Exercise 01](#ex-01)
#     - [Exercise 02](#ex-02)
#     - [Exercise 03](#ex-03)
#     - [Exercise 04](#ex-04)
#     - [Exercise 05](#ex-05)
#     - [Exercise 06](#ex-06)
#     - [Exercise 07](#ex-07)
# - [2 Develop n-gram based language models](#2)
#     - [Exercise 08](#ex-08)
#     - [Exercise 09](#ex-09)    
# - [3 Perplexity](#3)
#     - [Exercise 10](#ex-10)
# - [4 Build an auto-complete system](#4)
#     - [Exercise 11](#ex-11)

# A key building block for an auto-complete system is a language model.
# A language model assigns the probability to a sequence of words, in a way that more "likely" sequences receive higher scores.  For example, 
# >"I have a pen" 
# is expected to have a higher probability than 
# >"I am a pen"
# since the first one seems to be a more natural sentence in the real world.
# 
# You can take advantage of this probability calculation to develop an auto-complete system.  
# Suppose the user typed 
# >"I eat scrambled"
# Then you can find a word `x`  such that "I eat scrambled x" receives the highest probability.  If x = "eggs", the sentence would be
# >"I eat scrambled eggs"
# 
# While a variety of language models have been developed, this assignment uses **N-grams**, a simple but powerful method for language modeling.
# - N-grams are also used in machine translation and speech recognition. 
# 
# 
# Here are the steps of this assignment:
# 
# 1. Load and preprocess data
#     - Load and tokenize data.
#     - Split the sentences into train and test sets.
#     - Replace words with a low frequency by an unknown marker `<unk>`.
# 1. Develop N-gram based language models
#     - Compute the count of n-grams from a given data set.
#     - Estimate the conditional probability of a next word with k-smoothing.
# 1. Evaluate the N-gram models by computing the perplexity score.
# 1. Use your own model to suggest an upcoming word given your sentence. 

# In[ ]:


import math
import random
import numpy as np
import pandas as pd
import nltk
nltk.data.path.append('.')




#+end_example
