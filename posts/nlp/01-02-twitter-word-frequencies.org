#+BEGIN_COMMENT
.. title: Twitter Word Frequencies
.. slug: twitter-word-frequencies
.. date: 2020-07-07 18:19:19 UTC-07:00
.. tags: nlp,nltk,twitter
.. category: Word Frequencies
.. link: 
.. description: Looking at tweet word frequencies.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-9a4aeb7b-e200-4c23-b9f5-244f9fc113c9.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
** Setup
*** Imports
#+begin_src python :results none
# from python
from argparse import Namespace
from functools import partial

# from pypi
from bokeh.models import HoverTool
from nltk.corpus import twitter_samples
from tabulate import tabulate

import holoviews
import hvplot.pandas
import load_dotenv
import nltk
import numpy
import pandas

# this project
from neurotic.nlp.twitter.processor import TwitterProcessor
from neurotic.nlp.twitter.counter import WordCounter

# some helper stuff
from graeae import EmbedHoloviews
#+end_src
*** The Data
    First we'll load the training data that I set-up while building the tweet pre-processor.
#+begin_src python :results none
positive_tweets = twitter_samples.strings('positive_tweets.json')
negative_tweets = twitter_samples.strings('negative_tweets.json')
tweets = positive_tweets + negative_tweets
#+end_src

Now we need to make the labels for the tweets. We're going to label positive tweets with a =1= and negative tweets with a =0=. Since we concatenated the two sets of tweets we know that the first half will be all ones and the second half will be all zeros and we can create this master labels array using lists.

#+begin_src python :results output :exports both
NEGATIVE, POSITIVE = 0, 1
SENTIMENT = {
    POSITIVE:"positive",
    NEGATIVE:"negative",
}
labels = [1] * len(positive_tweets) + [0] * len(negative_tweets)
print(f"{len(labels):,}")
#+end_src

#+RESULTS:
: 10,000

*** Plotting and Printing
    Just some preliminary setup of the plotter and table-printer so I don't have to keep typing the same things over and over.

#+begin_src python :results none
Embed = partial(EmbedHoloviews,
                folder_path="../../files/posts/nlp/twitter-word-frequencies")

Plot = Namespace(
    width=990,
    height=780,
    color="#ddb377",
)
#+end_src

#+begin_src python :results none
TABLE = partial(tabulate, tablefmt="orgtbl", headers="keys", showindex=False)
#+end_src
* Middle
** Word Frequencies
   We're going to build up a dictionary of frequencies. The keys will be =(token, sentiment)= tuples and the values will be the counts for the token-sentiment pairs.
*** Tests
**** The Tangles
#+begin_src feature :tangle ../../tests/features/twitter/word_frequencies.feature
Feature: A Word Frequency Counter

In order to get a sense of how the words correlate with sentiment
I want to be able to count word-sentiment pairs.

<<counter-feature>>

<<call-feature>>
#+end_src

#+begin_src python :tangle ../../tests/functional/twitter/test_word_frequencies.py
# pypi
from expects import (
    be,
    equal,
    expect
    )

from pytest_bdd import (
    given,
    scenarios,
    then,
    when
)

# testing setup
from fixtures import katamari

# software under test
from neurotic.nlp.twitter.counter import WordCounter
from neurotic.nlp.twitter.processor import TwitterProcessor

scenarios("../../features/twitter/word_frequencies.feature")

<<test-creation>>


<<test-call>>
#+end_src
**** Setup
#+begin_src feature :noweb-ref counter-feature
Scenario: The Word Counter is created
  Given a word counter class
  When the word counter is created
  Then it has the expected attributes
#+end_src

#+begin_src python :noweb-ref test-creation
# Scenario: The Word Counter is created


@given("a word counter class")
def setup_class(katamari):
    katamari.definition = WordCounter
    return


@when("the word counter is created")
def create_word_counter(katamari, faker, mocker):
    katamari.tweets = mocker.Mock(list)
    katamari.labels = mocker.Mock(list)
    katamari.processor = mocker.Mock()
    katamari.counter = katamari.definition(tweets=katamari.tweets,
                                           labels=katamari.labels)
    katamari.counter._process = katamari.processor
    return


@then("it has the expected attributes")
def check_attributes(katamari):
    expect(katamari.counter.tweets).to(be(katamari.tweets))
    expect(katamari.counter.labels).to(be(katamari.labels))
    expect(katamari.counter.process).to(be(katamari.processor))
    return
#+end_src
**** The Call
#+begin_src feature :noweb-ref call-feature
Scenario: The Word Frequency counter is called
  Given a word frequency counter
  When the counter is called
  Then the counts are the expected
#+end_src

#+begin_src python :noweb-ref test-call
# Scenario: The Word Frequency counter is called


@given("a word frequency counter")
def setup_word_frequency_counter(katamari, mocker):
    processor = TwitterProcessor()
    katamari.tweets = ["a b aab a b c"]
    katamari.labels = [1] * len(katamari.tweets)
    katamari.counter = WordCounter(tweets=katamari.tweets,
                                   labels=katamari.labels)

    bad_sentiment = ["c aab aab"]
    katamari.tweets += bad_sentiment
    katamari.labels += [0]
    # since the tokenizer removes and changes words
    # I'm going to mock it out
    katamari.counter._process = mocker.MagicMock(TwitterProcessor)
    katamari.counter.process.side_effect = lambda x: x.split()
    katamari.expected = {("a", 1): 2, ("b", 1): 2, ("c", 1): 1, ("aab", 1):1,
                         ("c", 0): 1, ("aab", 0): 2}
    return


@when("the counter is called")
def call_counter(katamari):
    katamari.counts = katamari.counter.counts
    return


@then("the counts are the expected")
def check_counts(katamari):
    for key, value in katamari.counts.items():
        expect(katamari.expected[key]).to(equal(value))
    return
#+end_src
*** Implementation
    This is going to be a counter class that pre-processes the tweets and then counts the frequency of word-sentiment pairs.

#+begin_src python :tangle ../../neurotic/nlp/twitter/counter.py
# A Word Counter

# from python
from collections import Counter
import typing

# from pypi
import attr

# this project
from .processor import TwitterProcessor

@attr.s(auto_attribs=True)
class WordCounter:
    """A word-sentiment counter

    Args:
     tweets: list of unprocessed tweets
     labels: list of 1's (positive) and 0's that identifies sentiment for each tweet
    """
    tweets: typing.List[str]
    labels: typing.List[int]
    _process: TwitterProcessor = None
    _processed: list = None
    _counts: Counter = None

    @property
    def process(self) -> TwitterProcessor:
        """A callable to process tweets to lists of words"""
        if self._process is None:
            self._process = TwitterProcessor()
        return self._process

    @property
    def processed(self) -> list:
        """The processed and tokenized tweets"""
        if self._processed is None:
            self._processed = [self.process(tweet) for tweet in self.tweets]
        return self._processed

    @property
    def counts(self) -> Counter:
        """Processes the tweets and labels

        Returns:
         counts of word-sentiment pairs
        """
        if self._counts is None:
            assert len(self.tweets) == len(self.labels), \
                f"Tweets: {len(self.tweets)}, Labels: {len(self.labels)}"
            self._counts = Counter()
            for tweet, label in zip(self.processed, self.labels):
                for word in tweet:
                    self._counts[(word, label)] += 1
        return self._counts
#+end_src
** Counting
   Now we can do some counting.

#+begin_src python :results output :exports both
counter = WordCounter(tweets=tweets, labels=labels)
counts = counter.counts
print(f"Total token-sentiment pairs: {len(counts):,}")
#+end_src

#+RESULTS:
: Total token-sentiment pairs: 13,173

What are the most common? To make the rest of the post easier I'm going to set up a pandas DataFrame.

#+begin_src python :results none
tokens = []
top_counts = []
sentiments = []

for key, count in counts.most_common():
    token, sentiment = key
    tokens.append(token)
    sentiments.append(sentiment)
    top_counts.append(count)

top_counts = pandas.DataFrame.from_dict(dict(
    token=tokens,
    count=top_counts,
    sentiment=sentiments,
))

top_counts.loc[:, "sentiment"] = top_counts.sentiment.apply(lambda row: SENTIMENT[row])
#+end_src

#+begin_src python :results output :exports both
print(TABLE(top_counts.iloc[:20]))
#+end_src

| token   |   count | sentiment   |
|---------+---------+-------------|
| :(      |    4571 | negative    |
| :)      |    3568 | positive    |
| :-)     |     692 | positive    |
| :D      |     629 | positive    |
| thank   |     620 | positive    |
| :-(     |     493 | negative    |
| love    |     400 | positive    |
| follow  |     381 | positive    |
| i'm     |     343 | negative    |
| ...     |     331 | negative    |
| thi     |     318 | negative    |
| thi     |     303 | positive    |
| miss    |     301 | negative    |
| ...     |     289 | positive    |
| pleas   |     275 | negative    |
| follow  |     262 | negative    |
| day     |     246 | positive    |
| want    |     246 | negative    |
| wa      |     241 | negative    |
| good    |     238 | positive    |


It's interesting that the only repeated tokens in the top 20 are ellipses, "follow" and "thi" and that the four most common tokens were smileys, although that's not surprising, perhaps. I didn't notice this at first, but the most common token is a negative one.

** Plotting
   The counts themselves are interesting, but it might be more informative to look at their distribution as well as whether some tokens are more positive or negative.
*** Positive Vs Negative
#+begin_src python :results none
plot = top_counts.hvplot(kind="bar", x="sentiment", y="count").opts(
    width=Plot.width,
    height= Plot.height,
    title="Positive and Negative",
    fontscale=2,
    color=Plot.color,
)
embedded = Embed(plot=plot, file_name="positive_negative_distribution")
output = embedded()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="positive_negative_distribution.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

So it looks like negative sentiment is more common for the tokens, even though the tweets themselves were evenly split, suggesting that the negative tweets had a greater diversity of words.

*** Distribution
#+begin_src python :results none
tooltips = [
    ("Token", "@token"),
    ("Sentiment", "@sentiment"),
    ("Count", "@count"),
]

hover = HoverTool(tooltips=tooltips)

CUTOFF = 150

plot = top_counts[:CUTOFF].hvplot.bar(
    y="count", hover_cols=["token", "sentiment"],
    loglog=True).opts(
        tools=[hover],
        width=Plot.width,
        height=Plot.height,
        fontscale=2,
        color=Plot.color,
        line_color=Plot.color,
        xaxis=None,
        title=f"Log-Log Count Distribution (top {CUTOFF})")
output = Embed(plot=plot, file_name="count_distribution")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="count_distribution.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

This shows how steep the drop is from the two most common tokens which are then followed by a long tail. Without the logarithmic axes the drop is even more pronounced.

*** Positive Vs Negative by Tweet
#+begin_src python :results none
CUTOFF = 250

top_counts.loc[:, "positive"] = top_counts.apply(
    lambda row: row["count"] if row.sentiment=="positive" else 0,
    axis="columns")

top_counts.loc[:, "negative"] = top_counts.apply(
    lambda row: row["count"] if row.sentiment=="negative" else 0,
    axis="columns"
)

tooltips = [
    ("Token", "@token"),
    ("Positive", "@positive"),
    ("Negative", "@negative"),
]

hover = HoverTool(tooltips=tooltips)

grouped = top_counts.groupby("token").agg({"positive": "sum", "negative": "sum"})
to_plot = grouped.reset_index()

# log plots can't have zero values
MIN = 1
for column in ("positive", "negative"):
    to_plot.loc[:, column] = to_plot[column] + 1

MAX = to_plot.negative.max() + 1
line = holoviews.Curve(([MIN, MAX], [MIN, MAX])).opts(color="#ce7b6d")
scatter = to_plot.hvplot.scatter(
    color="#4687b7",
    x="positive", y="negative",
    hover_cols=["token"])
plot = (line * scatter ).opts(
        tools=[hover],
        width=Plot.width,
        height=Plot.height,
        logx=True,
        logy=True,
        xlabel="Positive",
        ylabel="Negative",
        fontscale=2,
        title="Log-Log Positive vs Negative")
output = Embed(plot=plot, file_name="scatter_plot")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="scatter_plot.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

So, we basically end up with two types of groupings - some tokens are lopsided to be either very negative or very positive and they show up in the straight line columns or rows, while other tokens are more evenly split, and they show up in the more distributed blob along the diagonal of the plot.

We can also see which tokens are the most negative (the highest along the y-axis) and the most positive (furthest along the x-axis).

The tokens along or around the red diagonal are evenly positive and negative so they probably aren't useful indicators of sentiment in and of themselves, while those furthest from the diagonal are the most biased to one side or the other so we might expect them to be useful in guessing a tweet's sentiment.

There are some unexpectedly negative tokens like "love" (400, 152) and "thank" (620, 107), but at this point we haven't really started to look at the sentiment yet so I'll leave further exploration for later.
* End
**Note:** This is a re-working of an exercise from Coursera's Natural Language Processing specialization.

