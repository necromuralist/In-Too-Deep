#+BEGIN_COMMENT
.. title: Sentiment Analysis: Defining the Model
.. slug: sentiment-analysis-defining-the-model
.. date: 2020-12-23 15:46:13 UTC-08:00
.. tags: nlp,sentiment analysis,deep learning
.. category: NLP
.. link: 
.. description: Defining the Deep Learning Model
.. type: text

#+END_COMMENT
* Beginning
  This continues a series on {{% lancelot title="sentiment analysis with deep learning" %}}sentiment-analysis-deep-learning-model{{% /lancelot %}}. In the {{% lancelot title="previous post" %}}sentiment-analysis-pre-processing-the-data{{% /lancelot %}} we loaded and processed our data set. In this post we'll see about actually defining the Neural Network.
* Ending
  Now that we have our Deep Learning model, we'll move on to {{% lancelot title="training it" %}}sentiment-analysis-training-the-model{{% /lancelot %}}.
* Raw
#+begin_example
# # Part 3:  Defining classes
# 
# In this part, you will write your own library of layers. It will be very similar
# to the one used in Trax and also in Keras and PyTorch. Writing your own small
# framework will help you understand how they all work and use them effectively
# in the future.
# 
# Your framework will be based on the following `Layer` class from utils.py.
# 
# ```CPP
# class Layer(object):
#     """ Base class for layers.
#     """
#       
#     # Constructor
#     def __init__(self):
#         # set weights to None
#         self.weights = None
# 
#     # The forward propagation should be implemented
#     # by subclasses of this Layer class
#     def forward(self, x):
#         raise NotImplementedError
# 
#     # This function initializes the weights
#     # based on the input signature and random key,
#     # should be implemented by subclasses of this Layer class
#     def init_weights_and_state(self, input_signature, random_key):
#         pass
# 
#     # This initializes and returns the weights, do not override.
#     def init(self, input_signature, random_key):
#         self.init_weights_and_state(input_signature, random_key)
#         return self.weights
#  
#     # __call__ allows an object of this class
#     # to be called like it's a function.
#     def __call__(self, x):
#         # When this layer object is called, 
#         # it calls its forward propagation function
#         return self.forward(x)
# ```

# <a name="3.1"></a>
# ## 3.1  ReLU class
# You will now implement the ReLU activation function in a class below. The ReLU function looks as follows: 
# <img src = "relu.jpg" style="width:300px;height:150px;"/>
# 
# $$ \mathrm{ReLU}(x) = \mathrm{max}(0,x) $$
# 

# <a name="ex03"></a>
# ### Exercise 03
# **Instructions:** Implement the ReLU activation function below. Your function should take in a matrix or vector and it should transform all the negative numbers into 0 while keeping all the positive numbers intact. 

# <details>    
# <summary>
#     <font size="3" color="darkgreen"><b>Hints</b></font>
# </summary>
# <p>
# <ul>
#     <li>Please use numpy.maximum(A,k) to find the maximum between each element in A and a scalar k</li>
# </ul>
# </p>
# 

# In[ ]:


# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: Relu
class Relu(Layer):
    """Relu activation function implementation"""
    def forward(self, x):
        '''
        Input: 
            - x (a numpy array): the input
        Output:
            - activation (numpy array): all positive or 0 version of x
        '''
        ### START CODE HERE (Replace instances of 'None' with your code) ###
        
        activation = None

        ### END CODE HERE ###
        
        return activation


# In[ ]:


# Test your relu function
x = np.array([[-2.0, -1.0, 0.0], [0.0, 1.0, 2.0]], dtype=float)
relu_layer = Relu()
print("Test data is:")
print(x)
print("Output of Relu is:")
print(relu_layer(x))


# ##### Expected Outout
# ```CPP
# Test data is:
# [[-2. -1.  0.]
#  [ 0.  1.  2.]]
# Output of Relu is:
# [[0. 0. 0.]
#  [0. 1. 2.]]
# ```

# <a name="3.2"></a>
# ## 3.2  Dense class 
# 
# ### Exercise
# 
# Implement the forward function of the Dense class. 
# - The forward function multiplies the input to the layer (`x`) by the weight matrix (`W`)
# 
# $$\mathrm{forward}(\mathbf{x},\mathbf{W}) = \mathbf{xW} $$
# 
# - You can use `numpy.dot` to perform the matrix multiplication.
# 
# Note that for more efficient code execution, you will use the trax version of `math`, which includes a trax version of `numpy` and also `random`.
# 
# Implement the weight initializer `new_weights` function
# - Weights are initialized with a random key.
# - The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols)
# - The num of rows for weights should equal the number of columns in x, because for forward propagation, you will multiply x times weights.
# 
# Please use `trax.fastmath.random.normal(key, shape, dtype=tf.float32)` to generate random values for the weight matrix. The key difference between this function
# and the standard `numpy` randomness is the explicit use of random keys, which
# need to be passed. While it can look tedious at the first sight to pass the random key everywhere, you will learn in Course 4 why this is very helpful when
# implementing some advanced models.
# - `key` can be generated by calling `random.get_prng(seed=)` and passing in a number for the `seed`.
# - `shape` is a tuple with the desired shape of the weight matrix.
#     - The number of rows in the weight matrix should equal the number of columns in the variable `x`.  Since `x` may have 2 dimensions if it reprsents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x.
#     - The number of columns in the weight matrix is the number of units chosen for that dense layer.  Look at the `__init__` function to see which variable stores the number of units.
# - `dtype` is the data type of the values in the generated matrix; keep the default of `tf.float32`. In this case, don't explicitly set the dtype (just let it use the default value).
# 
# Set the standard deviation of the random values to 0.1
# - The values generated have a mean of 0 and standard deviation of 1.
# - Set the default standard deviation `stdev` to be 0.1 by multiplying the standard deviation to each of the values in the weight matrix.

# In[ ]:


# use the fastmath module within trax
from trax import fastmath

# use the numpy module from trax
np = fastmath.numpy

# use the fastmath.random module from trax
random = fastmath.random


# In[ ]:


# See how the fastmath.trax.random.normal function works
tmp_key = random.get_prng(seed=1)
print("The random seed generated by random.get_prng")
display(tmp_key)

print("choose a matrix with 2 rows and 3 columns")
tmp_shape=(2,3)
display(tmp_shape)

# Generate a weight matrix
# Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow
# Just avoid setting the dtype and allow it to use the default data type
tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)

print("Weight matrix generated with a normal distribution with mean 0 and stdev of 1")
display(tmp_weight)


# <a name="ex04"></a>
# ### Exercise 04
# 
# Implement the `Dense` class.

# In[ ]:


# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: Dense

class Dense(Layer):
    """
    A dense (fully-connected) layer.
    """

    # __init__ is implemented for you
    def __init__(self, n_units, init_stdev=0.1):
        
        # Set the number of units in this layer
        self._n_units = n_units
        self._init_stdev = init_stdev

    # Please implement 'forward()'
    def forward(self, x):

### START CODE HERE (Replace instances of 'None' with your code) ###

        # Matrix multiply x and the weight matrix
        dense = None 
        
### END CODE HERE ###
        return dense

    # init_weights
    def init_weights_and_state(self, input_signature, random_key):
        
### START CODE HERE (Replace instances of 'None' with your code) ###
        # The input_signature has a .shape attribute that gives the shape as a tuple
        input_shape = None

        # Generate the weight matrix from a normal distribution, 
        # and standard deviation of 'stdev'        
        w = None
        
### END CODE HERE ###     
        self.weights = w
        return self.weights


# In[ ]:


# Testing your Dense layer 
dense_layer = Dense(n_units=10)  #sets  number of units in dense layer
random_key = random.get_prng(seed=0)  # sets random seed
z = np.array([[2.0, 7.0, 25.0]]) # input array 

dense_layer.init(z, random_key)
print("Weights are\n ",dense_layer.weights) #Returns randomly generated weights
print("Foward function output is ", dense_layer(z)) # Returns multiplied values of units and weights


# ##### Expected Outout
# ```CPP
# Weights are
#   [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126
#   -0.04265672  0.0986188  -0.05575325  0.00153249]
#  [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617
#   -0.03237354  0.16234995  0.02450038 -0.13809784]
#  [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459
#   -0.05933381 -0.01557652 -0.03832145 -0.11144515]]
# Foward function output is  [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209
#   -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]
# ```

# <a name="3.3"></a>
# ## 3.3  Model
# 
# Now you will implement a classifier using neural networks. Here is the model architecture you will be implementing. 
# 
# <img src = "nn.jpg" style="width:400px;height:250px;"/>
# 
# For the model implementation, you will use the Trax layers library `tl`.
# Note that the second character of `tl` is the lowercase of letter `L`, not the number 1. Trax layers are very similar to the ones you implemented above,
# but in addition to trainable weights also have a non-trainable state.
# State is used in layers like batch normalization and for inference, you will learn more about it in course 4.
# 
# First, look at the code of the Trax Dense layer and compare to your implementation above.
# - [tl.Dense](https://github.com/google/trax/blob/master/trax/layers/core.py#L29): Trax Dense layer implementation
# 
# One other important layer that you will use a lot is one that allows to execute one layer after another in sequence.
# - [tl.Serial](https://github.com/google/trax/blob/master/trax/layers/combinators.py#L26): Combinator that applies layers serially.  
#     - You can pass in the layers as arguments to `Serial`, separated by commas. 
#     - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))`
# 
# Please use the `help` function to view documentation for each layer.

# In[ ]:


# View documentation on tl.Dense
help(tl.Dense)


# In[ ]:


# View documentation on tl.Serial
help(tl.Serial)


# - [tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113): Layer constructor function for an embedding layer.  
#     - `tl.Embedding(vocab_size, d_feature)`.
#     - `vocab_size` is the number of unique words in the given vocabulary.
#     - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).

# In[ ]:


# View documentation for tl.Embedding
help(tl.Embedding)


# In[ ]:


tmp_embed = tl.Embedding(vocab_size=3, d_feature=2)
display(tmp_embed)


# - [tl.Mean](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276): Calculates means across an axis.  In this case, please choose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).  
# - For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements.

# In[ ]:


# view the documentation for tl.mean
help(tl.Mean)


# In[ ]:


# Pretend the embedding matrix uses 
# 2 elements for embedding the meaning of a word
# and has a vocabulary size of 3
# So it has shape (2,3)
tmp_embed = np.array([[1,2,3,],
                    [4,5,6]
                   ])

# take the mean along axis 0
print("The mean along axis 0 creates a vector whose length equals the vocabulary size")
display(np.mean(tmp_embed,axis=0))

print("The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding")
display(np.mean(tmp_embed,axis=1))


# - [tl.LogSoftmax](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242): Implements log softmax function
# - Here, you don't need to set any parameters for `LogSoftMax()`.

# In[ ]:


help(tl.LogSoftmax)


# **Online documentation**
# 
# - [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)
# 
# - [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators)
# 
# - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)
# 
# - [tl.Mean](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean)
# 
# - [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax)

# <a name="ex05"></a>
# ### Exercise 05
# Implement the classifier function. 

# In[ ]:


# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: classifier
def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):
        
### START CODE HERE (Replace instances of 'None' with your code) ###
    # create embedding layer
    embed_layer = tl.Embedding(
        vocab_size=None, # Size of the vocabulary
        d_feature=None)  # Embedding dimension
    
    # Create a mean layer, to create an "average" word embedding
    mean_layer = None
    
    # Create a dense layer, one unit for each output
    dense_output_layer = tl.Dense(n_units = None)

    
    # Create the log softmax layer (no parameters needed)
    log_softmax_layer = None
    
    # Use tl.Serial to combine all layers
    # and create the classifier
    # of type trax.layers.combinators.Serial
    model = tl.Serial(
      None, # embedding layer
      None, # mean layer
      None, # dense output layer 
      None # log softmax layer
    )
### END CODE HERE ###     
    
    # return the model of type
    return model


# In[ ]:


tmp_model = classifier()


# In[ ]:


print(type(tmp_model))
display(tmp_model)


# ##### Expected Outout
# ```CPP
# <class 'trax.layers.combinators.Serial'>
# Serial[
#   Embedding_9088_256
#   Mean
#   Dense_2
#   LogSoftmax
# ]
# ```

# <a name="4"></a>
#+end_example
