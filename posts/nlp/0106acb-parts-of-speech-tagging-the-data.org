#+BEGIN_COMMENT
.. title: Parts-of-Speech Tagging: The Data
.. slug: parts-of-speech-tagging-the-data
.. date: 2020-11-15 16:18:03 UTC-08:00
.. tags: nlp,pos tagging
.. category: NLP
.. link: 
.. description: Loading the Wall Street Journal POS data.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-6fa93b57-6166-4019-9c30-50c4eecde2f1-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
** Imports
#+begin_src python :results none
# python
from argparse import Namespace
from collections import Counter

import os
import random
import re
import string

# from pypi
from dotenv import load_dotenv
#+end_src
** Set Up
*** The Environment
#+begin_src python :results none
load_dotenv("posts/nlp/.env")
#+end_src

#+begin_src python :results none
Environment = Namespace(
    training_corpus="WALL_STREET_JOURNAL_POS",
    vocabulary="WALL_STREET_JOURNAL_VOCABULARY",
    test_corpus= "WALL_STREET_JOURNAL_TEST_POS",
    test_words="WALL_STREET_JOURNAL_TEST_WORDS",
)
#+end_src
* Middle
** The Training Corpus
   The training corpus is made up of words and parts-of-speech tags from the Wall Street Journal (previously looked at in {{% lancelot title="this post" %}}parts-of-speech-tagging-creating-a-vocabulary{{% /lancelot %}}).
#+begin_src python :results none
with open(os.environ[Environment.training_corpus]) as reader:
    training_corpus = reader.read().split("\n")
#+end_src

#+begin_src python :results output :exports both
for row in training_corpus[:5]:
    print(f" - {row}")
#+end_src

#+RESULTS:
:  - In	IN
:  - an	DT
:  - Oct.	NNP
:  - 19	CD
:  - review	NN

** The Vocabulary
There is also a pre-processed version of that same file that has only the words without the parts-of-speech tags that we can load. It has been altered slightly as well:
 - Words that appear only once have been removed
 - Words that don't exist in the original have been added so that there are some unknown words to handle.
#+begin_src python :results output :exports both
with open(os.environ[Environment.vocabulary]) as reader:
    vocabulary_words = reader.read().split("\n")

for row in vocabulary_words[:5]:
    print(f" - {row}")
#+end_src

#+RESULTS:
:  - !
:  - #
:  - $
:  - %
:  - &

Odd.

#+begin_src python :results output :exports both
for row in random.sample(vocabulary_words, 5):
    print(f" - {row}")
#+end_src

#+RESULTS:
:  - cabinet
:  - Byrd
:  - done
:  - Fueling
:  - investments

Our actual vocabulary is going to be a dictionary mapping each word to its index in the list we just loaded.

#+begin_src python :results none
vocabulary_words = sorted(vocabulary_words)
vocabulary = {key: index for index, key in enumerate(vocabulary_words)}
assert len(vocabulary) == len(vocabulary_words)
#+end_src

#+begin_src python :results output :exports both
training_counter = Counter([row.split("\t")[0] for row in training_corpus])
filtered = set(key for key, value in training_counter.items() if value > 1)
extra = set(vocabulary) - filtered
print(f"{len(extra):,}")
for item in random.sample(extra, 5):
    print(f" - {item}")
#+end_src

#+RESULTS:
: 9
:  - --unk_adj--
:  - --n--
:  - --unk--
:  - --unk_noun--
:  - --unk_adv--

So, it looks like the "unknowns" are pre-process tags.
** The Test Corpus
   This is another list of words taken from the Wall Street Journal with parts-of-speech tags added that we'll use as the test-set.

#+begin_src python :results none
with open(os.environ[Environment.test_corpus]) as reader:
    test_corpus = reader.read().split("\n")
#+end_src
** The Test Vocabulary
   There is also a set of words that we're going to try and tag. These need to be pre-processed.
*** Handle Empty
    We are going to replace empty lines with a special token.

#+begin_src python :results none
def handle_empty(words: list, empty_token="--n--"):
    """replace empty strings withh empty_token

    Args:
     words: list to process
     empty_token: what to replace empty strings with

    Yields:
     processed words
    """
    for word in words:
        if not word.strip():
            yield empty_token
        else:
            yield word
    return
#+end_src

*** Labeling Unknowns
**** Suffixes
#+begin_src python :results none
Suffixes = Namespace(
    noun = ["action", "age", "ance", "cy", "dom", "ee", "ence", "er", "hood",
            "ion", "ism", "ist", "ity", "ling", "ment", "ness", "or", "ry",
            "scape", "ship", "ty"],
    verb = ["ate", "ify", "ise", "ize"],
    adjective = ["able", "ese", "ful", "i", "ian", "ible", "ic", "ish", "ive",
                 "less", "ly", "ous"],
    adverb = ["ward", "wards", "wise"]
)
#+end_src
**** Labels for the Unknowns
#+begin_src python :results none
UNKNOWN = "--unknown-{}--"
Label = Namespace(
    digit=UNKNOWN.format("digit"),
    punctuation=UNKNOWN.format("punctuation"),
    uppercase=UNKNOWN.format("uppercase"),
    noun=UNKNOWN.format("noun"),    
    verb=UNKNOWN.format("verb"),
    adjective=UNKNOWN.format("adjective"),
    adverb=UNKNOWN.format("adverb"),
    unknown="--unknown--"
)
#+end_src
**** Bundle Them Up 
#+begin_src python :results none
Unknown = Namespace(
    punctuation = set(string.punctuation),
    suffix = Suffixes,
    label=Label,
    has_digit=re.compile(r"\d"),
    has_uppercase=re.compile("[A-Z]")
)
#+end_src
#+begin_src python :results none
def label_unknowns(words: str, vocabulary: set) -> str:
    """
    Assign tokens to unknown words

    Args:
     word: word not in our vocabulary
     vocabulary: something to check if it is a known word

    Yields:
     word or label for the word if unknown
    """
    for word in words:
        if word in vocabulary:
            yield word
            
        elif Unknown.has_digit.search(word):
            yield Unknown.label.digit
    
        elif not Unknown.punctuation.isdisjoint(set(word)):
            yield Unknown.label.punctuation
    
        elif Unknown.has_uppercase.search(word):
            yield Unknown.label.uppercase
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.noun):
            yield Unknown.label.noun
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.verb):
            yield Unknown.label.verb
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.adjective):
            yield Unknown.label.adjective
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.adverb):
            yield Unknown.label.adverb
        else:
            yield Unknown.label.unknown
    return
#+end_src
*** The Pre-Processor
#+begin_src python :results none
def preprocess(words: list, vocabulary: set) -> list:
    """Preprocess words

    Args:
     words: words to pre-process

    Returns:
     words with empty lines and unknown words labeled
    """
    processed = (word.strip() for word in words)
    processed = handle_empty(processed)
    processed = [word for word in label_unknowns(processed, vocabulary)]
    return processed
#+end_src

*** Load the Test Words
#+begin_src python :results output :exports both
with open(os.environ[Environment.test_words]) as reader:
    test_words = reader.read().strip()
before = len(test_words)
test_words = preprocess(test_words, vocabulary)
assert len(test_words) == before

print(f"{len(test_words):,}")
for word in random.sample(test_words, 5):
    print(f" - {word}")
#+end_src

#+RESULTS:
: 180,264
:  - --unknown--
:  - --n--
:  - e
:  - r
:  - --unknown--

Weird that the letters "e" and "r" are known words...
* End
  So, that's our data. To make it replicable for the next section I'm going to tangle it out.

#+begin_src python :tangle ../../neurotic/nlp/parts_of_speech/preprocessing.py
<<the-imports>>

<<the-environment>>

<<the-suffixes>>

<<the-label>>

<<the-unknown>>

<<the-empty>>


<<corpus-processor>>

    <<corpus-split>>

    <<corpus-handle-empty>>

    <<corpus-unknowns>>

    <<corpus-call>>


<<data-preprocessor-class>>

    <<data-handle-empty>>

    <<data-label-unknowns>>

    <<data-preprocessor-call>>



<<data-loader>>

    <<data-preprocessor>>

    <<data-vocabulary-words>>

    <<data-training-corpus>>

    <<data-processed-training>>

    <<data-vocabulary>>

    <<data-test-corpus>>

    <<data-test-words>>

    <<data-load>>
#+end_src
** The Code
*** Imports
#+begin_src python :noweb-ref the-imports
# from python
from argparse import Namespace

import os
import re
import string

# pypi
import attr
#+end_src    
*** The Environment
    I don't really know that I should save these keys, but I don't really want to cut and paste all the time...
#+begin_src python :noweb-ref the-environment
Environment = Namespace(
    training_corpus="WALL_STREET_JOURNAL_POS",
    vocabulary="WALL_STREET_JOURNAL_VOCABULARY",
    test_corpus= "WALL_STREET_JOURNAL_TEST_POS",
    test_words="WALL_STREET_JOURNAL_TEST_WORDS",
)
#+end_src
*** The Suffixes
#+begin_src python :noweb-ref the-suffixes
Suffixes = Namespace(
    noun = ["action", "age", "ance", "cy", "dom", "ee", "ence", "er", "hood",
            "ion", "ism", "ist", "ity", "ling", "ment", "ness", "or", "ry",
            "scape", "ship", "ty"],
    verb = ["ate", "ify", "ise", "ize"],
    adjective = ["able", "ese", "ful", "i", "ian", "ible", "ic", "ish", "ive",
                 "less", "ly", "ous"],
    adverb = ["ward", "wards", "wise"]
)
#+end_src
*** The Label
#+begin_src python :noweb-ref the-label
UNKNOWN = "--unknown-{}--"
Label = Namespace(
    digit=UNKNOWN.format("digit"),
    punctuation=UNKNOWN.format("punctuation"),
    uppercase=UNKNOWN.format("uppercase"),
    noun=UNKNOWN.format("noun"),    
    verb=UNKNOWN.format("verb"),
    adjective=UNKNOWN.format("adjective"),
    adverb=UNKNOWN.format("adverb"),
    unknown="--unknown--",
 )
#+end_src
*** The Unknown
#+begin_src python :noweb-ref the-unknown
Unknown = Namespace(
    punctuation = set(string.punctuation),
    suffix = Suffixes,
    label=Label,
    has_digit=re.compile(r"\d"),
    has_uppercase=re.compile("[A-Z]")
)
#+end_src
*** The Empty
#+begin_src python :noweb-ref the-empty
Empty = Namespace(
    word="--n--",
    tag="--s--",
)
#+end_src    
*** The Corpus Pre-Processor
#+begin_src python :noweb-ref corpus-processor
@attr.s(auto_attribs=True)
class CorpusProcessor:
    """Pre-processes the corpus

    Args:
     vocabulary: holder of our known words
    """
    vocabulary: dict
#+end_src

**** Split Tuples
#+begin_src python :noweb-ref corpus-split
def split_tuples(self, lines: list):
    """Generates tuples

    Args:
     lines: iterable of lines from the corpus

    Yields:
     whitespace split of line
    """
    for line in lines:
        yield line.split()
    return
#+end_src     
**** Handle Empty Lines
#+begin_src python :noweb-ref corpus-handle-empty
def handle_empty(self, tuples: list):
    """checks for empty strings

    Args:
     tuples: tuples of corpus lines

    Yields:
     line with empty string marked
    """
    for line in tuples:
        if not line:
            yield Empty.word, Empty.tag
        else:
            yield line
    return
#+end_src
**** Handle Unknowns
#+begin_src python :noweb-ref corpus-unknowns
def label_unknowns(self, tuples: list) -> str:
    """
    Assign tokens to unknown words

    Args:
     tuples: word, tag tuples

    Yields:
     word or label for the word if unknown, tag
    """
    for word, tag in tuples:
        if word in self.vocabulary:
            yield word, tag
            
        elif Unknown.has_digit.search(word):
            yield Unknown.label.digit, tag
    
        elif not Unknown.punctuation.isdisjoint(set(word)):
            yield Unknown.label.punctuation, tag
    
        elif Unknown.has_uppercase.search(word):
            yield Unknown.label.uppercase, tag
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.noun):
            yield Unknown.label.noun, tag
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.verb):
            yield Unknown.label.verb, tag
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.adjective):
            yield Unknown.label.adjective, tag
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.adverb):
            yield Unknown.label.adverb, tag
        else:
            yield Unknown.label.unknown, tag
    return
#+end_src
**** The Call
#+begin_src python :noweb-ref corpus-call
def __call__(self, tuples: list) -> list:
    """preprocesses the words and tags

    Args:
     tuples: list of words and tags to process
    
    Returns:
     preprocessed version of words, tags
    """
    processed = self.split_tuples(tuples)
    processed = self.handle_empty(processed)
    processed = [word for word in self.label_unknowns(processed)]
    return processed
#+end_src     
*** The Data Pre-Processor
#+begin_src python :noweb-ref data-preprocessor-class
@attr.s(auto_attribs=True)
class DataPreprocessor:
    """A pre-processor for the data

    Args:
     vocabulary: holder of our known words
     empty_token: what to use if a line is an empty string
    """
    vocabulary: dict
#+end_src
**** Handle Empty Lines
#+begin_src python :noweb-ref data-handle-empty
def handle_empty(self, words: list):
    """replace empty strings withh empty_token

    Args:
     words: list to process
     empty_token: what to replace empty strings with

    Yields:
     processed words
    """
    for word in words:
        if not word.strip():
            yield Empty.word
        else:
            yield word
    return
#+end_src
**** Label Unknowns
#+begin_src python :noweb-ref data-label-unknowns
def label_unknowns(self, words: list) -> str:
    """
    Assign tokens to unknown words

    Args:
     words: iterable of words to check

    Yields:
     word or label for the word if unknown
    """
    for word in words:
        if word in self.vocabulary:
            yield word
            
        elif Unknown.has_digit.search(word):
            yield Unknown.label.digit
    
        elif not Unknown.punctuation.isdisjoint(set(word)):
            yield Unknown.label.punctuation
    
        elif Unknown.has_uppercase.search(word):
            yield Unknown.label.uppercase
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.noun):
            yield Unknown.label.noun
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.verb):
            yield Unknown.label.verb
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.adjective):
            yield Unknown.label.adjective
    
        elif any(word.endswith(suffix) for suffix in Unknown.suffix.adverb):
            yield Unknown.label.adverb
        else:
            yield Unknown.label.unknown
    return
#+end_src    
**** The Call
#+begin_src python :noweb-ref data-preprocessor-call
def __call__(self, words: list) -> list:
    """preprocesses the words

    Args:
     words: list of words to process
    
    Returns:
     preprocessed version of words
    """
    processed = (word.strip() for word in words)
    processed = self.handle_empty(processed)
    processed = [word for word in self.label_unknowns(processed)]
    return processed
#+end_src         
*** The Data Loader
#+begin_src python :noweb-ref data-loader
@attr.s(auto_attribs=True)
class DataLoader:
    """Loads the traning and test data

    Args:
     environment: namespace with keys for the environment to load paths
    """
    environment: Namespace=Environment
    _preprocess: DataPreprocessor=None
    _vocabulary_words: list=None
    _vocabulary: dict=None
    _training_corpus: list=None
    _processed_training: list=None
    _test_corpus: list=None
    _test_words: list=None
#+end_src
*** The Preprocessor
#+begin_src python :noweb-ref data-preprocessor
@property
def preprocess(self) -> DataPreprocessor:
    """The Preprocessor for the data"""
    if self._preprocess is None:
        self._preprocess = DataPreprocessor(self.vocabulary)
    return self._preprocess
#+end_src        
*** The Vocabulary Words
#+begin_src python :noweb-ref data-vocabulary-words
@property
def vocabulary_words(self) -> list:
    """The list of vocabulary words for tranining"""
    if self._vocabulary_words is None:
        self._vocabulary_words = sorted(
            self.load(os.environ[self.environment.vocabulary]))
    return self._vocabulary_words
#+end_src
*** The Vocabulary
#+begin_src python :noweb-ref data-vocabulary
@property
def vocabulary(self) -> dict:
    """Converts the vocabulary list of words to a dict

    Returns:
     word to index of word in vocabulary words
    """
    if self._vocabulary is None:
        self._vocabulary = {
            word: index
            for index, word in enumerate(self.vocabulary_words)}
    return self._vocabulary
#+end_src        
*** The Training Corpus
#+begin_src python :noweb-ref data-training-corpus
@property
def training_corpus(self) -> list:
    """The corpus  for tranining"""
    if self._training_corpus is None:
        self._training_corpus = self.load(os.environ[self.environment.training_corpus])
    return self._training_corpus
#+end_src
*** Processed Training
#+begin_src python :noweb-ref data-processed-training
@property
def processed_training(self) -> list:
    """Pre-processes the training corpus"""
    if self._processed_training is None:
        processor = CorpusProcessor(self.vocabulary)
        self._processed_training = processor(self.training_corpus)
    return self._processed_training
#+end_src
*** The Test Corpus
#+begin_src python :noweb-ref data-test-corpus
@property
def test_corpus(self) -> list:
    """The corpus  for tranining"""
    if self._test_corpus is None:
        self._test_corpus = self.load(os.environ[self.environment.test_corpus])
    return self._test_corpus
#+end_src
*** Test Words
#+begin_src python :noweb-ref data-test-words
@property
def test_words(self) -> list:
    """The pre-processed test words"""
    if self._test_words is None:
        self._test_words = self.load(os.environ[self.environment.test_words])
        self._test_words = self.preprocess(self._test_words)
    return self._test_words
#+end_src        
*** Load Method
#+begin_src python :noweb-ref data-load
def load(self, path: str) -> list:
    """Loads the strings from the file

    Args:
     path: path to the text file

    Returns:
     list of lines from the file
    """
    with open(path) as reader:
        lines = reader.read().split("\n")
    return lines
#+end_src    
** Test it Out
#+begin_src python :results none
from neurotic.nlp.parts_of_speech.preprocessing import Environment, DataLoader
#+end_src

#+begin_src python :results output :exports both
loader = DataLoader(Environment)

print(f"{len(loader.vocabulary_words):,}")
print(random.sample(loader.vocabulary_words, 2))
assert len(loader.vocabulary_words) == len(vocabulary_words)

print(f"\n{len(loader.training_corpus):,}")
assert (len(loader.training_corpus)) == len(training_corpus)
print(random.sample(loader.training_corpus, 2))

print(f"\n{len(loader.vocabulary):,}")
assert len(loader.vocabulary) == len(vocabulary)

print(f"\n{len(loader.test_corpus):,}")
assert len(loader.test_corpus) == len(test_corpus)
print(random.sample(loader.test_corpus, 2))


print(f"\n{len(loader.test_words):,}")
print(random.sample(loader.test_words, 2))
#+end_src

#+RESULTS:
#+begin_example
23,777
['Island', 'Gibbons']

989,861
['nine\tCD', 'in\tIN']

23,777

34,200
['at\tIN', 'to\tTO']

34,200
['the', ';']
#+end_example


