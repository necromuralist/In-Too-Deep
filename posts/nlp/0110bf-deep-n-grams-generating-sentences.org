#+BEGIN_COMMENT
.. title: Deep N-Grams: Generating Sentences
.. slug: deep-n-grams-generating-sentences
.. date: 2021-01-05 16:49:26 UTC-08:00
.. tags: nlp,n-grams,rnn,gru
.. category: NLP
.. link: 
.. description: Generating sentences with our GRU model.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC

* Generating New Sentences
  - {{% lancelot title="First Post" %}}deep-n-grams{{% /lancelot %}}
  - {{% lancelot title="Previous Post" %}}deep-n-grams-evaluating-the-model{{% /lancelot %}}
#+begin_example python    
# # Part 5: Generating the language with your own model
# 
# We will now use your own language model to generate new sentences for that we need to make draws from a Gumble distribution.

# The Gumbel Probability Density Function (PDF) is defined as: 
# 
# $$ f(z) = {1\over{\beta}}e^{(-z+e^{(-z)})} $$
# 
# where: $$ z = {(x - \mu)\over{\beta}}$$
# 
# The maximum value, which is what we choose as the prediction in the last step of a Recursive Neural Network `RNN` we are using for text generation, in a sample of a random variable following an exponential distribution approaches the Gumbel distribution when the sample increases asymptotically. For that reason, the Gumbel distribution is used to sample from a categorical distribution.

# In[ ]:


# Run this cell to generate some news sentence
def gumbel_sample(log_probs, temperature=1.0):
    """Gumbel sampling from a categorical distribution."""
    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)
    g = -np.log(-np.log(u))
    return np.argmax(log_probs + g * temperature, axis=-1)

def predict(num_chars, prefix):
    inp = [ord(c) for c in prefix]
    result = [c for c in prefix]
    max_len = len(prefix) + num_chars
    for _ in range(num_chars):
        cur_inp = np.array(inp + [0] * (max_len - len(inp)))
        outp = model(cur_inp[None, :])  # Add batch dim.
        next_char = gumbel_sample(outp[0, len(inp)])
        inp += [int(next_char)]
       
        if inp[-1] == 1:
            break  # EOS
        result.append(chr(int(next_char)))
    
    return "".join(result)

print(predict(32, ""))


# In[ ]:


print(predict(32, ""))
print(predict(32, ""))
print(predict(32, ""))


# In the generated text above, you can see that the model generates text that makes sense capturing dependencies between words and without any input. A simple n-gram model would have not been able to capture all of that in one sentence.

# <a name='6'></a>
# ###  <span style="color:blue"> On statistical methods </span>
# 
# Using a statistical method like the one you implemented in course 2 will not give you results that are as good. Your model will not be able to encode information seen previously in the data set and as a result, the perplexity will increase. Remember from course 2 that the higher the perplexity, the worse your model is. Furthermore, statistical ngram models take up too much space and memory. As a result, it will be inefficient and too slow. Conversely, with deepnets, you can get a better perplexity. Note, learning about n-gram language models is still important and allows you to better understand deepnets.
# 
#+end_example
