<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Classifying tweet sentiment with naive bayes." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Naive Bayes Twitter Sentiment Classification | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/naive-bayes-twitter-sentiment-classification/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../visualizing-naive-bayes/" rel="prev" title="Visualizing Naive Bayes" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Naive Bayes Twitter Sentiment Classification" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/naive-bayes-twitter-sentiment-classification/" property="og:url">
<meta content="Classifying tweet sentiment with naive bayes." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-08-25T10:16:04-07:00" property="article:published_time">
<meta content="naive bayes" property="article:tag">
<meta content="nlp" property="article:tag">
<meta content="sentiment analysis" property="article:tag">
<meta content="twitter" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Naive Bayes Twitter Sentiment Classification</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-08-25T10:16:04-07:00" itemprop="datePublished" title="2020-08-25 10:16">2020-08-25 10:16</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgd1a7cdd">Beginning</a>
<ul>
<li><a href="#orgb9c06d2">Set Up</a></li>
</ul>
</li>
<li><a href="#orge03d773">Middle</a>
<ul>
<li><a href="#org1c00004">Training the Naive Bayes Classifier</a></li>
<li><a href="#org4424e41">Testing The Model</a></li>
<li><a href="#org74ba438">Filtering Words</a></li>
<li><a href="#org0c93422">Error Analysis</a></li>
<li><a href="#orgb9aa999">Predict Your Own Tweet</a></li>
</ul>
</li>
<li><a href="#org976e2b4">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgd1a7cdd">
<h2 id="orgd1a7cdd">Beginning</h2>
<div class="outline-text-2" id="text-orgd1a7cdd"></div>
<div class="outline-3" id="outline-container-orgb9c06d2">
<h3 id="orgb9c06d2">Set Up</h3>
<div class="outline-text-3" id="text-orgb9c06d2"></div>
<div class="outline-4" id="outline-container-org83bccf9">
<h4 id="org83bccf9">Imports</h4>
<div class="outline-text-4" id="text-org83bccf9">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>

<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.counter</span> <span class="kn">import</span> <span class="n">WordCounter</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org683bcd2">
<h4 id="org683bcd2">Tabulate</h4>
<div class="outline-text-4" id="text-org683bcd2">
<div class="highlight">
<pre><span></span><span class="n">TABLE</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">tabulate</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">"orgtbl"</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">"keys"</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgfdee454">
<h4 id="orgfdee454">The Dotenv</h4>
<div class="outline-text-4" id="text-orgfdee454">
<p>I put the path to the data files in a .env file so this loads it into the environment.</p>
<div class="highlight">
<pre><span></span><span class="n">env_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">env_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>
<span class="n">load_dotenv</span><span class="p">(</span><span class="n">env_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6834c6c">
<h4 id="org6834c6c">The Timer</h4>
<div class="outline-text-4" id="text-org6834c6c">
<p>This is a helper to keep track of how long things are taking.</p>
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org80cdaaa">
<h4 id="org80cdaaa">Load the Twitter Data</h4>
<div class="outline-text-4" id="text-org80cdaaa">
<p>I split the data <a href="../01-twitter-preprocessing-with-nltk/">previously</a> for the Logistic Regression twitter sentiment classifier so I'll load it here and skip building the sets.</p>
<div class="highlight">
<pre><span></span><span class="n">train_raw</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_feather</span><span class="p">(</span>
    <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_TRAINING_RAW"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">())</span>

<span class="n">test_raw</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_feather</span><span class="p">(</span>
    <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_TEST_RAW"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Training: {len(train_raw):,}"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Testing: {len(test_raw):,}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Training: 8,000
Testing: 2,000
</pre>
<p>I'll also re-use the WordCounter from the Logistic Regression. Despite the name it also does tokenizing and cleaning.</p>
<div class="highlight">
<pre><span></span><span class="n">counter</span> <span class="o">=</span> <span class="n">WordCounter</span><span class="p">(</span><span class="n">train_raw</span><span class="o">.</span><span class="n">tweet</span><span class="p">,</span> <span class="n">train_raw</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org2829255">
<h4 id="org2829255">Constants</h4>
<div class="outline-text-4" id="text-org2829255">
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_SENTIMENT"</span><span class="p">],</span> <span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">Sentiment</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">Sentiment</span><span class="p">)</span>
</pre></div>
<pre class="example">
Namespace(decode={1: 'positive', 0: 'negative'}, encode={'positive': 1, 'negative': 0}, negative=0, positive=1)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orge03d773">
<h2 id="orge03d773">Middle</h2>
<div class="outline-text-2" id="text-orge03d773"></div>
<div class="outline-3" id="outline-container-org1c00004">
<h3 id="org1c00004">Training the Naive Bayes Classifier</h3>
<div class="outline-text-3" id="text-org1c00004"></div>
<div class="outline-4" id="outline-container-org0b349f4">
<h4 id="org0b349f4">Priors and Log Priors</h4>
<div class="outline-text-4" id="text-org0b349f4">
<p>\(P(D_{pos})\) is the probability that the document is positive. \(P(D_{neg})\) is the probability that the document is negative.</p>
<p>Use the formulas as follows and store the values in a dictionary:</p>
\begin{align} P(D_{pos}) &amp;= \frac{D_{pos}}{D}\\ P(D_{neg}) &amp;= \frac{D_{neg}}{D}\\ \end{align}
<p>Where \(D\) is the total number of documents, or tweets in this case, \(D_{pos}\) is the total number of positive tweets and \(D_{neg}\) is the total number of negative tweets.</p>
<p>The prior is the ratio of the probabilities \(\frac{P(D_{pos})}{P(D_{neg})}\). If you look at the definitions of the probabilities, they have the same denominator (<i>D</i>) so taking the ratio of the probabilities means the denominator cancels out and we don't really need it.</p>
\begin{align} \frac{P(D_{pos})}{P(D_{neg})} &amp;= \frac{\frac{D_{pos}}{D}}{\frac{D_{neg}}{D}}\\ &amp;= \frac{\left( \frac{D_{pos}}{\cancel{D}}\right) \left(\frac{\cancel{D}}{D_{neg}}\right) }{ \cancel{\left(\frac{D_{neg}}{D}\right)} \cancel{\left(\frac{D}{D_{neg}}\right)} }\\ &amp;= \frac{D_{pos}}{D_{neg}}\\ \end{align}
<p>We can take the log of the prior to rescale it, and we'll call this the logprior.</p>
\begin{align} \text{logprior} &amp;= log \left( \frac{P(D_{pos})}{P(D_{neg})} \right) \\ &amp;= log \left( \frac{D_{pos}}{D_{neg}} \right)\\ \end{align}
<p>Note that \(log(\frac{A}{B})\) is the same as \(log(A) - log(B)\). So the logprior can also be calculated as the difference between two logs:</p>
\begin{align} \text{logprior} &amp;= \log (P(D_{pos})) - \log (P(D_{neg})) \\ &amp;= \log (D_{pos}) - \log (D_{neg})\\ \end{align}</div>
</div>
<div class="outline-4" id="outline-container-orgd5b2f39">
<h4 id="orgd5b2f39">Positive and Negative Probabilities</h4>
<div class="outline-text-4" id="text-orgd5b2f39">
<p>To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:</p>
<ul class="org-ul">
<li>\(freq_{pos}\) and \(freq_{neg}\) are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.</li>
<li>\(N_{pos}\) and \(N_{neg}\) are the total number of positive and negative words for all documents (for all tweets), respectively.</li>
<li><i>V</i> is the number of unique words in the entire set of documents, for all classes, whether positive or negative.</li>
</ul>
<p>We'll use these to compute the positive and negative probability for a specific word using this formula:</p>
\begin{align} P(W_{pos}) &amp;= \frac{freq_{pos} + 1}{N_{pos} + V}\\ P(W_{neg}) &amp;= \frac{freq_{neg} + 1}{N_{neg} + V}\\ \end{align}
<p>Notice that we add the "+1" in the numerator for additive smoothing. This <a href="https://en.wikipedia.org/wiki/Additive_smoothing">wiki article</a> explains more about additive smoothing.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org1029247">
<h4 id="org1029247">Log Likelihood</h4>
<div class="outline-text-4" id="text-org1029247">
<p>To compute the loglikelihood of that very same word, we can implement the following equations:</p>
<p>\[ \text{loglikelihood} = \log \left(\frac{P(W_{pos})}{P(W_{neg})} \right) \]</p>
</div>
</div>
<div class="outline-4" id="outline-container-org508a095">
<h4 id="org508a095">Calculating Stuff</h4>
<div class="outline-text-4" id="text-org508a095">
<ul class="org-ul">
<li><i>V</i> is the number of unique words in our word counter.</li>
<li>\(freq_{pos}\) and \(freq_{neg}\) are what <code>counter.counts</code> has for the values</li>
<li>\(N_{pos}\) and \(N_{neg}\) are the total number of positive and negative words respectively, which we can calculate from the counter</li>
<li><i>D</i> is the number of documents in the training set</li>
<li>\(D_{pos}\) and \(D_{neg}\) are the number of documents labeled positive and those labeled negative</li>
</ul>
</div>
</div>
<div class="outline-4" id="outline-container-orgb798006">
<h4 id="orgb798006">The Function</h4>
<div class="outline-text-4" id="text-orgb798006">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train_naive_bayes</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Input:</span>
<span class="sd">       freqs: dictionary from (word, label) to how often the word appears</span>
<span class="sd">       train_x: a list of tweets</span>
<span class="sd">       train_y: a list of labels correponding to the tweets (0,1)</span>
<span class="sd">    Output:</span>
<span class="sd">       logprior: the log prior. (equation 3 above)</span>
<span class="sd">       loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)</span>
<span class="sd">    '''</span>
    <span class="n">loglikelihood</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">logprior</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>

    <span class="c1"># calculate V, the number of unique words in the vocabulary</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">freqs</span><span class="o">.</span><span class="n">keys</span><span class="p">()])</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>

    <span class="c1"># calculate N_pos and N_neg</span>
    <span class="n">N_pos</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">freqs</span><span class="p">[(</span><span class="n">token</span><span class="p">,</span> <span class="n">sentiment</span><span class="p">)]</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="n">freqs</span>
                 <span class="k">if</span> <span class="n">sentiment</span> <span class="o">==</span> <span class="n">Sentiment</span><span class="o">.</span><span class="n">positive</span><span class="p">))</span>
    <span class="n">N_neg</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">freqs</span><span class="p">[(</span><span class="n">token</span><span class="p">,</span> <span class="n">sentiment</span><span class="p">)]</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="n">freqs</span>
                 <span class="k">if</span> <span class="n">sentiment</span> <span class="o">==</span> <span class="n">Sentiment</span><span class="o">.</span><span class="n">negative</span><span class="p">))</span>

    <span class="c1"># Calculate D, the number of documents</span>
    <span class="n">D</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>

    <span class="c1"># Calculate D_pos, the number of positive documents (*hint: use sum(&lt;np_array&gt;))</span>
    <span class="n">D_pos</span> <span class="o">=</span> <span class="n">train_y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Calculate D_neg, the number of negative documents (*hint: compute using D and D_pos)</span>
    <span class="n">D_neg</span> <span class="o">=</span> <span class="n">D</span> <span class="o">-</span> <span class="n">D_pos</span>

    <span class="c1"># Calculate logprior</span>
    <span class="n">logprior</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">D_pos</span><span class="p">)</span> <span class="o">-</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">D_neg</span><span class="p">)</span>

    <span class="c1"># For each word in the vocabulary...</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
        <span class="c1"># get the positive and negative frequency of the word</span>
        <span class="n">freq_pos</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="n">Sentiment</span><span class="o">.</span><span class="n">positive</span><span class="p">)]</span>
        <span class="n">freq_neg</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="n">Sentiment</span><span class="o">.</span><span class="n">negative</span><span class="p">)]</span>

        <span class="c1"># calculate the probability that each word is positive, and negative</span>
        <span class="n">p_w_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_pos</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N_pos</span> <span class="o">+</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">p_w_neg</span> <span class="o">=</span> <span class="p">(</span><span class="n">freq_neg</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">N_neg</span> <span class="o">+</span> <span class="n">V</span><span class="p">)</span>

        <span class="c1"># calculate the log likelihood of the word</span>
        <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_w_pos</span><span class="p">)</span> <span class="o">-</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_w_neg</span><span class="p">)</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">all_raw</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">train_raw</span><span class="p">,</span> <span class="n">test_raw</span><span class="p">])</span>
<span class="n">check</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
    <span class="n">all_raw</span><span class="p">[</span><span class="n">all_raw</span><span class="o">.</span><span class="n">label</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">4000</span><span class="p">],</span> <span class="n">all_raw</span><span class="p">[</span><span class="n">all_raw</span><span class="o">.</span><span class="n">label</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">4000</span><span class="p">]])</span>
<span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span> <span class="o">=</span> <span class="n">train_naive_bayes</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">check</span><span class="o">.</span><span class="n">tweet</span><span class="p">,</span> <span class="n">check</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Log Prior: {logprior}"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Log Likelihood: {len(loglikelihood)}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Log Prior: 0.0
Log Likelihood: 9172
</pre>
<div class="highlight">
<pre><span></span><span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span> <span class="o">=</span> <span class="n">train_naive_bayes</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">train_raw</span><span class="o">.</span><span class="n">tweet</span><span class="p">,</span> <span class="n">train_raw</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Log Prior: {logprior}"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Log Likelihood: {len(loglikelihood)}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
2020-08-27 17:53:26,369 graeae.timers.timer start: Started: 2020-08-27 17:53:26.368827
2020-08-27 17:53:26,423 graeae.timers.timer end: Ended: 2020-08-27 17:53:26.423193
2020-08-27 17:53:26,424 graeae.timers.timer end: Elapsed: 0:00:00.054366
Log Prior: -0.006500022885560952
Log Likelihood: 9172
</pre>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"{len(train_raw[train_raw.label==0]):,}"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"{len(train_raw[train_raw.label==1]):,}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
4,013
3,987
</pre>
<p>We end up with a negative prior in the second case because we don't have a perfect balance between the positive and negative counts.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org4424e41">
<h3 id="org4424e41">Testing The Model</h3>
<div class="outline-text-3" id="text-org4424e41">
<p>Implement the `naive_bayes_predict` function to make predictions on tweets.</p>
<ul class="org-ul">
<li>The function takes in the `tweet`, `logprior`, `loglikelihood`.</li>
<li>It returns the probability that the tweet belongs to the positive or negative class.</li>
<li>For each tweet, sum up loglikelihoods of each word in the tweet.</li>
<li>Also add the logprior to this sum to get the predicted sentiment of that tweet.</li>
</ul>
<p>\[ p = logprior + \sum_i^N (loglikelihood_i) \]</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">naive_bayes_predict</span><span class="p">(</span><span class="n">tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Input:</span>
<span class="sd">       tweet: a string</span>
<span class="sd">       logprior: a number</span>
<span class="sd">       loglikelihood: a dictionary of words mapping to numbers</span>
<span class="sd">    Output:</span>
<span class="sd">       p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)</span>

<span class="sd">    '''</span>
    <span class="c1"># process the tweet to get a list of words</span>
    <span class="n">word_l</span> <span class="o">=</span> <span class="n">counter</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span>

    <span class="c1"># initialize probability to zero</span>
    <span class="n">p</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># add the logprior</span>
    <span class="n">p</span> <span class="o">+=</span> <span class="n">logprior</span>

    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_l</span><span class="p">:</span>

        <span class="c1"># check if the word exists in the loglikelihood dictionary</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">loglikelihood</span><span class="p">:</span>
            <span class="c1"># add the log likelihood of that word to the probability</span>
            <span class="n">p</span> <span class="o">+=</span> <span class="n">loglikelihood</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">p</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">my_tweet</span> <span class="o">=</span> <span class="s1">'She smiled.'</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">my_tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'The expected output is'</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</pre></div>
<pre class="example">
The expected output is 1.44329786012511
</pre>
<p><b>Instructions</b>:</p>
<ul class="org-ul">
<li>Implement `test_naive_bayes` to check the accuracy of your predictions.</li>
<li>The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood</li>
<li>It returns the accuracy of your model.</li>
<li>First, use `naive_bayes_predict` function to make predictions for each tweet in text_x.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_naive_bayes</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Input:</span>
<span class="sd">       test_x: A list of tweets</span>
<span class="sd">       test_y: the corresponding labels for the list of tweets</span>
<span class="sd">       logprior: the logprior</span>
<span class="sd">       loglikelihood: a dictionary with the loglikelihoods for each word</span>
<span class="sd">    Output:</span>
<span class="sd">       accuracy: (# of tweets classified correctly)/(total # of tweets)</span>
<span class="sd">    """</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># return this properly</span>

    <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
    <span class="n">y_hats</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">test_x</span><span class="p">:</span>
        <span class="c1"># if the prediction is &gt; 0</span>
        <span class="k">if</span> <span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># the predicted class is 1</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># otherwise the predicted class is 0</span>
            <span class="n">y_hat_i</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># append the predicted class to the list y_hats</span>
        <span class="n">y_hats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_hat_i</span><span class="p">)</span>

    <span class="c1"># error is the average of the absolute values of the differences between y_hats and test_y</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_hats</span><span class="p">)</span> <span class="o">-</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_y</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="c1"># Accuracy is 1 minus the error</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">error</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s2">"Naive Bayes accuracy = </span><span class="si">%0.4f</span><span class="s2">"</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">test_naive_bayes</span><span class="p">(</span><span class="n">test_raw</span><span class="o">.</span><span class="n">tweet</span><span class="p">,</span> <span class="n">test_raw</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)))</span>
</pre></div>
<pre class="example">
Naive Bayes accuracy = 0.9955
</pre>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">'I am happy'</span><span class="p">,</span> <span class="s1">'I am bad'</span><span class="p">,</span> <span class="s1">'this movie should have been great.'</span><span class="p">,</span> <span class="s1">'great'</span><span class="p">,</span> <span class="s1">'great great'</span><span class="p">,</span> <span class="s1">'great great great'</span><span class="p">,</span> <span class="s1">'great great great great'</span><span class="p">]:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s1">'{tweet} -&gt; {p:.2f}'</span><span class="p">)</span>
</pre></div>
<pre class="example">
I am happy -&gt; 1.89
I am bad -&gt; -1.63
this movie should have been great. -&gt; 2.05
great -&gt; 2.06
great great -&gt; 4.13
great great great -&gt; 6.19
great great great great -&gt; 8.25
</pre>
<div class="highlight">
<pre><span></span><span class="n">my_tweet</span> <span class="o">=</span> <span class="s2">"the answer is in the umwelt"</span>
<span class="k">print</span><span class="p">(</span><span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">my_tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">))</span>
</pre></div>
<pre class="example">
-0.41441957689474407
</pre></div>
</div>
<div class="outline-3" id="outline-container-org74ba438">
<h3 id="org74ba438">Filtering Words</h3>
<div class="outline-text-3" id="text-org74ba438">
<ul class="org-ul">
<li>Some words have more positive counts than others, and can be considered "more positive". Likewise, some words can be considered more negative than others.</li>
<li>One way for us to define the level of positiveness or negativeness, without calculating the log likelihood, is to compare the positive to negative frequency of the word.</li>
<li>Note that we can also use the log likelihood calculations to compare relative positivity or negativity of words.</li>
<li>We can calculate the ratio of positive to negative frequencies of a word.</li>
<li>Once we're able to calculate these ratios, we can also filter a subset of words that have a minimum ratio of positivity / negativity or higher.</li>
<li>Similarly, we can also filter a subset of words that have a maximum ratio of positivity / negativity or lower (words that are at least as negative, or even more negative than a given threshold).</li>
<li>Given the `freqs` dictionary of words and a particular word, use `lookup(freqs,word,1)` to get the positive count of the word.</li>
<li>Similarly, use the `lookup()` function to get the negative count of that word.</li>
<li>Calculate the ratio of positive divided by negative counts</li>
</ul>
<p>\[ ratio = \frac{\text{pos_words} + 1}{\text{neg_words} + 1} \]</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Words</th>
<th class="org-right" scope="col">Positive word count</th>
<th class="org-right" scope="col">Negative Word Count</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">glad</td>
<td class="org-right">41</td>
<td class="org-right">2</td>
</tr>
<tr>
<td class="org-left">arriv</td>
<td class="org-right">57</td>
<td class="org-right">4</td>
</tr>
<tr>
<td class="org-left">:(</td>
<td class="org-right">1</td>
<td class="org-right">3663</td>
</tr>
<tr>
<td class="org-left">:-(</td>
<td class="org-right">0</td>
<td class="org-right">378</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_ratio</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Input:</span>
<span class="sd">       freqs: dictionary containing the words</span>
<span class="sd">       word: string to lookup</span>

<span class="sd">    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.</span>
<span class="sd">       Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}</span>
<span class="sd">    '''</span>
    <span class="n">pos_neg_ratio</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'positive'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">'negative'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">'ratio'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>

    <span class="c1"># use lookup() to find positive counts for the word (denoted by the integer 1)</span>
    <span class="n">pos_neg_ratio</span><span class="p">[</span><span class="s1">'positive'</span><span class="p">]</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>

    <span class="c1"># use lookup() to find negative counts for the word (denoted by integer 0)</span>
    <span class="n">pos_neg_ratio</span><span class="p">[</span><span class="s1">'negative'</span><span class="p">]</span> <span class="o">=</span> <span class="n">freqs</span><span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>

    <span class="c1"># calculate the ratio of positive to negative counts for the word</span>
    <span class="n">pos_neg_ratio</span><span class="p">[</span><span class="s1">'ratio'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">pos_neg_ratio</span><span class="p">[</span><span class="s2">"positive"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span>
        <span class="n">pos_neg_ratio</span><span class="p">[</span><span class="s2">"negative"</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">pos_neg_ratio</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">get_ratio</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="s1">'happi'</span><span class="p">))</span>
</pre></div>
<pre class="example">
{'positive': 160, 'negative': 23, 'ratio': 6.708333333333333}
</pre></div>
<div class="outline-4" id="outline-container-org1985c54">
<h4 id="org1985c54">get words by threshold</h4>
<div class="outline-text-4" id="text-org1985c54">
<ul class="org-ul">
<li>If we set the label to 1, then we'll look for all words whose threshold of positive/negative is at least as high as that threshold, or higher.</li>
<li>If we set the label to 0, then we'll look for all words whose threshold of positive/negative is at most as low as the given threshold, or lower.</li>
<li>Use the `get_ratio()` function to get a dictionary containing the positive count, negative count, and the ratio of positive to negative counts.</li>
<li>Append a dictionary to a list, where the key is the word, and the dictionary is the dictionary `pos_neg_ratio` that is returned by the `get_ratio()` function.</li>
</ul>
<p>An example key-value pair would have this structure:</p>
<div class="highlight">
<pre><span></span><span class="p">{</span><span class="s1">'happi'</span><span class="p">:</span>
     <span class="p">{</span><span class="s1">'positive'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">'negative'</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="s1">'ratio'</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}</span>
 <span class="p">}</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_words_by_threshold</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Input:</span>
<span class="sd">       freqs: dictionary of words</span>
<span class="sd">       label: 1 for positive, 0 for negative</span>
<span class="sd">       threshold: ratio that will be used as the cutoff for including a word in the returned dictionary</span>
<span class="sd">    Output:</span>
<span class="sd">       word_set: dictionary containing the word and information on its positive count, negative count, and ratio of positive to negative counts.</span>
<span class="sd">       example of a key value pair:</span>
<span class="sd">       {'happi':</span>
<span class="sd">           {'positive': 10, 'negative': 20, 'ratio': 0.5}</span>
<span class="sd">       }</span>
<span class="sd">    '''</span>
    <span class="n">word_list</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">:</span>
        <span class="n">word</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">key</span>

        <span class="c1"># get the positive/negative ratio for a word</span>
        <span class="n">pos_neg_ratio</span> <span class="o">=</span> <span class="n">get_ratio</span><span class="p">(</span><span class="n">freqs</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span>

        <span class="c1"># if the label is 1 and the ratio is greater than or equal to the threshold...</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">label</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">pos_neg_ratio</span><span class="p">[</span><span class="s2">"ratio"</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="ow">or</span>
            <span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">pos_neg_ratio</span><span class="p">[</span><span class="s2">"ratio"</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">)):</span>

            <span class="c1"># Add the pos_neg_ratio to the dictionary</span>
            <span class="n">word_list</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos_neg_ratio</span>

        <span class="c1"># otherwise, do not include this word in the list (do nothing)</span>

    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">word_list</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">passed</span> <span class="o">=</span> <span class="n">get_words_by_threshold</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">Sentiment</span><span class="o">.</span><span class="n">negative</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">passed</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"word: {word}</span><span class="se">\t</span><span class="s2">{info}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
word: :(        {'positive': 1, 'negative': 3705, 'ratio': 0.0005396654074473826}
word: :-(       {'positive': 0, 'negative': 407, 'ratio': 0.0024509803921568627}
word: ♛ {'positive': 0, 'negative': 162, 'ratio': 0.006134969325153374}
word: 》 {'positive': 0, 'negative': 162, 'ratio': 0.006134969325153374}
word: beli̇ev   {'positive': 0, 'negative': 27, 'ratio': 0.03571428571428571}
word: wi̇ll     {'positive': 0, 'negative': 27, 'ratio': 0.03571428571428571}
word: justi̇n   {'positive': 0, 'negative': 27, 'ratio': 0.03571428571428571}
word: ｓｅｅ       {'positive': 0, 'negative': 27, 'ratio': 0.03571428571428571}
word: ｍｅ        {'positive': 0, 'negative': 27, 'ratio': 0.03571428571428571}
word: sad       {'positive': 3, 'negative': 100, 'ratio': 0.039603960396039604}
word: &gt;:(    {'positive': 0, 'negative': 36, 'ratio': 0.02702702702702703}
</pre>
<div class="highlight">
<pre><span></span><span class="n">passed</span> <span class="o">=</span> <span class="n">get_words_by_threshold</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">counts</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">Sentiment</span><span class="o">.</span><span class="n">positive</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">passed</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"word: {word}</span><span class="se">\t</span><span class="s2">{info}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
word: :)        {'positive': 2967, 'negative': 1, 'ratio': 1484.0}
word: :-)       {'positive': 547, 'negative': 0, 'ratio': 548.0}
word: :D        {'positive': 537, 'negative': 0, 'ratio': 538.0}
word: :p        {'positive': 113, 'negative': 0, 'ratio': 114.0}
word: fback     {'positive': 22, 'negative': 0, 'ratio': 23.0}
word: blog      {'positive': 29, 'negative': 2, 'ratio': 10.0}
word: followfriday      {'positive': 19, 'negative': 0, 'ratio': 20.0}
word: recent    {'positive': 9, 'negative': 0, 'ratio': 10.0}
word: stat      {'positive': 52, 'negative': 0, 'ratio': 53.0}
word: arriv     {'positive': 57, 'negative': 4, 'ratio': 11.6}
word: thx       {'positive': 11, 'negative': 0, 'ratio': 12.0}
word: here'     {'positive': 19, 'negative': 0, 'ratio': 20.0}
word: influenc  {'positive': 16, 'negative': 0, 'ratio': 17.0}
word: bam       {'positive': 34, 'negative': 0, 'ratio': 35.0}
word: warsaw    {'positive': 34, 'negative': 0, 'ratio': 35.0}
word: welcom    {'positive': 58, 'negative': 4, 'ratio': 11.8}
word: vid       {'positive': 9, 'negative': 0, 'ratio': 10.0}
word: ceo       {'positive': 9, 'negative': 0, 'ratio': 10.0}
word: 1month    {'positive': 9, 'negative': 0, 'ratio': 10.0}
word: flipkartfashionfriday     {'positive': 14, 'negative': 0, 'ratio': 15.0}
word: inde      {'positive': 10, 'negative': 0, 'ratio': 11.0}
word: glad      {'positive': 35, 'negative': 2, 'ratio': 12.0}
word: braindot  {'positive': 9, 'negative': 0, 'ratio': 10.0}
word: ;)        {'positive': 21, 'negative': 0, 'ratio': 22.0}
word: goodnight {'positive': 19, 'negative': 1, 'ratio': 10.0}
word: youth     {'positive': 10, 'negative': 0, 'ratio': 11.0}
word: shout     {'positive': 9, 'negative': 0, 'ratio': 10.0}
word: fantast   {'positive': 10, 'negative': 0, 'ratio': 11.0}
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0c93422">
<h3 id="org0c93422">Error Analysis</h3>
<div class="outline-text-3" id="text-org0c93422">
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">'Truth Predicted Tweet'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">test_raw</span><span class="o">.</span><span class="n">itertuples</span><span class="p">():</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">row</span><span class="o">.</span><span class="n">tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">row</span><span class="o">.</span><span class="n">label</span> <span class="o">!=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span>
            <span class="n">f</span><span class="s2">"{row.label}</span><span class="se">\t</span><span class="s2">{numpy.sign(y_hat) &gt; 0:d}</span><span class="se">\t</span><span class="s2">"</span>
            <span class="n">f</span><span class="s2">"{' '.join(counter.process(row.tweet)).encode('ascii', 'ignore')}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Truth Predicted Tweet
0       1       b'whatev stil l young &gt;:-('
1       0       b'look fun kik va 642 kik kikgirl french model orgasm hannib phonesex :)'
0       1       b'great news thank let us know :( hope good weekend'
0       1       b"amb pleas harry' jean :) ): ): ):"
0       1       b'srsli fuck u unfollow hope ur futur child unpar u &gt;:-('
1       0       b'ate last cooki shir 0 &gt;:d'
1       0       b'snapchat jennyjean 22 snapchat kikmeboy model french kikchat sabadodeganarseguidor sexysasunday :)'
1       0       b'add kik ughtm 545 kik kikmeguy kissm nude likeforfollow musicbiz sexysasunday :)'
0       1       b'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgb9aa999">
<h3 id="orgb9aa999">Predict Your Own Tweet</h3>
<div class="outline-text-3" id="text-orgb9aa999">
<div class="highlight">
<pre><span></span><span class="n">my_tweet</span> <span class="o">=</span> <span class="s1">'my balls itch'</span>

<span class="n">p</span> <span class="o">=</span> <span class="n">naive_bayes_predict</span><span class="p">(</span><span class="n">my_tweet</span><span class="p">,</span> <span class="n">logprior</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"{my_tweet} is a positive tweet: {numpy.sign(p) &gt; 0}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
my balls itch is a positive tweet: True
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org976e2b4">
<h2 id="org976e2b4">End</h2>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/naive-bayes/" rel="tag">naive bayes</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/sentiment-analysis/" rel="tag">sentiment analysis</a></li>
<li><a class="tag p-category" href="../../../categories/twitter/" rel="tag">twitter</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../visualizing-naive-bayes/" rel="prev" title="Visualizing Naive Bayes">Previous post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
</script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
