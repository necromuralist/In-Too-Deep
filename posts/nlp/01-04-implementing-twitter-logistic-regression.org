#+BEGIN_COMMENT
.. title: Implementing Logistic Regression for Tweet Sentiment Analysis
.. slug: 01d-implementing-twitter-logistic-regression
.. date: 2020-07-14 16:16:22 UTC-07:00
.. tags: nlp,sentiment analysis,logistic regression,twitter
.. category: NLP
.. link: 
.. description: Implementing Logistic Regression for twitter sentiment analysis.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-f4ceef62-5b21-4710-a34a-36d2e299f186.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  In the previous post in this series ({{% doc %}}01c-the-tweet-vectorizer{{% /doc %}} I transformed some tweet data to vectors based on the sums of the positive and negative tokens in each tweet. This post will implement a Logistic Regression model to train on those vectors to classify tweets by sentiment.
** Set Up
*** Imports
#+begin_src python :results none
# from python
from argparse import Namespace
from functools import partial
from pathlib import Path
from typing import Union

import math
import os
import pickle

# from pypi
from dotenv import load_dotenv
from expects import (
    be_true,
    expect,
    equal
)
from nltk.corpus import twitter_samples
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegressionCV

import holoviews
import hvplot.pandas
import nltk
import numpy
import pandas

# this package
from neurotic.nlp.twitter.counter import WordCounter
from neurotic.nlp.twitter.sentiment import TweetSentiment
from neurotic.nlp.twitter.vectorizer import TweetVectorizer

# for plotting
from graeae import EmbedHoloviews
#+end_src
*** The Dotenv
    This loads the locations of previous data and object saves I made.

#+begin_src python :results none
load_dotenv("posts/nlp/.env")
#+end_src
*** The Data
    I had the data saved previously so I'll load them.

#+begin_src python :results output :exports both
training =  pandas.read_feather(
    Path(os.environ["TWITTER_TRAIN_VECTORS"]).expanduser())
testing = pandas.read_feather(
    Path(os.environ["TWITTER_TEST_VECTORS"]).expanduser())

print(f"Training: {training.shape}")
print(f"Testing: {testing.shape}")
#+end_src

#+RESULTS:
: Training: (8000, 4)
: Testing: (2000, 4)

*** For Plotting
#+begin_src python :results none
SLUG = "implementing-twitter-logistic-regression"
Embed = partial(EmbedHoloviews,
                folder_path=f"files/posts/nlp/{SLUG}")

with Path(os.environ["TWITTER_PLOT"]).expanduser().open("rb") as reader:
    Plot = pickle.load(reader)
#+end_src
*** Types
    Some stuff for type hinting.

#+begin_src python :results none
Tweet = Union[numpy.ndarray, float]
PositiveProbability = Tweet
#+end_src
* Middle
** Logistic Regression
   Now that we have the data it's time to implement the [[https://www.wikiwand.com/en/Logistic_regression][Logistic Regression]] model to classify tweets as positive or negative.
*** The Sigmoid
    Logistic Regression uses a version of [[https://www.wikiwand.com/en/Sigmoid_function][the Sigmoid Function]] called the Standard [[https://www.wikiwand.com/en/Logistic_function][Logistic Function]] to measure whether an entry has passed the threshold for classification. This is the mathematical definition:

\[
\sigma(z) = \frac{1}{1 + e^{-x \cdot \theta}}
\]

The numerator (1) determines the maximum value for the function, so in this case the range is from 0 to 1 and we can interpret $\sigma(z)$ as the probability that a tweet (/z/) is positive (/1/). The interpretation of $\sigma(z)$ is it's the probability that /z/ (a vector representation of a tweet times the weights) is classified as 1 (having a positive sentiment). So we could re-write this as:

\[
P(Y=1 | z) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}}
\]

Where $x_1$ is the sum of the positive tweet counts for the tokens in $x$ and $x_2$ is the sum of the negative tweet counts for the tokens. $\beta_0$ is our bias and $\beta_1$ and $\beta_2$ are the weights that we're going to find by training our model.

#+begin_src python :results none
def sigmoid(z: Tweet) -> PositiveProbability:
    """Calculates the logistic function value

    Args:
     z: input to the logistic function (float or array)

    Returns:
     calculated sigmoid for z
    """
    return 1/(1 + numpy.exp(-z))
#+end_src

**** A Little Test
    We have a couple of given values to test that our sigmoid is correct.

#+begin_src python :results none
expect(sigmoid(0)).to(equal(0.5))

expect(math.isclose(sigmoid(4.92), 0.9927537604041685)).to(be_true)

expected = numpy.array([0.5, 0.9927537604041685])
actual = sigmoid(numpy.array([0, 4.92]))

expect(all(actual==expected)).to(be_true)
#+end_src

**** Plotting It
    Let's see what the output looks like.

#+begin_src python :results none
min_x = -6
max_x = 6

x = numpy.linspace(min_x, max_x)
y = sigmoid(x)
halfway = sigmoid(0)

plot_data = pandas.DataFrame.from_dict(dict(x=x, y=y))
curve = plot_data.hvplot(x="x", y="y", color=Plot.color_cycle)

line = holoviews.Curve([(min_x, halfway), (max_x, halfway)], color=Plot.tan)

plot = (curve * line).opts(
    width=Plot.width,
    height=Plot.height,
    fontscale=Plot.font_scale,
    title="Sigmoid",
    show_grid=True,
)

embedded = Embed(plot=plot, file_name="sigmoid_function")
output = embedded()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="sigmoid_function.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

Looking at the plot you can see that the probability that a tweet is positive is 0.5 when the input is 0, becomes more likely the more positive the input is, and is less likely the more negative an input is. Next we'll need to look at how to train our model.

*** The Loss Function
    To train our model we need a way to measure how well (or in this case poorly) it's doing. For this we'll use the [[http://wiki.fast.ai/index.php/Log_Loss][Log Loss]] function which is the negative logarithm of our probability - so for each tweet, we'll calculate $\sigma$ (which is the probability that it's positive) and take the negative logarithm of it to get the log-loss.

The formula for loss:

\[
 Loss = - \left( y\log (p) + (1-y)\log (1-p) \right)
\]

$y$ is the classification of the tweet (1 or 0) so when the tweet is classified 1 (positive) the right term becomes 0 and when the tweet is classified 0 (negative) the left term becomes 0 so this is the equivalent of:

#+begin_src python
if y == 1:
    loss = -log(p)
else:
    loss = -log(1 - p)
#+end_src

Where $p$ is the probability that the tweet is positive and $1 - p$ is the probability that it isn't (so it's negative since that's the only alternative).  We take the negative of the logarithm because $log(p)$ is negative (all the values of $p$ are between 0 and 1) so negating it makes the output positive.

We can fill it in to make it match what we're going to actually calculate - for the $i^{th}$ item in our dataset $p = \sigma(z^i \cdot \theta)$ and the equation becomes:

\[
 Loss = - \left( y^{(i)}\log (\sigma(z^{(i)} \cdot \theta)) + (1-y^{(i)})\log (1-\sigma(z^{(i)} \cdot \theta)) \right)
\]


#+begin_src python :results none
epsilon = 1e-3
steps = 10**3
probabilities = numpy.linspace(epsilon, 1, num=steps)
losses = -1 * numpy.log(probabilities)
data = pandas.DataFrame.from_dict({
    "p": probabilities,
    "Log-Loss": losses 
})

plot = data.hvplot(x="p", y="Log-Loss", color=Plot.blue).opts(
    title="Log-Loss (Y=1)",
    width=Plot.width,
    height=Plot.height,
    fontscale=Plot.font_scale,
    ylim=(0, losses.max())
)

output = Embed(plot=plot, file_name="log_loss_example")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="log_loss_example.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

So what is this telling us? This is for the case where a tweet is labeled positive and at the far left, near 0 (=log(0)= is undefined so you can use a really small probability but not 0) our model is saying that it probably isn't a positive tweet, so the log-loss is fairly high, then as we move along the x-axis our model is saying that it is more and more likely that the tweet is positive so our log-loss goes down, until we reach the point where our model says that it's 100% guaranteed to be a positive tweet, at which point our log-loss drops to zero. Fairly intuitive.

Let's look at the case where the tweet is actually negative (/y=0/). Since /p/ is the probability that it's positive, when the label is 0 we need to take the log of /1-p/ to see what the model thinks the probability is that it's negative.

#+begin_src python :results none
epsilon = 1e-3
steps = 10**3
probabilities = numpy.linspace(epsilon, 1-epsilon, num=steps)
losses = -1 * (numpy.log(1 - probabilities))
data = pandas.DataFrame.from_dict({
    "p": probabilities,
    "Log-Loss": losses 
})

plot = data.hvplot(x="p", y="Log-Loss", color=Plot.blue).opts(
    title="Log-Loss (Y=0)",
    width=Plot.width,
    height=Plot.height,
    fontscale=Plot.font_scale,
    ylim=(0, losses.max())
)

output = Embed(plot=plot, file_name="log_loss_y_0_example")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="log_loss_y_0_example.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

So now we have basically the opposite loss. In this case the tweet is not positive so when the model puts a low likelihood that the tweet is positive the log-loss is small, but as you move along the x-axis the model is giving more probability to the notion that the tweet is positive so the log-loss gets larger.
*** Training the Model
    To train the model we're going to use [[https://www.wikiwand.com/en/Gradient_descent][Gradient Descent]]. What this means is that we're going to use the /gradient/ of our loss function to figure out how to update our weights. The /gradient/ is just the slope of the loss-function (but generalized to multiple dimensions).

How do we do this? First we calculate our model's estimate of the input being positive, then we calculate the gradient of its loss. If you remember from calculus the slope of a line is the derivative of its function so instead of calculating the loss, we'll calculate the derivative of the loss-function which is given as:

\[
\nabla_{\theta}L_{\theta} = \left [ \sigma(x \cdot \theta) - y \right] x_j
\]

The rightmost term $x_j$ represents one term in the input vector, the one that matches the weight - this has to be repeated for each $\beta$ in $\theta$ so in our case it will be repeated three times, with $x$ being 1 for the bias term.

It's called stochastic gradient descent because the inputs are chosen randomly from our training set. This turns out to not give you a smooth descent so we're going to do **batch  training** which changes our gradient a little.

\[
\nabla_{\theta_j}L_{\theta} = \frac{1}{m} \sum_{i=1}^m(\sigma(x \cdot \theta)-y)x_j
\]

Our gradient is now the average of the gradients for each of the inputs in our training set. We update the weights by subtracting a fraction of the difference between the current weights and the gradient. The fraction $\eta$ is called the /learning rate/ and it controls how much the weights change, representng how fast our model will learn. If it is too large we can miss the minimum and if it's too large it will take too long to train the model, so we need to choose the right value for it to reach the minima within a feasible time. 

Here's the algorithm in the rough.

 - /L/: Loss Function
 - $\sigma$: probability function parameterized by $\theta$
 - /x/: set of training inputs
 - /y/: set of training labels

#+begin_export html
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    });
</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js">
</script>

<pre id="gradientdescent" style="display:hidden">
\begin{algorithm}
\caption{Stochastic Gradient Descent}
\begin{algorithmic}
\STATE $\theta \gets 0$
\WHILE{not done}

 \FOR{each $(x^{(i)},y^{(i)})$ in training data}
  \State $\hat{y} \gets \sigma(x^{(i)}; \theta)$
  \State $loss \gets L(\hat{y}^{(i)}, y^{(i)})$
  \State $g \gets \nabla_{\theta} L(\hat{y}^{(i)}, y^{(i)})$
  \State $\theta \gets \theta - \eta g$
 \ENDFOR

\ENDWHILE
\end{algorithmic}
\end{algorithm}
</pre>
#+end_export

We can translate this a little more.

#+begin_export html
<pre id="gradientdescentengrish" style="display:hidden">
\begin{algorithm}
\caption{Stochastic Gradient Descent}
\begin{algorithmic}
\STATE Initialize the weights
\WHILE{the loss is still too high}

 \FOR{each $(x^{(i)},y^{(i)})$ in training data}
  \State What is our probability that the input is positive?
  \State How far off are we?
  \State What direction would we need to head to maximize the error?
  \State Let's go in the opposite direction.
 \ENDFOR

\ENDWHILE
\end{algorithmic}
\end{algorithm}
</pre>

<script>
    pseudocode.renderElement(document.getElementById("gradientdescent"));
    pseudocode.renderElement(document.getElementById("gradientdescentengrish"));
</script>
#+end_export

Note that the losses aren't needed for the algorithm to train the model, just for assessing how well the model did.
*** Implement It
**** The Function
#+begin_src python :results none
def gradient_descent(x: numpy.ndarray, y: numpy.ndarray,
                     weights: numpy.ndarray, learning_rate: float,
                     iterations: int=1):
    """Finds the weights for the model

    Args:
     x: the tweet vectors
     y: the positive/negative labels
     weights: the regression weights
     learning_rate: (eta) how much to update the weights
     iterations: the number of times to repeat training
    """
    assert len(x) == len(y)
    rows = len(x)
    losses = []
    learning_rate /= rows
    for iteration in range(iterations):
        y_hat = sigmoid(x.dot(weights))
        # average loss
        loss = numpy.squeeze(-((y.T.dot(numpy.log(y_hat))) +
                               (1 - y.T).dot(numpy.log(1 - y_hat))))/rows
        losses.append(loss)
        gradient = ((y_hat - y).T.dot(x)).sum(axis=0, keepdims=True)
        weights -= learning_rate * gradient.T
    return loss, weights, losses
#+end_src

If you look at the implementation you can see that there are some changes made to it from what I wrote earlier. This is because the algorithm I wrote in pseudocode came from a book while the implementation that I made came from a Coursera assignment. The main differences being that we use a set number of iterations to train the model and the learning rate is divided by the number of training examples. Of course, you could just divide the learning rate before passing it in to the function so it doesn't really change it that much. I also had to take into account the fact that you can't just take a dot product of two matrices if their shapes aren't compatible - the rows of the left hand matrix has to match the columns of the right hand matrix) so there's some transposing of matrices being done. Our actual implementation might be more like this.

#+begin_export html
<pre id="gradientdescentimplementation" style="display:hidden">
\begin{algorithm}
\caption{Stochastic Gradient Descent Implemented}
\begin{algorithmic}
\STATE $\theta \gets 0$
\STATE $m \gets rows(X)$
\FOR{$iteration \in$ \{0 $\ldots iterations-1$ \}}
  \STATE $\hat{Y} \gets \sigma(X \cdot \theta)$
  \STATE $loss \gets -\frac{1}{m}(Y^T \cdot \ln \hat{Y}) + (1 - Y)^T \cdot (\ln 1 - \hat{Y})$
  \STATE $\nabla \gets \sum (\hat{Y} - Y)^T \cdot x$
  \STATE $\theta \gets \theta - \frac{\eta}{m} \nabla^T$
 \ENDFOR
\end{algorithmic}
\end{algorithm}
</pre>

<script>
    pseudocode.renderElement(document.getElementById("gradientdescentimplementation"));
</script>
#+end_export

**** Test It
     First we'll make a fake (random) input set to make it easier to check the gradient descent.
#+begin_src python :results none
numpy.random.seed(1)
bias = numpy.ones((10, 1))
fake = numpy.random.rand(10, 2) * 2000
fake_tweet_vectors = numpy.append(bias, fake, axis=1)
#+end_src

Now, the fake labels - we'll make around 35% of them negative and the rest positive.

#+begin_src python :results none
fake_labels = (numpy.random.rand(10, 1) > 0.35).astype(float)
#+end_src

**** Do the Descent
     So now we can pass our test data into the gradient descent function and see what happens.

#+begin_src python :results output :exports both
fake_weights = numpy.zeros((3, 1))
fake_loss, fake_weights, losses = gradient_descent(x=fake_tweet_vectors,
                                           y=fake_labels, 
                                           weights=fake_weights,
                                           learning_rate=1e-8,
                                           iterations=700)
expect(math.isclose(fake_loss, 0.67094970, rel_tol=1e-8)).to(be_true)
print(f"The log-loss after training is {fake_loss:.8f}.")
print(f"The trained weights are {[round(t, 8) for t in numpy.squeeze(fake_weights)]}")
#+end_src

#+RESULTS:
: The log-loss after training is 0.67094970.
: The trained weights are [4.1e-07, 0.00035658, 7.309e-05]

** Train the Model
   Now that we have our parts let's actually train the model using the real training data. I originally did this expecting numpy arrays (like in earlier steps I was expecting python lists instead of numpy arrays - stuff changes) so I'll be extracting the relevant columns from the pandas DataFrame and converting them back to arrays.

#+begin_src python :results output :exports both
weights = numpy.zeros((3, 1))
eta = 1e-9
iterations = 1500
final_loss, weights, losses = gradient_descent(
    x=training[["bias", "positive", "negative"]].values,
    y=training.sentiment.values, weights=weights,
    learning_rate=eta, iterations=iterations)

print(f"The log-loss after training is {final_loss:.8f}.")
print(f"The resulting vector of weights is {[round(t, 8) for t in numpy.squeeze(weights)]}")
#+end_src

#+RESULTS:
: The log-loss after training is nan.
: The resulting vector of weights is [nan, nan, nan]


#+begin_src python :results none
plot_losses = pandas.DataFrame.from_dict({"Log-Loss": losses})
plot = plot_losses.hvplot().opts(title="Training Losses",
                            width=Plot.width,
                            height=Plot.height,
                            fontscale=Plot.font_scale,
                            color=Plot.blue
                            )

output = Embed(plot=plot, file_name="training_loss")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="training_loss.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

As you can see, the losses are still on the decline, but we'll stop here to see how it's doing.

** Test the Model
   This will be a class to predict the sentiment of a tweet using our model.

#+begin_src python :tangle ../../neurotic/nlp/twitter/sentiment.py
# pypi
import attr
import numpy

# this project
from .vectorizer import TweetVectorizer


@attr.s(auto_attribs=True)
class TweetSentiment:
    """Predicts the sentiment of a tweet

    Args:
     vectorizer: something to vectorize tweets
     theta: vector of weights for the logistic regression model
    """
    vectorizer: TweetVectorizer
    theta: numpy.ndarray

    def sigmoid(self, vectors: numpy.ndarray) -> float:
        """the logistic function

        Args:
         vectors: a matrix of bias, positive, negative counts

        Returns:
         array of probabilities that the tweets are positive
        """
        return 1/(1 + numpy.exp(-vectors))

    def probability_positive(self, tweet: str) -> float:
        """Calculates the probability of the tweet being positive

        Args:
         tweet: a tweet to classify

        Returns:
         the probability that the tweet is a positive one
        """
        x = self.vectorizer.extract_features(tweet, as_array=True)
        return numpy.squeeze(self.sigmoid(x.dot(self.theta)))

    def classify(self, tweet: str) -> int:
        """Decides if the tweet was positive or not

        Args:
         tweet: the tweet message to classify.
        """
        return int(numpy.round(self.probability_positive(tweet)))

    def __call__(self) -> numpy.ndarray:
        """Get the sentiments of the vectorized tweets
        
        Note:
         this assumes that the vectorizer passed in has the tweets

        Returns:
         array of predicted sentiments (1 for positive 0 for negative)
        """
        return numpy.round(self.sigmoid(self.vectorizer.vectors.dot(self.theta)))
#+end_src

To build the =TweetSentiment= I'll first load the vectorizer that I pickled in {{% doc %}}the-tweet-vectorizer{{% /doc %}}.

#+begin_src python :results none
with Path(os.environ["TWITTER_VECTORIZER"]).expanduser().open("rb") as reader:
    vectorizer = pickle.load(reader)
#+end_src

Now the =TweetSentiment=.

#+begin_src python :results output :exports both
sentiment = TweetSentiment(vectorizer, weights)
for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:
    print(f'{tweet} -> {sentiment.probability_positive(tweet)}')

#+end_src

#+RESULTS:
: I am happy -> 0.5000116603547458
: I am bad -> 0.5000042596595592
: this movie should have been great. -> 0.5000112046942453
: great -> 0.5000087076231243
: great great -> 0.5000173410082799
: great great great -> 0.5000259743934251
: great great great great -> 0.5000346077785548

Strangely very near the center. Probably because the words weren't that commonly used in our training set.

#+begin_src python :results output :exports both
totals = sum(vectorizer.counter.counts.values())
print(f"Great positive percentage: {100 * vectorizer.counter.counts[('great', 1)]/totals:.2f} %")
print(f"Great negative percentage: {100 * vectorizer.counter.counts[('great', 0)]/totals:.2f} % ")
#+end_src

#+RESULTS:
: Great positive percentage: 0.23 %
: Great negative percentage: 0.02 % 

Now we can see how it did overall.

#+begin_src python :results output :exports both
test_data = pandas.read_csv(Path(os.environ["TWITTER_TEST_SET"]).expanduser())
test_vectorizer = TweetVectorizer(test_data.tweet.values, vectorizer.counter)
sentiment = TweetSentiment(test_vectorizer, weights)

predictions = sentiment()[:, 0]
correct = sum(predictions == testing.sentiment.values)
print(f"Accuracy: {correct/len(test_data)}")
#+end_src

#+RESULTS:
: Accuracy: 0.4995835415625521

Whoops.

** The Wrong Stuff
#+begin_src python :results output :exports both
x_test = numpy.array(x_test).reshape((-1, 1))
wrong_places = predictions != y_test
wrong = x_test[wrong_places]
print(len(wrong))
#+end_src

#+RESULTS:
: 11

#+begin_src python :results output :exports both
wrong_ys = y_test[wrong_places]

for index, tweet in enumerate(wrong):
    print("*" * 10)
    print(f"Tweet: {tweet}")
    print(f"Tokens: {vectorizer.process(tweet)}")
    print(f"Probability Positive: {sentiment.probability_positive(tweet)}")
    print(f"Classification: {wrong_ys[index]}")
    print()
#+end_src

#+RESULTS:
#+begin_example
,**********
Tweet: I'm playing Brain Dots : ) #BrainDots
http://t.co/ilDzDRHf9d http://t.co/VTXNFCPFuI
Tokens: ["i'm", 'play', 'brain', 'dot', 'braindot']
Probability Positive: 0.48526803083166703
Classification: 1

,**********
Tweet: @ellekagaoan @chinmarquez Catch up once in a while :( &gt;:D&lt; @aditriphosphate @ErinMonzon
Tokens: ['catch', ':(', '>:d']
Probability Positive: 0.11389833615259448
Classification: 1

,**********
Tweet: off to the park to get some sunlight : )
Tokens: ['park', 'get', 'sunlight']
Probability Positive: 0.49574440240612333
Classification: 1

,**********
Tweet: Google has made @narendramodi really very sad about @ImranKhanPTI not becoming Prime Minister. :p @PTIofficial @pmln_org
Tokens: ['googl', 'made', 'realli', 'sad', 'becom', 'prime', 'minist', ':p']
Probability Positive: 0.49947331811099865
Classification: 1

,**********
Tweet: @planetjedward GoodMorning ! What's coming next? =:D =:D
Tokens: ['goodmorn', "what'", 'come', 'next', '=:', '=:']
Probability Positive: 0.49786597951455913
Classification: 1

,**********
Tweet: @_sarah_mae omg you can't just tell this and don't say more :p can't wait to know !!!! ❤️
Tokens: ['omg', "can't", 'tell', 'say', ':p', "can't", 'wait', 'know', '❤', '️']
Probability Positive: 0.48000079019523884
Classification: 1

,**********
Tweet: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5
Tokens: ["i'm", 'play', 'brain', 'dot', 'braindot']
Probability Positive: 0.48526803083166703
Classification: 1

,**********
Tweet: @samayanyan yes thank u!! Oh damn that hella sucks :-( but at least u had a really good time that's all that matters
Tokens: ['ye', 'thank', 'u', 'oh', 'damn', 'hella', 'suck', ':-(', 'least', 'u', 'realli', 'good', 'time', "that'", 'matter']
Probability Positive: 0.5145779186318544
Classification: 0

,**********
Tweet: @phenomyoutube u probs had more fun with david than me : (
Tokens: ['u', 'prob', 'fun', 'david']
Probability Positive: 0.5101260021527917
Classification: 0

,**********
Tweet: @wtfxmbs AMBS please it's harry's jeans :)):):):(
Tokens: ['amb', 'pleas', "harry'", 'jean', ':)', '):', '):', '):']
Probability Positive: 0.8218858541205992
Classification: 0

,**********
Tweet: @hinata_shouyno fuck u Neil u ruined it &gt;:-(
Tokens: ['fuck', 'u', 'neil', 'u', 'ruin', '>:-(']
Probability Positive: 0.5095962377275693
Classification: 0
#+end_example

The first thing to notice is that there's a duplicate tweet (the one about "Brain Dots", whatever that is). Another thing to note is that sometimes there are spaces between the characters in the emoticons, which the tokenizer probably can't figure out should be togethter, along with that weird =:)):):):(= emoticon. I'm not really soure that all of the positive tweets are actually positive, nor is it always obvious what they are about - what does "AMBS please it's harry's jeans" mean?

In at least in one case the NLTK vectorizer seems to mangle an emoticon as well:

#+begin_src python :results output :exports both
print(vectorizer.process.tokenizer.tokenize("=:D"))
#+end_src

#+RESULTS:
: ['=:', 'd']

That last one looks really wrong, though, let's take a look at it.

#+begin_src python :results output :exports both
for token in vectorizer.process(wrong[-1]):
    print(f"{token}: positive={counter.counts[(token, 1)]} negative={counter.counts[(token, 0)]}")
#+end_src

#+RESULTS:
: fuck: positive=23 negative=47
: u: positive=206 negative=148
: neil: positive=2 negative=0
: u: positive=206 negative=148
: ruin: positive=3 negative=10
: >:-(: positive=0 negative=2

So the big problem seems to be that the letter "u" is there twice and it's mostly seen as a positive. Why am I allowing single letters? There should probably be a minimum length or something. Anyway, I'm not sure you could get much better.

** Some Fresh Tweets
   First someone reacting to a post about the [[https://www.atlasobscura.com/places/clown-motel][Clown Motel]] in Tonopah, Nevada. The previous link was to Atlas Obscura, but the tweet came from [[https://www.thrillist.com/travel/nation/clown-motel-nevada-hame-anand][thrillist]].
#+begin_src python :results output :exports both
sentiments = {0: "negative", 1: "positive"}
tweet = "Nah dude. I drove by that at night and it was the creepiest thing ever. The whole town gave me bad vibes. I still shudder when I think about it."
print(f"Classified as {sentiments[sentiment.classify(tweet)]}")
#+end_src

#+RESULTS:
: Classified as negative

Seems reasonable.

#+begin_src python :results output :exports both
tweet = "This is just dope. Quaint! I’d love to have an ironic drive-in wedding in Las Vegas and then stay in a clown motel as newly weds for one night. I bet they have Big Clown Suits for newly weds, haha."

print(f"Classified as {sentiments[sentiment.classify(tweet)]}")
#+end_src

#+RESULTS:
: Classified as positive

** Compare to SKLearn
#+begin_src python :results output :exports both
classifier = LogisticRegressionCV(
    random_state=2020,
    max_iter=1500,
    scoring="neg_log_loss").fit(vectorizer.vectors, y_train)

predictions = classifier.predict(test_vectorizer.vectors).reshape((-1, 1))
correct = sum(predictions == y_test)
print(f"Accuracy: {correct[0]/len(y_test)}")
#+end_src

#+RESULTS:
: Accuracy: 0.9925

So it didn't do quite as well, but pretty much the same just using the default parameters. We could probably do a parameter search but that's okay for now.

* End

Let's save our weights for later. I was going to just write it to a file, but you seem to lose some precision converting the values to strings. numpy has a function called [[https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html][savetxt]] but it didn't behave exactly like I thought it would and I prefer pandas so I'll save it that way.

#+begin_src python :results none
load_dotenv("posts/nlp/.env")

path = Path(os.environ["TWITTER_REGRESSION_WEIGHTS"]).expanduser()
weights_frame = pandas.DataFrame(weights.T, columns="bias positive negative".split())
weights_frame.to_csv(path, index=False)
#+end_src

We should also save the counts because we're going to need that for later.

#+begin_src python :results none
load_dotenv("posts/nlp/.env")

counts = pandas.DataFrame(train_vectorizer.vectors, columns="bias positive negative".split())
counts["sentiment"] = y_train
path = Path(os.environ["TWITTER_REGRESSION_DATA"]).expanduser()
counts.to_csv(path, index=False)
#+end_src

**Note:** This is a re-working of an exercise from Coursera's Natural Language Processing specialization.

I also referred to this revision in progress:

 - Jurafsky, D. & Martin, J. (2020). Speech and language processing : an introduction to natural language processing, computational linguistics, and speech recognition. 3rd Edition draft. [[https://web.stanford.edu/~jurafsky/slp3/][(URL)]]
