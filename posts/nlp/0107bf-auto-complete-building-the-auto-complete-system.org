#+BEGIN_COMMENT
.. title: Auto-Complete: Building the Auto-Complete System
.. slug: auto-complete-building-the-auto-complete-system
.. date: 2020-12-04 15:21:47 UTC-08:00
.. tags: nlp,auto-complete,n-gram
.. category: NLP
.. link: 
.. description: Putting together the N-Gram auto-complete system.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-628b96bb-9fe2-4219-af43-264f81238d87-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  In the {{% lancelot title="previous post" %}}auto-complete-perplexity{{% /lancelot %}} we tested the perplexity of our N-Gram Language model. In this, the final post in the series that we began {{% lancelot title="with this post" %}}auto-complete{{% /lancelot %}}, we'll implement the final system.
* End
  So, now we have our system. Here are all the prior posts in this series.

  - {{% lancelot title="Overview" %}}auto-complete{{% /lancelot %}}
  - {{% lancelot title="Pre-Processing I" %}}auto-complete-pre-process-the-data-i{{% /lancelot %}}
  - {{% lancelot title="Pre-Processing II" %}}auto-complete-pre-process-the-data-ii{{% /lancelot %}}
  - {{% lancelot title="The N-Gram Model" %}}auto-complete-the-n-gram-model{{% /lancelot %}}
  - {{% lancelot title="Perplexity" %}}auto-complete-perplexity{{% /lancelot %}}
* Raw
#+begin_example
# ## Part 4: Build an auto-complete system
# 
# In this section, you will combine the language models developed so far to implement an auto-complete system. 
# 

# <a name='ex-11'></a>
# ### Exercise 11
# Compute probabilities for all possible next words and suggest the most likely one.
# - This function also take an optional argument `start_with`, which specifies the first few letters of the next words.

# <details>    
# <summary>
#     <font size="3" color="darkgreen"><b>Hints</b></font>
# </summary>
# <p>
# <ul>
#     <li><code>estimate_probabilities</code> returns a dictionary where the key is a word and the value is the word's probability.</li>
#     <li> Use <code>str1.startswith(str2)</code> to determine if a string starts with the letters of another string.  For example, <code>'learning'.startswith('lea')</code> returns True, whereas <code>'learning'.startswith('ear')</code> returns False. There are two additional parameters in <code>str.startswith()</code>, but you can use the default values for those parameters in this case.</li>
# </ul>
# </p>

# In[ ]:


# UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: suggest_a_word
def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):
    """
    Get suggestion for the next word
    
    Args:
        previous_tokens: The sentence you input where each token is a word. Must have length > n 
        n_gram_counts: Dictionary of counts of (n+1)-grams
        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams
        vocabulary: List of words
        k: positive constant, smoothing parameter
        start_with: If not None, specifies the first few letters of the next word
        
    Returns:
        A tuple of 
          - string of the most likely next word
          - corresponding probability
    """
    
    # length of previous words
    n = len(list(n_gram_counts.keys())[0]) 
    
    # From the words that the user already typed
    # get the most recent 'n' words as the previous n-gram
    previous_n_gram = previous_tokens[-n:]

    # Estimate the probabilities that each word in the vocabulary
    # is the next word,
    # given the previous n-gram, the dictionary of n-gram counts,
    # the dictionary of n plus 1 gram counts, and the smoothing constant
    probabilities = estimate_probabilities(previous_n_gram,
                                           n_gram_counts, n_plus1_gram_counts,
                                           vocabulary, k=k)
    
    # Initialize suggested word to None
    # This will be set to the word with highest probability
    suggestion = None
    
    # Initialize the highest word probability to 0
    # this will be set to the highest probability 
    # of all words to be suggested
    max_prob = 0
    
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    
    # For each word and its probability in the probabilities dictionary:
    for word, prob in None: # complete this line
        
        # If the optional start_with string is set
        if None: # complete this line
            
            # Check if the beginning of word does not match with the letters in 'start_with'
            if None: # complete this line

                # if they don't match, skip this word (move onto the next word)
                None # complete this line
        
        # Check if this word's probability
        # is greater than the current maximum probability
        if None: # complete this line
            
            # If so, save this word as the best suggestion (so far)
            suggestion = None
            
            # Save the new maximum probability
            max_prob = None

    ### END CODE HERE
    
    return suggestion, max_prob


# In[ ]:


# test your code
sentences = [['i', 'like', 'a', 'cat'],
             ['this', 'dog', 'is', 'like', 'a', 'cat']]
unique_words = list(set(sentences[0] + sentences[1]))

unigram_counts = count_n_grams(sentences, 1)
bigram_counts = count_n_grams(sentences, 2)

previous_tokens = ["i", "like"]
tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)
print(f"The previous words are 'i like',\n\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}")

print()
# test your code when setting the starts_with
tmp_starts_with = 'c'
tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)
print(f"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\n\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}")


# ### Expected output
# 
# ```CPP
# The previous words are 'i like',
# 	and the suggested word is `a` with a probability of 0.2727
# 
# The previous words are 'i like', the suggestion must start with `c`
# 	and the suggested word is `cat` with a probability of 0.0909
# 
# ```

# ### Get multiple suggestions
# 
# The function defined below loop over varioud n-gram models to get multiple suggestions.

# In[ ]:


def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):
    model_counts = len(n_gram_counts_list)
    suggestions = []
    for i in range(model_counts-1):
        n_gram_counts = n_gram_counts_list[i]
        n_plus1_gram_counts = n_gram_counts_list[i+1]
        
        suggestion = suggest_a_word(previous_tokens, n_gram_counts,
                                    n_plus1_gram_counts, vocabulary,
                                    k=k, start_with=start_with)
        suggestions.append(suggestion)
    return suggestions


# In[ ]:


# test your code
sentences = [['i', 'like', 'a', 'cat'],
             ['this', 'dog', 'is', 'like', 'a', 'cat']]
unique_words = list(set(sentences[0] + sentences[1]))

unigram_counts = count_n_grams(sentences, 1)
bigram_counts = count_n_grams(sentences, 2)
trigram_counts = count_n_grams(sentences, 3)
quadgram_counts = count_n_grams(sentences, 4)
qintgram_counts = count_n_grams(sentences, 5)

n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]
previous_tokens = ["i", "like"]
tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)

print(f"The previous words are 'i like', the suggestions are:")
display(tmp_suggest3)


# ### Suggest multiple words using n-grams of varying length
# 
# Congratulations!  You have developed all building blocks for implementing your own auto-complete systems.
# 
# Let's see this with n-grams of varying lengths (unigrams, bigrams, trigrams, 4-grams...6-grams).

# In[ ]:


n_gram_counts_list = []
for n in range(1, 6):
    print("Computing n-gram counts with n =", n, "...")
    n_model_counts = count_n_grams(train_data_processed, n)
    n_gram_counts_list.append(n_model_counts)


# In[ ]:


previous_tokens = ["i", "am", "to"]
tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)

print(f"The previous words are {previous_tokens}, the suggestions are:")
display(tmp_suggest4)


# In[ ]:


previous_tokens = ["i", "want", "to", "go"]
tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)

print(f"The previous words are {previous_tokens}, the suggestions are:")
display(tmp_suggest5)


# In[ ]:


previous_tokens = ["hey", "how", "are"]
tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)

print(f"The previous words are {previous_tokens}, the suggestions are:")
display(tmp_suggest6)


# In[ ]:


previous_tokens = ["hey", "how", "are", "you"]
tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)

print(f"The previous words are {previous_tokens}, the suggestions are:")
display(tmp_suggest7)


# In[ ]:


previous_tokens = ["hey", "how", "are", "you"]
tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with="d")

print(f"The previous words are {previous_tokens}, the suggestions are:")
display(tmp_suggest8)


# # Congratulations!
# 
# You've completed this assignment by building an autocomplete model using an n-gram language model!  
# 
# Please continue onto the fourth and final week of this course!
#+end_example
