#+BEGIN_COMMENT
.. title: Deep N-Grams
.. slug: deep-n-grams
.. date: 2021-01-05 16:30:51 UTC-08:00
.. tags: nlp,n-grams,rnn
.. category: NLP
.. link: 
.. description: Implementing a Deep Learning N-Gram model.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
* Deep N-Grams
  This is an exploration of Recurrent Neural Networks (RNN) using [[https://github.com/google/trax][trax]]. We're going to predict the next set of characters in a sentence given the previous characters.

Since this is so long I'm going to break it up into separate posts.

 - {{% lancelot title="Loading the Data" %}}deep-n-grams-loading-the-data{{% /lancelot %}}: Load the data and convert it to tensors
 - {{% lancelot title="Generating Data" %}}deep-n-grams-batch-generation{{% /lancelot %}}: Create a batch generator for the tensors
 - {{% lancelot title="Creating the Model" %}}deep-n-grams-creating-the-model{{% /lancelot %}}: Create a Gated Recurrent Unit (GRU) model
 - {{% lancelot title="Training the Model" %}}deep-n-grams-training-the-model{{% /lancelot %}}: Train the model
 - {{% lancelot title="Evaluating the Model" %}}deep-n-grams-evaluating-the-model{{% /lancelot %}}: Evaluate the model's perplexity
 - {{% lancelot title="Generating Sentences" %}}deep-n-grams-generating-sentences{{% /lancelot %}}: Generate new sentences using the model

First up:  - {{% lancelot title="Loading the Data" %}}deep-n-grams-loading-the-data{{% /lancelot %}}.
* Raw
#+begin_example python
# The figure above gives you a summary of what you are about to implement. 
# - You will get the embeddings;
# - Stack the embeddings on top of each other;
# - Run them through two layers with a relu activation in the middle;
# - Finally, you will compute the softmax. 
# 
# To predict the next character:
# - Use the softmax output and identify the word with the highest probability.
# - The word with the highest probability is the prediction for the next word.

# In[ ]:


import os
import trax
import trax.fastmath.numpy as np
import pickle
import numpy
import random as rnd
from trax import fastmath
from trax import layers as tl

# set random seed
trax.supervised.trainer_lib.init_random_number_generators(32)
rnd.seed(32)


# <a name='1'></a>

# <a name='5'></a>
#+end_example
