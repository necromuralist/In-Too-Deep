<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Calculating perplexity with Jax and Numpy." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Jax, Numpy, and Perplexity | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/jax-numpy-and-perplexity/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../hidden-state-activation/" rel="prev" title="Hidden State Activation" type="text/html">
<link href="../vanilla-rnns-and-grus/" rel="next" title="Vanilla RNNs and GRUs" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Jax, Numpy, and Perplexity" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/jax-numpy-and-perplexity/" property="og:url">
<meta content="Calculating perplexity with Jax and Numpy." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-31T21:41:39-08:00" property="article:published_time">
<meta content="nlp" property="article:tag">
<meta content="rnn" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Jax, Numpy, and Perplexity</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-31T21:41:39-08:00" itemprop="datePublished" title="2020-12-31 21:41">2020-12-31 21:41</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orged8f697">Beginning</a>
<ul>
<li><a href="#orga207ac1">Imports</a></li>
<li><a href="#org19b081d">Set Up</a>
<ul>
<li><a href="#org5d6dc68">The Data Paths</a></li>
<li><a href="#org6f8fb6f">The Random Seed</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org324bcd6">Middle</a>
<ul>
<li><a href="#orgf86673c">Numpy vs Trax</a></li>
<li><a href="#org0c1655b">Calculating Perplexity</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orged8f697">
<h2 id="orged8f697">Beginning</h2>
<div class="outline-text-2" id="text-orged8f697"></div>
<div class="outline-3" id="outline-container-orga207ac1">
<h3 id="orga207ac1">Imports</h3>
<div class="outline-text-3" id="text-orga207ac1">
<p><b>Note to future self:</b> The default jax installation from <code>pip</code> is CPU only, to get it to run on the GPU (which seems to be the main reason to use it) you need to specify it. Right now the command is:</p>
<pre class="example" id="org759d699">
pip install jaxlib==0.1.57+cuda111 -f https://storage.googleapis.com/jax-releases/jax_releases.html
</pre>
<p>Where <code>cuda111</code> refers to the fact that I have cuda 11.1 installed on the server, so I need that version. See the <a href="https://github.com/google/jax#installation">installation instructions</a> for more information (and to see if anything changes).</p>
<div class="highlight">
<pre><span></span><span class="c1"># from python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">trax</span>
<span class="kn">import</span> <span class="nn">trax.fastmath.numpy</span> <span class="k">as</span> <span class="nn">trax_numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org19b081d">
<h3 id="org19b081d">Set Up</h3>
<div class="outline-text-3" id="text-org19b081d"></div>
<div class="outline-4" id="outline-container-org5d6dc68">
<h4 id="org5d6dc68">The Data Paths</h4>
<div class="outline-text-4" id="text-org5d6dc68">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Paths</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">targets</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"RNN_TARGETS"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(),</span>
    <span class="n">predictions</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"RNN_PREDICTIONS"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6f8fb6f">
<h4 id="org6f8fb6f">The Random Seed</h4>
<div class="outline-text-4" id="text-org6f8fb6f">
<div class="highlight">
<pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># trax no longer has a global seed setting - pass it to the training.Loop</span>
<span class="c1"># trax.supervised.trainer_lib.init_random_number_generators(SEED)</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org324bcd6">
<h2 id="org324bcd6">Middle</h2>
<div class="outline-text-2" id="text-org324bcd6"></div>
<div class="outline-3" id="outline-container-orgf86673c">
<h3 id="orgf86673c">Numpy vs Trax</h3>
<div class="outline-text-3" id="text-orgf86673c">
<p>One important change to take into consideration is that the types of the resulting objects will be different depending on the version of numpy. With regular numpy you get <code>numpy.ndarray</code> but with Trax's numpy you will get <code>jax.interpreters.xla.DeviceArray</code>. These two types map to each other. So if you find some error logs mentioning DeviceArray type, don't worry about it, treat it like you would treat an ndarray and march ahead.</p>
<p>You can get a randomized numpy array by using the <code>numpy.random.random()</code> function.</p>
<p>This is one of the functionalities that Trax's numpy does not currently support in the same way as the regular numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The regular numpy array looks like this:</span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{</span><span class="n">numpy_array</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"It is of type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org083bcb1">
The regular numpy array looks like this:

 [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514
  0.10108656 0.92848807 0.60910917 0.59655344]
 [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249
  0.58940123 0.04993276 0.56179184 0.76635847]
 [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549
  0.16242374 0.70937058 0.16062408 0.81077677]
 [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613
  0.67483453 0.78238275 0.69520163 0.32895445]
 [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492
  0.39807245 0.72790532 0.86333097 0.02616954]]

It is of type: &lt;class 'numpy.ndarray'&gt;
</pre>
<p>You can easily cast regular numpy arrays or lists into trax numpy arrays using the <code>trax.fastmath.numpy.array()</code> function:</p>
<div class="highlight">
<pre><span></span><span class="n">trax_numpy_array</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The trax numpy array looks like this:</span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{</span><span class="n">trax_numpy_array</span><span class="si">}</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"It is of type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">trax_numpy_array</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org97aa771">
The trax numpy array looks like this:

 [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514
  0.10108656 0.9284881  0.60910916 0.59655344]
 [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246
  0.58940125 0.04993276 0.56179184 0.7663585 ]
 [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255
  0.16242374 0.7093706  0.16062407 0.81077677]
 [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613
  0.67483455 0.7823827  0.69520164 0.32895446]
 [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493
  0.39807245 0.72790533 0.86333096 0.02616954]]

It is of type: &lt;class 'jax.interpreters.xla._DeviceArray'&gt;
</pre>
<p>The previous section was a quick look at Trax's numpy. However this notebook also aims to teach you how you can calculate the perplexity of a trained model.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org0c1655b">
<h3 id="org0c1655b">Calculating Perplexity</h3>
<div class="outline-text-3" id="text-org0c1655b">
<p>The <i>perplexity</i> is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as:</p>
<p>\[ P(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}} \]</p>
<p>As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our <code>RNN</code>, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:</p>
\begin{align} log P(W) &amp;= {log\left(\sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}}\right)} \\ &amp;= {log\left({\prod_{i=1}^{N} \frac{1}{P(w_i| w_1,...,w_{n-1})}}\right)^{\frac{1}{N}}} \\ &amp;= {log\left({\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\right)^{-\frac{1}{N}}} \\ &amp;= -\frac{1}{N}{log\left({\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\right)} \\ &amp;= -\frac{1}{N}{\left({\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\right)} \end{align}
<p>We're going to use some pre-made arrays.</p>
<div class="highlight">
<pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Paths</span><span class="o">.</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Paths</span><span class="o">.</span><span class="n">targets</span><span class="p">)</span>
</pre></div>
<p>Now we'll cast the numpy arrays to jax.interpreters.xla.DeviceArrays.</p>
<div class="highlight">
<pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'predictions has shape: </span><span class="si">{</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'targets has shape: </span><span class="si">{</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
predictions has shape: (32, 64, 256)
targets has shape: (32, 64)
</pre>
<p>Notice that the predictions have an extra dimension - this is the same length as the size of the vocabulary used. Because of this you will need a way of reshaping <code>targets</code> to match this shape. For this we will use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.one_hot">trax.layers.one_hot</a>.</p>
<p>Also note that we can get the size of the last dimension using <code>predictions.shape[-1]</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">reshaped_targets</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">targets</span><span class="p">,</span> <span class="n">n_categories</span><span class="o">=</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'reshaped_targets has shape: </span><span class="si">{</span><span class="n">reshaped_targets</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
reshaped_targets has shape: (32, 64, 256)
</pre>
<p>By calculating the product of the predictions and the reshaped targets and summing across the last dimension, we can compute the total log perplexity.</p>
<div class="highlight">
<pre><span></span><span class="n">total_log_perplexity</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">predictions</span> <span class="o">*</span> <span class="n">reshaped_targets</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
<p>Now you will need to account for the padding so this metric is not artificially deflated (since a lower perplexity means a better model). To identify which elements are padding and which are not, you can use <code>np.equal()</code> and get a tensor with <code>True</code> in the positions of actual values and <code>False</code> where there are paddings.</p>
<div class="highlight">
<pre><span></span><span class="n">equals_zero</span> <span class="o">=</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">equals_zero</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[False False False ...  True  True  True]
 [False False False ...  True  True  True]
 [False False False ...  True  True  True]
 ...
 [False False False ...  True  True  True]
 [False False False ...  True  True  True]
 [False False False ...  True  True  True]]
</pre>
<p><code>equals_zero</code> is a boolean array that has <code>True</code> wherever the cell had a 0 and <code>False</code> everywhere else. To make it numeric we can subtract the boolean array from 1 (generally in python True is treated as 1 and False as 0).</p>
<div class="highlight">
<pre><span></span><span class="n">non_pad</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">equals_zero</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'non_pad has shape: </span><span class="si">{</span><span class="n">non_pad</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'non_pad looks like this: </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">non_pad</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org3503f0f">
non_pad has shape: (32, 64)

non_pad looks like this: 

 [[1. 1. 1. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]
 ...
 [1. 1. 1. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]
 [1. 1. 1. ... 0. 0. 0.]]
</pre>
<p>Now if we multiply <code>total_log_perplexity</code> by the <code>non_pad</code> we'll zero-out all the entries in <code>total_log_perplexity</code> where <code>non_pad</code> has zero.</p>
<div class="highlight">
<pre><span></span><span class="n">real_log_perplexity</span> <span class="o">=</span> <span class="n">total_log_perplexity</span> <span class="o">*</span> <span class="n">non_pad</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'real perplexity still has shape: </span><span class="si">{</span><span class="n">real_log_perplexity</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
real perplexity still has shape: (32, 64)
</pre>
<p>We can check the effect of filtering out the padding by looking at the two log perplexity tensors.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'log perplexity tensor before filtering padding: </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">total_log_perplexity</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'log perplexity tensor after filtering padding: </span><span class="se">\n\n</span><span class="s1"> </span><span class="si">{</span><span class="n">real_log_perplexity</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org10f067f">
log perplexity tensor before filtering padding: 

 [[ -5.396545    -1.0311184   -0.66916656 ... -22.37673    -23.18771
  -21.843483  ]
 [ -4.5857706   -1.1341286   -8.538033   ... -20.15686    -26.837097
  -23.57502   ]
 [ -5.2223887   -1.2824144   -0.17312431 ... -21.328228   -19.854412
  -33.88444   ]
 ...
 [ -5.396545   -17.291681    -4.360766   ... -20.825802   -21.065838
  -22.443115  ]
 [ -5.9313164  -14.247417    -0.2637329  ... -26.743248   -18.38433
  -22.355278  ]
 [ -5.670536    -0.10595131   0.         ... -23.332523   -28.087376
  -23.878807  ]]

log perplexity tensor after filtering padding: 

 [[ -5.396545    -1.0311184   -0.66916656 ...  -0.          -0.
   -0.        ]
 [ -4.5857706   -1.1341286   -8.538033   ...  -0.          -0.
   -0.        ]
 [ -5.2223887   -1.2824144   -0.17312431 ...  -0.          -0.
   -0.        ]
 ...
 [ -5.396545   -17.291681    -4.360766   ...  -0.          -0.
   -0.        ]
 [ -5.9313164  -14.247417    -0.2637329  ...  -0.          -0.
   -0.        ]
 [ -5.670536    -0.10595131   0.         ...  -0.          -0.
   -0.        ]]
</pre>
<p>To get a single average log perplexity across all the elements in the batch you can sum across both dimensions and divide by the number of elements. Note that the result will be the negative of the real log perplexity of the model.</p>
<div class="highlight">
<pre><span></span><span class="n">log_perplexity</span> <span class="o">=</span> <span class="o">-</span><span class="n">trax_numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">real_log_perplexity</span><span class="p">)</span> <span class="o">/</span> <span class="n">trax_numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">non_pad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"log perplexity: </span><span class="si">{</span><span class="n">log_perplexity</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">, "</span>
      <span class="sa">f</span><span class="s2">"perplexity: </span><span class="si">{</span><span class="n">trax_numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_perplexity</span><span class="p">)</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
log perplexity: 2.3281, perplexity: 10.2586
</pre></div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/rnn/" rel="tag">rnn</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../hidden-state-activation/" rel="prev" title="Hidden State Activation">Previous post</a></li>
<li class="next"><a href="../vanilla-rnns-and-grus/" rel="next" title="Vanilla RNNs and GRUs">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
