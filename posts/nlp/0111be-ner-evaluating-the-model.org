#+BEGIN_COMMENT
.. title: NER: Evaluating the Model
.. slug: ner-evaluating-the-model
.. date: 2021-01-13 15:02:42 UTC-08:00
.. tags: lstm,rnn,nlp,ner
.. category: NLP
.. link: 
.. description: Evaluating the NER model.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-31230fc3-119f-4c27-9dbf-87ade3b6be9c-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  - {{% lancelot title="The First Post" %}}named-entity-recognition{{% /lancelot %}}
  - {{% lancelot title="The Previous Post" %}}ner-training-the-model{{% /lancelot %}}       
  - {{% lancelot title="The Next Post" %}}ner-testing-the-model{{% /lancelot %}}


Now we'll evaluate our model using the test set. To do this we'll need to create a mask to avoid counting the padding tokens when computing the accuracy. 


 - *Step 1*: Calling =model(sentences)= will give us the predicted output. 
 - *Step 2*: The output will be the prediction with an added dimension. For each word in each sentence there will be a vector of probabilities for each tag type. For each word in each sentence we'll need to pick the maximum valued tag. This will require [[https://numpy.org/doc/stable/reference/generated/numpy.argmax.html][=np.argmax=]] and careful use of the =axis= argument.
 - *Step 3*: Create a mask to prevent counting pad characters. It will have the same dimensions as the output.
 - *Step 4*: Compute the accuracy metric by comparing the outputs against the test labels. Take the sum of that and divide by the total number of **unpadded** tokens. Use the mask value to mask the padded tokens.

** Imports
#+begin_src python :results none
# python
from collections import namedtuple
from pathlib import Path

# pypi
import jax
import numpy
import trax

# this project
from neurotic.nlp.named_entity_recognition import (DataGenerator,
                                                   NER,
                                                   NERData,
                                                   TOKEN)

#+end_src
** Set Up
#+begin_src python :results output :exports both
data = NERData().data
model = NER(vocabulary_size=len(data.vocabulary),
            tag_count=len(data.tags)).model

Settings = namedtuple("Settings", ["batch_size", "padding_id", "seed"])
SETTINGS = Settings(batch_size=64,
                    padding_id=data.vocabulary[TOKEN.pad],
                    seed=33)

model.init_from_file(Path("~/models/ner/model.pkl.gz").expanduser())
print(model)

random.seed(SETTINGS.seed)

test_generator = DataGenerator(x=ner.data.data_sets.x_test,
                                   y=data.data_sets.y_test,
                                   batch_size=SETTINGS.batch_size,
                                   padding=SETTINGS.padding_id)
#+end_src

#+RESULTS:
: Serial[
:   Embedding_35180_50
:   LSTM_50
:   Dense_18
:   LogSoftmax
: ]

* Middle
  As a reminder, here's what happens when you apply a boolean comparison to a numpy array.
#+begin_src python :results output :exports both
a = numpy.array([1, 2, 3, 4])
print(a == 2)
#+end_src  

#+RESULTS:
: [False  True False False]

** A Test Input
#+begin_src python :results output :exports both
x, y = next(test_generator)
print(f"x's shape: {x.shape} y's shape: {y.shape}")

predictions = model(x)
print(type(predictions))
print(f"predictions has shape: {predictions.shape}")
#+end_src

#+RESULTS:
: x's shape: (64, 44) y's shape: (64, 44)
: <class 'jax.interpreters.xla._DeviceArray'>
: predictions has shape: (64, 44, 18)

**Note:** the model's prediction has 3 axes:
 - the number of examples
 - the number of words in each example (padded to be as long as the longest sentence in the batch)
 - the number of possible targets (the 17 named entity tags).

#+begin_src python :results none
# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: evaluate_prediction
def evaluate_prediction(pred: jax.interpreters.xla._DeviceArray,
                        labels: numpy.ndarray,
                        pad: int=SETTINGS.padding_id) -> float:
    """Calculates the accuracy of a prediction
    
    Args:
      pred: prediction array with shape 
            (num examples, max sentence length in batch, num of classes)
      labels: array of size (batch_size, seq_len)
      pad: integer representing pad character

    Returns:
      accuracy: fraction of correct predictions
    """
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    ## step 1 ##
    outputs = numpy.argmax(pred, axis=-1)
    print("outputs shape:", outputs.shape)

## step 2 ##
    mask = y != pad
    print("mask shape:", mask.shape, "mask[0][20:30]:", mask[0][20:30])
## step 3 ##
    accuracy = numpy.sum((outputs==y)[mask])/numpy.sum(mask)
    ### END CODE HERE ###
    return accuracy
#+end_src

#+begin_src python :results output :exports both
accuracy = evaluate_prediction(model(x), y, SETTINGS.padding_id)
print("accuracy: ", accuracy)

#+end_src

#+RESULTS:
: outputs shape: (64, 44)
: mask shape: (64, 44) mask[0][20:30]: [ True False False False False False False False False False]
: accuracy:  0.9636752

Hmm, does pretty good.
