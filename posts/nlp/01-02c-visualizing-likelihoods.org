#+BEGIN_COMMENT
.. title: Visualizing Naive Bayes
.. slug: visualizing-naive-bayes
.. date: 2020-08-31 06:16:19 UTC-07:00
.. tags: naive bayes,nlp,visualization
.. category: NLP
.. link: 
.. description: Visualizing likelihoods and confidence ellipses.
.. type: text
.. has_math: true

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-afac48e3-293d-4bfb-b65f-14de7949c859-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
** Set Up
*** Imports
#+begin_src python :results none
# python
from argparse import Namespace
from functools import partial
from pathlib import Path

import os
import pickle

# from pypi
from dotenv import load_dotenv

import holoviews
import hvplot.pandas
import numpy
import pandas

# this project
from neurotic.nlp.twitter.counter import WordCounter

# graeae
from graeae import EmbedHoloviews
#+end_src
*** Plotting
#+begin_src python :results none
load_dotenv("posts/nlp/.env")
SLUG = "visualizing-naive-bayes"
Embed = partial(EmbedHoloviews,
                folder_path=f"files/posts/nlp/{SLUG}", create_folder=False)

plot_path = Path(os.environ["TWITTER_PLOT"])
assert plot_path.is_file()
with plot_path.open("rb") as reader:
    Plot = pickle.load(reader)
#+end_src
*** The Dotenv
#+begin_src python :results none
env_path = Path("posts/nlp/.env")
assert env_path.is_file()
load_dotenv(env_path)
#+end_src
*** The Data
#+begin_src python :results output :exports both
train_raw = pandas.read_feather(
    Path(os.environ["TWITTER_TRAINING_RAW"]).expanduser())

test_raw = pandas.read_feather(
    Path(os.environ["TWITTER_TEST_RAW"]).expanduser()
)

print(f"Training: {len(train_raw):,}")
print(f"Testing: {len(test_raw):,}")
#+end_src

#+RESULTS:
: Training: 8,000
: Testing: 2,000

*** The Word Counter
    This is a class to clean and tokenize the tweets and build up a Counter with the word counts.

#+begin_src python :results none
counter = WordCounter(train_raw.tweet, train_raw.label)
#+end_src

*** Constants
#+begin_src python :results none
Sentiment = Namespace(
    positive = 1,
    negative = 0,
)
#+end_src
* Middle
** Log Likelihoods
*** Calculating the Likelihoods
   The first thing to plot are the log-likelihoods for positive and negative tweets. When I implemented the Naive Bayes Classifier I took advantage of the fact that we're making a binary classifier and took the odds ratio when making predictions, but for our plot we're going to need to undo the division and plot the numerator against the denominator.


\begin{align}
log \frac{P(tweet|pos)}{P(tweet|neg)} &= log(P(tweet|pos)) - log(P(tweet|neg)) \\
positive = log(P(tweet|pos)) &= \sum_{i=0}^{n}{log P(W_i|pos)}\\
negative = log(P(tweet|neg)) &= \sum_{i=0}^{n}{log P(W_i|neg)}\\
\end{align}

So, let's get the log-likelihoods.

#+begin_src python :results none
COUNTS = counter.counts

positive_loglikelihood = {}
negative_loglikelihood = {}
log_ratio = {}

all_positive_words = sum(
    (counts[(token, sentiment)] for token, sentiment in COUNTS
     if sentiment == Sentiment.positive))
all_negative_words = sum(
    (counts[(token, sentiment)] for token, sentiment in COUNTS
     if sentiment == Sentiment.negative))

vocabulary = {key[0] for key in COUNTS}
vocabulary_size = len(vocabulary)

for word in vocabulary:
    this_word_positive_count = COUNTS[(word, Sentiment.positive)]
    this_word_negative_count = COUNTS[(word, Sentiment.negative)]

    probability_word_is_positive = ((this_word_positive_count + 1)/
                                    (all_positive_words + vocabulary_size))
    probability_word_is_negative = ((this_word_negative_count + 1)/
                                    (all_negative_words + vocabulary_size))
    positive_loglikelihood[word] = numpy.log(probability_word_is_positive)
    negative_loglikelihood[word] = numpy.log(probability_word_is_negative)
    log_ratio[word] = positive_loglikelihood[word] - negative_loglikelihood[word]
#+end_src

So now we have our positive and negative log-likelihoods and I'll put them into a pandas DataFrame to make it easier to plot.

#+begin_src python :results output :exports both
positive_document_likelihood = []
negative_document_likelihood = []
sentiment = []

for row in train_raw.itertuples():
    tokens = counter.process(row.tweet)
    
    positive_document_likelihood.append(sum(positive_loglikelihood.get(token, 0)
                                            for token in tokens))
    negative_document_likelihood.append(sum(negative_loglikelihood.get(token, 0)
                                            for token in tokens))
    sentiment.append(row.label)

features = pandas.DataFrame.from_dict(
    dict(
        positive = positive_document_likelihood,
        negative = negative_document_likelihood,
        sentiment=sentiment,
    )
)

print(features.head())
#+end_src

#+RESULTS:
:      positive   negative  sentiment
: 0  -26.305672 -33.940649          1
: 1  -30.909803 -37.634516          1
: 2  -42.936400 -33.403567          0
: 3  -15.983546 -25.501140          1
: 4 -107.899933 -99.191875          0

*** Plotting the Likelihoods
#+begin_src python :results none
plot = features.hvplot.scatter(x="positive", y="negative", by="sentiment",
                               color=Plot.color_cycle, fill_alpha=0).opts(
                                   title="Positive vs Negative",
                                   width=Plot.width,
                                   height=Plot.height,
                                   fontscale=Plot.font_scale,
                               )

outcome = Embed(plot=plot, file_name="positive_vs_negative_sentiment")()
#+end_src

#+begin_src python :results output html :exports both
print(outcome)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="positive_vs_negative_sentiment.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

It looks like the log likelihoods for the negatives are linearly separable.
* End
* Raw
#+begin_src python
# # Using Confidence Ellipses to interpret Naïve Bayes
# 
# In this section, we will use the [confidence ellipse]( https://matplotlib.org/3.1.1/gallery/statistics/confidence_ellipse.html#sphx-glr-gallery-statistics-confidence-ellipse-py) to give us an idea of what the Naïve Bayes model see.
# 
# A confidence ellipse is a way to visualize a 2D random variable. It is a better way than plotting the points over a cartesian plane because, with big datasets, the points can overlap badly and hide the real distribution of the data. Confidence ellipses summarize the information of the dataset with only four parameters: 
# 
# * Center: It is the numerical mean of the attributes
# * Height and width: Related with the variance of each attribute. The user must specify the desired amount of standard deviations used to plot the ellipse. 
# * Angle: Related with the covariance among attributes.
# 
# The parameter __n_std__ stands for the number of standard deviations bounded by the ellipse. Remember that for normal random distributions:
# 
# * About 68% of the area under the curve falls within 1 standard deviation around the mean.
# * About 95% of the area under the curve falls within 2 standard deviations around the mean.
# * About 99.7% of the area under the curve falls within 3 standard deviations around the mean.
# 
# <img src=std.jpg width="400" >
# 
# 
# In the next chart, we will plot the data and its corresponding confidence ellipses using 2 std and 3 std. 

# In[ ]:


# Plot the samples using columns 1 and 2 of the matrix
fig, ax = plt.subplots(figsize = (8, 8))

colors = ['red', 'green'] # Define a color palete

# Color base on sentiment

ax.scatter(data.positive, data.negative, c=[colors[int(k)] for k in data.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet

# Custom limits for this chart
plt.xlim(-200,40)  
plt.ylim(-200,40)

plt.xlabel("Positive") # x-axis label
plt.ylabel("Negative") # y-axis label

data_pos = data[data.sentiment == 1] # Filter only the positive samples
data_neg = data[data.sentiment == 0] # Filter only the negative samples

# Print confidence ellipses of 2 std
confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\sigma$' )
confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')

# Print confidence ellipses of 3 std
confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\sigma$')
confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')
ax.legend()

plt.show()


# In the next cell, we will modify the features of the samples with positive sentiment (1), in a way that the two distributions overlap. In this case, the Naïve Bayes method will produce a lower accuracy than with the original data.

# In[ ]:


data2 = data.copy() # Copy the whole data frame

# The following 2 lines only modify the entries in the data frame where sentiment == 1
data2.negative[data.sentiment == 1] =  data2.negative * 1.5 + 50 # Modify the negative attribute
data2.positive[data.sentiment == 1] =  data2.positive / 1.5 - 50 # Modify the positive attribute 


# Now let us plot the two distributions and the confidence ellipses

# In[ ]:


# Plot the samples using columns 1 and 2 of the matrix
fig, ax = plt.subplots(figsize = (8, 8))

colors = ['red', 'green'] # Define a color palete

# Color base on sentiment

#data.negative[data.sentiment == 1] =  data.negative * 2

ax.scatter(data2.positive, data2.negative, c=[colors[int(k)] for k in data2.sentiment], s = 0.1, marker='*')  # Plot a dot for tweet
# Custom limits for this chart
plt.xlim(-200,40)  
plt.ylim(-200,40)

plt.xlabel("Positive") # x-axis label
plt.ylabel("Negative") # y-axis label

data_pos = data2[data2.sentiment == 1] # Filter only the positive samples
data_neg = data[data2.sentiment == 0] # Filter only the negative samples

# Print confidence ellipses of 2 std
confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=2, edgecolor='black', label=r'$2\sigma$' )
confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=2, edgecolor='orange')

# Print confidence ellipses of 3 std
confidence_ellipse(data_pos.positive, data_pos.negative, ax, n_std=3, edgecolor='black', linestyle=':', label=r'$3\sigma$')
confidence_ellipse(data_neg.positive, data_neg.negative, ax, n_std=3, edgecolor='orange', linestyle=':')
ax.legend()

plt.show()


# To give away: Understanding the data allows us to predict if the method will perform well or not. Alternatively, it will allow us to understand why it worked well or bad.

#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: NameError                                 Traceback (most recent call last)
: <ipython-input-8-b30abfe0c3d2> in <module>
:      18 # Plot the samples using columns 1 and 2 of the matrix
:      19 
: ---> 20 fig, ax = plt.subplots(figsize = (8, 8)) #Create a new figure with a custom size
:      21 
:      22 colors = ['red', 'green'] # Define a color palete
: 
: NameError: name 'plt' is not defined
:END:
