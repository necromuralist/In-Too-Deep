#+BEGIN_COMMENT
.. title: Neural Machine Translation: Training the Model
.. slug: neural-machine-translation-training-the-model
.. date: 2021-02-14 14:54:34 UTC-08:00
.. tags: nlp,machine translation,encoder-decoder,attention
.. category: NLP
.. link: 
.. description: Training the Attention Model for Machine Translation.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-565aadae-d86e-4d2d-a679-4bafeb6333fe-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Training Our Model
  In the {{% lancelot title="previous post" %}}neural-machine-translation-the-attention-model{{% /lancelot %}} we defined our model for machine translation. In this post we'll train the model on our data.

  Doing supervised training in Trax is pretty straightforward (short example [[https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training][here]]). We will be instantiating three classes for this: =TrainTask=, =EvalTask=, and =Loop=. Let's take a closer look at each of these in the sections below.
** Imports
#+begin_src python :results none
# pypi
from trax.supervised import training

# this project
from neurotic.nlp.machine_translation import DataGenerator
#+end_src

** Set Up
#+begin_src python :results none
train_batch_stream = DataGenerator().batch_generator
eval_batch_stream = DataGenerator(training=False).batch_generator
#+end_src
* Training
** TrainTask

 The [[https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask][TrainTask]] class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights. 

#+begin_src python :results none 
train_task = training.TrainTask(
    
    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###
    
    # use the train batch stream as labeled data
    labeled_data= None,
    
    # use the cross entropy loss
    loss_layer= None,
    
    # use the Adam optimizer with learning rate of 0.01
    optimizer= None,
    
    # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule
    # have 1000 warmup steps with a max value of 0.01
    lr_schedule= None,
    
    # have a checkpoint every 10 steps
    n_steps_per_checkpoint= None,
    
    ### END CODE HERE ###
)
#+end_src

#+begin_src python :results none
def test_train_task(train_task):
    target = train_task
    success = 0
    fails = 0
     
    # Test the labeled data parameter
    try:
        strlabel = str(target._labeled_data)
        assert(strlabel.find("generator") and strlabel.find('add_loss_weights'))
        success += 1
    except:
        fails += 1
        print("Wrong labeled data parameter")
    
    # Test the cross entropy loss data parameter
    try:
        strlabel = str(target._loss_layer)
        assert(strlabel == "CrossEntropyLoss_in3")
        success += 1
    except:
        fails += 1
        print("Wrong loss functions. CrossEntropyLoss_in3 was expected")
        
     # Test the optimizer parameter
    try:
        assert(isinstance(target.optimizer, trax.optimizers.adam.Adam))
        success += 1
    except:
        fails += 1
        print("Wrong optimizer")
        
    # Test the schedule parameter
    try:
        assert(isinstance(target._lr_schedule,trax.supervised.lr_schedules._BodyAndTail))
        success += 1
    except:
        fails += 1
        print("Wrong learning rate schedule type")
    
    # Test the _n_steps_per_checkpoint parameter
    try:
        assert(target._n_steps_per_checkpoint==10)
        success += 1
    except:
        fails += 1
        print("Wrong checkpoint step frequency")
        
    if fails == 0:
        print("\033[92m All tests passed")
    else:
        print('\033[92m', success," Tests passed")
        print('\033[91m', fails, " Tests failed")
    return
#+end_src

#+begin_src python :results output :exports both
test_train_task(train_task)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: NameError                                 Traceback (most recent call last)
: <ipython-input-3-6735b260c743> in <module>
: ----> 1 test_train_task(train_task)
: 
: NameError: name 'train_task' is not defined
:END:
  
* End
  Now that we've trained the model in the {{% lancelot title="next post" %}}neural-machine-translation-testing-the-model{{% /lancelot %}} we'll test our model to see how well it does. The overview post with links to all the posts in this series is {{% lancelot title="here" %}}neural-machine-translation{{% /lancelot %}}.
* Raw
#+begin_example python

# <a name="3.2"></a>
# ## 3.2  EvalTask
# 
# The [EvalTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) on the other hand allows us to see how the model is doing while training. For our application, we want it to report the cross entropy loss and accuracy.

# In[ ]:


eval_task = training.EvalTask(
    
    ## use the eval batch stream as labeled data
    labeled_data=eval_batch_stream,
    
    ## use the cross entropy loss and accuracy as metrics
    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],
)


# <a name="3.3"></a>
# ## 3.3  Loop
# 
# The [Loop](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) class defines the model we will train as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps.

# In[ ]:


# define the output directory
output_dir = 'output_dir/'

# remove old model if it exists. restarts training.
get_ipython().system('rm -f ~/output_dir/model.pkl.gz  ')

# define the training loop
training_loop = training.Loop(NMTAttn(mode='train'),
                              train_task,
                              eval_tasks=[eval_task],
                              output_dir=output_dir)


# In[ ]:


# NOTE: Execute the training loop. This will take around 8 minutes to complete.
training_loop.run(10)


# <a name="4"></a>
#+end_example
