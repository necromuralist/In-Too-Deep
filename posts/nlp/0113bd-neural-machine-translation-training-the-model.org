#+BEGIN_COMMENT
.. title: Neural Machine Translation: Training the Model
.. slug: neural-machine-translation-training-the-model
.. date: 2021-02-14 14:54:34 UTC-08:00
.. tags: nlp,machine translation
.. category: NLP
.. link: 
.. description: Training the Attention Model for Machine Translation.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-95fb7f72-2980-4eed-b335-9f9a6c7ffbd5-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Training Our Model
  In the {{% lancelot title="previous post" %}}neural-machine-translation-the-attention-model{{% /lancelot %}} we defined our model for machine translation. In this post we'll train the model on our data.
* End
  Now that we've trained the model in the {{% lancelot title="next post" %}}neural-machine-translation-testing-the-model{{% /lancelot %}} we'll test our model to see how well it does. The overview post with links to all the posts in this series is {{% lancelot title="here" %}}neural-machine-translation{{% /lancelot %}}.
* Raw
#+begin_example python
# # Part 3:  Training
# 
# We will now be training our model in this section. Doing supervised training in Trax is pretty straightforward (short example [here](https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training)). We will be instantiating three classes for this: `TrainTask`, `EvalTask`, and `Loop`. Let's take a closer look at each of these in the sections below.
# 

# <a name="3.1"></a>
# ## 3.1  TrainTask
# 
# The [TrainTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights. 
# 
# <a name="ex05"></a>
# ### Exercise 05
# 
# **Instructions:** Instantiate a train task.

# In[ ]:


# UNQ_C5
# GRADED 
train_task = training.TrainTask(
    
    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###
    
    # use the train batch stream as labeled data
    labeled_data= None,
    
    # use the cross entropy loss
    loss_layer= None,
    
    # use the Adam optimizer with learning rate of 0.01
    optimizer= None,
    
    # use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule
    # have 1000 warmup steps with a max value of 0.01
    lr_schedule= None,
    
    # have a checkpoint every 10 steps
    n_steps_per_checkpoint= None,
    
    ### END CODE HERE ###
)


# In[ ]:


# BEGIN UNIT TEST
w1_unittest.test_train_task(train_task)
# END UNIT TEST


# <a name="3.2"></a>
# ## 3.2  EvalTask
# 
# The [EvalTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) on the other hand allows us to see how the model is doing while training. For our application, we want it to report the cross entropy loss and accuracy.

# In[ ]:


eval_task = training.EvalTask(
    
    ## use the eval batch stream as labeled data
    labeled_data=eval_batch_stream,
    
    ## use the cross entropy loss and accuracy as metrics
    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],
)


# <a name="3.3"></a>
# ## 3.3  Loop
# 
# The [Loop](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) class defines the model we will train as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps.

# In[ ]:


# define the output directory
output_dir = 'output_dir/'

# remove old model if it exists. restarts training.
get_ipython().system('rm -f ~/output_dir/model.pkl.gz  ')

# define the training loop
training_loop = training.Loop(NMTAttn(mode='train'),
                              train_task,
                              eval_tasks=[eval_task],
                              output_dir=output_dir)


# In[ ]:


# NOTE: Execute the training loop. This will take around 8 minutes to complete.
training_loop.run(10)


# <a name="4"></a>
#+end_example
