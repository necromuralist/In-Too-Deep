<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Data Preparation for word embeddings." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Word Embeddings: Data Preparation | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/word-embeddings-data-preparation/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../auto-complete-building-the-auto-complete-system/" rel="prev" title="Auto-Complete: Building the Auto-Complete System" type="text/html">
<link href="../introducing-the-cbow-model/" rel="next" title="Introducing the CBOW Model" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Word Embeddings: Data Preparation" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/word-embeddings-data-preparation/" property="og:url">
<meta content="Data Preparation for word embeddings." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-08T18:29:17-08:00" property="article:published_time">
<meta content="data preparation" property="article:tag">
<meta content="nlp" property="article:tag">
<meta content="word embeddings" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/Neurotic-Networking/"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Word Embeddings: Data Preparation</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-08T18:29:17-08:00" itemprop="datePublished" title="2020-12-08 18:29">2020-12-08 18:29</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgff73531">Beginning</a></li>
<li><a href="#org98eef07">Middle</a>
<ul>
<li><a href="#orgaa18d21">Cleaning and tokenization</a></li>
<li><a href="#org9bd69af">Tokenize</a></li>
<li><a href="#org5d6e3a4">More Processing</a></li>
<li><a href="#org04ef953">Make It a Function</a></li>
<li><a href="#orgdc7ea87">Sliding Window of Words</a></li>
<li><a href="#org21a4307">Transforming words into vectors for the training set</a>
<ul>
<li><a href="#org3f97153">Mapping words to indices and indices to words</a></li>
<li><a href="#orgc4289d6">Getting one-hot word vectors</a></li>
<li><a href="#orgd65f99f">Getting context word vectors</a></li>
</ul>
</li>
<li><a href="#orgf4b0edd">Building the training set</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgff73531">
<h2 id="orgff73531">Beginning</h2>
<div class="outline-text-2" id="text-orgff73531">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">emoji</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org98eef07">
<h2 id="org98eef07">Middle</h2>
<div class="outline-text-2" id="text-org98eef07"></div>
<div class="outline-3" id="outline-container-orgaa18d21">
<h3 id="orgaa18d21">Cleaning and tokenization</h3>
<div class="outline-text-3" id="text-orgaa18d21">
<p>To demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs. Let's define a corpus:</p>
<div class="highlight">
<pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="s1">'Who ❤️ "word embeddings" in 2020? I do!!!'</span>
</pre></div>
<p>First, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Corpus:  </span><span class="si">{</span><span class="n">corpus</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Do the substitution</span>
<span class="n">ONE_OR_MORE</span> <span class="o">=</span> <span class="s2">"+"</span>
<span class="n">PERIOD</span> <span class="o">=</span> <span class="s2">"."</span>
<span class="n">PUNCTUATION</span> <span class="o">=</span> <span class="s2">",!?;-"</span>
<span class="n">CHARACTERS</span> <span class="o">=</span> <span class="s2">"[</span><span class="si">{}</span><span class="s2">]"</span>
<span class="n">EXPRESSION</span> <span class="o">=</span> <span class="n">CHARACTERS</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">PUNCTUATION</span><span class="p">)</span> <span class="o">+</span> <span class="n">ONE_OR_MORE</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">EXPRESSION</span><span class="p">,</span> <span class="n">PERIOD</span><span class="p">,</span> <span class="n">corpus</span><span class="p">)</span>
<span class="c1"># Print cleaned corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"After cleaning punctuation:  '</span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Corpus:  Who ❤️ "word embeddings" in 2020? I do!!!
After cleaning punctuation:  'Who ❤️ "word embeddings" in 2020. I do.'
</pre></div>
</div>
<div class="outline-3" id="outline-container-org9bd69af">
<h3 id="org9bd69af">Tokenize</h3>
<div class="outline-text-3" id="text-org9bd69af">
<p>Next, use NLTK's tokenization engine to split the corpus into individual tokens.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Print cleaned corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Initial string:  </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Tokenize the cleaned corpus</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Print the tokenized version of the corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'After tokenization:  </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
Initial string:  Who ❤️ "word embeddings" in 2020. I do.
After tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', "''", 'in', '2020', '.', 'I', 'do', '.']
</pre></div>
</div>
<div class="outline-3" id="outline-container-org5d6e3a4">
<h3 id="org5d6e3a4">More Processing</h3>
<div class="outline-text-3" id="text-org5d6e3a4">
<p>Finally, as you saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Print the tokenized version of the corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Initial list of tokens:  </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Filter tokenized corpus using list comprehension</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span> <span class="n">character</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">character</span> <span class="ow">in</span> <span class="n">data</span>
         <span class="k">if</span> <span class="nb">any</span><span class="p">((</span><span class="n">character</span><span class="o">.</span><span class="n">isalpha</span><span class="p">(),</span>
                 <span class="n">character</span><span class="o">==</span> <span class="s1">'.'</span><span class="p">,</span>
                 <span class="n">emoji</span><span class="o">.</span><span class="n">get_emoji_regexp</span><span class="p">()</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">character</span><span class="p">)))</span>
       <span class="p">]</span>

<span class="c1"># Print the tokenized and filtered version of the corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"After cleaning:  '</span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Initial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', "''", 'in', '2020', '.', 'I', 'do', '.']
After cleaning:  '['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']'
</pre>
<p>Note that the heart emoji is considered a token just like any normal word.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org04ef953">
<h3 id="org04ef953">Make It a Function</h3>
<div class="outline-text-3" id="text-org04ef953">
<p>Now let's streamline the cleaning and tokenization process by wrapping the previous steps in a function.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">corpus</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""clean and tokenize the corpus</span>

<span class="sd">    Args:</span>
<span class="sd">     corpus: original source text</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of cleaned tokens from the corpus</span>
<span class="sd">    """</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">EXPRESSION</span><span class="p">,</span> <span class="n">PERIOD</span><span class="p">,</span> <span class="n">corpus</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span> <span class="n">character</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">character</span> <span class="ow">in</span> <span class="n">data</span>
             <span class="k">if</span> <span class="nb">any</span><span class="p">((</span><span class="n">character</span><span class="o">.</span><span class="n">isalpha</span><span class="p">(),</span>
                     <span class="n">character</span><span class="o">==</span> <span class="s1">'.'</span><span class="p">,</span>
                     <span class="n">emoji</span><span class="o">.</span><span class="n">get_emoji_regexp</span><span class="p">()</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">character</span><span class="p">)))</span>
       <span class="p">]</span>    
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
<p>Apply this function to the corpus that you'll be working on in the rest of this notebook: "I am happy because I am learning"</p>
<div class="highlight">
<pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="s1">'I am happy because I am learning'</span>

<span class="c1"># Print new corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Corpus:  </span><span class="si">{</span><span class="n">corpus</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Save tokenized version of corpus into 'words' variable</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># Print the tokenized version of the corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Words (tokens):  </span><span class="si">{</span><span class="n">words</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
Corpus:  I am happy because I am learning
Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']
</pre>
<p>An alternative sentence.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">"I'm tired of being a token! Where's all the other cheese-sniffing Gnomish at? I bet theres' at least 2 of us out there, or maybe more..."</span><span class="p">))</span>
</pre></div>
<pre class="example">
['i', 'tired', 'of', 'being', 'a', 'token', '.', 'where', 'all', 'the', 'other', 'gnomish', 'at', '.', 'i', 'bet', 'theres', 'at', 'least', 'of', 'us', 'out', 'there', '.', 'or', 'maybe', 'more']
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgdc7ea87">
<h3 id="orgdc7ea87">Sliding Window of Words</h3>
<div class="outline-text-3" id="text-orgdc7ea87">
<p>Now that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.</p>
<p>The <code>get_windows</code> function in the next cell was introduced in the lecture.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_windows</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">"""Generates windows of words</span>

<span class="sd">    Args:</span>
<span class="sd">     words: cleaned tokens</span>
<span class="sd">     half_window: number of words in the half-window</span>

<span class="sd">    Yields:</span>
<span class="sd">     the next window</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">center_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">half_window</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">center_index</span><span class="p">]</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">)</span> <span class="p">:</span> <span class="n">center_index</span><span class="p">]</span>
                         <span class="o">+</span> <span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):(</span><span class="n">center_index</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>
        <span class="k">yield</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span>
    <span class="k">return</span>
</pre></div>
<p>The first argument of this function is a list of words (or tokens). The second argument, <code>C</code>, is the context half-size. Recall that for a given center word, the context words are made of <code>C</code> words to the left and <code>C</code> words to the right of the center word.</p>
<p>Here is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">get_windows</span><span class="p">([</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">],</span> <span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
['i', 'am', 'because', 'i']     happy
['am', 'happy', 'i', 'am']      because
['happy', 'because', 'am', 'learning']  i
</pre>
<p>The first example of the training set is made of:</p>
<ul class="org-ul">
<li>the context words "i", "am", "because", "i",</li>
<li>and the center word to be predicted: "happy".</li>
</ul>
<p><b>Now try it out yourself. In the next cell, you can change both the sentence and the context half-size.</b></p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">get_windows</span><span class="p">(</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">"My baloney has a first name, it's Gerald."</span><span class="p">),</span> <span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
['my', 'baloney', 'a', 'first'] has
['baloney', 'has', 'first', 'name']     a
['has', 'a', 'name', '.']       first
['a', 'first', '.', 'it']       name
['first', 'name', 'it', 'gerald']       .
['name', '.', 'gerald', '.']    it
</pre></div>
</div>
<div class="outline-3" id="outline-container-org21a4307">
<h3 id="org21a4307">Transforming words into vectors for the training set</h3>
<div class="outline-text-3" id="text-org21a4307">
<p>To finish preparing the training set, you need to transform the context words and center words into vectors.</p>
</div>
<div class="outline-4" id="outline-container-org3f97153">
<h4 id="org3f97153">Mapping words to indices and indices to words</h4>
<div class="outline-text-4" id="text-org3f97153">
<p>The center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.</p>
<p>To create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, <code>get_dict</code>, that creates a Python dictionary that maps words to integers and back.</p>
<p>Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_dict</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Creates index to word mappings</span>

<span class="sd">    The index is based on sorted unique tokens in the data</span>

<span class="sd">    Args:</span>
<span class="sd">       data: the data you want to pull from</span>

<span class="sd">    Returns:</span>
<span class="sd">       word2Ind: returns dictionary mapping the word to its index</span>
<span class="sd">       Ind2Word: returns dictionary mapping the index to its word</span>
<span class="sd">    """</span>
    <span class="n">words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

    <span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="n">index_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_word</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">word2Ind</span><span class="p">,</span> <span class="n">Ind2word</span> <span class="o">=</span> <span class="n">get_dict</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">word2Ind</span><span class="p">)</span>
</pre></div>
<pre class="example">
{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}
</pre>
<p>You can use this dictionary to get the index of a word.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Index of the word 'i':  </span><span class="si">{</span><span class="n">word2Ind</span><span class="p">[</span><span class="s1">'i'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Index of the word 'i':  3
</pre>
<p>And conversely, here's the dictionary that maps indices to words.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">Ind2word</span><span class="p">)</span>
</pre></div>
<pre class="example">
{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word which has index 2:  '</span><span class="si">{</span><span class="n">Ind2word</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Word which has index 2:  'happy'
</pre>
<p>Finally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.</p>
<div class="highlight">
<pre><span></span><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2Ind</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of vocabulary: </span><span class="si">{</span><span class="n">vocabulary_size</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Size of vocabulary: 5
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgc4289d6">
<h4 id="orgc4289d6">Getting one-hot word vectors</h4>
<div class="outline-text-4" id="text-orgc4289d6">
<p>Recall from the lecture that you can easily convert an integer, <i>n</i>, into a one-hot vector.</p>
<p>Consider the word "happy". First, retrieve its numeric index.</p>
<div class="highlight">
<pre><span></span><span class="n">happy_index</span> <span class="o">=</span> <span class="n">word2Ind</span><span class="p">[</span><span class="s1">'happy'</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">happy_index</span><span class="p">)</span>
</pre></div>
<pre class="example">
2
</pre>
<p>Now create a vector with the size of the vocabulary, and fill it with zeros. Create vector with the same length as the vocabulary, filled with zeros</p>
<div class="highlight">
<pre><span></span><span class="n">center_word_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">center_word_vector</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">center_word_vector</span><span class="p">)</span> <span class="o">==</span> <span class="n">vocabulary_size</span>
</pre></div>
<pre class="example">
[0. 0. 0. 0. 0.]
</pre>
<p>Next, replace the 0 of the $n$-th element with a 1.</p>
<p>Replace element number 'n' with a 1</p>
<div class="highlight">
<pre><span></span><span class="n">center_word_vector</span><span class="p">[</span><span class="n">happy_index</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
<p>And you have your one-hot word vector.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">center_word_vector</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 1. 0. 0.]
</pre>
<p><b>You can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                           <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                           <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create a one-hot-vector with a 1 where the word is</span>


<span class="sd">    Args:</span>
<span class="sd">     word: known token to add to the vector</span>
<span class="sd">     word_to_index: dict mapping word: index</span>
<span class="sd">     vocabulary_size: how long to make the vector</span>

<span class="sd">    Returns:</span>
<span class="sd">     vector with zeros everywhere except where the word is</span>
<span class="sd">    """</span>
    <span class="n">one_hot_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
    <span class="n">one_hot_vector</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">one_hot_vector</span>
</pre></div>
<p>Check that it works as intended.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="s1">'happy'</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">))</span>
</pre></div>
<pre class="example">
[0. 0. 1. 0. 0.]
</pre>
<p><b>What is the word vector for "learning"?</b></p>
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="s1">'learning'</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">actual</span> <span class="o">==</span> <span class="n">expected</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 0. 0. 1.]
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgd65f99f">
<h4 id="orgd65f99f">Getting context word vectors</h4>
<div class="outline-text-4" id="text-orgd65f99f">
<p>To create the vectors that represent context words, you will calculate the average of the one-hot vectors representing the individual words.</p>
<p>Let's start with a list of context words.</p>
<div class="highlight">
<pre><span></span><span class="n">context_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">]</span>
</pre></div>
<p>Using Python's list comprehension construct and the <code>word_to_one_hot_vector</code> function that you created in the previous section, you can create a list of one-hot vectors representing each of the context words.</p>
<p>Create one-hot vectors for each context word using list comprehension</p>
<div class="highlight">
<pre><span></span><span class="n">context_words_vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">)</span>
</pre></div>
<pre class="example">
[array([0., 0., 0., 1., 0.]),
 array([1., 0., 0., 0., 0.]),
 array([0., 1., 0., 0., 0.]),
 array([0., 0., 0., 1., 0.])]
</pre>
<p>And you can now simply get the average of these vectors using numpy's <code>mean</code> function, to get the vector representation of the context words.</p>
<div class="highlight">
<pre><span></span><span class="n">ROWS</span><span class="p">,</span> <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">first</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROWS</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.25 0.25 0.   0.5  0.  ]
</pre>
<p><b>Now create the <code>context_words_to_vector</code> function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">context_words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                            <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create vector with the mean of the one-hot-vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to covert to one-hot vectors</span>
<span class="sd">     word_to_index: dict mapping word to index</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="n">context_words_vectors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROWS</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">words_to_vector</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">context_words_to_vector</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">word2Ind</span><span class="p">)</span>
<span class="n">second</span> <span class="o">=</span> <span class="n">words_to_vector</span><span class="p">([</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">second</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">first</span><span class="o">==</span><span class="n">second</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.25 0.25 0.   0.5  0.  ]
</pre>
<p><b>What is the vector representation of the context words "am happy i am"?</b></p>
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">words_to_vector</span><span class="p">([</span><span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span> <span class="p">,</span> <span class="mf">0.</span>  <span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.</span>  <span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">actual</span> <span class="o">==</span> <span class="n">expected</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.5  0.   0.25 0.25 0.  ]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf4b0edd">
<h3 id="orgf4b0edd">Building the training set</h3>
<div class="outline-text-3" id="text-orgf4b0edd">
<p>You can now combine the functions that you created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
<pre class="example">
['i', 'am', 'happy', 'because', 'i', 'am', 'learning']
</pre>
<p>To do this you need to use the sliding window function (<code>get_windows</code>) to extract the context words and center words, and you then convert these sets of words into a basic vector representation using <code>word_to_one_hot_vector</code> and <code>context_words_to_vector</code>.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="n">get_windows</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">half_window</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Context words:  </span><span class="si">{</span><span class="n">context_words</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="n">words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Center word:  </span><span class="si">{</span><span class="n">center_word</span><span class="si">}</span><span class="s2"> -&gt; "</span>
          <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">center_word</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
<pre class="example">
Context words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]
Center word:  happy -&gt; [0. 0. 1. 0. 0.]

Context words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]
Center word:  because -&gt; [0. 1. 0. 0. 0.]

Context words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]
Center word:  i -&gt; [0. 0. 0. 1. 0.]

</pre>
<p>In this practice notebook you'll be performing a single iteration of training using a single example, but in this week's assignment you'll train the CBOW model using several iterations and batches of example. Here is how you would use a Python generator function (remember the `yield` keyword from the lecture?) to make it easier to iterate over a set of examples.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_training_example</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="sd">"""generates training examples</span>

<span class="sd">    Args:</span>
<span class="sd">     words: source of words</span>
<span class="sd">     half_window: half the window size</span>
<span class="sd">     word_to_index: dict with word to index mapping</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="n">get_windows</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">),</span> <span class="n">word_to_one_hot_vector</span><span class="p">(</span>
            <span class="n">center_word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span>
            <span class="n">vocabulary_size</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<p>The output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">context_words_vector</span><span class="p">,</span> <span class="n">center_word_vector</span> <span class="ow">in</span> <span class="n">get_training_example</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Context words vector:  </span><span class="si">{</span><span class="n">context_words_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Center word vector:  </span><span class="si">{</span><span class="n">center_word_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
<pre class="example">
Context words vector:  [0.25 0.25 0.   0.5  0.  ]
Center word vector:  [0. 0. 1. 0. 0.]

Context words vector:  [0.5  0.   0.25 0.25 0.  ]
Center word vector:  [0. 1. 0. 0. 0.]

Context words vector:  [0.25 0.25 0.25 0.   0.25]
Center word vector:  [0. 0. 0. 1. 0.]

</pre>
<p>Now that the training set is ready, we can move on to the CBOW model itself which will be covered in the <a href="../introducing-the-cbow-model/">next post</a>.</p>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/data-preparation/" rel="tag">data preparation</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/word-embeddings/" rel="tag">word embeddings</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../auto-complete-building-the-auto-complete-system/" rel="prev" title="Auto-Complete: Building the Auto-Complete System">Previous post</a></li>
<li class="next"><a href="../introducing-the-cbow-model/" rel="next" title="Introducing the CBOW Model">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
