<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Assessing N-Gram model performance with perplexity." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Auto-Complete: Perplexity | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/auto-complete-perplexity/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../auto-complete-the-n-gram-model/" rel="prev" title="Auto-Complete: the N-Gram Model" type="text/html">
<link href="../auto-complete-building-the-auto-complete-system/" rel="next" title="Auto-Complete: Building the Auto-Complete System" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Auto-Complete: Perplexity" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/auto-complete-perplexity/" property="og:url">
<meta content="Assessing N-Gram model performance with perplexity." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-04T15:19:33-08:00" property="article:published_time">
<meta content="auto-complete" property="article:tag">
<meta content="n-gram" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Auto-Complete: Perplexity</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-04T15:19:33-08:00" itemprop="datePublished" title="2020-12-04 15:19">2020-12-04 15:19</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org30b1889">Perplexity</a>
<ul>
<li><a href="#org885d7fa">Imports</a></li>
</ul>
</li>
<li><a href="#org070fb62">Middle</a>
<ul>
<li><a href="#orgecbd264">The Probability Function</a></li>
<li><a href="#org0a09a8d">Calculating the Perplexity</a>
<ul>
<li><a href="#org916e2fe">Test It</a></li>
</ul>
</li>
<li><a href="#org75c6562">Using the Class-Based Version</a></li>
</ul>
</li>
<li><a href="#orgada2e72">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org30b1889">
<h2 id="org30b1889">Perplexity</h2>
<div class="outline-text-2" id="text-org30b1889">
<p>In the <a href="../auto-complete-the-n-gram-model/">previous post</a> we implemented the N-Gram Language Model for the auto-complete system that we began <a href="../auto-complete/">here</a>.</p>
<p>In this section, you will generate the perplexity score to evaluate your model on the test set.</p>
<ul class="org-ul">
<li>You will also use back-off when needed.</li>
<li>Perplexity is used as an evaluation metric of your language model.</li>
<li>To calculate the the perplexity score of the test set on an n-gram model, use:</li>
</ul>
<p>\[ PP(W) =\sqrt[N]{ \prod_{t=n+1}^N \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4} \]</p>
<ul class="org-ul">
<li>where <i>N</i> is the length of the sentence.</li>
<li><i>n</i> is the number of words in the n-gram (e.g. 2 for a bigram).</li>
<li>In math, the numbering starts at one and not zero.</li>
</ul>
<p>In code, array indexing starts at zero, so the code will use ranges for <i>t</i> according to this formula:</p>
<p>\[ PP(W) =\sqrt[N]{ \prod_{t=n}^{N-1} \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4.1} \]</p>
<p>The higher the probabilities are, the lower the perplexity will be.</p>
<ul class="org-ul">
<li>The more the n-grams tell us about the sentence, the lower the perplexity score will be.</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org885d7fa">
<h3 id="org885d7fa">Imports</h3>
<div class="outline-text-3" id="text-org885d7fa">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">expect</span><span class="p">,</span> <span class="n">be_true</span>

<span class="kn">import</span> <span class="nn">attr</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.autocomplete</span> <span class="kn">import</span> <span class="n">NGrams</span><span class="p">,</span><span class="n">NGramProbability</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org070fb62">
<h2 id="org070fb62">Middle</h2>
<div class="outline-text-2" id="text-org070fb62"></div>
<div class="outline-3" id="outline-container-orgecbd264">
<h3 id="orgecbd264">The Probability Function</h3>
<div class="outline-text-3" id="text-orgecbd264">
<p>This was already defined in the previous post, but the function following it assumes its existence so I'm temporarily re-defining it here.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">estimate_probability</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                         <span class="n">previous_n_gram</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> 
                         <span class="n">n_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">n_plus1_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span>

<span class="sd">    Args:</span>
<span class="sd">       word: next word</span>
<span class="sd">       previous_n_gram: A sequence of words of length n</span>
<span class="sd">       n_gram_counts: Dictionary of counts of n-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary_size: number of words in the vocabulary</span>
<span class="sd">       k: positive constant, smoothing parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">       A probability</span>
<span class="sd">    """</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>
    <span class="n">previous_n_gram_count</span> <span class="o">=</span> <span class="n">n_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">n_plus1_gram</span> <span class="o">=</span> <span class="n">previous_n_gram</span> <span class="o">+</span> <span class="p">(</span><span class="n">word</span><span class="p">,)</span>  
    <span class="n">n_plus1_gram_count</span> <span class="o">=</span> <span class="n">n_plus1_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n_plus1_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>       
    <span class="k">return</span> <span class="p">(</span><span class="n">n_plus1_gram_count</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">previous_n_gram_count</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">vocabulary_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0a09a8d">
<h3 id="org0a09a8d">Calculating the Perplexity</h3>
<div class="outline-text-3" id="text-org0a09a8d">
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: calculate_perplexity</span>
<span class="k">def</span> <span class="nf">calculate_perplexity</span><span class="p">(</span><span class="n">sentence</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                         <span class="n">n_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">n_plus1_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate perplexity for a list of sentences</span>

<span class="sd">    Args:</span>
<span class="sd">       sentence: List of strings</span>
<span class="sd">       n_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary_size: number of unique words in the vocabulary</span>
<span class="sd">       k: Positive smoothing constant</span>

<span class="sd">    Returns:</span>
<span class="sd">       Perplexity score</span>
<span class="sd">    """</span>
    <span class="c1"># length of previous words</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">n_gram_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span> 

    <span class="c1"># prepend &lt;s&gt; and append &lt;e&gt;</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"&lt;s&gt;"</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">]</span>

    <span class="c1"># Cast the sentence from a list to a tuple</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

    <span class="c1"># length of sentence (after adding &lt;s&gt; and &lt;e&gt; tokens)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

    <span class="c1"># The variable p will hold the product</span>
    <span class="c1"># that is calculated inside the n-root</span>
    <span class="c1"># Update this in the code below</span>
    <span class="n">product_pi</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># Index t ranges from n to N - 1, inclusive on both ends</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span> <span class="c1"># complete this line</span>

        <span class="c1"># get the n-gram preceding the word at position t</span>
        <span class="n">n_gram</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">t</span> <span class="o">-</span> <span class="n">n</span><span class="p">:</span> <span class="n">t</span><span class="p">]</span>

        <span class="c1"># get the word at position t</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

        <span class="c1"># Estimate the probability of the word given the n-gram</span>
        <span class="c1"># using the n-gram counts, n-plus1-gram counts,</span>
        <span class="c1"># vocabulary size, and smoothing constant</span>
        <span class="n">probability</span> <span class="o">=</span> <span class="n">estimate_probability</span><span class="p">(</span>
            <span class="n">word</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">previous_n_gram</span><span class="o">=</span><span class="n">n_gram</span><span class="p">,</span>
            <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">,</span>
            <span class="n">n_gram_counts</span><span class="o">=</span><span class="n">n_gram_counts</span><span class="p">,</span>
            <span class="n">n_plus1_gram_counts</span><span class="o">=</span><span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

        <span class="c1"># Update the product of the probabilities</span>
        <span class="c1"># This 'product_pi' is a cumulative product </span>
        <span class="c1"># of the (1/P) factors that are calculated in the loop</span>
        <span class="n">product_pi</span> <span class="o">*=</span> <span class="mi">1</span><span class="o">/</span><span class="n">probability</span>

    <span class="c1"># Take the Nth root of the product</span>
    <span class="n">perplexity</span> <span class="o">=</span> <span class="n">product_pi</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>

    <span class="c1">### END CODE HERE ### </span>
    <span class="k">return</span> <span class="n">perplexity</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org916e2fe">
<h4 id="org916e2fe">Test It</h4>
<div class="outline-text-4" id="text-org916e2fe">
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
                 <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">unigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>


<span class="n">perplexity_train1</span> <span class="o">=</span> <span class="n">calculate_perplexity</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                         <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span>
                                         <span class="nb">len</span><span class="p">(</span><span class="n">unique_words</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">2.8040</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Perplexity for first train sample: </span><span class="si">{</span><span class="n">perplexity_train1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">perplexity_train1</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">test_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">]</span>
<span class="n">perplexity_test</span> <span class="o">=</span> <span class="n">calculate_perplexity</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">,</span>
                                       <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span>
                                       <span class="nb">len</span><span class="p">(</span><span class="n">unique_words</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Perplexity for test sample: </span><span class="si">{</span><span class="n">perplexity_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">3.9654</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">perplexity_test</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
Perplexity for first train sample: 2.8040
Perplexity for test sample: 3.9654
</pre>
<p><b>Note:</b> If your sentence is really long, there will be underflow when multiplying many fractions.</p>
<ul class="org-ul">
<li>To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.</li>
</ul>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org75c6562">
<h3 id="org75c6562">Using the Class-Based Version</h3>
<div class="outline-text-3" id="text-org75c6562">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Perplexity</span><span class="p">:</span>
    <span class="sd">"""Calculate perplexity</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the tokenized training input</span>
<span class="sd">     n: the size of the n-grams</span>
<span class="sd">     augment_vocabulary: whether to augment the vocabulary for toy examples</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">augment_vocabulary</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>
    <span class="n">_probabilifier</span><span class="p">:</span> <span class="n">NGramProbability</span><span class="o">=</span><span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">probabilifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NGramProbability</span><span class="p">:</span>
        <span class="sd">"""Probability Calculator"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_probabilifier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_probabilifier</span> <span class="o">=</span> <span class="n">NGramProbability</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
                <span class="n">augment_vocabulary</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">augment_vocabulary</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_probabilifier</span>

    <span class="k">def</span> <span class="nf">perplexity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">"""Calculates the perplexity for the sentence"""</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="s2">"&lt;s&gt;"</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">])</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

        <span class="n">n_grams</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentence</span><span class="p">[</span><span class="n">position</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">:</span> <span class="n">position</span><span class="p">]</span>
                   <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentence</span><span class="p">[</span><span class="n">position</span><span class="p">]</span>
                 <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="n">words_n_grams</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">n_grams</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">probabilifier</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">n_gram</span><span class="p">)</span>
                         <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">n_gram</span> <span class="ow">in</span> <span class="n">words_n_grams</span><span class="p">)</span>
        <span class="n">product</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">probability</span> <span class="k">for</span> <span class="n">probability</span> <span class="ow">in</span> <span class="n">probabilities</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">product</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
                 <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Perplexity</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">augment_vocabulary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">perplexity</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">expected</span> <span class="o">=</span> <span class="mf">2.8040</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Perplexity for first train sample: </span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">test_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">]</span>
<span class="n">model</span>
<span class="n">perplexity_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">perplexity</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Perplexity for test sample: </span><span class="si">{</span><span class="n">perplexity_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">3.9654</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">perplexity_test</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgada2e72">
<h2 id="orgada2e72">End</h2>
<div class="outline-text-2" id="text-orgada2e72">
<p>In the next part we'll build our <a href="../auto-complete-building-the-auto-complete-system/">completed auto-complete system</a>.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/auto-complete/" rel="tag">auto-complete</a></li>
<li><a class="tag p-category" href="../../../categories/n-gram/" rel="tag">n-gram</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../auto-complete-the-n-gram-model/" rel="prev" title="Auto-Complete: the N-Gram Model">Previous post</a></li>
<li class="next"><a href="../auto-complete-building-the-auto-complete-system/" rel="next" title="Auto-Complete: Building the Auto-Complete System">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
