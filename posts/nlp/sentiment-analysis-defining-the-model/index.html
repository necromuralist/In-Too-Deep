<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Defining the Deep Learning Model" name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Sentiment Analysis: Defining the Model | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/sentiment-analysis-defining-the-model/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../sentiment-analysis-pre-processing-the-data/" rel="prev" title="Sentiment Analysis: Pre-processing the Data" type="text/html">
<link href="../sentiment-analysis-training-the-model/" rel="next" title="Sentiment Analysis: Training the Model" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Sentiment Analysis: Defining the Model" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/sentiment-analysis-defining-the-model/" property="og:url">
<meta content="Defining the Deep Learning Model" property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-23T15:46:13-08:00" property="article:published_time">
<meta content="deep learning" property="article:tag">
<meta content="nlp" property="article:tag">
<meta content="sentiment analysis" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Sentiment Analysis: Defining the Model</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-23T15:46:13-08:00" itemprop="datePublished" title="2020-12-23 15:46">2020-12-23 15:46</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org3bf2f7b">Beginning</a>
<ul>
<li><a href="#orgba131b9">Imports</a></li>
<li><a href="#orgee7612b">Set Up</a></li>
</ul>
</li>
<li><a href="#org9f00e72">Middle</a>
<ul>
<li><a href="#org657cebc">The Base Layer Class</a></li>
<li><a href="#org84de66e">The ReLU class</a>
<ul>
<li><a href="#orgc6e94b1">Test It</a></li>
</ul>
</li>
<li><a href="#org16953ca">The Dense class</a></li>
<li><a href="#orgb9f279c">The Layers for the Trax-Based Model</a></li>
<li><a href="#org1e987dd">Dense</a>
<ul>
<li><a href="#org3ea2899">Online Documentation</a></li>
</ul>
</li>
<li><a href="#orgd601006">The Classifier Function</a></li>
</ul>
</li>
<li><a href="#org82515a4">Ending</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org3bf2f7b">
<h2 id="org3bf2f7b">Beginning</h2>
<div class="outline-text-2" id="text-org3bf2f7b">
<p>This continues a series on <a href="../sentiment-analysis-deep-learning-model/">sentiment analysis with deep learning</a>. In the <a href="../sentiment-analysis-pre-processing-the-data/">previous post</a> we loaded and processed our data set. In this post we'll see about actually defining the Neural Network.</p>
<p>In this part we will write your own library of layers. It will be very similar to the one used in Trax and also in Keras and PyTorch. The intention is that in writing our own small framework will help us understand how they all work and use them more effectively in the future.</p>
</div>
<div class="outline-3" id="outline-container-orgba131b9">
<h3 id="orgba131b9">Imports</h3>
<div class="outline-text-3" id="text-orgba131b9">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">expect</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">fastmath</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">trax</span>
<span class="kn">import</span> <span class="nn">trax.layers</span> <span class="k">as</span> <span class="nn">trax_layers</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.twitter.tensor_generator</span> <span class="kn">import</span> <span class="n">TensorBuilder</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgee7612b">
<h3 id="orgee7612b">Set Up</h3>
<div class="outline-text-3" id="text-orgee7612b">
<p>Some aliases to get closer to what the notebook has.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy_fastmath</span> <span class="o">=</span> <span class="n">fastmath</span><span class="o">.</span><span class="n">numpy</span>
<span class="n">random</span> <span class="o">=</span> <span class="n">fastmath</span><span class="o">.</span><span class="n">random</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org9f00e72">
<h2 id="org9f00e72">Middle</h2>
<div class="outline-text-2" id="text-org9f00e72"></div>
<div class="outline-3" id="outline-container-org657cebc">
<h3 id="org657cebc">The Base Layer Class</h3>
<div class="outline-text-3" id="text-org657cebc">
<p>This will be the base class that the others will inherit from.</p>
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Layer</span><span class="p">:</span>
    <span class="sd">"""Base class for layers</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="sd">"""The forward propagation method</span>

<span class="sd">       Raises:</span>
<span class="sd">        NotImplementedError - method is called but child hasn't implemented it</span>
<span class="sd">       """</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">init_weights_and_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">,</span> <span class="n">random_key</span><span class="p">):</span>
        <span class="sd">"""method to initialize the weights</span>
<span class="sd">       based on the input signature and random key,</span>
<span class="sd">       be implemented by subclasses of this Layer class</span>
<span class="sd">       """</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">,</span> <span class="n">random_key</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""initializes and returns the weights</span>

<span class="sd">       Note:</span>
<span class="sd">        This is just an alias for the ``init_weights_and_state``</span>
<span class="sd">       method for some reason</span>

<span class="sd">       Args: </span>
<span class="sd">        input_signature: who knows?</span>
<span class="sd">        random_key: once again, who knows?</span>

<span class="sd">       Returns:</span>
<span class="sd">        the weights</span>
<span class="sd">       """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_weights_and_state</span><span class="p">(</span><span class="n">input_signature</span><span class="p">,</span> <span class="n">random_key</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""This is an alias for the ``forward`` method</span>

<span class="sd">       Args:</span>
<span class="sd">        x: input array</span>

<span class="sd">       Returns:</span>
<span class="sd">        whatever the ``forward`` method does</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org84de66e">
<h3 id="org84de66e">The ReLU class</h3>
<div class="outline-text-3" id="text-org84de66e">
<p>Here's the ReLU function:</p>
<p>\[ \mathrm{ReLU}(x) = \mathrm{max}(0,x) \]</p>
<p>We'll implement the ReLU activation function below. The function will take in a matrix or vector and it transform all the negative numbers into 0 while keeping all the positive numbers intact.</p>
<p>Please use numpy.maximum(A,k) to find the maximum between each element in A and a scalar k.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Relu</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""Relu activation function implementation"""</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">""""Performs the activation</span>

<span class="sd">       Args: </span>
<span class="sd">           - x: the input</span>

<span class="sd">       Returns:</span>
<span class="sd">           - activation: all positive or 0 version of x</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgc6e94b1">
<h4 id="orgc6e94b1">Test It</h4>
<div class="outline-text-4" id="text-orgc6e94b1">
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
<span class="n">relu_layer</span> <span class="o">=</span> <span class="n">Relu</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Test data is:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Output of Relu is:"</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">relu_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
Test data is:
[[-2. -1.  0.]
 [ 0.  1.  2.]]

Output of Relu is:
[[0. 0. 0.]
 [0. 1. 2.]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org16953ca">
<h3 id="org16953ca">The Dense class</h3>
<div class="outline-text-3" id="text-org16953ca">
<p>Implement the forward function of the Dense class.</p>
<ul class="org-ul">
<li>The forward function multiplies the input to the layer (<code>x</code>) by the weight matrix (<code>W</code>).</li>
</ul>
<p>\[ \mathrm{forward}(\mathbf{x},\mathbf{W}) = \mathbf{xW} \]</p>
<ul class="org-ul">
<li>You can use <code>numpy.dot</code> to perform the matrix multiplication.</li>
</ul>
<p>Note that for more efficient code execution, you will use the trax version of <code>math</code>, which includes a trax version of <code>numpy</code> and also <code>random</code>.</p>
<p>Implement the weight initializer <code>new_weights</code> function</p>
<ul class="org-ul">
<li>Weights are initialized with a random key.</li>
<li>The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols)</li>
<li>The num of rows for weights should equal the number of columns in x, because for forward propagation, you will multiply x times weights.</li>
</ul>
<p>Please use <code>trax.fastmath.random.normal(key, shape, dtype=tf.float32)</code> to generate random values for the weight matrix. The key difference between this function and the standard <code>numpy</code> randomness is the explicit use of random keys, which need to be passed in. While it can look tedious at the first sight to pass the random key everywhere, you will learn in Course 4 why this is very helpful when implementing some advanced models.</p>
<ul class="org-ul">
<li><code>key</code> can be generated by calling <code>random.get_prng(seed)</code> and passing in a number for the <code>seed</code>.</li>
<li><code>shape</code> is a tuple with the desired shape of the weight matrix.
<ul class="org-ul">
<li>The number of rows in the weight matrix should equal the number of columns in the variable <code>x</code>. Since <code>x</code> may have 2 dimensions if it represents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x.</li>
<li>The number of columns in the weight matrix is the number of units chosen for that dense layer. Look at the <code>__init__</code> function to see which variable stores the number of units.</li>
</ul>
</li>
<li><code>dtype</code> is the data type of the values in the generated matrix; keep the default of <code>tf.float32</code>. In this case, don't explicitly set the dtype (just let it use the default value).</li>
</ul>
<p>Set the standard deviation of the random values to 0.1</p>
<ul class="org-ul">
<li>The values generated have a mean of 0 and standard deviation of 1.</li>
<li>Set the default standard deviation <code>stdev</code> to be 0.1 by multiplying the standard deviation to each of the values in the weight matrix.</li>
</ul>
<p>See how the fastmath.trax.random.normal function works.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">get_prng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"The random seed generated by random.get_prng"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_key</span><span class="p">)</span>
</pre></div>
<pre class="example">
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
The random seed generated by random.get_prng
DeviceArray([0, 1], dtype=uint32)
</pre>
<p>For some reason tensorflow can't find the GPU. Setting the log level to 0 like the message suggests shows that it gives up after trying to find a TPU, there's no indication that it's looking for the GPU.</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">())</span>
</pre></div>
<p>Hmmm. I'll have to troubleshoot that.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"choose a matrix with 2 rows and 3 columns"</span><span class="p">)</span>
<span class="n">tmp_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
choose a matrix with 2 rows and 3 columns
(2, 3)
</pre>
<p>Generate a weight matrix Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow Just avoid setting the dtype and allow it to use the default data type</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_weight</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">tmp_key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">tmp_shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Weight matrix generated with a normal distribution with mean 0 and stdev of 1"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_weight</span><span class="p">)</span>
</pre></div>
<pre class="example">
Weight matrix generated with a normal distribution with mean 0 and stdev of 1
DeviceArray([[ 0.957307  , -0.9699291 ,  1.0070664 ],
             [ 0.36619022,  0.17294823,  0.29092228]], dtype=float32)
</pre>
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    A dense (fully-connected) layer.</span>

<span class="sd">    Args:</span>
<span class="sd">     - n_units: the number of columns for our weight matrix</span>
<span class="sd">     - init_stdev: standard deviation for our initial weights</span>
<span class="sd">    """</span>
    <span class="n">n_units</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">init_stdev</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.1</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""The dot product of the input and the weights</span>

<span class="sd">       Args:</span>
<span class="sd">        x: input to multipyl</span>

<span class="sd">       Returns:</span>
<span class="sd">        product of x and weights</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_weights_and_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_signature</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span>
                               <span class="n">random_key</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
        <span class="sd">"""initializes the weights</span>

<span class="sd">       Args:</span>
<span class="sd">        input_signature: tuple whose final dimension will be the number of rows</span>
<span class="sd">        random_ke: something to start the random normal generator with</span>
<span class="sd">       """</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">input_signature</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># to allow for more than two-dimensional matrices,</span>
        <span class="c1"># we use the last column of the input shape, rather than assuming it's</span>
        <span class="c1"># column 1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">random_key</span><span class="p">,</span>
                                      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_units</span><span class="p">))</span>
             <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_stdev</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">dense_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  <span class="c1">#sets  number of units in dense layer</span>
<span class="n">random_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">get_prng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># sets random seed</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">25.0</span><span class="p">]])</span> <span class="c1"># input array </span>

<span class="n">dense_layer</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">random_key</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Weights are</span><span class="se">\n</span><span class="s2"> "</span><span class="p">,</span><span class="n">dense_layer</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="c1">#Returns randomly generated weights</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">dense_layer</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Foward function output is "</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="c1"># Returns multiplied values of units and weights</span>

<span class="n">expected_weights</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.02837108</span><span class="p">,</span>  <span class="mf">0.09368162</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.10050076</span><span class="p">,</span>  <span class="mf">0.14165013</span><span class="p">,</span>  <span class="mf">0.10543301</span><span class="p">,</span>  <span class="mf">0.09108126</span><span class="p">,</span>
     <span class="o">-</span><span class="mf">0.04265672</span><span class="p">,</span>  <span class="mf">0.0986188</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.05575325</span><span class="p">,</span>  <span class="mf">0.00153249</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.20785688</span><span class="p">,</span>  <span class="mf">0.0554837</span><span class="p">,</span>   <span class="mf">0.09142365</span><span class="p">,</span>  <span class="mf">0.05744595</span><span class="p">,</span>  <span class="mf">0.07227863</span><span class="p">,</span>  <span class="mf">0.01210617</span><span class="p">,</span>
     <span class="o">-</span><span class="mf">0.03237354</span><span class="p">,</span>  <span class="mf">0.16234995</span><span class="p">,</span>  <span class="mf">0.02450038</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.13809784</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.06111237</span><span class="p">,</span>  <span class="mf">0.01403724</span><span class="p">,</span>  <span class="mf">0.08410042</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1094358</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.10775021</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11396459</span><span class="p">,</span>
     <span class="o">-</span><span class="mf">0.05933381</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01557652</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03832145</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11144515</span><span class="p">]])</span>

<span class="n">expected_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="o">-</span><span class="mf">3.0395496</span><span class="p">,</span>   <span class="mf">0.9266802</span><span class="p">,</span>   <span class="mf">2.5414743</span><span class="p">,</span>  <span class="o">-</span><span class="mf">2.050473</span><span class="p">,</span>   <span class="o">-</span><span class="mf">1.9769388</span><span class="p">,</span>  <span class="o">-</span><span class="mf">2.582209</span><span class="p">,</span>
      <span class="o">-</span><span class="mf">1.7952735</span><span class="p">,</span>   <span class="mf">0.94427425</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8980402</span><span class="p">,</span>  <span class="o">-</span><span class="mf">3.7497487</span><span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">dense_layer</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">expected_weights</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">expected_output</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
Weights are
  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126
  -0.04265672  0.0986188  -0.05575325  0.00153249]
 [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617
  -0.03237354  0.16234995  0.02450038 -0.13809784]
 [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459
  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]
Foward function output is  [[-3.03954965  0.92668021  2.54147445 -2.05047299 -1.97693891 -2.58220917
  -1.79527355  0.94427423 -0.89804017 -3.74974866]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgb9f279c">
<h3 id="orgb9f279c">The Layers for the Trax-Based Model</h3>
<div class="outline-text-3" id="text-orgb9f279c">
<p>For the model implementation we will use the Trax layers library. Trax layers are very similar to the ones we implemented above, but in addition to trainable weights they also have a non-trainable state. This state is used in layers like batch normalization and for inference - we will learn more about it later on.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org1e987dd">
<h3 id="org1e987dd">Dense</h3>
<div class="outline-text-3" id="text-org1e987dd">
<p>First, look at the code of the Trax Dense layer and compare to the implementation above.</p>
<ul class="org-ul">
<li><a href="https://github.com/google/trax/blob/master/trax/layers/core.py#L29">Trax Dense layer implementation</a></li>
</ul>
<p>Another other important layer that we will use a lot is the <a href="https://github.com/google/trax/blob/master/trax/layers/combinators.py#L26">Serial</a> layer which allows us to execute one layer after another in sequence.</p>
<ul class="org-ul">
<li>You can pass in the layers as arguments to <code>Serial</code>, separated by commas.</li>
<li>For example: <code>tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))</code></li>
</ul>
<p>The layer classes have pretty good docstrings, unlike the fastmath stuff, so it might be useful to look at it - but it's too long to include here.</p>
<p>We're also going to use an <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113">Embedding</a></p>
<ul class="org-ul">
<li><code>tl.Embedding(vocab_size, d_feature)</code>.</li>
<li><code>vocab_size</code> is the number of unique words in the given vocabulary.</li>
<li><code>d_feature</code> is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">tmp_embed</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">d_feature</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_embed</span><span class="p">)</span>
</pre></div>
<pre class="example">
Embedding_3_2
</pre>
<p>Another useful layer is the <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276">Mean</a> which calculates means across an axis. In this case, use axis = 1 (across rows) to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).</p>
<ul class="org-ul">
<li>For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements.</li>
</ul>
<p>Pretend the embedding matrix uses 2 elements for embedding the meaning of a word and has a vocabulary size of 3, so it has shape (2,3).</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_embed</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,],</span>
                         <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>
                         <span class="p">])</span>
</pre></div>
<p>First take the mean along axis 0, which creates a vector whose length equals the vocabulary size (the number of columns).</p>
<div class="highlight">
<pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp_embed</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
<pre class="example">
array([2.5, 3.5, 4.5])
</pre>
<p>If you take the mean along axis 1 it creates a vector whose length equals the number of elements in a word embedding (the rows).</p>
<div class="highlight">
<pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp_embed</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
<pre class="example">
array([2., 5.])
</pre>
<p>Finally, a <a href="https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242">LogSoftmax</a> layer gives you a log-softmax output.</p>
</div>
<div class="outline-4" id="outline-container-org3ea2899">
<h4 id="org3ea2899">Online Documentation</h4>
<div class="outline-text-4" id="text-org3ea2899">
<p>For completeness, here's some links to the Read the Docs documentation for these layers.</p>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">Dense</a></li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators">Serial</a></li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">Embedding</a></li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean">Mean</a></li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">LogSoftmax</a></li>
</ul>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd601006">
<h3 id="orgd601006">The Classifier Function</h3>
<div class="outline-text-3" id="text-orgd601006">
<div class="highlight">
<pre><span></span><span class="n">builder</span> <span class="o">=</span> <span class="n">TensorBuilder</span><span class="p">()</span>
<span class="n">size_of_vocabulary</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">builder</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">classifier</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">size_of_vocabulary</span><span class="p">,</span>
               <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
               <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">"""Creates the classifier model</span>

<span class="sd">    Args:</span>
<span class="sd">     vocab_size: number of tokens in the training vocabulary</span>
<span class="sd">     embedding_dim: output dimension for the Embedding layer</span>
<span class="sd">     output_dim: dimension for the Dense layer</span>

<span class="sd">    Returns:</span>
<span class="sd">     the composed layer-model</span>
<span class="sd">    """</span>
    <span class="n">embed_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="c1"># Size of the vocabulary</span>
        <span class="n">d_feature</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">)</span>  <span class="c1"># Embedding dimension</span>

    <span class="n">mean_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">dense_output_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span> <span class="o">=</span> <span class="n">output_dim</span><span class="p">)</span>

    <span class="n">log_softmax_layer</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">trax_layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
      <span class="n">embed_layer</span><span class="p">,</span>
      <span class="n">mean_layer</span><span class="p">,</span>
      <span class="n">dense_output_layer</span><span class="p">,</span>
      <span class="n">log_softmax_layer</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">tmp_model</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">tmp_model</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_model</span><span class="p">)</span>
</pre></div>
<pre class="example">
&lt;class 'trax.layers.combinators.Serial'&gt;
Serial[
  Embedding_9164_256
  Mean
  Dense_2
  LogSoftmax
]
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org82515a4">
<h2 id="org82515a4">Ending</h2>
<div class="outline-text-2" id="text-org82515a4">
<p>Now that we have our Deep Learning model, we'll move on to <a href="../sentiment-analysis-training-the-model/">training it</a>.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/deep-learning/" rel="tag">deep learning</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/sentiment-analysis/" rel="tag">sentiment analysis</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../sentiment-analysis-pre-processing-the-data/" rel="prev" title="Sentiment Analysis: Pre-processing the Data">Previous post</a></li>
<li class="next"><a href="../sentiment-analysis-training-the-model/" rel="next" title="Sentiment Analysis: Training the Model">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
