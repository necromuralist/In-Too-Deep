<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Introduction to google's Trax library to replace TensorFlow." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Introducing Trax | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/introducing-trax/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../word-embeddings-visualizing-the-embeddings/" rel="prev" title="Word Embeddings: Visualizing the Embeddings" type="text/html">
<link href="../data-generators/" rel="next" title="Data Generators" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Introducing Trax" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/introducing-trax/" property="og:url">
<meta content="Introduction to google's Trax library to replace TensorFlow." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-17T17:19:06-08:00" property="article:published_time">
<meta content="nlp" property="article:tag">
<meta content="trax" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Introducing Trax</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-17T17:19:06-08:00" itemprop="datePublished" title="2020-12-17 17:19">2020-12-17 17:19</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf447e0b">Background</a>
<ul>
<li><a href="#org09f090a">Why Trax and not TensorFlow or PyTorch?</a></li>
<li><a href="#org97ba895">Why not Keras then?</a></li>
<li><a href="#org4b32e47">How to Code in Trax</a></li>
<li><a href="#org413f5cd">Trax, JAX, TensorFlow and Tensor2Tensor</a></li>
<li><a href="#org7caaf79">Installing Trax</a></li>
<li><a href="#org4414ea7">Imports</a></li>
</ul>
</li>
<li><a href="#orgbf2610a">Middle</a>
<ul>
<li><a href="#org935d644">Layers</a>
<ul>
<li><a href="#orgc9d5903">Relu Layer</a></li>
<li><a href="#orge960c49">Concatenate Layer</a></li>
<li><a href="#orga6124f3">Configuring Layers</a></li>
<li><a href="#org2649efd">Layer Weights</a></li>
<li><a href="#org2b8ebba">Custom Layers</a></li>
</ul>
</li>
<li><a href="#org0551d27">Combinators</a>
<ul>
<li><a href="#org6630e1f">Serial Combinator</a></li>
</ul>
</li>
<li><a href="#orga86c419">JAX</a></li>
</ul>
</li>
<li><a href="#org14935be">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf447e0b">
<h2 id="orgf447e0b">Background</h2>
<div class="outline-text-2" id="text-orgf447e0b">
<p>This is going to be a first look at <a href="https://github.com/google/trax">Trax</a> a Deep Learning framework built by the Google Brain team.</p>
</div>
<div class="outline-3" id="outline-container-org09f090a">
<h3 id="org09f090a">Why Trax and not TensorFlow or PyTorch?</h3>
<div class="outline-text-3" id="text-org09f090a">
<p>TensorFlow and PyTorch are both extensive frameworks that can do almost anything in deep learning. They offer a lot of flexibility, but that often means verbosity of syntax and extra time to code.</p>
<p>Trax is much more concise. It runs on a TensorFlow backend but allows you to train models with 1 line commands. Trax also runs end to end, allowing you to get data, model and train all with a single terse statement. This means you can focus on learning, instead of spending hours on the idiosyncrasies of a big framework's implementation.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org97ba895">
<h3 id="org97ba895">Why not Keras then?</h3>
<div class="outline-text-3" id="text-org97ba895">
<p>Keras is now part of Tensorflow itself from 2.0 onwards. Also, trax is good for implementing new state of the art algorithms like Transformers, Reformers, BERT because it is actively maintained by Google Brain Team for advanced deep learning tasks. It runs smoothly on CPUs,GPUs and TPUs as well with comparatively lesser modifications in code.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org4b32e47">
<h3 id="org4b32e47">How to Code in Trax</h3>
<div class="outline-text-3" id="text-org4b32e47">
<p>Building models in Trax relies on 2 key concepts:- <b>layers</b> and <b>combinators</b>. Trax layers are simple objects that process data and perform computations. They can be chained together into composite layers using Trax combinators, allowing you to build layers and models of any complexity.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org413f5cd">
<h3 id="org413f5cd">Trax, JAX, TensorFlow and Tensor2Tensor</h3>
<div class="outline-text-3" id="text-org413f5cd">
<p>You already know that Trax uses Tensorflow as a backend, but it also uses the <a href="https://github.com/google/jax">JAX</a> library to speed up computation too. You can view JAX as an enhanced and optimized version of numpy.</p>
<p>You import their version of numpy using <code>import trax.fastmath.numpy</code>. If you see this line, remember that when calling <code>numpy</code> you are really calling Traxâ€™s version of numpy that is compatible with JAX.**</p>
<p>As a result of this, where you used to encounter the type <code>numpy.ndarray</code> now you will find the type <code>jax.interpreters.xla.DeviceArray</code>. The documentation for JAX is <a href="https://jax.readthedocs.io/en/latest/index.html">here</a> and specifically they have a page <a href="https://jax.readthedocs.io/en/latest/jax.numpy.html">with the numpy functions implemented so far</a>.</p>
<p><a href="https://tensorflow.github.io/tensor2tensor/">Tensor2Tensor</a> is another name you might have heard. It started as an end to end solution much like how Trax is designed, but it grew unwieldy and complicated. So you can view Trax as the new improved version that operates much faster and simpler.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org7caaf79">
<h3 id="org7caaf79">Installing Trax</h3>
<div class="outline-text-3" id="text-org7caaf79">
<p>Note that there is another library called <a href="https://trax.readthedocs.io/en/latest/">TraX</a> which is something different.</p>
<p>We're going to use Trax version 1.3.1 here, so to install it with pip:</p>
<div class="highlight">
<pre><span></span>pip install trax==1.3.1
</pre></div>
<p>Note the <code>==</code> for the version, not <code>=</code>. This is a very big install so maybe take a break after you run it. You aren't going to get the full benefit of JAX if you don't have CUDA set up can use TPUs so make sure to set up CUDA if you're not using google colab. I also had to install <code>cmake</code> to get <code>trax</code> to install.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org4414ea7">
<h3 id="org4414ea7">Imports</h3>
<div class="outline-text-3" id="text-org4414ea7">
<div class="highlight">
<pre><span></span><span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">shapes</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">fastmath</span>
</pre></div>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html">Layers</a> are the basic building blocks for Trax</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.html#module-trax.shapes">shapes</a> are used for data handling</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.fastmath.html">fastmath</a> is the JAX version of numpy that can run on GPUs and TPUs</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgbf2610a">
<h2 id="orgbf2610a">Middle</h2>
<div class="outline-text-2" id="text-orgbf2610a"></div>
<div class="outline-3" id="outline-container-org935d644">
<h3 id="org935d644">Layers</h3>
<div class="outline-text-3" id="text-org935d644">
<p>Layers are the core building blocks in Trax - they are the base classes. They take inputs, compute functions/custom calculations and return outputs.</p>
</div>
<div class="outline-4" id="outline-container-orgc9d5903">
<h4 id="orgc9d5903">Relu Layer</h4>
<div class="outline-text-4" id="text-orgc9d5903">
<p>First we'll build a ReLU activation function as a layer. A layer like this is one of the simplest types. Notice there is no object initialization so it works just like a math function.</p>
<p><b>Note: Activation functions are also layers in Trax, which might look odd if you have been using other frameworks for a longer time.</b></p>
<div class="highlight">
<pre><span></span><span class="n">relu</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">()</span>
</pre></div>
<p>You can inspect the properties of a layer:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">relu</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">relu</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">relu</span><span class="o">.</span><span class="n">n_out</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : Relu
expected inputs : 1
promised outputs : 1 

</pre>
<p>We'll make an input the layer using numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x :"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x : [-2 -1  0  1  2] 

</pre>
<p>And see what it puts out.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
-- Outputs --
y : [0 0 0 1 2]
</pre>
<p>I don't know why but JAX doesn't thing I have a GPU, even though tensorflow does. This whole thing is a little messed up right now because the current release of tensorflow doesn't work on Ubuntu 20.10. I'm running it with the nightly build (2.5) but I have to install all the Trax dependencies one at a time or it will clobber the tensorflow installation with the older version (the one that doesn't work) so there's a lot of places for error.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orge960c49">
<h4 id="orge960c49">Concatenate Layer</h4>
<div class="outline-text-4" id="text-orge960c49">
<p>Now a layer that takes 2 inputs. Notice the change in the expected inputs property from 1 to 2.</p>
<p>First create a concatenate trax layer and check out its properties.</p>
<div class="highlight">
<pre><span></span><span class="n">concatenate</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">concatenate</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">concatenate</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">concatenate</span><span class="o">.</span><span class="n">n_out</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : Concatenate
expected inputs : 2
promised outputs : 1 

</pre>
<p>Now create the two inputs.</p>
<div class="highlight">
<pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">])</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">/</span> <span class="o">-</span><span class="mi">10</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x1 :"</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x2 :"</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x1 : [-10 -20 -30]
x2 : [1. 2. 3.] 

</pre>
<p>And now feed the inputs through the concatenate layer.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Outputs --
y : [-10. -20. -30.   1.   2.   3.]
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga6124f3">
<h4 id="orga6124f3">Configuring Layers</h4>
<div class="outline-text-4" id="text-orga6124f3">
<p>You can change the default settings of layers. For example, you can change the expected inputs for a concatenate layer from 2 to 3 using the optional parameter <code>n_items</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">concatenate_three</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">(</span><span class="n">n_items</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">concatenate_three</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">concatenate_three</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">concatenate_three</span><span class="o">.</span><span class="n">n_out</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : Concatenate
expected inputs : 3
promised outputs : 1 

</pre>
<p>Create some inputs.</p>
<div class="highlight">
<pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">])</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">/</span> <span class="o">-</span><span class="mi">10</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">*</span> <span class="mf">0.99</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x1 :"</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x2 :"</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x3 :"</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x1 : [-10 -20 -30]
x2 : [1. 2. 3.]
x3 : [0.99 1.98 2.97] 

</pre>
<p>And now do the concatenation.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">concatenate_three</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Outputs --
y : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]
</pre></div>
</div>
<div class="outline-4" id="outline-container-org2649efd">
<h4 id="org2649efd">Layer Weights</h4>
<div class="outline-text-4" id="text-org2649efd">
<p>Some layer types include mutable weights and biases that are used in computation and training. Layers of this type require initialization before use.</p>
<p>For example the <code>LayerNorm</code> layer calculates normalized data, that is also scaled by weights and biases. During initialization you pass the data shape and data type of the inputs, so the layer can initialize compatible arrays of weights and biases.</p>
<p>Initialize it.</p>
<div class="highlight">
<pre><span></span><span class="n">norm</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">()</span>
</pre></div>
<p>Now some input data.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">"float"</span><span class="p">)</span>
</pre></div>
<p>Use the input data signature to get the shape and type for the initializing weights and biases. We need to convert the input datatype from the usual ndarray to a trax ShapeDtype</p>
<div class="highlight">
<pre><span></span><span class="n">norm</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> 
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Normal shape:"</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"Data Type:"</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Shapes Trax:"</span><span class="p">,</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="s2">"Data Type:"</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
<pre class="example">
Normal shape: (4,) Data Type: &lt;class 'tuple'&gt;
Shapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: &lt;class 'trax.shapes.ShapeDtype'&gt;
</pre>
<p>Here are its properties.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">n_out</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : LayerNorm
expected inputs : 1
promised outputs : 1
</pre>
<p>And the weights and biases.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"weights :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"biases :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>
</pre></div>
<pre class="example">
weights : [1. 1. 1. 1.]
biases : [0. 0. 0. 0.]
</pre>
<p>We have our input array.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x :"</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x : [0. 1. 2. 3.]
</pre>
<p>So we can inspect what the layer did to it.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Outputs --
y : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]
</pre>
<p>If you look at it you can see that the positives cancel out the negatives, giving us a sum of 0. I don't know why that's the norm, but maybe it'll become obvious later.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org2b8ebba">
<h4 id="org2b8ebba">Custom Layers</h4>
<div class="outline-text-4" id="text-org2b8ebba">
<p>You can create your own custom layers too and define custom functions for computations by using <code>layers.Fn</code>. Let me show you how.</p>
<div class="highlight">
<pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org54ac6b0">
Help on function Fn in module trax.layers.base:

Fn(name, f, n_out=1)
    Returns a layer with no weights that applies the function `f`.
    
    `f` can take and return any number of arguments, and takes only positional
    arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).
    The following, for example, would create a layer that takes two inputs and
    returns two outputs -- element-wise sums and maxima:
    
        `Fn('SumAndMax', lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`
    
    The layer's number of inputs (`n_in`) is automatically set to number of
    positional arguments in `f`, but you must explicitly set the number of
    outputs (`n_out`) whenever it's not the default value 1.
    
    Args:
      name: Class-like name for the resulting layer; for use in debugging.
      f: Pure function from input tensors to output tensors, where each input
          tensor is a separate positional arg, e.g., `f(x0, x1) --&gt; x0 + x1`.
          Output tensors must be packaged as specified in the `Layer` class
          docstring.
      n_out: Number of outputs promised by the layer; default value 1.
    
    Returns:
      Layer executing the function `f`.
</pre></div>
<ul class="org-ul">
<li><a id="org08e4d47"></a>Define a custom layer<br>
<div class="outline-text-5" id="text-org08e4d47">
<p>In this example we'll create a layer to calculate the input times 2.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">double_it</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">:</span>
    <span class="sd">"""A custom layer function that doubles any inputs</span>


<span class="sd">    Returns:</span>
<span class="sd">     a custom function that takes one numeric argument and doubles it</span>
<span class="sd">    """</span>
    <span class="n">layer_name</span> <span class="o">=</span> <span class="s2">"TimesTwo"</span>

    <span class="c1"># Custom function for the custom layer</span>
    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>

    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">(</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="orgee07e31"></a>Test it<br>
<div class="outline-text-5" id="text-orgee07e31">
<div class="highlight">
<pre><span></span><span class="n">double</span> <span class="o">=</span> <span class="n">double_it</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">double</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">double</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">double</span><span class="o">.</span><span class="n">n_out</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : TimesTwo
expected inputs : 1
promised outputs : 1
</pre>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x :"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">double</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x : [1 2 3] 

-- Outputs --
y : [2 4 6]
</pre></div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0551d27">
<h3 id="org0551d27">Combinators</h3>
<div class="outline-text-3" id="text-org0551d27">
<p>You can combine layers to build more complex layers. Trax provides a set of objects named combinator layers to make this happen. Combinators are themselves layers, so behavior commutes.</p>
</div>
<div class="outline-4" id="outline-container-org6630e1f">
<h4 id="org6630e1f">Serial Combinator</h4>
<div class="outline-text-4" id="text-org6630e1f">
<p>This is the most common and easiest to use. You could, for example, build a simple neural network by combining layers into a single layer using the <code>Serial</code> combinator. This new layer then acts just like a single layer, so you can inspect intputs, outputs and weights. Or even combine it into another layer! Combinators can then be used as trainable models. <i>Try adding more layers.</i></p>
<p><b>Note:As you must have guessed, if there is serial combinator, there must be a parallel combinator as well. Do try to explore about combinators and other layers from the trax documentation and look at the repo to understand how these layers are written.</b></p>
<div class="highlight">
<pre><span></span><span class="n">serial</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(),</span>
    <span class="n">double</span><span class="p">,</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span> 
<span class="p">)</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org2b697ed"></a>Initialization<br>
<div class="outline-text-5" id="text-org2b697ed">
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="c1">#input</span>
<span class="n">serial</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Serial Model --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">serial</span><span class="p">,</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"sublayers :"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">n_out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"weights & biases:"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org1604a4b">
-- Serial Model --
Serial[
  LayerNorm
  Relu
  TimesTwo
  Dense_2
  Dense_1
  LogSoftmax
] 

-- Properties --
name : Serial
sublayers : [LayerNorm, Relu, TimesTwo, Dense_2, Dense_1, LogSoftmax]
expected inputs : 1
promised outputs : 1
weights & biases: [(DeviceArray([1, 1, 1, 1, 1], dtype=int32), DeviceArray([0, 0, 0, 0, 0], dtype=int32)), (), (), (DeviceArray([[ 0.19178385,  0.1832077 ],
             [-0.36949775, -0.03924937],
             [ 0.43800744,  0.788491  ],
             [ 0.43107533, -0.3623491 ],
             [ 0.6186575 ,  0.04764405]], dtype=float32), DeviceArray([-3.0051979e-06,  1.4359505e-06], dtype=float32)), (DeviceArray([[-0.6747592],
             [-0.8550365]], dtype=float32), DeviceArray([-8.9325863e-07], dtype=float32)), ()] 
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x :"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">serial</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x : [-2 -1  0  1  2] 

-- Outputs --
y : [0.]
</pre></div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orga86c419">
<h3 id="orga86c419">JAX</h3>
<div class="outline-text-3" id="text-orga86c419">
<p>Just remember to lookout for which numpy you are using, the regular numpy or Trax's JAX compatible numpy. Watch those import blocks. Numpy and fastmath.numpy have different data types.</p>
<p>Regular numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">x_numpy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"good old numpy : "</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_numpy</span><span class="p">),</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
good old numpy :  &lt;class 'numpy.ndarray'&gt; 

</pre>
<p>Fastmath and jax numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">x_jax</span> <span class="o">=</span> <span class="n">fastmath</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"jax trax numpy : "</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_jax</span><span class="p">))</span>
</pre></div>
<pre class="example">
jax trax numpy :  &lt;class 'jax.interpreters.xla._DeviceArray'&gt;
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org14935be">
<h2 id="org14935be">End</h2>
<div class="outline-text-2" id="text-org14935be">
<ul class="org-ul">
<li>Trax is a concise framework, built on TensorFlow, for end to end machine learning. The key building blocks are layers and combinators.</li>
<li>This was a lab that was part of coursera's <b>Natural Language Processing with Sequence Models</b> course put up by DeepLearning.AI.</li>
</ul>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/trax/" rel="tag">trax</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../word-embeddings-visualizing-the-embeddings/" rel="prev" title="Word Embeddings: Visualizing the Embeddings">Previous post</a></li>
<li class="next"><a href="../data-generators/" rel="next" title="Data Generators">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
