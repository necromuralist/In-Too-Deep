<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Training our Continuous Bag of Words Model." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Training the CBOW Model | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/training-the-cbow-model/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../introducing-the-cbow-model/" rel="prev" title="Introducing the CBOW Model" type="text/html">
<link href="../extracting-word-embeddings/" rel="next" title="Extracting Word Embeddings" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Training the CBOW Model" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/training-the-cbow-model/" property="og:url">
<meta content="Training our Continuous Bag of Words Model." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-09T18:34:27-08:00" property="article:published_time">
<meta content="cbow" property="article:tag">
<meta content="nlp" property="article:tag">
<meta content="word embeddings" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/Neurotic-Networking/"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Training the CBOW Model</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-09T18:34:27-08:00" itemprop="datePublished" title="2020-12-09 18:34">2020-12-09 18:34</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org80c300e">Beginning</a>
<ul>
<li><a href="#org5126391">Imports</a></li>
<li><a href="#org069baa5">Functions from Previous Posts</a>
<ul>
<li><a href="#orgfa2f344">Data Preparation Functions</a></li>
<li><a href="#org4219a3f">Activation Functions</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org7d7f8b2">Word Embeddings: Training the CBOW model</a>
<ul>
<li><a href="#org93ab497">Neural Network Initialization</a></li>
<li><a href="#org3ce8d2b">Initialization of the weights and biases</a>
<ul>
<li><a href="#orge8d692e">Define the first matrix of weights</a></li>
<li><a href="#org42d4226">Define the second matrix of weights</a></li>
<li><a href="#orge366336">Define the first vector of biases</a></li>
<li><a href="#org0b60557">Define the second vector of biases</a></li>
<li><a href="#orgcf7339f">Define the tokenized version of the corpus</a></li>
<li><a href="#org11a1223">Get 'word_to_index' and 'Ind2word' dictionaries for the tokenized corpus</a></li>
</ul>
</li>
<li><a href="#org9f9ce10">The First Training Example</a></li>
<li><a href="#org47aa186">Forward Propagation</a>
<ul>
<li><a href="#orgda8737c">The Hidden Layer</a></li>
<li><a href="#orgf4fe83b">The Output Layer</a></li>
</ul>
</li>
<li><a href="#org7789a92">Cross-Entropy Loss</a></li>
<li><a href="#orga03c441">Backpropagation</a></li>
<li><a href="#org1c2ad3f">Gradient descent</a></li>
</ul>
</li>
<li><a href="#orgacbf6ca">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org80c300e">
<h2 id="org80c300e">Beginning</h2>
<div class="outline-text-2" id="text-org80c300e">
<p>Previously we looked at <a href="../word-embeddings-data-preparation/">preparing the data</a> and how to set up the <a href="../introducing-the-cbow-model/">CBOW Model</a>, now we'll look at training the model.</p>
</div>
<div class="outline-3" id="outline-container-org5126391">
<h3 id="org5126391">Imports</h3>
<div class="outline-text-3" id="text-org5126391">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">be_true</span><span class="p">,</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org069baa5">
<h3 id="org069baa5">Functions from Previous Posts</h3>
<div class="outline-text-3" id="text-org069baa5"></div>
<div class="outline-4" id="outline-container-orgfa2f344">
<h4 id="orgfa2f344">Data Preparation Functions</h4>
<div class="outline-text-4" id="text-orgfa2f344">
<p>These were previously defined in <a href="../word-embeddings-data-preparation/">Word Embeddings: Data Preparation</a> post.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">window_generator</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">"""Generates windows of words</span>

<span class="sd">    Args:</span>
<span class="sd">     words: cleaned tokens</span>
<span class="sd">     half_window: number of words in the half-window</span>

<span class="sd">    Yields:</span>
<span class="sd">     the next window</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">center_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">half_window</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">center_index</span><span class="p">]</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">)</span> <span class="p">:</span> <span class="n">center_index</span><span class="p">]</span>
                         <span class="o">+</span> <span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):(</span><span class="n">center_index</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>
        <span class="k">yield</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">index_word_maps</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Creates index to word mappings</span>

<span class="sd">    The index is based on sorted unique tokens in the data</span>

<span class="sd">    Args:</span>
<span class="sd">       data: the data you want to pull from</span>

<span class="sd">    Returns:</span>
<span class="sd">       word2Ind: returns dictionary mapping the word to its index</span>
<span class="sd">       Ind2Word: returns dictionary mapping the index to its word</span>
<span class="sd">    """</span>
    <span class="n">words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

    <span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="n">index_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_word</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create a one-hot-encoded vector</span>

<span class="sd">    Args:</span>
<span class="sd">     word: the word from the corpus that we're encoding</span>
<span class="sd">     word_to_index: map of the word to the index</span>
<span class="sd">     vocabulary_size: the size of the vocabulary</span>

<span class="sd">    Returns:</span>
<span class="sd">     vector with all zeros except where the word is</span>
<span class="sd">    """</span>
    <span class="n">one_hot_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
    <span class="n">one_hot_vector</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">one_hot_vector</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">def</span> <span class="nf">context_words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                            <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create vector with the mean of the one-hot-vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to covert to one-hot vectors</span>
<span class="sd">     word_to_index: dict mapping word to index</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="n">context_words_vectors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROWS</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">training_example_generator</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="sd">"""generates training examples</span>

<span class="sd">    Args:</span>
<span class="sd">     words: source of words</span>
<span class="sd">     half_window: half the window size</span>
<span class="sd">     word_to_index: dict with word to index mapping</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="n">window_generator</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">context_words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">),</span>
               <span class="n">word_to_one_hot_vector</span><span class="p">(</span>
                   <span class="n">center_word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">))</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4219a3f">
<h4 id="org4219a3f">Activation Functions</h4>
<div class="outline-text-4" id="text-org4219a3f">
<p>These functions were defined in the <a href="../introducing-the-cbow-model/">Introducing the CBOW Model</a> post.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Get the ReLU for the input array</span>

<span class="sd">    Args:</span>
<span class="sd">     z: an array of numbers</span>

<span class="sd">    Returns:</span>
<span class="sd">     ReLU of z</span>
<span class="sd">    """</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">result</span><span class="p">[</span><span class="n">result</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate Softmax for the input</span>

<span class="sd">    Args:</span>
<span class="sd">     v: array of values</span>

<span class="sd">    Returns:</span>
<span class="sd">     array of probabilities</span>
<span class="sd">    """</span>
    <span class="n">e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">sum_e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">e_z</span> <span class="o">/</span> <span class="n">sum_e_z</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7d7f8b2">
<h2 id="org7d7f8b2">Word Embeddings: Training the CBOW model</h2>
<div class="outline-text-2" id="text-org7d7f8b2">
<p>In previous lecture notebooks you saw how to prepare data before feeding it to a continuous bag-of-words model, the model itself, its architecture and activation functions. This notebook will walk you through:</p>
<ul class="org-ul">
<li>Forward propagation.</li>
<li>Cross-entropy loss.</li>
<li>Backpropagation.</li>
<li>Gradient descent.</li>
</ul>
<p>Which are concepts necessary to understand how the training of the model works.</p>
</div>
<div class="outline-3" id="outline-container-org93ab497">
<h3 id="org93ab497">Neural Network Initialization</h3>
<div class="outline-text-3" id="text-org93ab497">
<p>Let's dive into the neural network itself, which is shown below with all the dimensions and formulas you'll need.</p>
<p>Set <i>N</i> equal to 3. Remember that <i>N</i> is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.</p>
<p>Also set <i>V</i> equal to 5, which is the size of the vocabulary we have used so far.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Define the size of the word embedding vectors and save it in the variable 'N'</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Define V. Remember this was the size of the vocabulary in the previous lecture notebooks</span>
<span class="n">V</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org3ce8d2b">
<h3 id="org3ce8d2b">Initialization of the weights and biases</h3>
<div class="outline-text-3" id="text-org3ce8d2b">
<p>Before you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.</p>
<p>In the assignment you will implement a function to do this yourself using <code>numpy.random.rand</code>. In this notebook, we've pre-populated these matrices and vectors for you.</p>
</div>
<div class="outline-4" id="outline-container-orge8d692e">
<h4 id="orge8d692e">Define the first matrix of weights</h4>
<div class="outline-text-4" id="text-orge8d692e">
<div class="highlight">
<pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.41687358</span><span class="p">,</span>  <span class="mf">0.08854191</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23495225</span><span class="p">,</span>  <span class="mf">0.28320538</span><span class="p">,</span>  <span class="mf">0.41800106</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.32735501</span><span class="p">,</span>  <span class="mf">0.22795148</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23951958</span><span class="p">,</span>  <span class="mf">0.4117634</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.23924344</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.26637602</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23846886</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.37770863</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11399446</span><span class="p">,</span>  <span class="mf">0.34008124</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org42d4226">
<h4 id="org42d4226">Define the second matrix of weights</h4>
<div class="outline-text-4" id="text-org42d4226">
<div class="highlight">
<pre><span></span><span class="n">W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.22182064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43008631</span><span class="p">,</span>  <span class="mf">0.13310965</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.08476603</span><span class="p">,</span>  <span class="mf">0.08123194</span><span class="p">,</span>  <span class="mf">0.1772054</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.1871551</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.06107263</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1790735</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.07055222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02015138</span><span class="p">,</span>  <span class="mf">0.36107434</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.33480474</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.39423389</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43959196</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge366336">
<h4 id="orge366336">Define the first vector of biases</h4>
<div class="outline-text-4" id="text-orge366336">
<div class="highlight">
<pre><span></span><span class="n">b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.09688219</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.29239497</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.27364426</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org0b60557">
<h4 id="org0b60557">Define the second vector of biases</h4>
<div class="outline-text-4" id="text-org0b60557">
<div class="highlight">
<pre><span></span><span class="n">b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.0352008</span> <span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.36393384</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.12775555</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.34802326</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.07017815</span><span class="p">]])</span>
</pre></div>
<p><b>Check that the dimensions of these matrices are correct.</b></p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'V (vocabulary size): </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'N (embedding size / size of the hidden layer): </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of W1: </span><span class="si">{</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (NxV)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of b1: </span><span class="si">{</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Nx1)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of W2: </span><span class="si">{</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (VxN)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of b2: </span><span class="si">{</span><span class="n">b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Vx1)'</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example">
V (vocabulary size): 5
N (embedding size / size of the hidden layer): 3
size of W1: (3, 5) (NxV)
size of b1: (3, 1) (Nx1)
size of W2: (5, 3) (VxN)
size of b2: (5, 1) (Vx1)
</pre>
<p>Before moving forward, you will need some functions and variables defined in previous notebooks. They can be found next. Be sure you understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgcf7339f">
<h4 id="orgcf7339f">Define the tokenized version of the corpus</h4>
<div class="outline-text-4" id="text-orgcf7339f">
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org11a1223">
<h4 id="org11a1223">Get 'word_to_index' and 'Ind2word' dictionaries for the tokenized corpus</h4>
<div class="outline-text-4" id="text-org11a1223">
<div class="highlight">
<pre><span></span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_word</span> <span class="o">=</span> <span class="n">index_word_maps</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org9f9ce10">
<h3 id="org9f9ce10">The First Training Example</h3>
<div class="outline-text-3" id="text-org9f9ce10">
<p>Run the next cells to get the first training example, made of the vector representing the context words "i am because i", and the target which is the one-hot vector representing the center word "happy".</p>
<div class="highlight">
<pre><span></span><span class="n">training_examples</span> <span class="o">=</span> <span class="n">training_example_generator</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span>
<span class="n">x_array</span><span class="p">,</span> <span class="n">y_array</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">training_examples</span><span class="p">)</span>
</pre></div>
<p>In this notebook <code>next</code> is used because you will only be performing one iteration of training. In this week's assignment with the full training over several iterations you'll use regular <code>for</code> loops with the iterator that supplies the training examples.</p>
<p>The vector representing the context words, which will be fed into the neural network, is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_array</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.25 0.25 0.   0.5  0.  ]
</pre>
<p>The one-hot vector representing the center word to be predicted is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_array</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 1. 0. 0.]
</pre>
<p>Now convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook.</p>
<div class="highlight">
<pre><span></span> <span class="c1"># Copy vector</span>
 <span class="n">x</span> <span class="o">=</span> <span class="n">x_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

 <span class="c1"># Reshape it</span>
 <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

 <span class="c1"># Print it</span>
 <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'x:</span><span class="se">\n</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

 <span class="c1"># Copy vector</span>
 <span class="n">y</span> <span class="o">=</span> <span class="n">y_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

 <span class="c1"># Reshape it</span>
 <span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

 <span class="c1"># Print it</span>
 <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'y:</span><span class="se">\n</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
x:
[[0.25]
 [0.25]
 [0.  ]
 [0.5 ]
 [0.  ]]

y:
[[0.]
 [0.]
 [1.]
 [0.]
 [0.]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org47aa186">
<h3 id="org47aa186">Forward Propagation</h3>
<div class="outline-text-3" id="text-org47aa186"></div>
<div class="outline-4" id="outline-container-orgda8737c">
<h4 id="orgda8737c">The Hidden Layer</h4>
<div class="outline-text-4" id="text-orgda8737c">
<p>Now that you have initialized all the variables that you need for forward propagation, you can calculate the values of the hidden layer using the following formulas:</p>
\begin{align} \mathbf{z_1} = \mathbf{W_1}\mathbf{x} + \mathbf{b_1} \tag{1} \\ \mathbf{h} = \mathrm{ReLU}(\mathbf{z_1}) \tag{2} \\ \end{align}
<p>First, you can calculate the value of \(\mathbf{z_1}\).</p>
<p>Compute z1 (values of first hidden layer before applying the ReLU function)</p>
<div class="highlight">
<pre><span></span><span class="n">z1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
</pre></div>
<p>As expected you get an \(N\) by 1 matrix, or column vector with <i>N</i> elements, where <i>N</i> is equal to the embedding size, which is 3 in this example.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.36483875]
 [ 0.63710329]
 [-0.3236647 ]]
</pre>
<p>You can now take the ReLU of \(\mathbf{z_1}\) to get \(\mathbf{h}\), the vector with the values of the hidden layer.</p>
<p>Compute h (z1 after applying ReLU function)</p>
<div class="highlight">
<pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.36483875]
 [0.63710329]
 [0.        ]]
</pre>
<p>Applying ReLU means that the negative element of \(\mathbf{z_1}\) has been replaced with a zero.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgf4fe83b">
<h4 id="orgf4fe83b">The Output Layer</h4>
<div class="outline-text-4" id="text-orgf4fe83b">
<p>Here are the formulas you need to calculate the values of the output layer, represented by the vector \(\mathbf{\hat y}\):</p>
\begin{align} \mathbf{z_2} &amp;= \mathbf{W_2}\mathbf{h} + \mathbf{b_2} \tag{3} \\ \mathbf{\hat y} &amp;= \mathrm{softmax}(\mathbf{z_2}) \tag{4} \\ \end{align}
<p><b>First, calculate \(\mathbf{z_2}\).</b></p>
<p>Compute z2 (values of the output layer before applying the softmax function)</p>
<div class="highlight">
<pre><span></span><span class="n">z2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.31973737</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.28125477</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.09838369</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.33512159</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.19919612</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[-0.31973737]
 [-0.28125477]
 [-0.09838369]
 [-0.33512159]
 [-0.19919612]]
</pre>
<p>This is a <i>V</i> by 1 matrix, where <i>V</i> is the size of the vocabulary, which is 5 in this example.</p>
<p><b>Now calculate the value of \(\mathbf{\hat y}\).</b></p>
<p>Compute y_hat (z2 after applying softmax function)</p>
<div class="highlight">
<pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.18519074</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.19245626</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.23107446</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.18236353</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.20891502</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.18519074]
 [0.19245626]
 [0.23107446]
 [0.18236353]
 [0.20891502]]
</pre>
<p>As you've performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.</p>
<p><b>That being said, what word did the neural network predict?</b></p>
<div class="highlight">
<pre><span></span><span class="n">prediction</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The predicted word at index </span><span class="si">{</span><span class="n">prediction</span><span class="si">}</span><span class="s2"> is '</span><span class="si">{</span><span class="n">index_to_word</span><span class="p">[</span><span class="n">prediction</span><span class="p">]</span><span class="si">}</span><span class="s2">'."</span><span class="p">)</span>
</pre></div>
<pre class="example">
The predicted word at index 2 is 'happy'.
</pre>
<p>The neural network predicted the word "happy": the largest element of \(\mathbf{\hat y}\) is the third one, and the third word of the vocabulary is "happy".</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7789a92">
<h3 id="org7789a92">Cross-Entropy Loss</h3>
<div class="outline-text-3" id="text-org7789a92">
<p>Now that you have the network's prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.</p>
<p>Remember that you are working on a single training example, not on a batch of examples, which is why you are using <b>loss</b> and not <b>cost</b>, which is the generalized form of loss.</p>
<p>First let's recall what the prediction was.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.18519074]
 [0.19245626]
 [0.23107446]
 [0.18236353]
 [0.20891502]]
</pre>
<p>And the actual target value is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.]
 [0.]
 [1.]
 [0.]
 [0.]]
</pre>
<p>The formula for cross-entropy loss is:</p>
<p>\[ J=-\sum\limits_{k=1}^{V}y_k\log{\hat{y}_k} \tag{6} \]</p>
<p><b>Try implementing the cross-entropy loss function so you get more familiar working with numpy.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                       <span class="n">y_actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate cross-entropy loss  for the prediction</span>

<span class="sd">    Args:</span>
<span class="sd">     y_predicted: what our model predicted</span>
<span class="sd">     y_actual: the known labels</span>

<span class="sd">    Returns:</span>
<span class="sd">     cross-entropy loss for y_predicted</span>
<span class="sd">    """</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_actual</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
<p>Hint 1:</p>
<p>To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, you can simply use the &lt;code&gt;*&lt;/code&gt; operator.</p>
<p>Hint 2:</p>
<p>Once you have a vector equal to the element-wise multiplication of <code>y</code> and <code>y_hat</code>, you can use <code>numpy.sum</code> to calculate the sum of the elements of this vector.</p>
<p><b>Now use this function to calculate the loss with the actual values of \(\mathbf{y}\) and \(\mathbf{\hat y}\).</b></p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">1.4650152923611106</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
1.465
</pre>
<p>This value is neither good nor bad, which is expected as the neural network hasn't learned anything yet.</p>
<p>The actual learning will start during the next phase: backpropagation.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orga03c441">
<h3 id="orga03c441">Backpropagation</h3>
<div class="outline-text-3" id="text-orga03c441">
<p>The formulas that you will implement for backpropagation are the following.</p>
\begin{align} \frac{\partial J}{\partial \mathbf{W_1}} &amp;= \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right )\mathbf{x}^\top \tag{7}\\ \frac{\partial J}{\partial \mathbf{W_2}} &amp;= (\mathbf{\hat{y}} - \mathbf{y})\mathbf{h^\top} \tag{8}\\ \frac{\partial J}{\partial \mathbf{b_1}} &amp;= \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right ) \tag{9}\\ \frac{\partial J}{\partial \mathbf{b_2}} &amp;= \mathbf{\hat{y}} - \mathbf{y} \tag{10} \end{align}
<p><b>*Note:</b> these formulas are slightly simplified compared to the ones in the lecture as you're working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you'll be implementing the latter.</p>
<p>Let's start with an easy one.</p>
<p><b>Calculate the partial derivative of the loss function with respect to \(\mathbf{b_2}\), and store the result in <code>grad_b2</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{b_2}} = \mathbf{\hat{y}} - \mathbf{y} \tag{10} \]</p>
<p>Compute vector with partial derivatives of loss function with respect to b2</p>
<div class="highlight">
<pre><span></span><span class="n">grad_b2</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_b2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.18519074</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.19245626</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.76892554</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.18236353</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.20891502</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_b2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.18519074]
 [ 0.19245626]
 [-0.76892554]
 [ 0.18236353]
 [ 0.20891502]]
</pre>
<p><b>Next, calculate the partial derivative of the loss function with respect to \(\mathbf{W_2}\), and store the result in <code>grad_W2</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{W_2}} = (\mathbf{\hat{y}} - \mathbf{y})\mathbf{h^\top} \tag{8} \]</p>
<p>Hint: use <code>.T</code> to get a transposed matrix, e.g. <code>h.T</code> returns \(\mathbf{h^\top}\).</p>
<p>Compute matrix with partial derivatives of loss function with respect to W2.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_W2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.06756476</span><span class="p">,</span>  <span class="mf">0.11798563</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.0702155</span> <span class="p">,</span>  <span class="mf">0.12261452</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.28053384</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.48988499</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.06653328</span><span class="p">,</span>  <span class="mf">0.1161844</span> <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.07622029</span><span class="p">,</span>  <span class="mf">0.13310045</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_W2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.06756476  0.11798563  0.        ]
 [ 0.0702155   0.12261452  0.        ]
 [-0.28053384 -0.48988499  0.        ]
 [ 0.06653328  0.1161844   0.        ]
 [ 0.07622029  0.13310045  0.        ]]
</pre>
<p><b>Now calculate the partial derivative with respect to \(\mathbf{b_1}\) and store the result in <code>grad_b1</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{b_1}} = \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right ) \tag{9} \]</p>
<p>Compute vector with partial derivatives of loss function with respect to b1.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_b1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_b1</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.17045858</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_b1</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.        ]
 [0.        ]
 [0.17045858]]
</pre>
<p><b>Finally, calculate the partial derivative of the loss with respect to \(\mathbf{W_1}\), and store it in <code>grad_W1</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{W_1}} = \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right )\mathbf{x}^\top \tag{7} \] Compute matrix with partial derivatives of loss function with respect to W1.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">relu</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)),</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_W1</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.04261464</span><span class="p">,</span> <span class="mf">0.04261464</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.08522929</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_W1</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.04261464 0.04261464 0.         0.08522929 0.        ]]
</pre>
<p>Before moving on to gradient descent, double-check that all the matrices have the expected dimensions.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'V (vocabulary size): </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'N (embedding size / size of the hidden layer): </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_W1: </span><span class="si">{</span><span class="n">grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (NxV)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_b1: </span><span class="si">{</span><span class="n">grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Nx1)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_W2: </span><span class="si">{</span><span class="n">grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (VxN)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_b2: </span><span class="si">{</span><span class="n">grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Vx1)'</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example">
V (vocabulary size): 5
N (embedding size / size of the hidden layer): 3
size of grad_W1: (3, 5) (NxV)
size of grad_b1: (3, 1) (Nx1)
size of grad_W2: (5, 3) (VxN)
size of grad_b2: (5, 1) (Vx1)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org1c2ad3f">
<h3 id="org1c2ad3f">Gradient descent</h3>
<div class="outline-text-3" id="text-org1c2ad3f">
<p>During the gradient descent phase, you will update the weights and biases by subtracting \(\alpha\) times the gradient from the original matrices and vectors, using the following formulas.</p>
\begin{align} \mathbf{W_1} &amp;\gets \mathbf{W_1} - \alpha \frac{\partial J}{\partial \mathbf{W_1}} \tag{11}\\ \mathbf{W_2} &amp;\gets \mathbf{W_2} - \alpha \frac{\partial J}{\partial \mathbf{W_2}} \tag{12}\\ \mathbf{b_1} &amp;\gets \mathbf{b_1} - \alpha \frac{\partial J}{\partial \mathbf{b_1}} \tag{13}\\ \mathbf{b_2} &amp;\gets \mathbf{b_2} - \alpha \frac{\partial J}{\partial \mathbf{b_2}} \tag{14}\\ \end{align}
<p>First, let set a value for \(\alpha\).</p>
<div class="highlight">
<pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.03</span>
</pre></div>
<p>The updated weight matrix \(\mathbf{W_1}\) will be:</p>
<div class="highlight">
<pre><span></span><span class="n">W1_new</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
</pre></div>
<p>Let's compare the previous and new values of \(\mathbf{W_1}\):</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'old value of W1:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'new value of W1:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W1_new</span><span class="p">)</span>
</pre></div>
<pre class="example">
old value of W1:
[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]
 [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]
 [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]

new value of W1:
[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]
 [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]
 [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]
</pre>
<p>The difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.</p>
<p><b>Now calculate the new values of \(\mathbf{W_2}\) (to be stored in <code>W2_new</code>), \(\mathbf{b_1}\) (in <code>b1_new</code>), and \(\mathbf{b_2}\) (in <code>b2_new</code>).</b></p>
\begin{align} \mathbf{W_2} &amp;\gets \mathbf{W_2} - \alpha \frac{\partial J}{\partial \mathbf{W_2}} \tag{12}\\ \mathbf{b_1} &amp;\gets \mathbf{b_1} - \alpha \frac{\partial J}{\partial \mathbf{b_1}} \tag{13}\\ \mathbf{b_2} &amp;\gets \mathbf{b_2} - \alpha \frac{\partial J}{\partial \mathbf{b_2}} \tag{14}\\ \end{align}
<p>Compute updated W2.</p>
<div class="highlight">
<pre><span></span><span class="n">W2_new</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
</pre></div>
<p>Compute updated b1.</p>
<div class="highlight">
<pre><span></span><span class="n">b1_new</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
</pre></div>
<p>Compute updated b2.</p>
<div class="highlight">
<pre><span></span><span class="n">b2_new</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'W2_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W2_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b1_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b1_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b2_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b2_new</span><span class="p">)</span>

<span class="n">w2_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span><span class="o">-</span><span class="mf">0.22384758</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43362588</span><span class="p">,</span>  <span class="mf">0.13310965</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.08265956</span><span class="p">,</span>  <span class="mf">0.0775535</span> <span class="p">,</span>  <span class="mf">0.1772054</span> <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.19557112</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04637608</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1790735</span> <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.06855622</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02363691</span><span class="p">,</span>  <span class="mf">0.36107434</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.33251813</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3982269</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.43959196</span><span class="p">]])</span>

<span class="n">b1_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span> <span class="mf">0.09688219</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.29239497</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.27875802</span><span class="p">]])</span>

<span class="n">b2_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span> <span class="mf">0.02964508</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.36970753</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.10468778</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.35349417</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.0764456</span> <span class="p">]]</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">actual</span><span class="p">,</span> <span class="n">expected</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">((</span><span class="n">W2_new</span><span class="p">,</span> <span class="n">b1_new</span><span class="p">,</span> <span class="n">b2_new</span><span class="p">),</span> <span class="p">(</span><span class="n">w2_expected</span><span class="p">,</span> <span class="n">b1_expected</span><span class="p">,</span> <span class="n">b2_expected</span><span class="p">)):</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
W2_new
[[-0.22384758 -0.43362588  0.13310965]
 [ 0.08265956  0.0775535   0.1772054 ]
 [ 0.19557112 -0.04637608 -0.1790735 ]
 [ 0.06855622 -0.02363691  0.36107434]
 [ 0.33251813 -0.3982269  -0.43959196]]

b1_new
[[ 0.09688219]
 [ 0.29239497]
 [-0.27875802]]

b2_new
[[ 0.02964508]
 [-0.36970753]
 [-0.10468778]
 [-0.35349417]
 [-0.0764456 ]]
</pre>
<p>Congratulations, you have completed one iteration of training using one training example!</p>
<p>You'll need many more iterations to fully train the neural network, and you can optimize the learning process by training on batches of examples, as described in the lecture. You will get to do this during this week's assignment.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgacbf6ca">
<h2 id="orgacbf6ca">End</h2>
<div class="outline-text-2" id="text-orgacbf6ca">
<p>Now that we know how to train the CBOW Model, we'll move on to <a href="../extracting-word-embeddings/">extracting word embeddings</a> from the model.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/cbow/" rel="tag">cbow</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/word-embeddings/" rel="tag">word embeddings</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../introducing-the-cbow-model/" rel="prev" title="Introducing the CBOW Model">Previous post</a></li>
<li class="next"><a href="../extracting-word-embeddings/" rel="next" title="Extracting Word Embeddings">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
