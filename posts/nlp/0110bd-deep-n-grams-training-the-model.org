#+BEGIN_COMMENT
.. title: Deep N-Grams: Training the Model
.. slug: deep-n-grams-training-the-model
.. date: 2021-01-05 16:48:29 UTC-08:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT
* Training The Model
  - {{% lancelot title="First Post" %}}deep-n-grams{{% /lancelot %}}
  - {{% lancelot title="Previous Post" %}}deep-n-grams-creating-the-model{{% /lancelot %}}
  - {{% lancelot title="Next Post" %}}deep-n-grams-evaluating-the-model{{% /lancelot %}}
#+begin_example python    
# # Part 3: Training
# 
# Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a `gpu` or `cpu`. You also have to feed in a built model. Before, going into the training, we re-introduce the `TrainTask` and `EvalTask` abstractions from the last week's assignment.
# 
# To train a model on a task, Trax defines an abstraction `trax.supervised.training.TrainTask` which packages the train data, loss and optimizer (among other things) together into an object.
# 
# Similarly to evaluate a model, Trax defines an abstraction `trax.supervised.training.EvalTask` which packages the eval data and metrics (among other things) into another object.
# 
# The final piece tying things together is the `trax.supervised.training.Loop` abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.
# Using `training.Loop` will save you a lot of code compared to always writing the training loop by hand, like you did in courses 1 and 2. More importantly, you are less likely to have a bug in that code that would ruin your training.

# In[ ]:


batch_size = 32
max_length = 64


# An `epoch` is traditionally defined as one pass through the dataset.
# 
# Since the dataset was divided in `batches` you need several `steps` (gradient evaluations) in order to complete an `epoch`. So, one `epoch` corresponds to the number of examples in a `batch` times the number of `steps`. In short, in each `epoch` you go over all the dataset. 
# 
# The `max_length` variable defines the maximum length of lines to be used in training our data, lines longer that that length are discarded. 
# 
# Below is a function and results that indicate how many lines conform to our criteria of maximum length of a sentence in the entire dataset and how many `steps` are required in order to cover the entire dataset which in turn corresponds to an `epoch`.

# In[ ]:


def n_used_lines(lines, max_length):
    '''
    Args: 
    lines: all lines of text an array of lines
    max_length - max_length of a line in order to be considered an int
    Return:
    n_lines -number of efective examples
    '''

    n_lines = 0
    for l in lines:
        if len(l) <= max_length:
            n_lines += 1
    return n_lines

num_used_lines = n_used_lines(lines, 32)
print('Number of used lines from the dataset:', num_used_lines)
print('Batch size (a power of 2):', int(batch_size))
steps_per_epoch = int(num_used_lines/batch_size)
print('Number of steps to cover one epoch:', steps_per_epoch)


# **Expected output:** 
# 
# Number of used lines from the dataset: 25881
# 
# Batch size (a power of 2): 32
# 
# Number of steps to cover one epoch: 808

# <a name='3.1'></a>
# ### 3.1 Training the model
# 
# You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. 
# 
# <a name='ex04'></a>
# ### Exercise 04
# 
# **Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do:
# 
# - Create a `trax.supervised.trainer.TrainTask` object, this encapsulates the aspects of the dataset and the problem at hand:
#     - labeled_data = the labeled data that we want to *train* on.
#     - loss_fn = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html?highlight=CrossEntropyLoss#trax.layers.metrics.CrossEntropyLoss)
#     - optimizer = [trax.optimizers.Adam()](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html?highlight=Adam#trax.optimizers.adam.Adam) with learning rate = 0.0005
# 
# - Create a `trax.supervised.trainer.EvalTask` object, this encapsulates aspects of evaluating the model:
#     - labeled_data = the labeled data that we want to *evaluate* on.
#     - metrics = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss) and [tl.Accuracy()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy)
#     - How frequently we want to evaluate and checkpoint the model.
# 
# - Create a `trax.supervised.trainer.Loop` object, this encapsulates the following:
#     - The previously created `TrainTask` and `EvalTask` objects.
#     - the training model = [GRULM](#ex03)
#     - optionally the evaluation model, if different from the training model. NOTE: in presence of Dropout etc we usually want the evaluation model to behave slightly differently than the training model.
# 
# You will be using a cross entropy loss, with Adam optimizer. Please read the [trax](https://trax-ml.readthedocs.io/en/latest/index.html) documentation to get a full understanding. Make sure you use the number of steps provided as a parameter to train for the desired number of steps.
# 
# **NOTE:** Don't forget to wrap the data generator in `itertools.cycle` to iterate on it for multiple epochs.

# In[ ]:


from trax.supervised import training

# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: train_model
def train_model(model, data_generator, batch_size=32, max_length=64, lines=lines, eval_lines=eval_lines, n_steps=1, output_dir='model/'): 
    """Function that trains the model

    Args:
        model (trax.layers.combinators.Serial): GRU model.
        data_generator (function): Data generator function.
        batch_size (int, optional): Number of lines per batch. Defaults to 32.
        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.
        lines (list, optional): List of lines to use for training. Defaults to lines.
        eval_lines (list, optional): List of lines to use for evaluation. Defaults to eval_lines.
        n_steps (int, optional): Number of steps to train. Defaults to 1.
        output_dir (str, optional): Relative path of directory to save model. Defaults to "model/".

    Returns:
        trax.supervised.training.Loop: Training loop for the model.
    """
    
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    bare_train_generator = None
    infinite_train_generator = None
    
    bare_eval_generator = None
    infinite_eval_generator = None
   
    train_task = training.TrainTask(
        labeled_data=None, # Use infinite train data generator
        loss_layer=None,   # Don't forget to instantiate this object
        optimizer=None     # Don't forget to add the learning rate parameter
    )

    eval_task = training.EvalTask(
        labeled_data=None,    # Use infinite eval data generator
        metrics=[None, None], # Don't forget to instantiate these objects
        n_eval_batches=3      # For better evaluation accuracy in reasonable time
    )
    
    training_loop = training.Loop(model,
                                  train_task,
                                  eval_task=eval_task,
                                  output_dir=output_dir)

    training_loop.run(n_steps=None)
    
    ### END CODE HERE ###
    
    # We return this because it contains a handle to the model, which has the weights etc.
    return training_loop


# In[ ]:


# Train the model 1 step and keep the `trax.supervised.training.Loop` object.
training_loop = train_model(GRULM(), data_generator)


# The model was only trained for 1 step due to the constraints of this environment. Even on a GPU accelerated environment it will take many hours for it to achieve a good level of accuracy. For the rest of the assignment you will be using a pretrained model but now you should understand how the training can be done using Trax.

# <a name='4'></a>
# # Part 4:  Evaluation  
# <a name='4.1'></a>
#+end_example
