#+BEGIN_COMMENT
.. title: Auto-Complete: Pre-Process the Data II
.. slug: auto-complete-pre-process-the-data-ii
.. date: 2020-12-04 15:12:52 UTC-08:00
.. tags: nlp,auto-complete,n-gram
.. category: NLP
.. link: 
.. description: Adding counts to the twitter auto-complete data for the n-gram model.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-628b96bb-9fe2-4219-af43-264f81238d87-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  This is the third post in a series that begins with {{% doc %}}auto-complete{{% /doc %}}. In the {{% lancelot title="previous entry" %}}auto-complete-pre-process-the-data-i{{% /lancelot %}} we did some basic preprocessing to transform the raw tweet data into a form closer to what we wanted. In this post we'll add some counts to the data so that we can use it to build our model.
* End
  Now that we have the data in the basic form we want we'll move on to building the {{% lancelot title="N-Gram Language Model" %}}auto-complete-the-n-gram-model{{% /lancelot %}}.
* Raw
#+begin_example
# <a name='1'></a>
# ### Exercise 04
# 
# You won't use all the tokens (words) appearing in the data for training.  Instead, you will use the more frequently used words.  
# - You will focus on the words that appear at least N times in the data.
# - First count how many times each word appears in the data.
# 
# You will need a double for-loop, one for sentences and the other for tokens within a sentence.
# 

# <details>    
# <summary>
#     <font size="3" color="darkgreen"><b>Hints</b></font>
# </summary>
# <p>
# <ul>
#     <li>If you decide to import and use defaultdict, remember to cast the dictionary back to a regular 'dict' before returning it. </li>
# </ul>
# </p>
# 

# In[ ]:


# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
### GRADED_FUNCTION: count_words ###
def count_words(tokenized_sentences):
    """
    Count the number of word appearence in the tokenized sentences
    
    Args:
        tokenized_sentences: List of lists of strings
    
    Returns:
        dict that maps word (str) to the frequency (int)
    """
        
    word_counts = {}
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    
    # Loop through each sentence
    for sentence in None: # complete this line
        
        # Go through each token in the sentence
        for token in None: # complete this line

            # If the token is not in the dictionary yet, set the count to 1
            if None: # complete this line
                word_counts[token] = None
            
            # If the token is already in the dictionary, increment the count by 1
            else:
                word_counts[token] += None

    ### END CODE HERE ###
    
    return word_counts


# In[ ]:


# test your code
tokenized_sentences = [['sky', 'is', 'blue', '.'],
                       ['leaves', 'are', 'green', '.'],
                       ['roses', 'are', 'red', '.']]
count_words(tokenized_sentences)


# ##### Expected output
# 
# Note that the order may differ.
# 
# ```CPP
# {'sky': 1,
#  'is': 1,
#  'blue': 1,
#  '.': 3,
#  'leaves': 1,
#  'are': 2,
#  'green': 1,
#  'roses': 1,
#  'red': 1}
# ```

# ### Handling 'Out of Vocabulary' words
# 
# If your model is performing autocomplete, but encounters a word that it never saw during training, it won't have an input word to help it determine the next word to suggest. The model will not be able to predict the next word because there are no counts for the current word. 
# - This 'new' word is called an 'unknown word', or <b>out of vocabulary (OOV)</b> words.
# - The percentage of unknown words in the test set is called the <b> OOV </b> rate. 
# 
# To handle unknown words during prediction, use a special token to represent all unknown words 'unk'. 
# - Modify the training data so that it has some 'unknown' words to train on.
# - Words to convert into "unknown" words are those that do not occur very frequently in the training set.
# - Create a list of the most frequent words in the training set, called the <b> closed vocabulary </b>. 
# - Convert all the other words that are not part of the closed vocabulary to the token 'unk'. 

# <a name='ex-05'></a>
# ### Exercise 05
# 
# You will now create a function that takes in a text document and a threshold `count_threshold`.
# - Any word whose count is greater than or equal to the threshold `count_threshold` is kept in the closed vocabulary.
# - Returns the word closed vocabulary list.  

# In[ ]:


# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
### GRADED_FUNCTION: get_words_with_nplus_frequency ###
def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):
    """
    Find the words that appear N times or more
    
    Args:
        tokenized_sentences: List of lists of sentences
        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.
    
    Returns:
        List of words that appear N times or more
    """
    # Initialize an empty list to contain the words that
    # appear at least 'minimum_freq' times.
    closed_vocab = []
    
    # Get the word couts of the tokenized sentences
    # Use the function that you defined earlier to count the words
    word_counts = count_words(tokenized_sentences)
    
    ### START CODE HERE (Replace instances of 'None' with your code) ###

    # for each word and its count
    for word, cnt in None: # complete this line
        
        # check that the word's count
        # is at least as great as the minimum count
        if None:
            
            # append the word to the list
            None
    ### END CODE HERE ###
    
    return closed_vocab


# In[ ]:


# test your code
tokenized_sentences = [['sky', 'is', 'blue', '.'],
                       ['leaves', 'are', 'green', '.'],
                       ['roses', 'are', 'red', '.']]
tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)
print(f"Closed vocabulary:")
print(tmp_closed_vocab)


# ##### Expected output
# 
# ```CPP
# Closed vocabulary:
# ['.', 'are']
# ```

# <a name='ex-06'></a>
# ### Exercise 06
# 
# The words that appear `count_threshold` times or more are in the closed vocabulary. 
# - All other words are regarded as `unknown`.
# - Replace words not in the closed vocabulary with the token `<unk>`.

# In[ ]:


# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
### GRADED_FUNCTION: replace_oov_words_by_unk ###
def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token="<unk>"):
    """
    Replace words not in the given vocabulary with '<unk>' token.
    
    Args:
        tokenized_sentences: List of lists of strings
        vocabulary: List of strings that we will use
        unknown_token: A string representing unknown (out-of-vocabulary) words
    
    Returns:
        List of lists of strings, with words not in the vocabulary replaced
    """
    
    # Place vocabulary into a set for faster search
    vocabulary = set(vocabulary)
    
    # Initialize a list that will hold the sentences
    # after less frequent words are replaced by the unknown token
    replaced_tokenized_sentences = []
    
    # Go through each sentence
    for sentence in tokenized_sentences:
        
        # Initialize the list that will contain
        # a single sentence with "unknown_token" replacements
        replaced_sentence = []
        ### START CODE HERE (Replace instances of 'None' with your code) ###

        # for each token in the sentence
        for token in None: # complete this line
            
            # Check if the token is in the closed vocabulary
            if token in None: # complete this line
                # If so, append the word to the replaced_sentence
                None
            else:
                # otherwise, append the unknown token instead
                None
        ### END CODE HERE ###
        
        # Append the list of tokens to the list of lists
        replaced_tokenized_sentences.append(replaced_sentence)
        
    return replaced_tokenized_sentences


# In[ ]:


tokenized_sentences = [["dogs", "run"], ["cats", "sleep"]]
vocabulary = ["dogs", "sleep"]
tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)
print(f"Original sentence:")
print(tokenized_sentences)
print(f"tokenized_sentences with less frequent words converted to '<unk>':")
print(tmp_replaced_tokenized_sentences)


# ### Expected answer
# 
# ```CPP
# Original sentence:
# [['dogs', 'run'], ['cats', 'sleep']]
# tokenized_sentences with less frequent words converted to '<unk>':
# [['dogs', '<unk>'], ['<unk>', 'sleep']]
# ```

# <a name='ex-07'></a>
# ### Exercise 07
# 
# Now we are ready to process our data by combining the functions that you just implemented.
# 
# 1. Find tokens that appear at least count_threshold times in the training data.
# 1. Replace tokens that appear less than count_threshold times by "<unk\>" both for training and test data.

# In[ ]:


# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
### GRADED_FUNCTION: preprocess_data ###
def preprocess_data(train_data, test_data, count_threshold):
    """
    Preprocess data, i.e.,
        - Find tokens that appear at least N times in the training data.
        - Replace tokens that appear less than N times by "<unk>" both for training and test data.        
    Args:
        train_data, test_data: List of lists of strings.
        count_threshold: Words whose count is less than this are 
                      treated as unknown.
    
    Returns:
        Tuple of
        - training data with low frequent words replaced by "<unk>"
        - test data with low frequent words replaced by "<unk>"
        - vocabulary of words that appear n times or more in the training data
    """
    ### START CODE HERE (Replace instances of 'None' with your code) ###

    # Get the closed vocabulary using the train data
    vocabulary = None
    
    # For the train data, replace less common words with "<unk>"
    train_data_replaced = None
    
    # For the test data, replace less common words with "<unk>"
    test_data_replaced = None
    
    ### END CODE HERE ###
    return train_data_replaced, test_data_replaced, vocabulary


# In[ ]:


# test your code
tmp_train = [['sky', 'is', 'blue', '.'],
     ['leaves', 'are', 'green']]
tmp_test = [['roses', 'are', 'red', '.']]

tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, 
                                                           tmp_test, 
                                                           count_threshold = 1)

print("tmp_train_repl")
print(tmp_train_repl)
print()
print("tmp_test_repl")
print(tmp_test_repl)
print()
print("tmp_vocab")
print(tmp_vocab)


# ##### Expected outcome
# 
# ```CPP
# tmp_train_repl
# [['sky', 'is', 'blue', '.'], ['leaves', 'are', 'green']]
# 
# tmp_test_repl
# [['<unk>', 'are', '<unk>', '.']]
# 
# tmp_vocab
# ['sky', 'is', 'blue', '.', 'leaves', 'are', 'green']
# ```

# ### Preprocess the train and test data
# Run the cell below to complete the preprocessing both for training and test sets.

# In[ ]:


minimum_freq = 2
train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, 
                                                                        test_data, 
                                                                        minimum_freq)


# In[ ]:


print("First preprocessed training sample:")
print(train_data_processed[0])
print()
print("First preprocessed test sample:")
print(test_data_processed[0])
print()
print("First 10 vocabulary:")
print(vocabulary[0:10])
print()
print("Size of vocabulary:", len(vocabulary))


# ##### Expected output
# 
# ```CPP
# First preprocessed training sample:
# ['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the', 'team', 'local', 'company', 'and', 'quality', 'production']
# 
# First preprocessed test sample:
# ['that', 'picture', 'i', 'just', 'seen', 'whoa', 'dere', '!', '!', '>', '>', '>', '>', '>', '>', '>']
# 
# First 10 vocabulary:
# ['i', 'personally', 'would', 'like', 'as', 'our', 'official', 'glove', 'of', 'the']
# 
# Size of vocabulary: 14821
# ```

# You are done with the preprocessing section of the assignment.
# Objects `train_data_processed`, `test_data_processed`, and `vocabulary` will be used in the rest of the exercises.

# <a name='2'></a>
#+end_example
