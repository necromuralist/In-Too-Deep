#+BEGIN_COMMENT
.. title: Sentiment Analysis: Testing the Model
.. slug: sentiment-analysis-testing-the-model
.. date: 2020-12-23 15:52:18 UTC-08:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text

#+END_COMMENT
* Beginning
  Having trained our Deep Learning model for Sentiment Analysis {{% lancelot title="previously" %}}sentiment-analysis-training-the-model{{% /lancelot %}} we're now going to test how well it did.
* End
  So, there you have it, a Deep Learning Model for Sentiment Analysis built using Trax. Here are the prior posts in this series.

 - {{% lancelot title="Introduction" %}}sentiment-analysis-deep-learning-model{{% /lancelot %}}
 - {{% lancelot title="Loading the Data" %}}sentiment-analysis-pre-processing-the-data{{% /lancelot %}}
 - {{% lancelot title="Defining the Model" %}}sentiment-analysis-defining-the-model{{% /lancelot %}}
 - {{% lancelot title="Training the Model" %}}sentiment-analysis-training-the-model{{% /lancelot %}}
* Raw
#+begin_example
# ## 5.2  Testing your model on Validation Data
# 
# Now you will write test your model's prediction accuracy on validation data. 
# 
# This program will take in a data generator and your model. 
# - The generator allows you to get batches of data. You can use it with a `for` loop:
# 
# ```
# for batch in iterator: 
#    # do something with that batch
# ```
# 
# `batch` has dimensions `(X, Y, weights)`. 
# - Column 0 corresponds to the tweet as a tensor (input).
# - Column 1 corresponds to its target (actual label, positive or negative sentiment).
# - Column 2 corresponds to the weights associated (example weights)
# - You can feed the tweet into model and it will return the predictions for the batch. 
# 

# <a name="ex08"></a>
# ### Exercise 08
# 
# **Instructions:** 
# - Compute the accuracy over all the batches in the validation iterator. 
# - Make use of `compute_accuracy`, which you recently implemented, and return the overall accuracy.

# In[ ]:


# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: test_model
def test_model(generator, model):
    '''
    Input: 
        generator: an iterator instance that provides batches of inputs and targets
        model: a model instance 
    Output: 
        accuracy: float corresponding to the accuracy
    '''
    
    accuracy = 0.
    total_num_correct = 0
    total_num_pred = 0
    
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    for batch in generator: 
        
        # Retrieve the inputs from the batch
        inputs = None
        
        # Retrieve the targets (actual labels) from the batch
        targets = None
        
        # Retrieve the example weight.
        example_weight = None

        # Make predictions using the inputs
        pred = None
        
        # Calculate accuracy for the batch by comparing its predictions and targets
        batch_accuracy, batch_num_correct, batch_num_pred = None
        
        # Update the total number of correct predictions
        # by adding the number of correct predictions from this batch
        total_num_correct += None
        
        # Update the total number of predictions 
        # by adding the number of predictions made for the batch
        total_num_pred += None

    # Calculate accuracy over all examples
    accuracy = None
    
    ### END CODE HERE ###
    return accuracy


# In[ ]:


# DO NOT EDIT THIS CELL
# testing the accuracy of your model: this takes around 20 seconds
model = training_loop.eval_model
accuracy = test_model(test_generator(16), model)

print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )


# ##### Expected Output (Approximately)
# 
# ```CPP
# The accuracy of your model on the validation set is 0.9931
# ```

# <a name="6"></a>
# # Part 6:  Testing with your own input
# 
# Finally you will test with your own input. You will see that deepnets are more powerful than the older methods you have used before. Although you go close to 100% accuracy on the first two assignments, the task was way easier. 

# In[ ]:


# this is used to predict on your own sentnece
def predict(sentence):
    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))
    
    # Batch size 1, add dimension for batch, to work with the model
    inputs = inputs[None, :]  
    
    # predict with the model
    preds_probs = model(inputs)
    
    # Turn probabilities into categories
    preds = int(preds_probs[0, 1] > preds_probs[0, 0])
    
    sentiment = "negative"
    if preds == 1:
        sentiment = 'positive'

    return preds, sentiment


# In[ ]:


# try a positive sentence
sentence = "It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe"
tmp_pred, tmp_sentiment = predict(sentence)
print(f"The sentiment of the sentence \n***\n\"{sentence}\"\n***\nis {tmp_sentiment}.")

print()
# try a negative sentence
sentence = "I hated my day, it was the worst, I'm so sad."
tmp_pred, tmp_sentiment = predict(sentence)
print(f"The sentiment of the sentence \n***\n\"{sentence}\"\n***\nis {tmp_sentiment}.")


# Notice that the model works well even for complex sentences.

# ### On Deep Nets
# 
# Deep nets allow you to understand and capture dependencies that you would have not been able to capture with a simple linear regression, or logistic regression. 
# - It also allows you to better use pre-trained embeddings for classification and tends to generalize better.

#+end_example
