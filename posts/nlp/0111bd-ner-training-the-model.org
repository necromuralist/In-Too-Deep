#+BEGIN_COMMENT
.. title: NER: Training the Model
.. slug: ner-training-the-model
.. date: 2021-01-13 15:01:58 UTC-08:00
.. tags: lstm,rnn,nlp,ner
.. category: NLP
.. link: 
.. description: Training the NER model.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-9974ba11-9b71-4b8e-8dc9-4b5779900b41-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  - {{% lancelot title="The First Post" %}}named-entity-recognition{{% /lancelot %}}
  - {{% lancelot title="The Previous Post" %}}ner-building-the-model{{% /lancelot %}}    
  - {{% lancelot title="The Next Post" %}}ner-evaluating-the-model{{% /lancelot %}}    
* Raw
#+begin_example python
# # Part 3:  Train the Model 
# 
# This section will train your model.
# 
# Before you start, you need to create the data generators for training and validation data. It is important that you mask padding in the loss weights of your data, which can be done using the `id_to_mask` argument of `trax.supervised.inputs.add_loss_weights`.

# In[ ]:


from trax.supervised import training

rnd.seed(33)

batch_size = 64

# Create training data, mask pad id=35180 for training.
train_generator = trax.supervised.inputs.add_loss_weights(
    data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], True),
    id_to_mask=vocab['<PAD>'])

# Create validation data, mask pad id=35180 for training.
eval_generator = trax.supervised.inputs.add_loss_weights(
    data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], True),
    id_to_mask=vocab['<PAD>'])

# ### 3.1 Training the model
# 
# You will now write a function that takes in your model and trains it.
# 
# As you've seen in the previous assignments, you will first create the [TrainTask](https://trax-ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.TrainTask) and [EvalTask](https://trax-ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.EvalTask) using your data generator. Then you will use the `training.Loop` to train your model.
# 
# <a name="ex03"></a>
# ### Exercise 03
# 
# **Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do: 
# - Create the trainer object by calling [`trax.supervised.training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) and pass in the following:
# 
#     - model = [NER](#ex02)
#     - [training task](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) that uses the train data generator defined in the cell above
#         - loss_layer = [tl.CrossEntropyLoss()](https://github.com/google/trax/blob/22765bb18608d376d8cd660f9865760e4ff489cd/trax/layers/metrics.py#L71)
#         - optimizer = [trax.optimizers.Adam(0.01)](https://github.com/google/trax/blob/03cb32995e83fc1455b0c8d1c81a14e894d0b7e3/trax/optimizers/adam.py#L23)
#     - [evaluation task](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) that uses the validation data generator defined in the cell above
#         - metrics for `EvalTask`: `tl.CrossEntropyLoss()` and `tl.Accuracy()`
#         - in `EvalTask` set `n_eval_batches=10` for better evaluation accuracy
#     - output_dir = output_dir
# 
# You'll be using a [cross entropy loss](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss), with an [Adam optimizer](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam). Please read the [trax](https://trax-ml.readthedocs.io/en/latest/trax.html) documentation to get a full understanding. The [trax GitHub](https://github.com/google/trax) also contains some useful information and a link to a colab notebook.

# In[ ]:


# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: train_model
def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):
    '''
    Input: 
        NER - the model you are building
        train_generator - The data generator for training examples
        eval_generator - The data generator for validation examples,
        train_steps - number of training steps
        output_dir - folder to save your model
    Output:
        training_loop - a trax supervised training Loop
    '''
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    train_task = training.TrainTask(
      None, # A train data generator
      loss_layer = None, # A cross-entropy loss function
      optimizer = None,  # The adam optimizer
    )

    eval_task = training.EvalTask(
      labeled_data = None, # A labeled data generator
      metrics = [None, None], # Evaluate with cross-entropy loss and accuracy
      n_eval_batches = None # Number of batches to use on each evaluation
    )

    training_loop = training.Loop(
        None, # A model to train
        None, # A train task
        eval_task = None, # The evaluation task
        output_dir = None) # The output directory

    # Train with train_steps
    training_loop.run(n_steps = None)
    ### END CODE HERE ###
    return training_loop


# On your local machine, you can run this training for 1000 train_steps and get your own model. This training takes about 5 to 10 minutes to run.

# In[ ]:


train_steps = 100            # In coursera we can only train 100 steps
get_ipython().system("rm -f 'model/model.pkl.gz'  # Remove old model.pkl if it exists")

# Train the model
training_loop = train_model(NER(), train_generator, eval_generator, train_steps)


# **Expected output (Approximately)**
# 
# ```
# ...
# Step      1: train CrossEntropyLoss |  2.94375849
# Step      1: eval  CrossEntropyLoss |  1.93172036
# Step      1: eval          Accuracy |  0.78727312
# Step    100: train CrossEntropyLoss |  0.57727730
# Step    100: eval  CrossEntropyLoss |  0.36356260
# Step    100: eval          Accuracy |  0.90943187
# ...
# ```
# This value may change between executions, but it must be around 90% of accuracy on train and validations sets, after 100 training steps.

# We have trained the model longer, and we give you such a trained model. In that way, we ensure you can continue with the rest of the assignment even if you had some troubles up to here, and also we are sure that everybody will get the same outputs for the last example. However, you are free to try your model, as well. 

# In[ ]:


#+end_example
