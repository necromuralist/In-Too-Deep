<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="How to handle out-of-vocabulary words with N-grams." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>N-Grams: Out-of-Vocabulary Words | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/n-grams-out-of-vocabulary-words/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../n-gram-building-the-language-model/" rel="prev" title="N-Gram: Building the Language Model" type="text/html">
<link href="../auto-complete/" rel="next" title="Auto-Complete" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="N-Grams: Out-of-Vocabulary Words" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/n-grams-out-of-vocabulary-words/" property="og:url">
<meta content="How to handle out-of-vocabulary words with N-grams." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-03T14:35:57-08:00" property="article:published_time">
<meta content="n-grams" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">N-Grams: Out-of-Vocabulary Words</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-03T14:35:57-08:00" itemprop="datePublished" title="2020-12-03 14:35">2020-12-03 14:35</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org05ba3c0">Beginning</a></li>
<li><a href="#org420c1f5">Middle</a>
<ul>
<li><a href="#org9ddccac">Build the vocabulary</a></li>
<li><a href="#orgce13c1b">Replace Unknown Words</a>
<ul>
<li><a href="#orgba154c1">A Specific Frequency</a></li>
</ul>
</li>
<li><a href="#orgb6ce569">Too Many Unknowns</a></li>
<li><a href="#org3927a3d">Smoothing</a></li>
<li><a href="#org0fce1b4">Back-Off</a></li>
<li><a href="#orgd31916b">Interpolation</a></li>
</ul>
</li>
<li><a href="#org785e1da">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org05ba3c0">
<h2 id="org05ba3c0">Beginning</h2>
<div class="outline-text-2" id="text-org05ba3c0">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org420c1f5">
<h2 id="org420c1f5">Middle</h2>
<div class="outline-text-2" id="text-org420c1f5">
<p>We're going to look at a method of deciding whether an unknown word belongs to our vocabulary. It requires that we know the target size of the vocabulary in advance and the vocabulary has the words and their counts from the training set.</p>
</div>
<div class="outline-3" id="outline-container-org9ddccac">
<h3 id="org9ddccac">Build the vocabulary</h3>
<div class="outline-text-3" id="text-org9ddccac">
<p>First we'll define the vocabulary target size.</p>
<div class="highlight">
<pre><span></span><span class="n">vocabulary_target_size</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>
<p>Now build a counter - with a real vocabulary we could use the <code>Counter</code> object to build the counts directly, but since we don't have a real corpus we can create it with a dict.</p>
<div class="highlight">
<pre><span></span><span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">({</span><span class="s2">"happy"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
          <span class="s2">"because"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
          <span class="s2">"i"</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span>
          <span class="s2">"am"</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span>
          <span class="s2">"learning"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
          <span class="s2">"."</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</pre></div>
<p>Now trim it down to our target size.</p>
<div class="highlight">
<pre><span></span><span class="n">vocabulary</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">vocabulary_target_size</span><span class="p">)</span>
</pre></div>
<p>Now get the words only.</p>
<div class="highlight">
<pre><span></span><span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The </span><span class="si">{</span><span class="n">vocabulary_target_size</span><span class="si">}</span><span class="s2"> most frequent words: </span><span class="si">{</span><span class="n">vocabulary</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
The 3 most frequent words: {'because', 'learning', 'happy'}
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgce13c1b">
<h3 id="orgce13c1b">Replace Unknown Words</h3>
<div class="outline-text-3" id="text-orgce13c1b">
<div class="highlight">
<pre><span></span><span class="n">original</span> <span class="o">=</span> <span class="s2">"am i learning"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">original</span><span class="p">:</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">word</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span> <span class="k">else</span> <span class="s2">"&lt;UNK&gt;"</span>
    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Original: </span><span class="si">{</span><span class="n">original</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Processed: </span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Original: ['am', 'i', 'learning']
Processed: ['&lt;UNK&gt;', '&lt;UNK&gt;', 'learning']
</pre></div>
<div class="outline-4" id="outline-container-orgba154c1">
<h4 id="orgba154c1">A Specific Frequency</h4>
<div class="outline-text-4" id="text-orgba154c1">
<p>There might also be cases where we need to filter by a specific frequency instead of just the largest frequencies. Here's one way to do it.</p>
<div class="highlight">
<pre><span></span><span class="n">match</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">word_counts</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"happy"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
               <span class="s2">"because"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
               <span class="s2">"i"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
               <span class="s2">"am"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
               <span class="s2">"learning"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
               <span class="s2">"."</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">matches</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">count</span><span class="o">==</span><span class="n">match</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
<pre class="example">
because
learning
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb6ce569">
<h3 id="orgb6ce569">Too Many Unknowns</h3>
<div class="outline-text-3" id="text-orgb6ce569">
<p>We're going to use perplexity to assess the performance of our model. If you have too many unknowns your perplexity will be low even though your model isn't doing well. Here's an example of this effect.</p>
<p>Rather than going through the trouble of creating the corpus, let's just pretend we calculated the probabilities (the bigram-probabilities for the training set were calculated in <a href="../n-gram-building-the-language-model/">the previous post</a>).</p>
<p>Here's the case where everything is known.</p>
<div class="highlight">
<pre><span></span><span class="n">training_set</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">,</span> <span class="s1">'.'</span><span class="p">]</span>

<span class="c1"># pre-calculated probabilities</span>
<span class="n">bigram_probabilities</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">):</span> <span class="mf">1.0</span><span class="p">,</span>
                        <span class="p">(</span><span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">):</span> <span class="mf">0.5</span><span class="p">,</span>
                        <span class="p">(</span><span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">):</span> <span class="mf">1.0</span><span class="p">,</span>
                        <span class="p">(</span><span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">):</span> <span class="mf">1.0</span><span class="p">,</span>
                        <span class="p">(</span><span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">):</span> <span class="mf">0.5</span><span class="p">,</span>
                        <span class="p">(</span><span class="s1">'learning'</span><span class="p">,</span> <span class="s1">'.'</span><span class="p">):</span> <span class="mf">1.0</span><span class="p">}</span>
<span class="n">test_set</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">]</span>
</pre></div>
<p>And here's the case where the training set has a lot of unknowns (Out-of-Vocabulary words).</p>
<div class="highlight">
<pre><span></span><span class="n">training_set_unk</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'&lt;UNK&gt;'</span><span class="p">,</span> <span class="s1">'&lt;UNK&gt;'</span><span class="p">,</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'&lt;UNK&gt;'</span><span class="p">,</span> <span class="s1">'&lt;UNK&gt;'</span><span class="p">]</span>

<span class="n">test_set_unk</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'&lt;UNK&gt;'</span><span class="p">]</span>
</pre></div>
<p>And here's our bigram probabilities for the set with unknowns.</p>
<div class="highlight">
<pre><span></span><span class="n">bigram_probabilities_unk</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">):</span> <span class="mf">1.0</span><span class="p">,</span>
                            <span class="p">(</span><span class="s1">'am'</span><span class="p">,</span> <span class="s1">'&lt;UNK&gt;'</span><span class="p">):</span> <span class="mf">1.0</span><span class="p">,</span>
                            <span class="p">(</span><span class="s1">'&lt;UNK&gt;'</span><span class="p">,</span> <span class="s1">'&lt;UNK&gt;'</span><span class="p">):</span> <span class="mf">0.5</span><span class="p">,</span>
                            <span class="p">(</span><span class="s1">'&lt;UNK&gt;'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">):</span> <span class="mf">0.25</span><span class="p">}</span>
</pre></div>
<p>"i" is always followed by "am" so the first probability is going to be 1. "am" is always followed by "&lt;UNK&gt;" so the second probability will also be 1. Two of the four "&lt;UNK&gt;"s are followed by an "&lt;UNK&gt;" so the third probability is 1/2 and "&lt;UNK&gt;" is followed by "i" once, so the last probability is 1/4.</p>
<div class="highlight">
<pre><span></span><span class="n">M</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span>
<span class="n">probability</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">probability_unk</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">n</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">),</span> <span class="mi">2</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">bigram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">test_set</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">N</span><span class="p">])</span>
    <span class="n">probability</span> <span class="o">=</span> <span class="n">probability</span> <span class="o">*</span> <span class="n">bigram_probabilities</span><span class="p">[</span><span class="n">bigram</span><span class="p">]</span>

    <span class="n">bigram_unk</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">test_set_unk</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">N</span><span class="p">])</span>
    <span class="n">probability_unk</span> <span class="o">=</span> <span class="n">probability_unk</span> <span class="o">*</span> <span class="n">bigram_probabilities_unk</span><span class="p">[</span><span class="n">bigram_unk</span><span class="p">]</span>

<span class="c1"># calculate perplexity for both original test set and test set with &lt;UNK&gt;</span>
<span class="n">perplexity</span> <span class="o">=</span> <span class="n">probability</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">M</span><span class="p">)</span>
<span class="n">perplexity_unk</span> <span class="o">=</span> <span class="n">probability_unk</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">M</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"perplexity for the training set: </span><span class="si">{</span><span class="n">perplexity</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"perplexity for the training set with &lt;UNK&gt;: </span><span class="si">{</span><span class="n">perplexity_unk</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
perplexity for the training set: 1.2599210498948732
perplexity for the training set with &lt;UNK&gt;: 1.0
</pre>
<p>So our training set with unknown words does better than our training set with all the words in our test set. It's a little mysterious to me why you would choose to put all these unknowns in the training set, unless you're trying to save space or something. I'll have to go back and read about that.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org3927a3d">
<h3 id="org3927a3d">Smoothing</h3>
<div class="outline-text-3" id="text-org3927a3d">
<p>As with prior cases where we had to calculate probabilities, we need to be able to handle probabilities for n-grams that we didn't learn. We're going to use <code>add-k</code> smoothing here as an example.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">add_k_smoothing</span><span class="p">(</span><span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_gram_count</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                    <span class="n">n_gram_prefix_count</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="k">return</span>  <span class="p">((</span><span class="n">n_gram_count</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span><span class="o">/</span>
             <span class="p">(</span><span class="n">n_gram_prefix_count</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">vocabulary_size</span><span class="p">))</span>
</pre></div>
<p>We'll take a look at <code>k=1</code> (Laplacian) smoothing for a trigram.</p>
<div class="highlight">
<pre><span></span><span class="n">trigram_probabilities</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">)</span> <span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="n">bigram_probabilities</span> <span class="o">=</span> <span class="p">{(</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">)</span> <span class="p">:</span> <span class="mi">10</span><span class="p">}</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">probability_known_trigram</span> <span class="o">=</span> <span class="n">add_k_smoothing</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">trigram_probabilities</span><span class="p">[(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">)],</span> 
                           <span class="n">bigram_probabilities</span><span class="p">[(</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">)])</span>

<span class="n">probability_unknown_trigram</span> <span class="o">=</span> <span class="n">add_k_smoothing</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"probability_known_trigram: </span><span class="si">{</span><span class="n">probability_known_trigram</span><span class="si">:</span><span class="s2"> 0.03f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"probability_unknown_trigram: </span><span class="si">{</span><span class="n">probability_unknown_trigram</span><span class="si">:</span><span class="s2"> 0.03f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
probability_known_trigram:  0.200
probability_unknown_trigram:  0.200
</pre>
<p>So, here's a problem with <i>add-k</i> smoothing - when the n-gram is unknown, we still get a 20% probability, which in this case happens to be the same as a trigram that was in the training set.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org0fce1b4">
<h3 id="org0fce1b4">Back-Off</h3>
<div class="outline-text-3" id="text-org0fce1b4">
<p>Here's an alternate way to handle unknown n-grams - if the n-gram isn't known, use a probability for a smaller <i>n</i>.</p>
<p>Here are our pre-calculated probabilities of all types of n-grams.</p>
<div class="highlight">
<pre><span></span><span class="n">trigram_probabilities</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">):</span> <span class="mi">0</span><span class="p">}</span>
<span class="n">bigram_probabilities</span> <span class="o">=</span> <span class="p">{(</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">):</span> <span class="mf">0.3</span><span class="p">}</span>
<span class="n">unigram_probabilities</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'happy'</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">}</span>
</pre></div>
<p>Here's the trigram that we want the probability for. As you can see, we don't have "you" in our known n-grams.</p>
<div class="highlight">
<pre><span></span><span class="n">trigram</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'are'</span><span class="p">,</span> <span class="s1">'you'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">)</span>
<span class="n">bigram</span><span class="p">,</span> <span class="n">unigram</span> <span class="o">=</span> <span class="n">trigram</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span> <span class="mi">3</span><span class="p">],</span> <span class="n">trigram</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
<p>Now we can do a brute-force search for the probabilities. \(\lambda\) was discovered experimentally.</p>
<div class="highlight">
<pre><span></span><span class="n">lambda_factor</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">probability_hat_trigram</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># search for first non-zero probability starting with the trigram</span>
<span class="c1"># to generalize this for any order of n-gram hierarchy, </span>
<span class="c1"># you could loop through the probability dictionaries instead of if/else cascade</span>
<span class="k">if</span> <span class="n">trigram</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">trigram_probabilities</span> <span class="ow">or</span> <span class="n">trigram_probabilities</span><span class="p">[</span><span class="n">trigram</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"probability for trigram </span><span class="si">{</span><span class="n">trigram</span><span class="si">}</span><span class="s2"> not found"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">bigram</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">bigram_probabilities</span> <span class="ow">or</span> <span class="n">bigram_probabilities</span><span class="p">[</span><span class="n">bigram</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"probability for bigram </span><span class="si">{</span><span class="n">bigram</span><span class="si">}</span><span class="s2"> not found"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">unigram</span> <span class="ow">in</span> <span class="n">unigram_probabilities</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"probability for unigram </span><span class="si">{</span><span class="n">unigram</span><span class="si">}</span><span class="s2"> found</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
            <span class="n">probability_hat_trigram</span> <span class="o">=</span> <span class="n">lambda_factor</span> <span class="o">*</span> <span class="n">lambda_factor</span> <span class="o">*</span> <span class="n">unigram_probabilities</span><span class="p">[</span><span class="n">unigram</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">probability_hat_trigram</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">probability_hat_trigram</span> <span class="o">=</span> <span class="n">lambda_factor</span> <span class="o">*</span> <span class="n">bigram_probabilities</span><span class="p">[</span><span class="n">bigram</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">probability_hat_trigram</span> <span class="o">=</span> <span class="n">trigram_probabilities</span><span class="p">[</span><span class="n">trigram</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"probability for trigram </span><span class="si">{</span><span class="n">trigram</span><span class="si">}</span><span class="s2"> estimated as </span><span class="si">{</span><span class="n">probability_hat_trigram</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
probability for trigram ('are', 'you', 'happy') not found
probability for bigram ('you', 'happy') not found
probability for unigram happy found

probability for trigram ('are', 'you', 'happy') estimated as 0.064
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgd31916b">
<h3 id="orgd31916b">Interpolation</h3>
<div class="outline-text-3" id="text-orgd31916b">
<p>Yet another way to handle unknown n-grams. In this case you always use trigrams, bigrams, and unigrams, thus eliminating some of the overhead and use a weighted value instead. As always, there's no free lunch - you have to find the best weights to make this work (but we'll take some pre-made ones).</p>
<p>Pre-calculated probabilities of all types of n-grams.</p>
<div class="highlight">
<pre><span></span><span class="n">trigram_probabilities</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">):</span> <span class="mf">0.15</span><span class="p">}</span>
<span class="n">bigram_probabilities</span> <span class="o">=</span> <span class="p">{(</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">):</span> <span class="mf">0.3</span><span class="p">}</span>
<span class="n">unigram_probabilities</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'happy'</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">}</span>
</pre></div>
<p>The weights come from optimization on a validation set.</p>
<div class="highlight">
<pre><span></span><span class="n">lambda_1</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">lambda_2</span> <span class="o">=</span> <span class="mf">0.15</span>
<span class="n">lambda_3</span> <span class="o">=</span> <span class="mf">0.05</span>
</pre></div>
<p>And now the trigram whose probability we want to estimate as well as derived bigrams and unigrams.</p>
<div class="highlight">
<pre><span></span><span class="n">trigram</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">)</span>
<span class="n">bigram</span><span class="p">,</span> <span class="n">unigram</span> <span class="o">=</span> <span class="n">trigram</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span> <span class="mi">3</span><span class="p">],</span> <span class="n">trigram</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">probability_hat_trigram</span> <span class="o">=</span> <span class="n">lambda_1</span> <span class="o">*</span> <span class="n">trigram_probabilities</span><span class="p">[</span><span class="n">trigram</span><span class="p">]</span> 
<span class="o">+</span> <span class="n">lambda_2</span> <span class="o">*</span> <span class="n">bigram_probabilities</span><span class="p">[</span><span class="n">bigram</span><span class="p">]</span>
<span class="o">+</span> <span class="n">lambda_3</span> <span class="o">*</span> <span class="n">unigram_probabilities</span><span class="p">[</span><span class="n">unigram</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"estimated probability of the input trigram </span><span class="si">{</span><span class="n">trigram</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">probability_hat_trigram</span><span class="si">:</span><span class="s2"> 0.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
estimated probability of the input trigram ('i', 'am', 'happy') is  0.1200
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org785e1da">
<h2 id="org785e1da">End</h2>
<div class="outline-text-2" id="text-org785e1da">
<p>So, there's various ways to handle both individual words as well as n-grams we don't recognize.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/n-grams/" rel="tag">n-grams</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../n-gram-building-the-language-model/" rel="prev" title="N-Gram: Building the Language Model">Previous post</a></li>
<li class="next"><a href="../auto-complete/" rel="next" title="Auto-Complete">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
