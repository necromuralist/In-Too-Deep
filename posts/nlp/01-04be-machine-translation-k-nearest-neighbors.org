#+BEGIN_COMMENT
.. title: Implementing k-Nearest Neighbors for Machine Translation
.. slug: machine-translation-k-nearest-neighbors
.. date: 2020-10-22 17:38:25 UTC-07:00
.. tags: nlp,machine translation,assignment
.. category: NLP
.. link: 
.. description: Implementing machine translation using k-Nearest Neighbors.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-62e03387-23bf-4675-96a8-2b918ac0854c-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  This continues from the post where we found the {{% lancelot title="transformation matrix" %}}machine-translation-transformation-matrix{{% /lancelot %}}. It's part of a series of posts whose links are gathered in the {{% doc %}}machine-translation{{% /doc %}} post.
* Middle
** Testing the translation
*** k-Nearest neighbors algorithm

[[https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm][k-Nearest neighbors algorithm]]

- k-NN is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it. 
- The 'k' is the number of "nearest neighbors" to find (e.g. k=2 finds the closest two neighbors).
 
*** Searching for the translation embedding
 Since we're approximating the translation function from English to French embeddings by a linear transformation matrix \(\mathbf{R}\), most of the time we won't get the exact embedding of a French word when we transform embedding \(\mathbf{e}\) of some particular English word into the French embedding space. 

 This is where /k/-NN becomes really useful! By using /1/-NN with \(\mathbf{eR}\) as input, we can search for an embedding \(\mathbf{f}\) (as a row) in the matrix \(\mathbf{Y}\) which is the closest to the transformed vector \(\mathbf{eR}\).

*** Cosine similarity
 Cosine similarity between vectors /u/ and /v/ calculated as the cosine of the angle between them.

 The formula is:

 \[
\cos(u,v)=\frac{u\cdot v}{\left\|u\right\|\left\|v\right\|}
\]

 - \(\cos(u,v) = 1\) when /u/ and /v/ lie on the same line and have the same direction.
 - \(\cos(u,v) = -1\) when they have exactly opposite directions.
 - \(\cos(u,v) = 0\) when the vectors are orthogonal (perpendicular) to each other.

 **Note:** Distance and similarity are pretty much opposite things.

 - We can obtain distance metric from cosine similarity, but the cosine similarity can't be used directly as the distance metric. 
 - When the cosine similarity increases (towards /1/), the "distance" between the two vectors decreases (towards /0/).
 - We can define the cosine distance between /u/ and /v/ as

\[
d_{\text{cos}}(u,v)=1-\cos(u,v)
\]

 **Exercise 05**: Complete the function =nearest_neighbor()=

Inputs:
 - Vector /v/
 - A set of possible nearest neighbors /candidates/
 - /k/ nearest neighbors to find.
 - The distance metric should be based on cosine similarity.
 - /cosine_similarity/ function is already implemented and imported for you. It's arguments are two vectors and it returns the cosine of the angle between them.
 - Iterate over rows in /candidates/, and save the result of similarities between current row and vector /v/ in a python list. Take care that similarities are in the same order as row vectors of /candidates/.
 - Now you can use [[https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort][numpy argsort]] to sort the indices for the rows of /candidates/.

***** Hints
      - =numpy.argsort= sorts values from most negative to most positive (smallest to largest)
      - The candidates that are nearest to /v/ should have the highest cosine similarity
      - To get the last element of a list /tmp/, the notation is =tmp[-1:]=

#+begin_src python :results none
# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
def nearest_neighbor(v, candidates, k=1):
    """
    Input:
      - v, the vector you are going find the nearest neighbor for
      - candidates: a set of vectors where we will find the neighbors
      - k: top k nearest neighbors to find
    Output:
      - k_idx: the indices of the top k closest vectors in sorted form
    """
    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###
    similarity_l = []

    # for each candidate vector...
    for row in candidates:
        # get the cosine similarity
        cos_similarity = None

        # append the similarity to the list
        None
        
    # sort the similarity list and get the indices of the sorted list
    sorted_ids = None

    # get the indices of the k most similar candidate vectors
    k_idx = None
    ### END CODE HERE ###
    return k_idx
#+end_src
** Test your implementation:
#+begin_src python :results none
v = np.array([1, 0, 1])
candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])
print(candidates[nearest_neighbor(v, candidates, 3)])
#+end_src


**Expected Output**:

#+begin_example
[[9 9 9]
[1 0 5]
[2 0 1]]
#+end_example
** Test your translation and compute its accuracy
 **Exercise 06**:
Complete the function =test_vocabulary= which takes in English embedding matrix /X/, French embedding matrix /Y/ and the /R/ matrix and returns the accuracy of translations from /X/ to /Y/ by /R/.

 - Iterate over transformed English word embeddings and check if the closest French word vector belongs to French word that is the actual translation.
 - Obtain an index of the closest French embedding by using =nearest_neighbor= (with argument /k=1/), and compare it to the index of the English embedding you have just transformed.
 - Keep track of the number of times you get the correct translation.
 - Calculate accuracy as

   \[
    \text{accuracy}=\frac{\#(\text{correct predictions})}{\#(\text{total predictions})}
    \]


#+begin_src python :results none
# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
def test_vocabulary(X, Y, R):
    '''
    Input:
        X: a matrix where the columns are the English embeddings.
        Y: a matrix where the columns correspong to the French embeddings.
        R: the transform matrix which translates word embeddings from
        English to French word vector space.
    Output:
        accuracy: for the English to French capitals
    '''

    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###
    # The prediction is X times R
    pred = None

    # initialize the number correct to zero
    num_correct = 0

    # loop through each row in pred (each transformed embedding)
    for i in range(len(pred)):
        # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y
        pred_idx = None

        # if the index of the nearest neighbor equals the row of i... \
        if pred_idx == i:
            # increment the number correct by 1.
            num_correct += None

    # accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)
    accuracy = None

    ### END CODE HERE ###

    return accuracy
#+end_src

Let's see how is your translation mechanism working on the unseen data:

#+begin_src python :results none
X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)
#+end_src

You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything.

#+begin_src python :results output :exports both
acc = test_vocabulary(X_val, Y_val, R_train)  # this might take a minute or two
print(f"accuracy on test set is {acc:.3f}")
#+end_src

 **Expected Output**:

#+RESULTS
 0.557

 You managed to translate words from one language to another language without ever seing them with almost 56% accuracy by using some basic linear algebra and learning a mapping of words from one language to another!
* End
  - The next post in this series is {{% doc %}}machine-translation-with-locality-sensitive-hashing{{% /doc %}}
