#+BEGIN_COMMENT
.. title: Twitter Logistic Regression Visualization
.. slug: twitter-logistic-regression
.. date: 2020-07-10 23:08:03 UTC-07:00
.. tags: nlp,twitter,logistic regression,sentiment analysis
.. category: NLP
.. link: 
.. description: Creating a Logistic Regression Model to predict tweet sentiment.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-34f205ad-639b-4cb7-888e-469777f94b2a.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  We are going to extend our previous plotting of the word frequencies by creating a [[https://www.wikiwand.com/en/Logistic_regression][Logistic Regression]] model of the tweet sentiment and then plot the output of the model along with the counts.
** Set Up
*** Imports
#+begin_src python :results none
# from python
from argparse import Namespace
from functools import partial
from pathlib import Path

import os

# from pypi
from dotenv import load_dotenv
from nltk.corpus import twitter_samples 
from sklearn.model_selection import train_test_split

import holoviews
import hvplot.pandas
import nltk
import pandas

# This project
from neurotic.nlp.twitter.counter import WordCounter

# Some helper code
from graeae import EmbedHoloviews
#+end_src
*** Plotting
#+begin_src python :results none
SLUG = "twitter-logistic-regression"
Embed = partial(EmbedHoloviews,
                folder_path=f"../../files/posts/nlp/{SLUG}")

Plot = Namespace(
    width=990,
    height=780,
    tan="#ddb377",
    blue="#4687b7",
    red="#ce7b6d",
    font_scale=2,
    color_cycle = holoviews.Cycle(["#4687b7", "#ce7b6d"])
)

#+end_src
** The Tweet Vectors
   In the previous post we built a dictionary-like set to count the number of times each token was in a positive tweet and in a negative tweet. To represent a tweet as a vector for training the model you then sum the total counts for the tokens in the tweet when they are positive and when they are positive. 

Come again?

Lets say you have a tweet ="a b c"= which tokenizes to =a, b, c=. Look up the positive and negative tweet counts for each token and then add them:

| Token | Positive | Negative |
|-------+----------+----------|
| a     |        1 |        4 |
| b     |        2 |        5 |
| c     |        3 |        6 |
|-------+----------+----------|
| Total |        6 | 15       |

So to represent this tweet you would create a vector of the form:

\begin{align}
\hat{v} &= \langle bias, positive, negative \rangle\\
&= \langle 1, 6, 15\rangle\\
\end{align}

**Note:** The bias is always one (it just is).

We're skipping the step where we actually create the vectors and just loading a prepared set.

#+begin_src python :results output :exports both
load_dotenv(override=True)
FEATURES = Path(os.environ["TWITTER_REGRESSION_DATA"]).expanduser()
data = pandas.read_csv(FEATURES)
print(data.iloc[0])
#+end_src

#+RESULTS:
: bias            1.0
: positive     3020.0
: negative       61.0
: sentiment       1.0
: Name: 0, dtype: float64

This first row is the vector representation for a tweet that I was talking about (plus a label column). The =bias= is always one, the =positive= value is the sum of the positive tweet counts for each token in the tweet and the =negative= values is the sum of the negative tweet counts for each of the tokens.

Since the =data= has the label column mixed in with the other columns we'll split them apart next.

#+begin_src python :results output :exports both
x_data = data[["bias", "positive", "negative"]]
y_data = data["sentiment"]
print(x_data.shape)
#+end_src

#+RESULTS:
: (8000, 3)

*** The Weights
    Okay, we're just given the weights for the model next. Presumably this was gained by training the model on the data we loaded just now.

#+begin_src python :results none
theta = [7e-08, 0.0005239, -0.00055517]

Weights = Namespace(
    bias=0,
    positive=1,
    negative=2
)
#+end_src
* Middle
** Plot The Vectors
   We can plot the positive vs negative counts for each tweet to see how correlated they seem to be.

#+begin_src python :results none
plot = data.hvplot.scatter(x="positive", y="negative", by="sentiment", color=Plot.color_cycle).opts(
    height=Plot.height,
    width=Plot.width,
    fontscale=Plot.font_scale,
    title="Positive vs Negative",
)

output = Embed(plot=plot, file_name="positive_negative_scatter")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="positive_negative_scatter.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

Looking at the plot you can see that representing the tweets this way seems to have created a fairly separable dataset (although there's some mixing when the counts are low).
*** Add the Model
    Since we've been given the model's weights we can plot it's output when fed the vectors to see how it separates the data. To get the equation for the separation line we need to solve for one of the positive or negative terms when the product of the weights and the vector is 0 ($\theta \times x = 0$, where /x/ is our vector $\langle bias, positive, negative \rangle$).

Get ready for some algebra.

\begin{align}
\theta \times x &= 0\\
\theta \times \langle bias, positive, negative \rangle &= 0\\
\theta \times \langle 1, positive, negative \rangle &= 0\\
\theta_0 + \theta_1 \times positive + \theta_2 \times negative &= 0\\
\theta_2 \times negative &= -\theta_0 - \theta_1 \times positive\\
negative &= \frac{-\theta_0 - \theta_1 \times positive}{\theta_2}\\
\end{align}

This is the equation for our separation line, which we can translate to a function to apply to our data.

#+begin_src python :results none
def negative(theta: list, positive: float) -> float:
    """Calculate the negative value

    This calculates the value for the separation line

    Args:
     theta: list of weights for the logistic regression
     positive: count of positive tweets matching tweet

    Returns:
     the calculated negative value for the separation line
    """
    return (-theta[Weights.bias]
            - positive * theta[Weights.positive])/theta[Weights.negative]
#+end_src
*** Some Direction Line
    To make the visualization clearer we can add lines that are perpendicular to the separation line.

#+begin_src python :results none
def direction(theta: list, positive: float) -> float:
    return positive * theta[Weights.negative]/theta[Weights.positive]
#+end_src
*** Plot Again
#+begin_src python :results none
scatter = data.hvplot.scatter(x="positive", y="negative", by="sentiment", color=Plot.color_cycle)

most_positive = data.positive.max()
line = holoviews.Curve([(0, 0),
                       (negative(theta, 0), negative(theta, most_positive))],
                       color="gray")
plot = (scatter * line).opts(
    height=Plot.height,
    width=Plot.width,
    fontscale=Plot.font_scale,
    title="Positive vs Negative",
)
output = Embed(plot=plot, file_name="positive_negative_separated")()
#+end_src

#+begin_src python :results output html :exports both
print(output)
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="positive_negative_separated.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

* End
* Raw
#+begin_src python

# The red and green lines that point in the direction of the corresponding sentiment are calculated using a perpendicular line to the separation line calculated in the previous equations(neg function). It must point in the same direction as the derivative of the Logit function, but the magnitude may differ. It is only for a visual representation of the model. 
# 
# $$direction = pos * \theta_2 / \theta_1$$


# Equation for the direction of the sentiments change
# We don't care about the magnitude of the change. We are only interested 
# in the direction. So this direction is just a perpendicular function to the 
# separation plane
# df(pos, W) = pos * w2 / w1
def direction(theta, pos):
    return    pos * theta[2] / theta[1]


# The green line in the chart points in the direction where z > 0 and the red line points in the direction where z < 0. The direction of these lines are given by the weights $\theta_1$ and $\theta_2$

# In[ ]:


# Plot the samples using columns 1 and 2 of the matrix
fig, ax = plt.subplots(figsize = (8, 8))

colors = ['red', 'green']

# Color base on the sentiment Y
ax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words
plt.xlabel("Positive")
plt.ylabel("Negative")

# Now lets represent the logistic regression model in this chart. 
maxpos = np.max(X[:,1])

offset = 5000 # The pos value for the direction vectors origin

# Plot a gray line that divides the 2 areas.
ax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') 

# Plot a green line pointing to the positive direction
ax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=500, head_length=500, fc='g', ec='g')
# Plot a red line pointing to the negative direction
ax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=500, head_length=500, fc='r', ec='r')

plt.show()


# **Note that more critical than the Logistic regression itself, are the features extracted from tweets that allow getting the right results in this exercise.**
# 
# That is all, folks. Hopefully, now you understand better what the Logistic regression model represents, and why it works that well for this specific problem. 

#+end_src
