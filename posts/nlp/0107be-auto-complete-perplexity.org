#+BEGIN_COMMENT
.. title: Auto-Complete: Perplexity
.. slug: auto-complete-perplexity
.. date: 2020-12-04 15:19:33 UTC-08:00
.. tags: nlp,auto-complete,n-gram
.. category: NLP
.. link: 
.. description: Assessing N-Gram model performance with perplexity.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3

#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-628b96bb-9fe2-4219-af43-264f81238d87-ssh.json

#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  In the {{% lancelot title="previous post" %}}auto-complete-the-n-gram-model{{% /lancelot %}} we implemented the N-Gram Language Model for the auto-complete system that we began {{% lancelot title="here" %}}auto-complete{{% /lancelot %}}.
* End
  In the next part we'll build our {{% lancelot title="completed auto-complete system" %}}auto-complete-building-the-auto-complete-system{{{% /lancelot %}}.
* Raw
#+begin_example
# <a name='3'></a>
# ## Part 3: Perplexity
# 
# In this section, you will generate the perplexity score to evaluate your model on the test set. 
# - You will also use back-off when needed. 
# - Perplexity is used as an evaluation metric of your language model. 
# - To calculate the  the perplexity score of the test set on an n-gram model, use: 
# 
# $$ PP(W) =\sqrt[N]{ \prod_{t=n+1}^N \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4}$$
# 
# - where $N$ is the length of the sentence.
# - $n$ is the number of words in the n-gram (e.g. 2 for a bigram).
# - In math, the numbering starts at one and not zero.
# 
# In code, array indexing starts at zero, so the code will use ranges for $t$ according to this formula:
# 
# $$ PP(W) =\sqrt[N]{ \prod_{t=n}^{N-1} \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4.1}$$
# 
# The higher the probabilities are, the lower the perplexity will be. 
# - The more the n-grams tell us about the sentence, the lower the perplexity score will be. 

# <a name='ex-10'></a>
# ### Exercise 10
# Compute the perplexity score given an N-gram count matrix and a sentence. 

# <details>    
# <summary>
#     <font size="3" color="darkgreen"><b>Hints</b></font>
# </summary>
# <p>
# <ul>
#     <li>Remember that <code>range(2,4)</code> produces the integers [2, 3] (and excludes 4).</li>
# </ul>
# </p>
# 

# In[ ]:


# UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
# GRADED FUNCTION: calculate_perplexity
def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):
    """
    Calculate perplexity for a list of sentences
    
    Args:
        sentence: List of strings
        n_gram_counts: Dictionary of counts of (n+1)-grams
        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams
        vocabulary_size: number of unique words in the vocabulary
        k: Positive smoothing constant
    
    Returns:
        Perplexity score
    """
    # length of previous words
    n = len(list(n_gram_counts.keys())[0]) 
    
    # prepend <s> and append <e>
    sentence = ["<s>"] * n + sentence + ["<e>"]
    
    # Cast the sentence from a list to a tuple
    sentence = tuple(sentence)
    
    # length of sentence (after adding <s> and <e> tokens)
    N = len(sentence)
    
    # The variable p will hold the product
    # that is calculated inside the n-root
    # Update this in the code below
    product_pi = 1.0
    
    ### START CODE HERE (Replace instances of 'None' with your code) ###
    
    # Index t ranges from n to N - 1, inclusive on both ends
    for t in range(None, None): # complete this line

        # get the n-gram preceding the word at position t
        n_gram = None
        
        # get the word at position t
        word = None
        
        # Estimate the probability of the word given the n-gram
        # using the n-gram counts, n-plus1-gram counts,
        # vocabulary size, and smoothing constant
        probability = None
        
        # Update the product of the probabilities
        # This 'product_pi' is a cumulative product 
        # of the (1/P) factors that are calculated in the loop
        product_pi *= None

    # Take the Nth root of the product
    perplexity = None
    
    ### END CODE HERE ### 
    return perplexity


# In[ ]:


# test your code

sentences = [['i', 'like', 'a', 'cat'],
                 ['this', 'dog', 'is', 'like', 'a', 'cat']]
unique_words = list(set(sentences[0] + sentences[1]))

unigram_counts = count_n_grams(sentences, 1)
bigram_counts = count_n_grams(sentences, 2)


perplexity_train1 = calculate_perplexity(sentences[0],
                                         unigram_counts, bigram_counts,
                                         len(unique_words), k=1.0)
print(f"Perplexity for first train sample: {perplexity_train1:.4f}")

test_sentence = ['i', 'like', 'a', 'dog']
perplexity_test = calculate_perplexity(test_sentence,
                                       unigram_counts, bigram_counts,
                                       len(unique_words), k=1.0)
print(f"Perplexity for test sample: {perplexity_test:.4f}")


# ### Expected Output
# 
# ```CPP
# Perplexity for first train sample: 2.8040
# Perplexity for test sample: 3.9654
# ```
# 
# <b> Note: </b> If your sentence is really long, there will be underflow when multiplying many fractions.
# - To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.

# <a name='4'></a>

#+end_example
