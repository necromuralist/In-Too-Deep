#+BEGIN_COMMENT
.. title: Pytorch 60 Minute Blitz
.. slug: pytorch-60-minute-blitz
.. date: 2019-04-03 12:36:06 UTC-07:00
.. tags: pytorch,tutorial
.. category: PyTorch
.. link: 
.. description: A walk through the pytorch 60 Minute Blitz.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 4
#+BEGIN_SRC ipython :session blitz :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* The Departure
  This is a replication of [[https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html][Deep Learning With Pytorch: A 60 Minute Blitz]] to get me back into using [[https://pytorch.org][PyTorch]].
** Imports
*** PyPi
    Although the project is called PyTorch, the package is named =torch=.
#+BEGIN_SRC ipython :session blitz :results none
import torch
#+END_SRC

And we're going to use numpy a little.
#+BEGIN_SRC ipython :session blitz :results none
import numpy
#+END_SRC
* The Initiation
** Getting Started
*** Tensors
    In PyTorch, [[https://pytorch.org/docs/stable/tensors.html#torch.Tensor][tensors]] are similar to numpy's [[https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html][ndarrays]] (n-dimensional arrays). You can create an unitialized one using the =empty= function.
**** Empty
#+BEGIN_SRC ipython :session blitz :results output :exports both
empty_tensor = torch.empty(5, 3)
print(empty_tensor)
#+END_SRC

#+RESULTS:
: tensor([[-2.3492e+02,  4.5902e-41, -2.3492e+02],
:         [ 4.5902e-41,  3.1766e+30,  1.7035e+25],
:         [ 4.0498e-43,  0.0000e+00, -2.3492e+02],
:         [ 4.5902e-41,  2.6417e-37,  0.0000e+00],
:         [ 1.4607e-19,  1.8469e+25,  1.0901e+27]])

Here's the docstring for =empty=:

#+BEGIN_SRC ipython :session blitz :results output :exports both
print(torch.empty.__doc__)
#+END_SRC

#+RESULTS:
#+begin_example

empty(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

Returns a tensor filled with uninitialized data. The shape of the tensor is
defined by the variable argument :attr:`sizes`.

Args:
    sizes (int...): a sequence of integers defining the shape of the output tensor.
        Can be a variable number of arguments or a collection like a list or tuple.
    out (Tensor, optional): the output tensor
    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
        Default: ``torch.strided``.
    device (:class:`torch.device`, optional): the desired device of returned tensor.
        Default: if ``None``, uses the current device for the default tensor type
        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
        for CPU tensor types and the current CUDA device for CUDA tensor types.
    requires_grad (bool, optional): If autograd should record operations on the
        returned tensor. Default: ``False``.

Example::

    >>> torch.empty(2, 3)
    tensor(1.00000e-08 *
           [[ 6.3984,  0.0000,  0.0000],
            [ 0.0000,  0.0000,  0.0000]])


#+end_example
**** Random
#+BEGIN_SRC ipython :session blitz :results output :exports both
print(torch.rand(5, 3))
#+END_SRC

#+RESULTS:
: tensor([[0.1767, 0.9520, 0.1488],
:         [0.5592, 0.4836, 0.2645],
:         [0.8066, 0.8864, 0.1083],
:         [0.9206, 0.7311, 0.1278],
:         [0.0140, 0.5370, 0.3123]])

The arguments are the same as for empty.
**** Zeros
     Here we'll create a tensor of zeros as long integers.
#+BEGIN_SRC ipython :session blitz :results output :exports both
print(torch.zeros(5, 3, dtype=torch.long))
#+END_SRC

#+RESULTS:
: tensor([[0, 0, 0],
:         [0, 0, 0],
:         [0, 0, 0],
:         [0, 0, 0],
:         [0, 0, 0]])

Once again the argument for =zeros= is the same as those for =empty=.
**** From Data
#+BEGIN_SRC ipython :session blitz :results output :exports both
print(torch.tensor([5.5, 3]))
#+END_SRC

#+RESULTS:
: tensor([5.5000, 3.0000])
**** From A Tensor
     You can create a new tensor from a previously constructed one. This preserves any parameters you passed in that you don't subsequently override.

#+BEGIN_SRC ipython :session blitz :results output :exports both
x = torch.tensor([5, 3], dtype=torch.int)
print(x)
y = x.new_ones(5, 3)
print(y)
#+END_SRC

#+RESULTS:
: tensor([5, 3], dtype=torch.int32)
: tensor([[1, 1, 1],
:         [1, 1, 1],
:         [1, 1, 1],
:         [1, 1, 1],
:         [1, 1, 1]], dtype=torch.int32)

PyTorch also has another syntax for creating a random tensor from another tensor.

#+BEGIN_SRC ipython :session blitz :results output :exports both
print(torch.randn_like(x, dtype=torch.float))
#+END_SRC

#+RESULTS:
: tensor([ 0.6447, -0.9750])

So in this case it kept the shape but used our dtype. The values seemed odd at first, but that's because the =randn= indicates it comes from a standard-normal distribution centered at 0, not some value in the range from zero to one (non-inclusive) like a regular random function would.

**** Tensor Size
Like pandas, the tensor has a shape, but confusingly it's called =Size= and can be accessed either from the =size= method of the =shape= attribute.

#+BEGIN_SRC ipython :session blitz :results output :exports both
print(y.size())
#+END_SRC

#+RESULTS:
: torch.Size([5, 3])

#+BEGIN_SRC ipython :session blitz :results output :exports both
print(y.shape)
#+END_SRC

#+RESULTS:
: torch.Size([5, 3])

#+BEGIN_SRC ipython :session blitz :results output :exports both
print(torch.Size.__base__)
#+END_SRC

#+RESULTS:
: <class 'tuple'>

The =Size= object inherits from tuples and supports all the tuple operations.
*** Operations
**** Addition
    For some operations you can use either the operators (like =+=) or method calls. Here's two ways to do addition.

#+BEGIN_SRC ipython :session blitz :results output :exports both
SIZE = (5, 3)
x = torch.rand(*SIZE)
y = torch.rand(*SIZE)
output = x + y
print(output)
print()
print(torch.add(x, y))
#+END_SRC

#+RESULTS:
#+begin_example
tensor([[0.4370, 1.4905, 0.8806],
        [1.7555, 0.9883, 0.8121],
        [1.1988, 0.6291, 1.2755],
        [1.2424, 1.1548, 1.1025],
        [0.8627, 0.9954, 1.1028]])

tensor([[0.4370, 1.4905, 0.8806],
        [1.7555, 0.9883, 0.8121],
        [1.1988, 0.6291, 1.2755],
        [1.2424, 1.1548, 1.1025],
        [0.8627, 0.9954, 1.1028]])
#+end_example
**** Pre-Made Tensors
One advantage to using the function is that you can pass in a tensor, rather than having pytorch create the output-tensor for you.
#+BEGIN_SRC ipython :session blitz :results output :exports both
summation = torch.empty(SIZE)
torch.add(x, y, out=summation)
print(summation)
#+END_SRC

#+RESULTS:
: tensor([[0.4370, 1.4905, 0.8806],
:         [1.7555, 0.9883, 0.8121],
:         [1.1988, 0.6291, 1.2755],
:         [1.2424, 1.1548, 1.1025],
:         [0.8627, 0.9954, 1.1028]])
**** In-Place Operations
     Tensors also have methods that let you update them instead of creating a new tensor.

#+BEGIN_SRC ipython :session blitz :results output :exports both
x.add_(y)
print(x)
#+END_SRC

#+RESULTS:
: tensor([[0.4370, 1.4905, 0.8806],
:         [1.7555, 0.9883, 0.8121],
:         [1.1988, 0.6291, 1.2755],
:         [1.2424, 1.1548, 1.1025],
:         [0.8627, 0.9954, 1.1028]])

**** Slicing
     The slicing follows what numpy's arrays do. Here's how to get all the rows of the second column.

#+BEGIN_SRC ipython :session blitz :results output :exports both
print(x[:, 1])
#+END_SRC

#+RESULTS:
: tensor([1.4905, 0.9883, 0.6291, 1.1548, 0.9954])

**** Reshaping
     You can create a new tensor with the same data but a different shape using the [[https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view][view]] method.

#+BEGIN_SRC ipython :session blitz :results output :exports both
y = x.view(15)
z = x.view(-1, 5)
print(x.shape)
print(y.shape)
print(z.shape)
#+END_SRC

#+RESULTS:
: torch.Size([5, 3])
: torch.Size([15])
: torch.Size([3, 5])

Using =-1= tells pytorch to infer the dimension based on the original and the dimension that you did pass in.
*** Torch to Numpy
    While there are advantages to using torch for operations (it can use the GPU, for instance), there might be times when you want to convert the tensor to a numpy array.
#+BEGIN_SRC ipython :session blitz :results output :exports both
x = torch.zeros(5)
print(x)
y = x.numpy()
print(y)
x.add_(1)
print(x)
print(y)
print(type(y))
#+END_SRC

#+RESULTS:
: tensor([0., 0., 0., 0., 0.])
: [0. 0. 0. 0. 0.]
: tensor([1., 1., 1., 1., 1.])
: [1. 1. 1. 1. 1.]
: <class 'numpy.ndarray'>

Somehow updating the tensor in place updates the numpy array as well, even though it's an ndarray.
*** Numpy to Torch
    You can go the other way as well.

#+BEGIN_SRC ipython :session blitz :results output :exports both
x = numpy.zeros(5)
print(x)
y = torch.from_numpy(x)
print(y)
x += 5
print(y)
#+END_SRC

#+RESULTS:
: [0. 0. 0. 0. 0.]
: tensor([0., 0., 0., 0., 0.], dtype=torch.float64)
: tensor([5., 5., 5., 5., 5.], dtype=torch.float64)

So updating the array (in place) updates the tensor.

*** Cuda
    As I mentioned before, an advantage of pytorch tensors is that they can be run on the GPU - unfortunately the computer I'm on is old and CUDA doesn't run on it, but we can make a check to see if it will first using =torch.cuda.is_available()

#+BEGIN_SRC ipython :session blitz :results output :exports both
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print(device)

x = torch.ones(5)

# pass in the device
y = torch.ones_like(x, device=device)

# or move the tensor to the device (not an inplace operation)
x = x.to(device)

z = x + y
print(z)
#+END_SRC
* The Return
