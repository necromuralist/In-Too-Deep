#+BEGIN_COMMENT
.. title: Dog and Cat Breed Classification
.. slug: dog-breed-classification
.. date: 2019-04-13 16:14:46 UTC-07:00
.. tags: fastai,deep learning,cnn
.. category: CNN
.. link: 
.. description: Classifying images of dogs and cats by breed.
.. type: text
.. updated: 2020-05-03 16:14:46 UTC-07:00
#+END_COMMENT
#+OPTIONS: ^:{}
#+OPTIONS: H:5
#+TOC: headlines 2
#+PROPERTY: header-args :session /home/athena/.local/share/jupyter/runtime/kernel-e9eb0f6e-5f68-4678-8c81-e17380d39f97-ssh.json
#+BEGIN_SRC python :results none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  This is lesson one from the [[https://www.fast.ai][fastai]] course [[https://course.fast.ai/index.html][Practical Deep Learning for Coders, v3]], which I assume is the third version of the course, and not a reference to a [[https://www.wikiwand.com/en/Kamen_Rider_V3][Japanese television show]]. It uses the [[http://www.fast.ai/2018/10/02/fastai-ai/][fastai V1 library]] which uses [[https://hackernoon.com/pytorch-1-0-468332ba5163][Pytorch 1.0]] but is an [[https://www.wikiwand.com/en/Convention_over_configuration][opinionated framework]] that bundles some sensible defaults so you don't have to spend as much time building the networks.

Val the veterinarian has decided to set up a website to identify dog and cat breeds for people who upload a photo of their pet to his site. To do this he is going to train a neural network to identify the breeds of dogs and cats based on photos of them. It uses the [[http://www.robots.ox.ac.uk/~vgg/data/pets/][Oxford-IIT Pet Dataset]] which was created by researchers at Oxford University's [[http://www.robots.ox.ac.uk/~vgg/][Visual Geometry Group]].
** Imports
*** Python
    Other than the [[https://docs.python.org/3.4/library/re.html][=re=]] none of the python imports were part of the original lesson. 
#+begin_src python :results none
from argparse import Namespace
from functools import partial
from pathlib import Path

import os
import re
#+end_src
*** PyPi

I'll keep the =fast.ai= imports separate to maybe make it easier to reference what comes from where.

#+begin_src python :results none
from fastai.datasets import untar_data, URLs
from fastai.metrics import accuracy, error_rate
from fastai.train import ClassificationInterpretation
from fastai.vision import open_image
from fastai.vision.data import (
    get_image_files, 
    imagenet_stats, 
    ImageDataBunch)
from fastai.vision.learner import cnn_learner
from fastai.vision.models import resnet18, resnet34, resnet50
from fastai.vision.transform import get_transforms
#+end_src

And the rest... 
#+begin_src python :results none
from dotenv import load_dotenv
from ipyexperiments import IPyExperimentsPytorch
from tabulate import tabulate
import holoviews
import matplotlib.pyplot as pyplot
import matplotlib.image as matplotlib_image
import numpy
import pandas
import seaborn
import torch
#+end_src
*** My Stuff
    This is just some convenience stuff wrapped around other people's code (my lite-version of opinionated code).
#+begin_src python :results none
from graeae.tables import CountPercentage
from graeae.timers import Timer
from graeae.visualization import EmbedHoloview
#+end_src
** Setup
*** Some Constants
    There's a lot of values scattered all over the place and I just wanted one place to keep track of them and maybe change them if needed.
#+begin_src python :results none
Net = Namespace(
    random_seed=2,
    batch_size=64,
    low_memory_batch_size=16,
)
#+end_src
*** The Random Seed
    To make this reproducible I'll set the random seed in numpy.
#+begin_src python :results none
numpy.random.seed(Net.random_seed)
#+end_src
*** The Path
    This loads the path where I put the image data-set.
#+begin_src python :results none
load_dotenv(".env", override=True)
DATA_PATH = Path(os.environ.get("OXFORD_PET_DATASET")).expanduser()
#+end_src
*** Plotting
    Although I'd prefer to plot things in HoloViews/bokeh, some of their stuff is too tightly bundled to make it easy (and the image plots maybe don't need to be interactive) so this sets up some formatting for the matplotlib plots.
**** Matplotlib
#+BEGIN_SRC python :results none
get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=1)
#+END_SRC

**** The Bokeh
     This sets up some stuff for the javascript-based plotting.
#+BEGIN_SRC python :results none
holoviews.extension("bokeh")
SLUG = "dog-breed-classification"
OUTPUT_FOLDER = Path("../../files/posts/fastai/")/SLUG
Embed = partial(EmbedHoloview, folder_path=OUTPUT_FOLDER)
#+END_SRC

This is where I'm going to put the settings for the javascript-based plotting.
#+BEGIN_SRC python :results none
Plot = Namespace(
    width = 1000,
    height = 800,
)
#+END_SRC
*** The Timer
    This times how long things take so I can estimate how long it will take if I re-run cells. It also speaks a message so I can do something else and will know that the code is done running without having to watch the messages.

#+BEGIN_SRC python :results none
TIMER = Timer()
#+END_SRC
*** Tabulate
    This is to format tables in the org-mode format (since I'm running this in emacs org-babel).
#+BEGIN_SRC python :results none
ORG_TABLE = partial(tabulate, headers="keys", 
                    showindex=False, 
                    tablefmt="orgtbl")
#+END_SRC
* Middle
** Downloading the Data

As I mentioned before, the data will be the [[http://www.robots.ox.ac.uk/~vgg/data/pets/][Oxford-IIIT Pet Dataset]] by [[http://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf][O. M. Parkhi et al., 2012]]. In the dataset there are twelve breeds of cat and twenty-five breeds of dog. When the researchers performed their experiments in 2012 the best accuracy they got was 59.21 %.

The original lesson uses the [[https://docs.fast.ai/datasets.html#untar_data][untar_data]] function to download the data-set.

#+begin_src python :results output :exports both
help(untar_data)
#+end_src

#+RESULTS:
: Help on function untar_data in module fastai.datasets:
: 
: untar_data(url: str, fname: Union[pathlib.Path, str] = None, dest: Union[pathlib.Path, str] = None, data=True, force_download=False, verbose=False) -> pathlib.Path
:     Download `url` to `fname` if `dest` doesn't exist, and un-tgz to folder `dest`.
: 
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.001
: ･ CPU:          0          0      2,332 MB |
: ･ GPU:          0          0        663 MB |

But, see the next section for why I don't use it.

#+begin_src python :results none
# path = untar_data(URLs.PETS)
# print(path)
#+end_src

This data set is 774 Megabytes and given my over-priced yet still incredibly slow CenturyLink speeds I found downloading it directly from the [[https://course.fast.ai/datasets#image-classification][fastai datasets page]] a little more satisfactory, since the progress widget that runs during the download when =untar_data= downloads the dataset doesn't show up in emacs so it just looks like it's hung up.

#+begin_src python :results output :exports both
assert DATA_PATH.is_dir()
print(DATA_PATH.name)
#+end_src

#+RESULTS:
: oxford-iiit-pet
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.001
: ･ CPU:          0          0      2,332 MB |
: ･ GPU:          0          0        663 MB |

Now let's see what's in the folder.

#+begin_src python :results output raw :exports both
for path in DATA_PATH.iterdir():
    print(f" - {path.name}")
#+end_src

#+RESULTS:
:  - images
:  - annotations
:  - README.org
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.001
: ･ CPU:          0          0      2,332 MB |
: ･ GPU:          0          0        663 MB |

For convenience we can set up two paths - one for the images and one for the annotations (the labels).
#+begin_src python :results none
PATH = Namespace(
    to_annotations = DATA_PATH/'annotations',
    to_images = DATA_PATH/'images',
)
#+end_src
** Looking At the Data
*** Getting the Labels
   Here's where we peek at our data set. The dataset is set up so that the breeds are used in the names of the image files. =fast.ai= has a convenient classmethod named [[https://docs.fast.ai/vision.data.html#ImageDataBunch.from_name_re][ImageDataBunch.from_name_re]] that will extract the labels from the filenames using a [[https://docs.python.org/3.6/library/re.html][regular expression]].

Before we get to that, though, we can take a look at some file names using [[https://docs.fast.ai/vision.data.html#get_image_files][get_image_files]].

#+begin_src python :results output raw :exports both
file_names = get_image_files(PATH.to_images)
for path in file_names[:5]:
    print(f" - {path.name}")
#+end_src

#+RESULTS:
:  - japanese_chin_16.jpg
:  - pug_144.jpg
:  - Sphynx_40.jpg
:  - boxer_12.jpg
:  - Russian_Blue_232.jpg
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.144
: ･ CPU:          1          0      2,332 MB |
: ･ GPU:          0          0        663 MB |

One thing to notice about the names is that the case for the names is important - the names that start with a capital letter represents cats and the names with only lower-case names are dogs.

It also says that the format for the names is <class>_<ID>, where the class is the breed and the ID is a numeric index (each breed has more than one file representing it, so you need this keeps them separated). Besides the naming convention, there is a file named =list.txt= with the species (cat or dog) and a numeric identifier for the breed for each image.

Now I'll construct the pattern to match the file-name.

#+begin_src python :results none
UNDERSCORE = "_"
is_not_a = "^"
end_of_line = "$"
one_or_more = "+"
digit = r"\d"
forward_slash = "/"
character_class = "[{}]"
group = "({})"

anything_but_a_slash = character_class.format(f"{is_not_a}{forward_slash}")

index = rf"{digit}{one_or_more}"
label = group.format(f'{anything_but_a_slash}{one_or_more}')
file_extension = ".jpg"

expression = rf'{forward_slash}{label}{UNDERSCORE}{index}{file_extension}{end_of_line}'
test = "/home/tester/data/datasets/images/oxford-iiit-pet/images/saint_bernard_195.jpg"
assert re.search(expression, test).groups()[0] == "saint_bernard"

test = "/home/tester/data/datasets/images/oxford-iiit-pet/images/Ragdoll_196.jpg"
#+end_src

The reason for the forward slash at the beginning of the expression is that we're passing in the entire path to each image, not just the name of the image.

Now on to the =ImageDataBunch=. Here's the arguments we need to pass in.

#+begin_src python :results output :exports both
print(help(ImageDataBunch.from_name_re))
#+end_src

#+RESULTS:
: Help on method from_name_re in module fastai.vision.data:
: 
: from_name_re(path: Union[pathlib.Path, str], fnames: Collection[pathlib.Path], pat: str, valid_pct: float = 0.2, **kwargs) method of builtins.type instance
:     Create from list of `fnames` in `path` with re expression `pat`.
: 
: None
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.002
: ･ CPU:          0          0      2,332 MB |
: ･ GPU:          0          0        663 MB |

Here's the arguments that we'll pass in.

| Argument  | Description                                                     |
|-----------+-----------------------------------------------------------------|
| =path=    | The path to the folder for temporary files                      |
| =fnames=  | A list of file names                                            |
| =pat=     | Regular expression to extract the labels from the names         |
| =ds_tfms= | A tuple of data transformation functions to apply to the images |
| =size=    | Argument to the data transform (augmentation) functions         |
| =bs=      | The batch size                                                  |

Okay, so let's get the labels.

#+begin_src python :results none
data = ImageDataBunch.from_name_re(PATH.to_images, 
                                   file_names, 
                                   expression, 
                                   ds_tfms=get_transforms(), 
                                   size=224, 
                                   bs=Net.batch_size
                                  ).normalize(imagenet_stats)
#+end_src

One of the arguments we passed in (=ds_tfms=) isn't particularly obviously named, unless you already know about applying transforms to images, but here's what we passed to it.

#+begin_src python :results output :exports both
print(help(get_transforms))
#+end_src

#+RESULTS:
: Help on function get_transforms in module fastai.vision.transform:
: 
: get_transforms(do_flip: bool = True, flip_vert: bool = False, max_rotate: float = 10.0, max_zoom: float = 1.1, max_lighting: float = 0.2, max_warp: float = 0.2, p_affine: float = 0.75, p_lighting: float = 0.75, xtra_tfms: Union[Collection[fastai.vision.image.Transform], NoneType] = None) -> Collection[fastai.vision.image.Transform]
:     Utility func to easily create a list of flip, rotate, `zoom`, warp, lighting transforms.
: 
: None
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.002
: ･ CPU:          0          0      2,332 MB |
: ･ GPU:          0          0        663 MB |

[[https://docs.fast.ai/vision.transform.html#get_transforms][get_transforms]] adds random changes to the images to augment the datasets for our training.

We also added a call to [[https://docs.fast.ai/vision.data.html#normalize][normalize]] which sets the mean and standard deviation of the images to match those of the images used to train the model that we're going to use ([[https://arxiv.org/abs/1512.03385][ResNet]]).

*** Looking at Some of the Images
The [[https://docs.fast.ai/basic_data.html#DataBunch.show_batch][show_batch]] method will plot some of the images in matplotlib. It retrieves them randomly so calling the method repeatedly will pull up different images. Unfortunately you can't pass in a figure or axes so it isn't easily configurable.

#+begin_src python :results output :exports both
help(data.show_batch)
#+end_src

#+RESULTS:
: Help on method show_batch in module fastai.basic_data:
: 
: show_batch(rows: int = 5, ds_type: fastai.basic_data.DatasetType = <DatasetType.Train: 1>, reverse: bool = False, **kwargs) -> None method of fastai.vision.data.ImageDataBunch instance
:     Show a batch of data in `ds_type` on a few `rows`.
: 
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.001
: ･ CPU:          0          0      2,332 MB |
: ･ GPU:          0          0        663 MB |

Now I'll call it to get the batch.

#+begin_src python :results raw drawer :file ../../files/posts/fastai/dog-breed-classification/show_batch.png
data.show_batch(rows=3, figsize=(7,6))
#+end_src

#+RESULTS:
:RESULTS:
: /home/hades/.virtualenvs/In-Too-Deep/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
:   warnings.warn("The default behavior for interpolate/upsample with float scale_factor will change "
: /home/hades/.virtualenvs/In-Too-Deep/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
:   warnings.warn("The default behavior for interpolate/upsample with float scale_factor will change "
: /home/hades/.virtualenvs/In-Too-Deep/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
:   warnings.warn("The default behavior for interpolate/upsample with float scale_factor will change "
: /home/hades/.virtualenvs/In-Too-Deep/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
:   warnings.warn("The default behavior for interpolate/upsample with float scale_factor will change "
: /home/hades/.virtualenvs/In-Too-Deep/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
:   warnings.warn("The default behavior for interpolate/upsample with float scale_factor will change "
#+attr_org: :width 449 :height 419
[[file:../../files/posts/fastai/dog-breed-classification/show_batch.png]]
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:04.439
: ･ CPU:          1         10      2,332 MB |
: ･ GPU:          0        114        663 MB |
:END:

[[file:show_batch.png]]


I'm guessing that the reason why so many images look "off" is because the of the data-transforms being added, and not that the photographers were horrible (or drunk). Why don't we look at the representation of the data bunch?

#+begin_src python :results output :exports both
print(data)
#+end_src

#+RESULTS:
#+begin_example
ImageDataBunch;

Train: LabelList (5912 items)
x: ImageList
Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
y: CategoryList
japanese_chin,pug,Sphynx,Russian_Blue,saint_bernard
Path: /home/hades/data/datasets/images/oxford-iiit-pet/images;

Valid: LabelList (1478 items)
x: ImageList
Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)
y: CategoryList
Abyssinian,Bombay,great_pyrenees,english_cocker_spaniel,english_setter
Path: /home/hades/data/datasets/images/oxford-iiit-pet/images;

Test: None
･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.182
･ CPU:          0          3      2,332 MB |
･ GPU:          0          0        663 MB |
/home/hades/.virtualenvs/In-Too-Deep/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn("The default behavior for interpolate/upsample with float scale_factor will change "
#+end_example

So it looks like the =ImageDataBunch= created a training and a validation set and based on the shapes, each of the images has three channels and is 224 x 224 pixels. Also note that although it only displays five labels (y) it actually has more.

#+begin_src python :results output :exports both
print(len(set(data.label_list.y)))
#+end_src

#+RESULTS:
: 37
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.241
: ･ CPU:          0          0      2,332 MB |
: ･ GPU:          0          0        663 MB |

** Training: resnet34

 Here's where we train the model - a [[http://cs231n.github.io/convolutional-networks/][convolutional neural network]] in the back with a fully-connected network at the end.

I'll use =fast.ai's= [[https://docs.fast.ai/vision.learner.html#cnn_learner][cnn_learner]] to load the data, pre-trained model (=resnet34=), and  metric to use when training ([[https://docs.fast.ai/metrics.html#error_rate][error_rate]]). If you look at the [[https://github.com/fastai/fastai/blob/master/fastai/vision/models/__init__.py][fast ai code]] they are importing the =resnet34= model from [[https://pytorch.org/docs/stable/torchvision/models.html#id3][pytorch's torchvision]].

This next block sets up the [[https://github.com/stas00/ipyexperiments/blob/master/docs/ipyexperiments.md][IPyExperiments]] which will delete all the variables that were created after it was created when it is deleted. Okay, that's a weird sentence - it's going to clean up stuff for me. This is to free up memory because the =resnet= architecture takes up a lot of memory on the GPU.

#+begin_src python :results output :exports both
experiment = IPyExperimentsPytorch()
#+end_src

#+RESULTS:
#+begin_example

,*** Experiment started with the Pytorch backend
Device: ID 0, GeForce GTX 1070 Ti (8118 RAM)


,*** Current state:
RAM:    Used    Free   Total       Util
CPU:   2,332  29,014  32,099 MB   7.27% 
GPU:     663   7,454   8,118 MB   8.17% 


･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.179
･ CPU:          0          0      2,332 MB |
･ GPU:          0          0        663 MB |

IPyExperimentsPytorch: Finishing

,*** Experiment finished in 00:25:15 (elapsed wallclock time)

,*** Experiment memory:
RAM: Consumed       Reclaimed
CPU:      -33        0 MB ( -0.00%)
GPU:        0        0 MB (100.00%)

,*** Current state:
RAM:    Used    Free   Total       Util
CPU:   2,332  29,014  32,099 MB   7.27% 
GPU:     663   7,454   8,118 MB   8.17% 


･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.176
･ CPU:          0          0      2,332 MB |
･ GPU:          0          0        663 MB |
#+end_example


#+begin_src python :results output :exports both
learn = cnn_learner(data, resnet34, metrics=error_rate)
#+end_src

#+RESULTS:
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:01.085
: ･ CPU:          0          0      2,417 MB |
: ･ GPU:         86          0        749 MB |

#+begin_example
Downloading: "https://download.pytorch.org/models/resnet34-333f7ec4.pth" to /home/athena/.torch/models/resnet34-333f7ec4.pth
87306240it [00:26, 3321153.99it/s]
#+end_example

As you can see, it downloaded the stored model parameters from pytorch. This is because I've never downloaded this particular model before - if you run it again it shouldn't need to re-download it. Since this is a [[https://pytorch.org][pytorch]] model we can look at it's represetantion to see the architecture of the network.

#+begin_src python :results output :exports both
print(learn.model)
#+end_src

#+RESULTS:
#+begin_example
Sequential(
  (0): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (1): Sequential(
    (0): AdaptiveConcatPool2d(
      (ap): AdaptiveAvgPool2d(output_size=1)
      (mp): AdaptiveMaxPool2d(output_size=1)
    )
    (1): Flatten()
    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Dropout(p=0.25, inplace=False)
    (4): Linear(in_features=1024, out_features=512, bias=True)
    (5): ReLU(inplace=True)
    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): Dropout(p=0.5, inplace=False)
    (8): Linear(in_features=512, out_features=37, bias=True)
  )
)
･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.008
･ CPU:          0          0      2,341 MB |
･ GPU:          0          0        749 MB |
#+end_example

That's a pretty big network, but the main thing to notice is the last layer, which has 37 =out_features= which corresponds to the number of breeds we have in our data-set. If you were working directly with pytorch you'd have to remove the last layer and add it back yourself, but =fast.ai= has done this for us.

Now we need to train it using the [[https://docs.fast.ai/train.html#fit_one_cycle][fit_one_cycle]] method. At first I thought 'one cycle' meant just one pass through the batches but according to the [[https://docs.fast.ai/callbacks.one_cycle.html][documentation]], this is a reference to a training method called the [[https://sgugger.github.io/the-1cycle-policy.html][1Cycle Policy]] proposed by [[https://arxiv.org/abs/1803.09820][Leslie N. Smith]] that changes the hyperparameters to make the model train faster.

#+BEGIN_SRC python :results output :exports both
TIMER.message = "Finished fitting the ResNet 34 Model."
with TIMER:
    learn.fit_one_cycle(4)
#+END_SRC

Depending on how busy the computer is this takes two to three minutes when I run it. Next let's store the parameters for the trained model to disk.

#+BEGIN_SRC python :results none
MODELS = Path(os.environ["MODELS"]).expanduser()/"fastai/dogs-and-cats"
learn.save(MODELS/'stage-1')
#+END_SRC

   Let's look at how the model did. If I was running this in a jupyter notebook there would be a table output of the accuracy, but I'm not, and I can't find any documentation on how to get that myself, so, tough luck, then. We can look at some things after the fact, though - the [[https://docs.fast.ai/train.html#ClassificationInterpretation][ClassificationInterpretation]] class contains methods to help look at how the model did.

#+BEGIN_SRC python :results none
interpreter = ClassificationInterpretation.from_learner(learn)
#+END_SRC

The [[https://docs.fast.ai/vision.learner.html#ClassificationInterpretation.top_losses][top_losses]] method returns a tuple of the highest losses along with the indices of the data that gave those losses. By default it actually gives all the losses sorted from largest to smallest, but you can pass in an integer to limit how much it returns.

#+BEGIN_SRC python :results output :exports both
losses, indexes = interpreter.top_losses()
print(losses)
print(indexes)
assert len(data.valid_ds)==len(losses)==len(indexes)
#+END_SRC

#+RESULTS:
: tensor([9.0430e+00, 7.5851e+00, 6.4739e+00,  ..., 4.7684e-07, 1.1921e-07,
:         -0.0000e+00])
: tensor([1057, 1100,  624,  ...,  848,  415,  980])
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.003
: ･ CPU:          0          0      2,341 MB |
: ･ GPU:          0          0        689 MB |


#+BEGIN_SRC python :results none
plot = holoviews.Distribution(losses).opts(title="Loss Distribution", 
                                           xlabel="Loss", 
                                           width=Plot.width, 
                                           height=Plot.height)
output = Embed(plot=plot, file_name="loss_distribution")
output()
#+END_SRC

#+BEGIN_SRC python :results html :exports both
print(output.source)
#+END_SRC

#+RESULTS:
#+begin_export html
: <object type="text/html" data="loss_distribution.html" style="width:100%" height=800>
:   <p>Figure Missing</p>
: </object>
#+end_export

Although it looks like there are negative losses, that's just the way the distribution works out, most of the losses are around zero.

#+BEGIN_SRC python :results output :exports both
print(losses.max())
print(losses.min())
#+END_SRC

#+RESULTS:
: tensor(9.0430)
: tensor(-0.)
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.003
: ･ CPU:          0          0      2,341 MB |
: ･ GPU:          0          0        689 MB |

Here's a count of the losses when they are broken up into ten bins.

#+BEGIN_SRC python :results output raw :exports both
bins = pandas.cut(losses.tolist(), bins=10).value_counts().reset_index()
total = bins[0].sum()
percentage = 100 * bins[0]/total
bins["percent"] = percentage
print(ORG_TABLE(bins, headers="Range Count Percent(%)".split()))
#+END_SRC

#+RESULTS:
| Range             |   Count |   Percent(%) |
|-------------------+---------+--------------|
| (-0.00904, 0.904] |    1381 |    93.4371   |
| (0.904, 1.809]    |      38 |     2.57104  |
| (1.809, 2.713]    |      28 |     1.89445  |
| (2.713, 3.617]    |      17 |     1.1502   |
| (3.617, 4.521]    |       6 |     0.405954 |
| (4.521, 5.426]    |       4 |     0.270636 |
| (5.426, 6.33]     |       1 |     0.067659 |
| (6.33, 7.234]     |       1 |     0.067659 |
| (7.234, 8.139]    |       1 |     0.067659 |
| (8.139, 9.043]    |       1 |     0.067659 |

#+begin_src python :resultos output :exports both
print(learn.loss_func)
#+end_src

#+RESULTS:
: FlattenedLoss of CrossEntropyLoss()

So our "loss" represents [[https://www.wikiwand.com/en/Cross_entropy][cross-entropy loss]] Any time you see the workd "entropy" in a Computer Science context you should remember that it's one of the main ideas behind Information Theory - and then you should slowly back away and try not to make any sudden movements that might lead anyone to think that you're actively involved with this scene. Another thing we can do is plot the images that had the highest losses.

#+begin_src python :results output :exports both
print(numpy.median(losses.tolist()))
#+end_src

#+RESULTS:
: 0.0029419257771223783

So the median error rate is 0.3 %, which seems like a good rate.

#+begin_src python :results raw drawer :file ../../files/posts/fastai/dog-breed-classification/top_losses.png
interpreter.plot_top_losses(9, figsize=(15,11))
#+END_SRC

#+RESULTS:
:RESULTS:
#+attr_org: :width 833 :height 691
[[file:../../files/posts/fastai/dog-breed-classification/top_losses.png]]
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:03.513
: ･ CPU:          2         15      2,341 MB |
: ･ GPU:          0          0        689 MB |
:END:

[[file:top_losses.png]]


#+begin_src python :results output :exports both
print(help(interpreter.plot_top_losses))
#+END_SRC

#+RESULTS:
: Help on method _cl_int_plot_top_losses in module fastai.vision.learner:
: 
: _cl_int_plot_top_losses(k, largest=True, figsize=(12, 12), heatmap: bool = False, heatmap_thresh: int = 16, alpha: float = 0.6, cmap: str = 'magma', show_text: bool = True, return_fig: bool = None) -> Union[matplotlib.figure.Figure, NoneType] method of fastai.train.ClassificationInterpretation instance
:     Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.
: 
: None
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.003
: ･ CPU:          0          0      2,341 MB |
: ･ GPU:          0          0        689 MB |

*Note:* in the original notebook they were using a function called [[https://github.com/fastai/fastai/blob/master/fastai/gen_doc/nbdoc.py#L126][doc]], which tries to open another window and will thus hang when run in emacs. They /really/ want you to use jupyter.

Next let's look at the [[https://www.wikiwand.com/en/Confusion_matrix][confusion matrix]].

#+begin_src python :results raw drawer :file ../../files/posts/fastai/dog-breed-classification/confusion_matrix.png
interpreter.plot_confusion_matrix(figsize=(12,12), dpi=60)
#+END_SRC

#+RESULTS:
:RESULTS:
#+attr_org: :width 712 :height 724
[[file:../../files/posts/fastai/dog-breed-classification/confusion_matrix.png]]
: ･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:07.730
: ･ CPU:          0         43      2,342 MB |
: ･ GPU:          0          0        689 MB |
:END:

[[file:confusion_matrix.png]]

One way to interpret this is to look at the x-axis (the actual breed) and sweep vertically up to see the counts for the y-axis (what our model predicted it was). The diagonal cells from the top left to the bottom right is where the predicted matched the actual. In this case, the fact that almost all the counts are in the diagonal means our model did pretty well at predicting the breeds in the images.

If you compare the images with the worst losses to the confusion matrix you'll notice that they don't seem to correlate with the worst performances overall - the worst losses were one-offs. I should also note that when I ran the notebook previously the most confused were images that had a flare effect applied to them (presumably during the data augmentation step) but that isn't the case here. Perhaps improvements have been made upstream to the model... The most confused was the /Egyptian Mau/ being confused for a /Bengal/.

Here's the breeds that were the hardest for the model to predict.

#+begin_src python :results output raw :exports both
print(ORG_TABLE(interpreter.most_confused(min_val=3), 
                headers="Actual Predicted Count".split()))
#+END_SRC

#+RESULTS:
| Actual                    | Predicted                  |   Count |
|---------------------------+----------------------------+---------|
| Egyptian_Mau              | Bengal                     |       6 |
| american_pit_bull_terrier | staffordshire_bull_terrier |       6 |
| Birman                    | Ragdoll                    |       4 |
| Ragdoll                   | Birman                     |       4 |
| British_Shorthair         | Russian_Blue               |       3 |
| Russian_Blue              | British_Shorthair          |       3 |
| Siamese                   | Birman                     |       3 |
| american_bulldog          | staffordshire_bull_terrier |       3 |
| american_pit_bull_terrier | american_bulldog           |       3 |
| american_pit_bull_terrier | miniature_pinscher         |       3 |
| beagle                    | basset_hound               |       3 |

It doesn't look too bad, actually, other that the first few entries, maybe. I should note here that sometimes when I train the model it ends up with different cases for most confused - it's generally the same suspects but in a different order (e.g. sometimes the American Pit Bull Terrier is confused for the Staffordshire Bull Terrier more than the other breeds are confused).

* Return
** Looking at a Labrador
  Okay, it's nice that the confusion matrix looks okay, but we want to actually make predictions on images. Let's start with a Labrador.

[[file:labrador.jpg]]

(Source: [[https://www.needpix.com/photo/1101375/labrador-retriever-puppy-dog-lab-black-canine-young-breed][needpix.com]])

So we know it's a Labrador Retriever, but what does our model think?

*Note:* for this next block to work you need to edit the =.env= file on your remote machine to have the =CATS_AND_DOGS= entry and have a directory on the machine with the matching folder.

#+begin_src python :results output :exports both
TEST_PATH = Path(os.environ["CATS_AND_DOGS"]).expanduser()
assert TEST_PATH.is_dir()
image = open_image(TEST_PATH/"labrador.jpg")
classification = learn.predict(image)
print(classification[0])
#+end_src

#+RESULTS:
: staffordshire_bull_terrier

So, it thinks our labrador is a Staffordshire Bull Terrier? Despite the good results on the training set, it's starting to look a little suspect. At least it got the species right.
** Maybe a Ragdoll

[[file:simba-ragdoll.jpg]]



#+begin_src python :results output :exports both
image = open_image(TEST_PATH/"Simba_ragdoll_cat.jpg")
classification = learn.predict(image)
print(classification[0])
#+end_src

#+RESULTS:
: Birman
So it thinks our ragdoll is a birman... that's actually not out of bound for our training set outcomes.

** Try a Staffordshire Bull Terrier

[[file:Staffordshire-bull-terrier-white-2748733.jpg]]

(Source: [[https://pixabay.com/photos/staffordshire-bull-terrier-staffy-2748733/][pixabay]])

#+begin_src python :results output :exports both
image = open_image(TEST_PATH/"Staffordshire-bull-terrier-white-2748733.jpg")
classification = learn.predict(image)
print(classification[0])
#+end_src

#+RESULTS:
: american_bulldog

So it has the same problems that it did with the training set, not really impressive so far. Although, truthfully, I have no idea how to tell any of these breeds apart (other than the Labrador, how could anyone not recognize a Labrador?).
** Okay, So Maybe a Newfoundland?

[[file:newfoundland-609531_1280.jpg]]


(Source: [[https://pixabay.com/photos/newfoundland-dog-black-609531/][pixabay]])

#+begin_src python :results output :exports both
image = open_image(TEST_PATH/"newfoundland-609531_1280.jpg")
classification = learn.predict(image)
print(classification[0])
#+end_src

#+RESULTS:
: newfoundland

Well, so it got one right...
