#+BEGIN_COMMENT
.. title: NLP Classification Exercise
.. slug: nlp-classification-exercise
.. date: 2019-09-29 11:28:06 UTC-07:00
.. tags: nlp,embeddings
.. category: NLP
.. link: 
.. description: Walking through an embeddings exercise.
.. type: text
#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
* Beginning
** Imports
*** Python
#+begin_src ipython :session kernel-3945-ssh.json :results none
from argparse import Namespace
from functools import partial
from pathlib import Path
#+end_src
*** PyPi
#+begin_src ipython :session kernel-3945-ssh.json :results none
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import hvplot.pandas
import numpy
import pandas
import tensorflow
#+end_src
*** Others
#+begin_src ipython :session kernel-3945-ssh.json :results none
from graeae import (CountPercentage,
                    EmbedHoloviews,
                    SubPathLoader,
                    Timer,
                    ZipDownloader)
#+end_src
** Set Up
*** The Timer
#+begin_src ipython :session kernel-3945-ssh.json :results none
TIMER = Timer()
#+end_src
*** The Plotting
#+begin_src ipython :session kernel-3945-ssh.json :results none
slug = "nlp-classification-exercise"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{slug}")
#+end_src
*** The Dataset
    It isn't mentioned in the notebook where the data originally came from, but it looks like it's the [[http://help.sentiment140.com/home][Sentiment140]] dataset, which consists of tweets whose sentiment was inferred by emoticons in each tweet.
#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
url = "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
path = Path("~/data/datasets/texts/sentiment140/").expanduser()
download = ZipDownloader(url, path)
download()
#+end_src

#+RESULTS:
: Files exist, not downloading

#+begin_src ipython :session kernel-3945-ssh.json :results none
columns = ["polarity", "tweet_id", "datetime", "query", "user", "text"]
training = pandas.read_csv(path/"training.1600000.processed.noemoticon.csv", 
                           encoding="latin-1", names=columns, header=None)
testing = pandas.read_csv(path/"testdata.manual.2009.06.14.csv", 
                           encoding="latin-1", names=columns, header=None)
#+end_src

*** Some Constants
#+begin_src ipython :session kernel-3945-ssh.json :results none
Text = Namespace(
    embedding_dim = 100,
    max_length = 16,
    trunc_type='post',
    padding_type='post',
    oov_tok = "<OOV>",
    training_size=16000,
)
#+end_src
* Middle
** The Data
#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
print(training.sample().iloc[0])
#+end_src

#+RESULTS:
: polarity                                                    0
: tweet_id                                           1999685047
: datetime                         Mon Jun 01 20:38:41 PDT 2009
: query                                                NO_QUERY
: user                                                     JETX
: text        Just had to freaking fight tech support to giv...
: Name: 304226, dtype: object

#+begin_src ipython :session kernel-3945-ssh.json :results output raw :exports both
CountPercentage(training.polarity)()
#+end_src

#+RESULTS:
| Value | Count   | Percent (%) |
|-------+---------+-------------|
|     4 | 800,000 |       50.00 |
|     0 | 800,000 |       50.00 |

The =polarity= is what might also be called the "sentiment" of the tweet - /0/ means a negative tweet and /4/ means a positive tweet.

But, for our purposes, we would be better off if the positive polarity was =1=, not =4=, so let's convert it.

#+begin_src ipython :session kernel-3945-ssh.json :results output raw :exports both
training.loc[training.polarity==4, "polarity"] = 1
counts = CountPercentage(training.polarity)()
#+end_src

#+RESULTS:
| Value | Count   | Percent (%) |
|-------+---------+-------------|
|     1 | 800,000 |       50.00 |
|     0 | 800,000 |       50.00 |

** The Tokenizer
   As you can see from the sample, the data is still in text form so we need to convert it to a numeric form with a Tokenizer. 

First I'll Lower-case it.

#+begin_src ipython :session kernel-3945-ssh.json :results none
training.loc[:, "text"] = training.text.str.lower()
#+end_src

Next we'll fit it to our text.

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
tokenizer = Tokenizer()
with TIMER:
    tokenizer.fit_on_texts(training.text.values)
#+end_src

#+RESULTS:
: 2019-10-06 18:44:00,612 graeae.timers.timer start: Started: 2019-10-06 18:44:00.612015
: WARNING: Logging before flag parsing goes to stderr.
: I1006 18:44:00.612909 140055379531584 timer.py:70] Started: 2019-10-06 18:44:00.612015
: 2019-10-06 18:44:37,167 graeae.timers.timer end: Ended: 2019-10-06 18:44:37.167109
: I1006 18:44:37.167169 140055379531584 timer.py:77] Ended: 2019-10-06 18:44:37.167109
: 2019-10-06 18:44:37,170 graeae.timers.timer end: Elapsed: 0:00:36.555094
: I1006 18:44:37.170241 140055379531584 timer.py:78] Elapsed: 0:00:36.555094

Now, we can store some of it's values in variables for convenience.

#+begin_src ipython :session kernel-3945-ssh.json :results none
word_index = tokenizer.word_index
vocabulary_size = len(tokenizer.word_index)
#+end_src

Now, we'll convert the texts to sequences and pad them so they are all the same length.

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
with TIMER:
    sequences = tokenizer.texts_to_sequences(training.text.values)
    padded = pad_sequences(sequences, maxlen=Text.max_length,
                           truncating=Text.trunc_type)

    splits = train_test_split(
        padded, training.polarity, test_size=.2)

    training_sequences, test_sequences, training_labels, test_labels = splits
#+end_src

#+RESULTS:
: 2019-10-06 18:50:31,373 graeae.timers.timer start: Started: 2019-10-06 18:50:31.373126
: I1006 18:50:31.373161 140055379531584 timer.py:70] Started: 2019-10-06 18:50:31.373126
: 2019-10-06 18:51:12,694 graeae.timers.timer end: Ended: 2019-10-06 18:51:12.694651
: I1006 18:51:12.694694 140055379531584 timer.py:77] Ended: 2019-10-06 18:51:12.694651
: 2019-10-06 18:51:12,697 graeae.timers.timer end: Elapsed: 0:00:41.321525
: I1006 18:51:12.697135 140055379531584 timer.py:78] Elapsed: 0:00:41.321525

** GloVe
   GloVe is short for /Global Vectors for Word Representation/. It is an /unsupervised/ algorithm that creates vector representations for words. They have a [[https://nlp.stanford.edu/projects/glove/][site]] where you can download pre-trained models or get the code and train one yourself. We're going to use one of their pre-trained models.

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
path = Path("~/models/glove/").expanduser()
url = "http://nlp.stanford.edu/data/glove.6B.zip"
ZipDownloader(url, path)()
#+end_src

#+RESULTS:
: Files exist, not downloading

The GloVe data is stored as a series of space separated lines with the first column being the word that's encoded and the rest of the columns being the values for the vector. To make this work we're going to split the word off from the vector and put each into a dictionary.

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
embeddings = {}
with TIMER:
    with open(path/"glove.6B.100d.txt") as lines:
        for line in lines:
            tokens = line.split()
            embeddings[tokens[0]] = numpy.array(tokens[1:])
#+end_src

#+RESULTS:
: 2019-10-06 18:55:11,592 graeae.timers.timer start: Started: 2019-10-06 18:55:11.592880
: I1006 18:55:11.592908 140055379531584 timer.py:70] Started: 2019-10-06 18:55:11.592880
: 2019-10-06 18:55:21,542 graeae.timers.timer end: Ended: 2019-10-06 18:55:21.542689
: I1006 18:55:21.542738 140055379531584 timer.py:77] Ended: 2019-10-06 18:55:21.542689
: 2019-10-06 18:55:21,544 graeae.timers.timer end: Elapsed: 0:00:09.949809
: I1006 18:55:21.544939 140055379531584 timer.py:78] Elapsed: 0:00:09.949809

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
print(f"{len(embeddings):,}")
#+end_src

#+RESULTS:
: 400,000

So, our vocabulary consists of 400,000 "words" (tokens is more accurate, since they also include punctuation). The problem we have to deal with next is that our data set wasn't part of the dataset used to train the embeddings, so there will probably be some tokens in our data set that aren't in the embeddings. To handle this we need to add zeroed embeddings for the extra tokens.

Rather than adding to the dict, we'll create a matrix of zeros with rows for each word in our datasets vocabulary, then we'll iterate over the words in our dataset and if there's a match in the GloVE embeddings we'll insert it into the matrix.

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
with TIMER:
    embeddings_matrix = numpy.zeros((vocabulary_size+1, Text.embedding_dim));
    for word, index in word_index.items():
        embedding_vector = embeddings.get(word);
        if embedding_vector is not None:
            embeddings_matrix[index] = embedding_vector;
#+end_src

#+RESULTS:
: 2019-10-06 18:55:46,577 graeae.timers.timer start: Started: 2019-10-06 18:55:46.577855
: I1006 18:55:46.577886 140055379531584 timer.py:70] Started: 2019-10-06 18:55:46.577855
: 2019-10-06 18:55:51,374 graeae.timers.timer end: Ended: 2019-10-06 18:55:51.374706
: I1006 18:55:51.374763 140055379531584 timer.py:77] Ended: 2019-10-06 18:55:51.374706
: 2019-10-06 18:55:51,377 graeae.timers.timer end: Elapsed: 0:00:04.796851
: I1006 18:55:51.377207 140055379531584 timer.py:78] Elapsed: 0:00:04.796851

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
print(f"{len(embeddings_matrix):,}")
#+end_src

#+RESULTS:
: 690,961
** The Models
*** A CNN
**** Build
#+begin_src ipython :session kernel-3945-ssh.json :results none
convoluted_model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(
        vocabulary_size + 1,
        Text.embedding_dim,
        input_length=Text.max_length,
        weights=[embeddings_matrix],
        trainable=False),
    tensorflow.keras.layers.Conv1D(filters=128,
                                   kernel_size=5,
    activation='relu'),
    tensorflow.keras.layers.GlobalMaxPooling1D(),
    tensorflow.keras.layers.Dense(24, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
convoluted_model.compile(loss="binary_crossentropy", optimizer="rmsprop",
                         metrics=["accuracy"])
#+end_src

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
print(convoluted_model.summary())
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 16, 100)           69096100  
_________________________________________________________________
conv1d (Conv1D)              (None, 12, 128)           64128     
_________________________________________________________________
global_max_pooling1d (Global (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 24)                3096      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 25        
=================================================================
Total params: 69,163,349
Trainable params: 67,249
Non-trainable params: 69,096,100
_________________________________________________________________
None
#+end_example

**** Train
#+begin_src ipython :session kernel-3945-ssh.json :results none
Training = Namespace(
    size = 0.75,
    epochs = 2,
    verbosity = 2,
    batch_size=128,
    )
#+end_src

#+begin_src ipython :session kernel-3945-ssh.json :results output :exports both
with TIMER:
    cnn_history = convoluted_model.fit(training_sequences,
                                       training_labels.values,
                                       epochs=Training.epochs,
                                       batch_size=Training.batch_size,
                                       validation_data=(test_sequences, test_labels.values),
                                       verbose=Training.verbosity)
#+end_src
**** Some Plotting

#+begin_src ipython :session kernel-3945-ssh.json :results output raw :exports both
performance = pandas.DataFrame(cnn_history.history)
plot = performance.hvplot().opts(title="CNN Twitter Sentiment Training Performance",
                                 width=1000,
                                 height=800)
Embed(plot=plot, file_name="cnn_training")()
#+end_src
* End
** Citations
   - Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. 
* Raw
#+begin_comment
import json
import tensorflow as tf
import csv
import random
import numpy as np

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import regularizers


embedding_dim = 100
max_length = 16
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"
training_size=#Your dataset size here. Experiment using smaller values (i.e. 16000), but don't forget to train on at least 160000 to see the best effects
test_portion=.1

corpus = []


# In[ ]:



# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader
# You can do that yourself with:
# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv
# I then hosted it on my site to make it easier to use in this notebook

get_ipython().system('wget --no-check-certificate     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv     -O /tmp/training_cleaned.csv')

num_sentences = 0

with open("/tmp/training_cleaned.csv") as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    for row in reader:
      # Your Code here. Create list items where the first item is the text, found in row[5], and the second is the label. Note that the label is a '0' or a '4' in the text. When it's the former, make
      # your label to be 0, otherwise 1. Keep a count of the number of sentences in num_sentences
        list_item=[]
        # YOUR CODE HERE
        num_sentences = num_sentences + 1
        corpus.append(list_item)



# In[ ]:


print(num_sentences)
print(len(corpus))
print(corpus[1])

# Expected Output:
# 1600000
# 1600000
# ["is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!", 0]


# In[ ]:


sentences=[]
labels=[]
random.shuffle(corpus)
for x in range(training_size):
    sentences.append(# YOUR CODE HERE)
    labels.append(# YOUR CODE HERE)


tokenizer = Tokenizer()
tokenizer.fit_on_texts(# YOUR CODE HERE)

word_index = tokenizer.word_index
vocab_size=len(# YOUR CODE HERE)

sequences = tokenizer.texts_to_sequences(# YOUR CODE HERE)
padded = pad_sequences(# YOUR CODE HERE)

split = int(test_portion * training_size)

test_sequences = padded[# YOUR CODE HERE]
training_sequences = padded[# YOUR CODE HERE]
test_labels = labels[# YOUR CODE HERE]
training_labels = labels[# YOUR CODE HERE]


# In[ ]:


print(vocab_size)
print(word_index['i'])
# Expected Output
# 138858
# 1


# In[ ]:


# Note this is the 100 dimension version of GloVe from Stanford
# I unzipped and hosted it on my site to make this notebook easier
get_ipython().system('wget --no-check-certificate     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt     -O /tmp/glove.6B.100d.txt')
embeddings_index = {};
with open('/tmp/glove.6B.100d.txt') as f:
    for line in f:
        values = line.split();
        word = values[0];
        coefs = np.asarray(values[1:], dtype='float32');
        embeddings_index[word] = coefs;

embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word);
    if embedding_vector is not None:
        embeddings_matrix[i] = embedding_vector;


# In[ ]:


print(len(embeddings_matrix))
# Expected Output
# 138859


# In[ ]:


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),
    # YOUR CODE HERE - experiment with combining different types, such as convolutions and LSTMs
])
model.compile(# YOUR CODE HERE)
model.summary()

num_epochs = 50
history = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=2)

print("Training Complete")


# In[ ]:


import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

#-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
acc=history.history['acc']
val_acc=history.history['val_acc']
loss=history.history['loss']
val_loss=history.history['val_loss']

epochs=range(len(acc)) # Get number of epochs

#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and validation accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Accuracy", "Validation Accuracy"])

plt.figure()

#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.title('Training and validation loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Loss", "Validation Loss"])

plt.figure()


# Expected Output
# A chart where the validation loss does not increase sharply!
#+end_comment
