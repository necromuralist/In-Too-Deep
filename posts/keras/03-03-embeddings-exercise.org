#+BEGIN_COMMENT
.. title: NLP Classification Exercise
.. slug: nlp-classification-exercise
.. date: 2019-09-29 11:28:06 UTC-07:00
.. tags: nlp,embeddings
.. category: NLP
.. link: 
.. description: Walking through an embeddings exercise.
.. type: text
#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
* Beginning
** Imports
*** Python
#+begin_src ipython :session kernel-17746-ssh.json :results none
from argparse import Namespace
from functools import partial
from pathlib import Path
#+end_src
*** PyPi
#+begin_src ipython :session kernel-17746-ssh.json :results none
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import hvplot.pandas
import numpy
import pandas
import tensorflow
#+end_src
*** Others
#+begin_src ipython :session kernel-17746-ssh.json :results none
from graeae import (CountPercentage,
                    EmbedHoloviews,
                    SubPathLoader,
                    Timer,
                    ZipDownloader)
#+end_src
** Set Up
*** The Timer
#+begin_src ipython :session kernel-17746-ssh.json :results none
TIMER = Timer()
#+end_src
*** The Plotting
#+begin_src ipython :session kernel-17746-ssh.json :results none
slug = "nlp-classification-exercise"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{slug}")
#+end_src
*** The Dataset
    It isn't mentioned in the notebook where the data originally came from, but it looks like it's the [[http://help.sentiment140.com/home][Sentiment140]] dataset, which consists of tweets whose sentiment was inferred by emoticons in each tweet.
#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
url = "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
path = Path("~/data/datasets/texts/sentiment140/").expanduser()
download = ZipDownloader(url, path)
download()
#+end_src

#+RESULTS:
: Files exist, not downloading

#+begin_src ipython :session kernel-17746-ssh.json :results none
columns = ["polarity", "tweet_id", "datetime", "query", "user", "text"]
training = pandas.read_csv(path/"training.1600000.processed.noemoticon.csv", 
                           encoding="latin-1", names=columns, header=None)
testing = pandas.read_csv(path/"testdata.manual.2009.06.14.csv", 
                           encoding="latin-1", names=columns, header=None)
#+end_src

*** Some Constants
    Right now I'm not able to train on the GPU (the embeddings are too large) experiment with the =embedding_dim= until it works.
#+begin_src ipython :session kernel-17746-ssh.json :results none
Text = Namespace(
    embedding_dim = 100,
    max_length = 16,
    trunc_type='post',
    padding_type='post',
    oov_tok = "<OOV>",
    training_size=16000,
)
#+end_src
#+begin_src ipython :session kernel-17746-ssh.json :results none
Data = Namespace(
    batch_size = 64,
    shuffle_buffer_size=100,
)
#+end_src
* Middle
** The Data
#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
print(training.sample().iloc[0])
#+end_src

#+RESULTS:
: polarity                                                    4
: tweet_id                                           1468852290
: datetime                         Tue Apr 07 04:04:10 PDT 2009
: query                                                NO_QUERY
: user                                              leawoodward
: text        Def off now...unexpected day out tomorrow so s...
: Name: 806643, dtype: object

#+begin_src ipython :session kernel-17746-ssh.json :results output raw :exports both
CountPercentage(training.polarity)()
#+end_src

#+RESULTS:
| Value | Count   | Percent (%) |
|-------+---------+-------------|
|     4 | 800,000 |       50.00 |
|     0 | 800,000 |       50.00 |

The =polarity= is what might also be called the "sentiment" of the tweet - /0/ means a negative tweet and /4/ means a positive tweet.

But, for our purposes, we would be better off if the positive polarity was =1=, not =4=, so let's convert it.

#+begin_src ipython :session kernel-17746-ssh.json :results output raw :exports both
training.loc[training.polarity==4, "polarity"] = 1
counts = CountPercentage(training.polarity)()
#+end_src

#+RESULTS:
| Value | Count   | Percent (%) |
|-------+---------+-------------|
|     1 | 800,000 |       50.00 |
|     0 | 800,000 |       50.00 |

** The Tokenizer
   As you can see from the sample, the data is still in text form so we need to convert it to a numeric form with a Tokenizer. 

First I'll Lower-case it.

#+begin_src ipython :session kernel-17746-ssh.json :results none
training.loc[:, "text"] = training.text.str.lower()
#+end_src

Next we'll fit it to our text.

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
tokenizer = Tokenizer()
with TIMER:
    tokenizer.fit_on_texts(training.text.values)
#+end_src

#+RESULTS:
: 2019-10-10 07:25:09,065 graeae.timers.timer start: Started: 2019-10-10 07:25:09.065039
: WARNING: Logging before flag parsing goes to stderr.
: I1010 07:25:09.065394 140436771002176 timer.py:70] Started: 2019-10-10 07:25:09.065039
: 2019-10-10 07:25:45,389 graeae.timers.timer end: Ended: 2019-10-10 07:25:45.389540
: I1010 07:25:45.389598 140436771002176 timer.py:77] Ended: 2019-10-10 07:25:45.389540
: 2019-10-10 07:25:45,391 graeae.timers.timer end: Elapsed: 0:00:36.324501
: I1010 07:25:45.391984 140436771002176 timer.py:78] Elapsed: 0:00:36.324501

Now, we can store some of it's values in variables for convenience.

#+begin_src ipython :session kernel-17746-ssh.json :results none
word_index = tokenizer.word_index
vocabulary_size = len(tokenizer.word_index)
#+end_src

Now, we'll convert the texts to sequences and pad them so they are all the same length.

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
with TIMER:
    sequences = tokenizer.texts_to_sequences(training.text.values)
    padded = pad_sequences(sequences, maxlen=Text.max_length,
                           truncating=Text.trunc_type)

    splits = train_test_split(
        padded, training.polarity, test_size=.2)

    training_sequences, test_sequences, training_labels, test_labels = splits
#+end_src

#+RESULTS:
: 2019-10-10 07:25:51,057 graeae.timers.timer start: Started: 2019-10-10 07:25:51.057684
: I1010 07:25:51.057712 140436771002176 timer.py:70] Started: 2019-10-10 07:25:51.057684
: 2019-10-10 07:26:33,530 graeae.timers.timer end: Ended: 2019-10-10 07:26:33.530338
: I1010 07:26:33.530381 140436771002176 timer.py:77] Ended: 2019-10-10 07:26:33.530338
: 2019-10-10 07:26:33,531 graeae.timers.timer end: Elapsed: 0:00:42.472654
: I1010 07:26:33.531477 140436771002176 timer.py:78] Elapsed: 0:00:42.472654

Now convert them to [[https://www.tensorflow.org/tutorials/load_data/numpy][datasets]].

#+begin_src ipython :session kernel-17746-ssh.json :results none
training_dataset = tensorflow.data.Dataset.from_tensor_slices(
    (training_sequences, training_labels)
)

testing_dataset = tensorflow.data.Dataset.from_tensor_slices(
    (test_sequences, test_labels)
)

training_dataset = training_dataset.shuffle(Data.shuffle_buffer_size).batch(Data.batch_size)
testing_dataset = testing_dataset.shuffle(Data.shuffle_buffer_size).batch(Data.batch_size)
#+end_src


** GloVe
   GloVe is short for /Global Vectors for Word Representation/. It is an /unsupervised/ algorithm that creates vector representations for words. They have a [[https://nlp.stanford.edu/projects/glove/][site]] where you can download pre-trained models or get the code and train one yourself. We're going to use one of their pre-trained models.

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
path = Path("~/models/glove/").expanduser()
url = "http://nlp.stanford.edu/data/glove.6B.zip"
ZipDownloader(url, path)()
#+end_src

#+RESULTS:
: Files exist, not downloading

The GloVe data is stored as a series of space separated lines with the first column being the word that's encoded and the rest of the columns being the values for the vector. To make this work we're going to split the word off from the vector and put each into a dictionary.

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
embeddings = {}
with TIMER:
    with open(path/"glove.6B.100d.txt") as lines:
        for line in lines:
            tokens = line.split()
            embeddings[tokens[0]] = numpy.array(tokens[1:])
#+end_src

#+RESULTS:
: 2019-10-10 07:26:37,537 graeae.timers.timer start: Started: 2019-10-10 07:26:37.537829
: I1010 07:26:37.537865 140436771002176 timer.py:70] Started: 2019-10-10 07:26:37.537829
: 2019-10-10 07:26:48,207 graeae.timers.timer end: Ended: 2019-10-10 07:26:48.207054
: I1010 07:26:48.207099 140436771002176 timer.py:77] Ended: 2019-10-10 07:26:48.207054
: 2019-10-10 07:26:48,208 graeae.timers.timer end: Elapsed: 0:00:10.669225
: I1010 07:26:48.208894 140436771002176 timer.py:78] Elapsed: 0:00:10.669225

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
print(f"{len(embeddings):,}")
#+end_src

#+RESULTS:
: 400,000

So, our vocabulary consists of 400,000 "words" (tokens is more accurate, since they also include punctuation). The problem we have to deal with next is that our data set wasn't part of the dataset used to train the embeddings, so there will probably be some tokens in our data set that aren't in the embeddings. To handle this we need to add zeroed embeddings for the extra tokens.

Rather than adding to the dict, we'll create a matrix of zeros with rows for each word in our datasets vocabulary, then we'll iterate over the words in our dataset and if there's a match in the GloVE embeddings we'll insert it into the matrix.

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
with TIMER:
    embeddings_matrix = numpy.zeros((vocabulary_size+1, Text.embedding_dim));
    for word, index in word_index.items():
        embedding_vector = embeddings.get(word);
        if embedding_vector is not None:
            embeddings_matrix[index] = embedding_vector;
#+end_src

#+RESULTS:
: 2019-10-10 07:26:49,760 graeae.timers.timer start: Started: 2019-10-10 07:26:49.760827
: I1010 07:26:49.760850 140436771002176 timer.py:70] Started: 2019-10-10 07:26:49.760827
: 2019-10-10 07:26:54,125 graeae.timers.timer end: Ended: 2019-10-10 07:26:54.125867
: I1010 07:26:54.125922 140436771002176 timer.py:77] Ended: 2019-10-10 07:26:54.125867
: 2019-10-10 07:26:54,127 graeae.timers.timer end: Elapsed: 0:00:04.365040
: I1010 07:26:54.127669 140436771002176 timer.py:78] Elapsed: 0:00:04.365040

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
print(f"{len(embeddings_matrix):,}")
#+end_src

#+RESULTS:
: 690,961
** The Models
*** A CNN
**** Build
#+begin_src ipython :session kernel-17746-ssh.json :results none
convoluted_model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(
        vocabulary_size + 1,
        Text.embedding_dim,
        input_length=Text.max_length,
        weights=[embeddings_matrix],
        trainable=False),
    tensorflow.keras.layers.Conv1D(filters=128,
                                   kernel_size=5,
    activation='relu'),
    tensorflow.keras.layers.GlobalMaxPooling1D(),
    tensorflow.keras.layers.Dense(24, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
convoluted_model.compile(loss="binary_crossentropy", optimizer="rmsprop",
                         metrics=["accuracy"])
#+end_src

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
print(convoluted_model.summary())
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 16, 100)           69096100  
_________________________________________________________________
conv1d (Conv1D)              (None, 12, 128)           64128     
_________________________________________________________________
global_max_pooling1d (Global (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 24)                3096      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 25        
=================================================================
Total params: 69,163,349
Trainable params: 67,249
Non-trainable params: 69,096,100
_________________________________________________________________
None
#+end_example

**** Train
#+begin_src ipython :session kernel-17746-ssh.json :results none
Training = Namespace(
    size = 0.75,
    epochs = 2,
    verbosity = 2,
    batch_size=128,
    )
#+end_src

#+begin_src ipython :session kernel-17746-ssh.json :results output :exports both
with TIMER:
    cnn_history = convoluted_model.fit(training_dataset,
                                       epochs=Training.epochs,
                                       validation_data=testing_dataset,
                                       verbose=Training.verbosity)
#+end_src

#+RESULTS:
#+begin_example
2019-10-10 07:27:04,921 graeae.timers.timer start: Started: 2019-10-10 07:27:04.921617
I1010 07:27:04.921657 140436771002176 timer.py:70] Started: 2019-10-10 07:27:04.921617
Epoch 1/2
W1010 07:27:05.154920 140436771002176 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20000/20000 - 4964s - loss: 0.5091 - accuracy: 0.7454 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/2
20000/20000 - 4935s - loss: 0.4790 - accuracy: 0.7671 - val_loss: 0.4782 - val_accuracy: 0.7677
2019-10-10 10:12:04,382 graeae.timers.timer end: Ended: 2019-10-10 10:12:04.382359
I1010 10:12:04.382491 140436771002176 timer.py:77] Ended: 2019-10-10 10:12:04.382359
2019-10-10 10:12:04,384 graeae.timers.timer end: Elapsed: 2:44:59.460742
I1010 10:12:04.384716 140436771002176 timer.py:78] Elapsed: 2:44:59.460742
#+end_example

**** Some Plotting

#+begin_src ipython :session kernel-17746-ssh.json :results output raw :exports both
performance = pandas.DataFrame(cnn_history.history)
plot = performance.hvplot().opts(title="CNN Twitter Sentiment Training Performance",
                                 width=1000,
                                 height=800)
Embed(plot=plot, file_name="cnn_training")()
#+end_src
* End
** Citations
   - Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. 
* Raw
#+begin_comment
import json
import tensorflow as tf
import csv
import random
import numpy as np

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import regularizers


embedding_dim = 100
max_length = 16
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"
training_size=#Your dataset size here. Experiment using smaller values (i.e. 16000), but don't forget to train on at least 160000 to see the best effects
test_portion=.1

corpus = []


# In[ ]:



# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader
# You can do that yourself with:
# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv
# I then hosted it on my site to make it easier to use in this notebook

get_ipython().system('wget --no-check-certificate     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv     -O /tmp/training_cleaned.csv')

num_sentences = 0

with open("/tmp/training_cleaned.csv") as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    for row in reader:
      # Your Code here. Create list items where the first item is the text, found in row[5], and the second is the label. Note that the label is a '0' or a '4' in the text. When it's the former, make
      # your label to be 0, otherwise 1. Keep a count of the number of sentences in num_sentences
        list_item=[]
        # YOUR CODE HERE
        num_sentences = num_sentences + 1
        corpus.append(list_item)



# In[ ]:


print(num_sentences)
print(len(corpus))
print(corpus[1])

# Expected Output:
# 1600000
# 1600000
# ["is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!", 0]


# In[ ]:


sentences=[]
labels=[]
random.shuffle(corpus)
for x in range(training_size):
    sentences.append(# YOUR CODE HERE)
    labels.append(# YOUR CODE HERE)


tokenizer = Tokenizer()
tokenizer.fit_on_texts(# YOUR CODE HERE)

word_index = tokenizer.word_index
vocab_size=len(# YOUR CODE HERE)

sequences = tokenizer.texts_to_sequences(# YOUR CODE HERE)
padded = pad_sequences(# YOUR CODE HERE)

split = int(test_portion * training_size)

test_sequences = padded[# YOUR CODE HERE]
training_sequences = padded[# YOUR CODE HERE]
test_labels = labels[# YOUR CODE HERE]
training_labels = labels[# YOUR CODE HERE]


# In[ ]:


print(vocab_size)
print(word_index['i'])
# Expected Output
# 138858
# 1


# In[ ]:


# Note this is the 100 dimension version of GloVe from Stanford
# I unzipped and hosted it on my site to make this notebook easier
get_ipython().system('wget --no-check-certificate     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt     -O /tmp/glove.6B.100d.txt')
embeddings_index = {};
with open('/tmp/glove.6B.100d.txt') as f:
    for line in f:
        values = line.split();
        word = values[0];
        coefs = np.asarray(values[1:], dtype='float32');
        embeddings_index[word] = coefs;

embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word);
    if embedding_vector is not None:
        embeddings_matrix[i] = embedding_vector;


# In[ ]:


print(len(embeddings_matrix))
# Expected Output
# 138859


# In[ ]:


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),
    # YOUR CODE HERE - experiment with combining different types, such as convolutions and LSTMs
])
model.compile(# YOUR CODE HERE)
model.summary()

num_epochs = 50
history = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=2)

print("Training Complete")


# In[ ]:


import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

#-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
acc=history.history['acc']
val_acc=history.history['val_acc']
loss=history.history['loss']
val_loss=history.history['val_loss']

epochs=range(len(acc)) # Get number of epochs

#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and validation accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Accuracy", "Validation Accuracy"])

plt.figure()

#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.title('Training and validation loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Loss", "Validation Loss"])

plt.figure()


# Expected Output
# A chart where the validation loss does not increase sharply!
#+end_comment
