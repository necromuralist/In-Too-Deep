<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Building a classifier for the BBC news." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>BBC News Classification | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/bbc-news-classification/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../cleaning-the-bbc-news-archive/" rel="prev" title="Cleaning the BBC News Archive" type="text/html">
<link href="../imdb-reviews-tensorflow-dataset/" rel="next" title="IMDB Reviews Tensorflow Dataset" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="BBC News Classification" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/keras/bbc-news-classification/" property="og:url">
<meta content="Building a classifier for the BBC news." property="og:description">
<meta content="article" property="og:type">
<meta content="2019-08-26T15:28:56-07:00" property="article:published_time">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">BBC News Classification</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2019-08-26T15:28:56-07:00" itemprop="datePublished" title="2019-08-26 15:28">2019-08-26 15:28</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5fa5fce">Beginning</a>
<ul>
<li><a href="#org4ddcfa5">Imports</a>
<ul>
<li><a href="#orgddbb38b">Python</a></li>
<li><a href="#orga939111">PyPi</a></li>
<li><a href="#org46cc4dd">Graeae</a></li>
</ul>
</li>
<li><a href="#orgc41914d">Setup</a>
<ul>
<li><a href="#org661c69f">The Timer</a></li>
<li><a href="#org19f8982">The Environment</a></li>
<li><a href="#org0e5d114">Spacy</a></li>
<li><a href="#orge88d4a9">Plotting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org22566cb">Middle</a>
<ul>
<li><a href="#org313a478">Load the Datasets</a></li>
<li><a href="#org8565f0d">The Tokenizers</a></li>
<li><a href="#org8ba4ae0">Making the Sequences</a></li>
<li><a href="#orgbfc3432">Make training and testing sets</a></li>
<li><a href="#orgf9e4a49">The Model</a></li>
<li><a href="#orgeba52d6">Plotting the Performance</a></li>
</ul>
</li>
<li><a href="#org87429ac">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org5fa5fce">
<h2 id="org5fa5fce">Beginning</h2>
<div class="outline-text-2" id="text-org5fa5fce"></div>
<div class="outline-3" id="outline-container-org4ddcfa5">
<h3 id="org4ddcfa5">Imports</h3>
<div class="outline-text-3" id="text-org4ddcfa5"></div>
<div class="outline-4" id="outline-container-orgddbb38b">
<h4 id="orgddbb38b">Python</h4>
<div class="outline-text-4" id="text-orgddbb38b">
<div class="highlight">
<pre><span></span>from functools import partial
from pathlib import Path
import csv
import random
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga939111">
<h4 id="orga939111">PyPi</h4>
<div class="outline-text-4" id="text-orga939111">
<div class="highlight">
<pre><span></span>from sklearn.model_selection import train_test_split
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import hvplot.pandas
import numpy
import pandas
import spacy
import tensorflow
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org46cc4dd">
<h4 id="org46cc4dd">Graeae</h4>
<div class="outline-text-4" id="text-org46cc4dd">
<div class="highlight">
<pre><span></span>from graeae import EmbedHoloviews, SubPathLoader, Timer
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc41914d">
<h3 id="orgc41914d">Setup</h3>
<div class="outline-text-3" id="text-orgc41914d"></div>
<div class="outline-4" id="outline-container-org661c69f">
<h4 id="org661c69f">The Timer</h4>
<div class="outline-text-4" id="text-org661c69f">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org19f8982">
<h4 id="org19f8982">The Environment</h4>
<div class="outline-text-4" id="text-org19f8982">
<div class="highlight">
<pre><span></span>ENVIRONMENT = SubPathLoader('DATASETS')
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org0e5d114">
<h4 id="org0e5d114">Spacy</h4>
<div class="outline-text-4" id="text-org0e5d114">
<div class="highlight">
<pre><span></span>spacy.prefer_gpu()
nlp = spacy.load("en_core_web_lg")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge88d4a9">
<h4 id="orge88d4a9">Plotting</h4>
<div class="outline-text-4" id="text-orge88d4a9">
<div class="highlight">
<pre><span></span>SLUG = "bbc-news-classification"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{SLUG}")
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org22566cb">
<h2 id="org22566cb">Middle</h2>
<div class="outline-text-2" id="text-org22566cb"></div>
<div class="outline-3" id="outline-container-org313a478">
<h3 id="org313a478">Load the Datasets</h3>
<div class="outline-text-3" id="text-org313a478">
<div class="highlight">
<pre><span></span>path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()

texts = []
labels = []
with TIMER:
    with path.open() as csvfile:
        lines = csv.DictReader(csvfile)
        for line in lines:
            labels.append(line["category"])
            texts.append(nlp(line["text"]))
</pre></div>
<pre class="example">
WARNING: Logging before flag parsing goes to stderr.
I0908 13:32:14.804769 139839933974336 environment.py:35] Environment Path: /home/athena/.env
I0908 13:32:14.806000 139839933974336 environment.py:90] Environment Path: /home/athena/.config/datasets/env
2019-09-08 13:32:14,806 graeae.timers.timer start: Started: 2019-09-08 13:32:14.806861
I0908 13:32:14.806965 139839933974336 timer.py:70] Started: 2019-09-08 13:32:14.806861
2019-09-08 13:33:37,430 graeae.timers.timer end: Ended: 2019-09-08 13:33:37.430228
I0908 13:33:37.430259 139839933974336 timer.py:77] Ended: 2019-09-08 13:33:37.430228
2019-09-08 13:33:37,431 graeae.timers.timer end: Elapsed: 0:01:22.623367
I0908 13:33:37.431128 139839933974336 timer.py:78] Elapsed: 0:01:22.623367
</pre>
<div class="highlight">
<pre><span></span>print(texts[random.randrange(len(texts))])
</pre></div>
<pre class="example">
candidate resigns over bnp link a prospective candidate for the uk independence party (ukip) has resigned after admitting a  brief attachment  to the british national party(bnp).  nicholas betts-green  who had been selected to fight the suffolk coastal seat  quit after reports in a newspaper that he attended a bnp meeting. the former teacher confirmed he had attended the meeting but said that was the only contact he had with the group. mr betts-green resigned after being questioned by the party s leadership. a ukip spokesman said mr betts-green s resignation followed disclosures in the east anglian daily times last month about his attendance at a bnp meeting.  he did once attend a bnp meeting. he did not like what he saw and heard and will take no further part of it   the spokesman added. a meeting of suffolk coastal ukip members is due to be held next week to discuss a replacement. mr betts-green  of woodbridge  suffolk  has also resigned as ukip s branch chairman.
</pre>
<p>So, it looks like the text has been lower-cased but there's still punctuation and extra white-space.</p>
<div class="highlight">
<pre><span></span>print(f"Rows: {len(labels):,}")
print(f"Unique Labels: {len(set(labels)):,}")
</pre></div>
<pre class="example">
Rows: 2,225
Unique Labels: 5
</pre>
<p>Since there's only five maybe we should plot it.</p>
<div class="highlight">
<pre><span></span>labels_frame = pandas.DataFrame({"label": labels})
counts = labels_frame.label.value_counts().reset_index().rename(
    columns={"index": "Category", "label": "Articles"})
plot = counts.hvplot.bar("Category", "Articles").opts(
    title="Count of BBC News Articles by Category",
    height=800, width=1000)
Embed(plot=plot, file_name="bbc_category_counts")()
</pre></div>
<object data="bbc_category_counts.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>It looks like the categories are somewhat unevenly distributed. Now to normalize the tokens.</p>
<div class="highlight">
<pre><span></span>with TIMER:
    cleaned = [[token.lemma_ for token in text if not any((token.is_stop, token.is_space, token.is_punct))]
               for text in texts]
</pre></div>
<pre class="example">
2019-09-08 13:33:40,257 graeae.timers.timer start: Started: 2019-09-08 13:33:40.257908
I0908 13:33:40.257930 139839933974336 timer.py:70] Started: 2019-09-08 13:33:40.257908
2019-09-08 13:33:40,810 graeae.timers.timer end: Ended: 2019-09-08 13:33:40.810135
I0908 13:33:40.810176 139839933974336 timer.py:77] Ended: 2019-09-08 13:33:40.810135
2019-09-08 13:33:40,811 graeae.timers.timer end: Elapsed: 0:00:00.552227
I0908 13:33:40.811067 139839933974336 timer.py:78] Elapsed: 0:00:00.552227
</pre></div>
</div>
<div class="outline-3" id="outline-container-org8565f0d">
<h3 id="org8565f0d">The Tokenizers</h3>
<div class="outline-text-3" id="text-org8565f0d">
<p>Even though I've already tokenized the texts, we need to eventually one-hot-encode them so I'll use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">tensorflow keras Tokenizer</a>.</p>
<p><b>Note:</b> The labels tokenizer doesn't get the out-of-vocabulary token, only the text-tokenizer does.</p>
<div class="highlight">
<pre><span></span>tokenizer = Tokenizer(num_words=1000, oov_token="&lt;OOV&gt;")
labels_tokenizer = Tokenizer()
labels_tokenizer.fit_on_texts(labels)
</pre></div>
<p>The <code>num_words</code> is the total amount of words that will be kept in the word index - I don't know why a thousand, I just found that in the "answer" notebook. The <code>oov_token</code> is what's used when a word is encountered outside of the words we're building into our word-index (<i>Out Of Vocabulary</i>). The next step is to create the word-index by fitting the tokenizer to the text.</p>
<div class="highlight">
<pre><span></span>with TIMER:
    tokenizer.fit_on_texts(cleaned)
</pre></div>
<pre class="example">
2019-09-08 14:59:30,671 graeae.timers.timer start: Started: 2019-09-08 14:59:30.671536
I0908 14:59:30.671563 139839933974336 timer.py:70] Started: 2019-09-08 14:59:30.671536
2019-09-08 14:59:30,862 graeae.timers.timer end: Ended: 2019-09-08 14:59:30.862483
I0908 14:59:30.862523 139839933974336 timer.py:77] Ended: 2019-09-08 14:59:30.862483
2019-09-08 14:59:30,863 graeae.timers.timer end: Elapsed: 0:00:00.190947
I0908 14:59:30.863504 139839933974336 timer.py:78] Elapsed: 0:00:00.190947
</pre>
<p>The tokenizer now has a dictionary named <code>word_index</code> that holds the words:index pairs for all the tokens found (it only uses the <code>num_words</code> when you call tokenizer's methods according to <a href="https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work">Stack Overflow</a>).</p>
<div class="highlight">
<pre><span></span>print(f"{len(tokenizer.word_index):,}")
</pre></div>
<pre class="example">
24,339
</pre></div>
</div>
<div class="outline-3" id="outline-container-org8ba4ae0">
<h3 id="org8ba4ae0">Making the Sequences</h3>
<div class="outline-text-3" id="text-org8ba4ae0">
<p>I've trained the Tokenizer so that it has a word-index, but now we have to one hot encode our texts and pad them so they're all the same length.</p>
<div class="highlight">
<pre><span></span>MAX_LENGTH = 120
sequences = tokenizer.texts_to_sequences(cleaned)
padded = pad_sequences(sequences, padding="post", maxlen=MAX_LENGTH)
labels_sequenced = labels_tokenizer.texts_to_sequences(labels)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgbfc3432">
<h3 id="orgbfc3432">Make training and testing sets</h3>
<div class="outline-text-3" id="text-orgbfc3432">
<div class="highlight">
<pre><span></span>TESTING = 0.2
x_train, x_test, y_train, y_test = train_test_split(
    padded, labels_sequenced,
    test_size=TESTING)
x_train, x_validation, y_train, y_validation = train_test_split(
    x_train, y_train, test_size=TESTING)

y_train = numpy.array(y_train)
y_test = numpy.array(y_test)
y_validation = numpy.array(y_validation)

print(f"Training: {x_train.shape}")
print(f"Validation: {x_validation.shape}")
print(f"Testing: {x_test.shape}")
</pre></div>
<pre class="example">
Training: (1424, 120)
Validation: (356, 120)
Testing: (445, 120)
</pre>
<p><b>Note:</b> I originally forgot to pass the <code>TESTING</code> variable with the keyword <code>test_size</code> and got an error that I couldn't use a Singleton array - don't forget the keywords when you pass in anything other than the data to <code>train_test_split</code>.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgf9e4a49">
<h3 id="orgf9e4a49">The Model</h3>
<div class="outline-text-3" id="text-orgf9e4a49">
<div class="highlight">
<pre><span></span>vocabulary_size = 1000
embedding_dimension = 16
max_length=120

model = tensorflow.keras.Sequential([
    layers.Embedding(vocabulary_size, embedding_dimension,
                     input_length=max_length),
    layers.GlobalAveragePooling1D(),
    layers.Dense(24, activation="relu"),
    layers.Dense(6, activation="softmax"),
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 120, 16)           16000     
_________________________________________________________________
global_average_pooling1d_1 ( (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 24)                408       
_________________________________________________________________
dense_3 (Dense)              (None, 6)                 150       
=================================================================
Total params: 16,558
Trainable params: 16,558
Non-trainable params: 0
_________________________________________________________________
None
</pre>
<div class="highlight">
<pre><span></span>model.fit(x_train, y_train, epochs=30,
          validation_data=(x_validation, y_validation), verbose=2)
</pre></div>
<pre class="example">
Train on 1424 samples, validate on 356 samples
Epoch 1/30
1424/1424 - 0s - loss: 1.7623 - accuracy: 0.2879 - val_loss: 1.7257 - val_accuracy: 0.5000
Epoch 2/30
1424/1424 - 0s - loss: 1.6871 - accuracy: 0.5190 - val_loss: 1.6332 - val_accuracy: 0.5281
Epoch 3/30
1424/1424 - 0s - loss: 1.5814 - accuracy: 0.4782 - val_loss: 1.5118 - val_accuracy: 0.4944
Epoch 4/30
1424/1424 - 0s - loss: 1.4417 - accuracy: 0.4677 - val_loss: 1.3543 - val_accuracy: 0.5365
Epoch 5/30
1424/1424 - 0s - loss: 1.2706 - accuracy: 0.5934 - val_loss: 1.1850 - val_accuracy: 0.7022
Epoch 6/30
1424/1424 - 0s - loss: 1.1075 - accuracy: 0.6749 - val_loss: 1.0387 - val_accuracy: 0.8006
Epoch 7/30
1424/1424 - 0s - loss: 0.9606 - accuracy: 0.8483 - val_loss: 0.9081 - val_accuracy: 0.8567
Epoch 8/30
1424/1424 - 0s - loss: 0.8244 - accuracy: 0.8869 - val_loss: 0.7893 - val_accuracy: 0.8848
Epoch 9/30
1424/1424 - 0s - loss: 0.6963 - accuracy: 0.9164 - val_loss: 0.6747 - val_accuracy: 0.8961
Epoch 10/30
1424/1424 - 0s - loss: 0.5815 - accuracy: 0.9228 - val_loss: 0.5767 - val_accuracy: 0.9185
Epoch 11/30
1424/1424 - 0s - loss: 0.4831 - accuracy: 0.9375 - val_loss: 0.4890 - val_accuracy: 0.9270
Epoch 12/30
1424/1424 - 0s - loss: 0.3991 - accuracy: 0.9473 - val_loss: 0.4195 - val_accuracy: 0.9326
Epoch 13/30
1424/1424 - 0s - loss: 0.3321 - accuracy: 0.9508 - val_loss: 0.3669 - val_accuracy: 0.9438
Epoch 14/30
1424/1424 - 0s - loss: 0.2800 - accuracy: 0.9572 - val_loss: 0.3268 - val_accuracy: 0.9494
Epoch 15/30
1424/1424 - 0s - loss: 0.2385 - accuracy: 0.9656 - val_loss: 0.2936 - val_accuracy: 0.9438
Epoch 16/30
1424/1424 - 0s - loss: 0.2053 - accuracy: 0.9740 - val_loss: 0.2693 - val_accuracy: 0.9466
Epoch 17/30
1424/1424 - 0s - loss: 0.1775 - accuracy: 0.9761 - val_loss: 0.2501 - val_accuracy: 0.9466
Epoch 18/30
1424/1424 - 0s - loss: 0.1557 - accuracy: 0.9789 - val_loss: 0.2332 - val_accuracy: 0.9494
Epoch 19/30
1424/1424 - 0s - loss: 0.1362 - accuracy: 0.9831 - val_loss: 0.2189 - val_accuracy: 0.9522
Epoch 20/30
1424/1424 - 0s - loss: 0.1209 - accuracy: 0.9853 - val_loss: 0.2082 - val_accuracy: 0.9551
Epoch 21/30
1424/1424 - 0s - loss: 0.1070 - accuracy: 0.9860 - val_loss: 0.1979 - val_accuracy: 0.9579
Epoch 22/30
1424/1424 - 0s - loss: 0.0952 - accuracy: 0.9888 - val_loss: 0.1897 - val_accuracy: 0.9551
Epoch 23/30
1424/1424 - 0s - loss: 0.0854 - accuracy: 0.9902 - val_loss: 0.1815 - val_accuracy: 0.9579
Epoch 24/30
1424/1424 - 0s - loss: 0.0765 - accuracy: 0.9916 - val_loss: 0.1761 - val_accuracy: 0.9522
Epoch 25/30
1424/1424 - 0s - loss: 0.0689 - accuracy: 0.9930 - val_loss: 0.1729 - val_accuracy: 0.9579
Epoch 26/30
1424/1424 - 0s - loss: 0.0618 - accuracy: 0.9951 - val_loss: 0.1680 - val_accuracy: 0.9551
Epoch 27/30
1424/1424 - 0s - loss: 0.0559 - accuracy: 0.9958 - val_loss: 0.1633 - val_accuracy: 0.9551
Epoch 28/30
1424/1424 - 0s - loss: 0.0505 - accuracy: 0.9958 - val_loss: 0.1594 - val_accuracy: 0.9579
Epoch 29/30
1424/1424 - 0s - loss: 0.0457 - accuracy: 0.9965 - val_loss: 0.1559 - val_accuracy: 0.9522
Epoch 30/30
1424/1424 - 0s - loss: 0.0416 - accuracy: 0.9972 - val_loss: 0.1544 - val_accuracy: 0.9551
</pre>
<p>It seems to get good suprisingly fast - it might be overfitting toward the end.</p>
<div class="highlight">
<pre><span></span>loss, accuracy =model.evaluate(x_test, y_test, verbose=0)
print(f"Loss: {loss: .2f} Accuracy: {accuracy:.2f}")
</pre></div>
<pre class="example">
Loss:  0.16 Accuracy: 0.95
</pre>
<p>It does pretty well, even on the test set.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgeba52d6">
<h3 id="orgeba52d6">Plotting the Performance</h3>
<div class="outline-text-3" id="text-orgeba52d6">
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(model.history.history)
plot = data.hvplot().opts(title="Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="model_performance")()
</pre></div>
<object data="model_performance.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Unlike with the image classifications, the validation performance never quite matches the training performance (although it's quite good), probably because we aren't doing any kind of augmentation the way you tend to do with images.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org87429ac">
<h2 id="org87429ac">End</h2>
<div class="outline-text-2" id="text-org87429ac">
<p>Okay, so we seem to have a decent model, but is that really the end-game? No, we want to be able to predict what classification a new input should get.</p>
<div class="highlight">
<pre><span></span>index_to_label = {value:key for (key, value) in labels_tokenizer.word_index.items()}

def category(text: str) -&gt; None:
    """Categorizes the text

    Args:
     text: text to categorize
    """
    text = tokenizer.texts_to_sequences([text])
    predictions = model.predict(pad_sequences(text, maxlen=MAX_LENGTH))
    print(f"Predicted Category: {index_to_label[predictions.argmax()]}")
    return
</pre></div>
<div class="highlight">
<pre><span></span>text = "crickets are nutritious and delicious but make for such a silly game"
category(text)
</pre></div>
<pre class="example">
Predicted Category: sport
</pre>
<div class="highlight">
<pre><span></span>text = "i like butts that are big and round, something something like a xxx throw down, and so does the house of parliament"
category(text)
</pre></div>
<pre class="example">
Predicted Category: sport
</pre>
<p>It kind of looks like it's biased toward sports.</p>
<div class="highlight">
<pre><span></span>text = "tv future hand viewer home theatre"
category(text)
</pre></div>
<pre class="example">
Predicted Category: sport
</pre>
<p>Something isn't right here.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../cleaning-the-bbc-news-archive/" rel="prev" title="Cleaning the BBC News Archive">Previous post</a></li>
<li class="next"><a href="../imdb-reviews-tensorflow-dataset/" rel="next" title="IMDB Reviews Tensorflow Dataset">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
