<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Walking through an embeddings exercise." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>NLP Classification Exercise | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/nlp-classification-exercise/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../embeddings-from-scratch/" rel="prev" title="Embeddings from Scratch" type="text/html">
<link href="../../link-collection/" rel="next" title="Link-Collection" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="NLP Classification Exercise" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/keras/nlp-classification-exercise/" property="og:url">
<meta content="Walking through an embeddings exercise." property="og:description">
<meta content="article" property="og:type">
<meta content="2019-09-29T11:28:06-07:00" property="article:published_time">
<meta content="embeddings" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">NLP Classification Exercise</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2019-09-29T11:28:06-07:00" itemprop="datePublished" title="2019-09-29 11:28">2019-09-29 11:28</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org920f178">Beginning</a>
<ul>
<li><a href="#org707d0a0">Imports</a>
<ul>
<li><a href="#orgb686cbb">Python</a></li>
<li><a href="#orgb6a35a6">PyPi</a></li>
<li><a href="#org8421d86">Others</a></li>
</ul>
</li>
<li><a href="#org79cef76">Set Up</a>
<ul>
<li><a href="#org60b750a">The Timer</a></li>
<li><a href="#org128c0ec">The Plotting</a></li>
<li><a href="#org309b81c">The Dataset</a></li>
<li><a href="#org7b69f51">Some Constants</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org806d5f2">Middle</a>
<ul>
<li><a href="#org2540b03">The Data</a></li>
<li><a href="#orgc10e107">The Tokenizer</a></li>
<li><a href="#org817e0e5">GloVe</a></li>
<li><a href="#org81c4bfa">The Models</a>
<ul>
<li><a href="#org93682b3">A CNN</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org16b70f5">End</a>
<ul>
<li><a href="#orgece0d38">Citations</a></li>
</ul>
</li>
<li><a href="#orgc874f0e">Raw</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org920f178">
<h2 id="org920f178">Beginning</h2>
<div class="outline-text-2" id="text-org920f178"></div>
<div class="outline-3" id="outline-container-org707d0a0">
<h3 id="org707d0a0">Imports</h3>
<div class="outline-text-3" id="text-org707d0a0"></div>
<div class="outline-4" id="outline-container-orgb686cbb">
<h4 id="orgb686cbb">Python</h4>
<div class="outline-text-4" id="text-orgb686cbb">
<div class="highlight">
<pre><span></span>from argparse import Namespace
from functools import partial
from pathlib import Path
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb6a35a6">
<h4 id="orgb6a35a6">PyPi</h4>
<div class="outline-text-4" id="text-orgb6a35a6">
<div class="highlight">
<pre><span></span>from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import hvplot.pandas
import numpy
import pandas
import tensorflow
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8421d86">
<h4 id="org8421d86">Others</h4>
<div class="outline-text-4" id="text-org8421d86">
<div class="highlight">
<pre><span></span>from graeae import (CountPercentage,
                    EmbedHoloviews,
                    SubPathLoader,
                    Timer,
                    ZipDownloader)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org79cef76">
<h3 id="org79cef76">Set Up</h3>
<div class="outline-text-3" id="text-org79cef76"></div>
<div class="outline-4" id="outline-container-org60b750a">
<h4 id="org60b750a">The Timer</h4>
<div class="outline-text-4" id="text-org60b750a">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org128c0ec">
<h4 id="org128c0ec">The Plotting</h4>
<div class="outline-text-4" id="text-org128c0ec">
<div class="highlight">
<pre><span></span>slug = "nlp-classification-exercise"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{slug}")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org309b81c">
<h4 id="org309b81c">The Dataset</h4>
<div class="outline-text-4" id="text-org309b81c">
<p>It isn't mentioned in the notebook where the data originally came from, but it looks like it's the <a href="http://help.sentiment140.com/home">Sentiment140</a> dataset, which consists of tweets whose sentiment was inferred by emoticons in each tweet.</p>
<div class="highlight">
<pre><span></span>url = "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
path = Path("~/data/datasets/texts/sentiment140/").expanduser()
download = ZipDownloader(url, path)
download()
</pre></div>
<pre class="example">
Files exist, not downloading
</pre>
<div class="highlight">
<pre><span></span>columns = ["polarity", "tweet_id", "datetime", "query", "user", "text"]
training = pandas.read_csv(path/"training.1600000.processed.noemoticon.csv", 
                           encoding="latin-1", names=columns, header=None)
testing = pandas.read_csv(path/"testdata.manual.2009.06.14.csv", 
                           encoding="latin-1", names=columns, header=None)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org7b69f51">
<h4 id="org7b69f51">Some Constants</h4>
<div class="outline-text-4" id="text-org7b69f51">
<div class="highlight">
<pre><span></span>Text = Namespace(
    embedding_dim = 100,
    max_length = 16,
    trunc_type='post',
    padding_type='post',
    oov_tok = "&lt;OOV&gt;",
    training_size=16000,
)
</pre></div>
<div class="highlight">
<pre><span></span>Data = Namespace(
    batch_size = 64,
    shuffle_buffer_size=100,
)
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org806d5f2">
<h2 id="org806d5f2">Middle</h2>
<div class="outline-text-2" id="text-org806d5f2"></div>
<div class="outline-3" id="outline-container-org2540b03">
<h3 id="org2540b03">The Data</h3>
<div class="outline-text-3" id="text-org2540b03">
<div class="highlight">
<pre><span></span>print(training.sample().iloc[0])
</pre></div>
<pre class="example">
polarity                                                    4
tweet_id                                           1468852290
datetime                         Tue Apr 07 04:04:10 PDT 2009
query                                                NO_QUERY
user                                              leawoodward
text        Def off now...unexpected day out tomorrow so s...
Name: 806643, dtype: object
</pre>
<div class="highlight">
<pre><span></span>CountPercentage(training.polarity)()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Value</th>
<th class="org-left" scope="col">Count</th>
<th class="org-right" scope="col">Percent (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">4</td>
<td class="org-left">800,000</td>
<td class="org-right">50.00</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-left">800,000</td>
<td class="org-right">50.00</td>
</tr>
</tbody>
</table>
<p>The <code>polarity</code> is what might also be called the "sentiment" of the tweet - <i>0</i> means a negative tweet and <i>4</i> means a positive tweet.</p>
<p>But, for our purposes, we would be better off if the positive polarity was <code>1</code>, not <code>4</code>, so let's convert it.</p>
<div class="highlight">
<pre><span></span>training.loc[training.polarity==4, "polarity"] = 1
counts = CountPercentage(training.polarity)()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Value</th>
<th class="org-left" scope="col">Count</th>
<th class="org-right" scope="col">Percent (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">800,000</td>
<td class="org-right">50.00</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-left">800,000</td>
<td class="org-right">50.00</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-3" id="outline-container-orgc10e107">
<h3 id="orgc10e107">The Tokenizer</h3>
<div class="outline-text-3" id="text-orgc10e107">
<p>As you can see from the sample, the data is still in text form so we need to convert it to a numeric form with a Tokenizer.</p>
<p>First I'll Lower-case it.</p>
<div class="highlight">
<pre><span></span>training.loc[:, "text"] = training.text.str.lower()
</pre></div>
<p>Next we'll fit it to our text.</p>
<div class="highlight">
<pre><span></span>tokenizer = Tokenizer()
with TIMER:
    tokenizer.fit_on_texts(training.text.values)
</pre></div>
<pre class="example">
2019-10-10 07:25:09,065 graeae.timers.timer start: Started: 2019-10-10 07:25:09.065039
WARNING: Logging before flag parsing goes to stderr.
I1010 07:25:09.065394 140436771002176 timer.py:70] Started: 2019-10-10 07:25:09.065039
2019-10-10 07:25:45,389 graeae.timers.timer end: Ended: 2019-10-10 07:25:45.389540
I1010 07:25:45.389598 140436771002176 timer.py:77] Ended: 2019-10-10 07:25:45.389540
2019-10-10 07:25:45,391 graeae.timers.timer end: Elapsed: 0:00:36.324501
I1010 07:25:45.391984 140436771002176 timer.py:78] Elapsed: 0:00:36.324501
</pre>
<p>Now, we can store some of it's values in variables for convenience.</p>
<div class="highlight">
<pre><span></span>word_index = tokenizer.word_index
vocabulary_size = len(tokenizer.word_index)
</pre></div>
<p>Now, we'll convert the texts to sequences and pad them so they are all the same length.</p>
<div class="highlight">
<pre><span></span>with TIMER:
    sequences = tokenizer.texts_to_sequences(training.text.values)
    padded = pad_sequences(sequences, maxlen=Text.max_length,
                           truncating=Text.trunc_type)

    splits = train_test_split(
        padded, training.polarity, test_size=.2)

    training_sequences, test_sequences, training_labels, test_labels = splits
</pre></div>
<pre class="example">
2019-10-10 07:25:51,057 graeae.timers.timer start: Started: 2019-10-10 07:25:51.057684
I1010 07:25:51.057712 140436771002176 timer.py:70] Started: 2019-10-10 07:25:51.057684
2019-10-10 07:26:33,530 graeae.timers.timer end: Ended: 2019-10-10 07:26:33.530338
I1010 07:26:33.530381 140436771002176 timer.py:77] Ended: 2019-10-10 07:26:33.530338
2019-10-10 07:26:33,531 graeae.timers.timer end: Elapsed: 0:00:42.472654
I1010 07:26:33.531477 140436771002176 timer.py:78] Elapsed: 0:00:42.472654
</pre>
<p>Now convert them to <a href="https://www.tensorflow.org/tutorials/load_data/numpy">datasets</a>.</p>
<div class="highlight">
<pre><span></span>training_dataset = tensorflow.data.Dataset.from_tensor_slices(
    (training_sequences, training_labels)
)

testing_dataset = tensorflow.data.Dataset.from_tensor_slices(
    (test_sequences, test_labels)
)

training_dataset = training_dataset.shuffle(Data.shuffle_buffer_size).batch(Data.batch_size)
testing_dataset = testing_dataset.shuffle(Data.shuffle_buffer_size).batch(Data.batch_size)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org817e0e5">
<h3 id="org817e0e5">GloVe</h3>
<div class="outline-text-3" id="text-org817e0e5">
<p>GloVe is short for <i>Global Vectors for Word Representation</i>. It is an <i>unsupervised</i> algorithm that creates vector representations for words. They have a <a href="https://nlp.stanford.edu/projects/glove/">site</a> where you can download pre-trained models or get the code and train one yourself. We're going to use one of their pre-trained models.</p>
<div class="highlight">
<pre><span></span>path = Path("~/models/glove/").expanduser()
url = "http://nlp.stanford.edu/data/glove.6B.zip"
ZipDownloader(url, path)()
</pre></div>
<pre class="example">
Files exist, not downloading
</pre>
<p>The GloVe data is stored as a series of space separated lines with the first column being the word that's encoded and the rest of the columns being the values for the vector. To make this work we're going to split the word off from the vector and put each into a dictionary.</p>
<div class="highlight">
<pre><span></span>embeddings = {}
with TIMER:
    with open(path/"glove.6B.100d.txt") as lines:
        for line in lines:
            tokens = line.split()
            embeddings[tokens[0]] = numpy.array(tokens[1:])
</pre></div>
<pre class="example">
2019-10-06 18:55:11,592 graeae.timers.timer start: Started: 2019-10-06 18:55:11.592880
I1006 18:55:11.592908 140055379531584 timer.py:70] Started: 2019-10-06 18:55:11.592880
2019-10-06 18:55:21,542 graeae.timers.timer end: Ended: 2019-10-06 18:55:21.542689
I1006 18:55:21.542738 140055379531584 timer.py:77] Ended: 2019-10-06 18:55:21.542689
2019-10-06 18:55:21,544 graeae.timers.timer end: Elapsed: 0:00:09.949809
I1006 18:55:21.544939 140055379531584 timer.py:78] Elapsed: 0:00:09.949809
</pre>
<div class="highlight">
<pre><span></span>print(f"{len(embeddings):,}")
</pre></div>
<pre class="example">
400,000
</pre>
<p>So, our vocabulary consists of 400,000 "words" (tokens is more accurate, since they also include punctuation). The problem we have to deal with next is that our data set wasn't part of the dataset used to train the embeddings, so there will probably be some tokens in our data set that aren't in the embeddings. To handle this we need to add zeroed embeddings for the extra tokens.</p>
<p>Rather than adding to the dict, we'll create a matrix of zeros with rows for each word in our datasets vocabulary, then we'll iterate over the words in our dataset and if there's a match in the GloVE embeddings we'll insert it into the matrix.</p>
<div class="highlight">
<pre><span></span>with TIMER:
    embeddings_matrix = numpy.zeros((vocabulary_size+1, Text.embedding_dim));
    for word, index in word_index.items():
        embedding_vector = embeddings.get(word);
        if embedding_vector is not None:
            embeddings_matrix[index] = embedding_vector;
</pre></div>
<pre class="example">
2019-10-06 18:55:46,577 graeae.timers.timer start: Started: 2019-10-06 18:55:46.577855
I1006 18:55:46.577886 140055379531584 timer.py:70] Started: 2019-10-06 18:55:46.577855
2019-10-06 18:55:51,374 graeae.timers.timer end: Ended: 2019-10-06 18:55:51.374706
I1006 18:55:51.374763 140055379531584 timer.py:77] Ended: 2019-10-06 18:55:51.374706
2019-10-06 18:55:51,377 graeae.timers.timer end: Elapsed: 0:00:04.796851
I1006 18:55:51.377207 140055379531584 timer.py:78] Elapsed: 0:00:04.796851
</pre>
<div class="highlight">
<pre><span></span>print(f"{len(embeddings_matrix):,}")
</pre></div>
<pre class="example">
690,961
</pre></div>
</div>
<div class="outline-3" id="outline-container-org81c4bfa">
<h3 id="org81c4bfa">The Models</h3>
<div class="outline-text-3" id="text-org81c4bfa"></div>
<div class="outline-4" id="outline-container-org93682b3">
<h4 id="org93682b3">A CNN</h4>
<div class="outline-text-4" id="text-org93682b3"></div>
<ul class="org-ul">
<li><a id="orgb13acd3"></a>Build<br>
<div class="outline-text-5" id="text-orgb13acd3">
<div class="highlight">
<pre><span></span>convoluted_model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(
        vocabulary_size + 1,
        Text.embedding_dim,
        input_length=Text.max_length,
        weights=[embeddings_matrix],
        trainable=False),
    tensorflow.keras.layers.Conv1D(filters=128,
                                   kernel_size=5,
    activation='relu'),
    tensorflow.keras.layers.GlobalMaxPooling1D(),
    tensorflow.keras.layers.Dense(24, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
convoluted_model.compile(loss="binary_crossentropy", optimizer="rmsprop",
                         metrics=["accuracy"])
</pre></div>
<div class="highlight">
<pre><span></span>print(convoluted_model.summary())
</pre></div>
<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 16, 100)           69096100  
_________________________________________________________________
conv1d (Conv1D)              (None, 12, 128)           64128     
_________________________________________________________________
global_max_pooling1d (Global (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 24)                3096      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 25        
=================================================================
Total params: 69,163,349
Trainable params: 67,249
Non-trainable params: 69,096,100
_________________________________________________________________
None
</pre></div>
</li>
<li><a id="org5eea0f0"></a>Train<br>
<div class="outline-text-5" id="text-org5eea0f0">
<div class="highlight">
<pre><span></span>Training = Namespace(
    size = 0.75,
    epochs = 2,
    verbosity = 2,
    batch_size=128,
    )
</pre></div>
<div class="highlight">
<pre><span></span>with TIMER:
    cnn_history = convoluted_model.fit(training_dataset,
                                       epochs=Training.epochs,
                                       validation_data=testing_dataset,
                                       verbose=Training.verbosity)
</pre></div>
<pre class="example">
2019-10-10 07:27:04,921 graeae.timers.timer start: Started: 2019-10-10 07:27:04.921617
I1010 07:27:04.921657 140436771002176 timer.py:70] Started: 2019-10-10 07:27:04.921617
Epoch 1/2
W1010 07:27:05.154920 140436771002176 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20000/20000 - 4964s - loss: 0.5091 - accuracy: 0.7454 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/2
20000/20000 - 4935s - loss: 0.4790 - accuracy: 0.7671 - val_loss: 0.4782 - val_accuracy: 0.7677
2019-10-10 10:12:04,382 graeae.timers.timer end: Ended: 2019-10-10 10:12:04.382359
I1010 10:12:04.382491 140436771002176 timer.py:77] Ended: 2019-10-10 10:12:04.382359
2019-10-10 10:12:04,384 graeae.timers.timer end: Elapsed: 2:44:59.460742
I1010 10:12:04.384716 140436771002176 timer.py:78] Elapsed: 2:44:59.460742
</pre></div>
</li>
<li><a id="org8ff3785"></a>Some Plotting<br>
<div class="outline-text-5" id="text-org8ff3785">
<div class="highlight">
<pre><span></span>performance = pandas.DataFrame(cnn_history.history)
plot = performance.hvplot().opts(title="CNN Twitter Sentiment Training Performance",
                                 width=1000,
                                 height=800)
Embed(plot=plot, file_name="cnn_training")()
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org16b70f5">
<h2 id="org16b70f5">End</h2>
<div class="outline-text-2" id="text-org16b70f5"></div>
<div class="outline-3" id="outline-container-orgece0d38">
<h3 id="orgece0d38">Citations</h3>
<div class="outline-text-3" id="text-orgece0d38">
<ul class="org-ul">
<li>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc874f0e">
<h2 id="orgc874f0e">Raw</h2>
<div class="outline-text-2" id="text-orgc874f0e"></div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/embeddings/" rel="tag">embeddings</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../embeddings-from-scratch/" rel="prev" title="Embeddings from Scratch">Previous post</a></li>
<li class="next"><a href="../../link-collection/" rel="next" title="Link-Collection">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
