<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Classifying hands for rock-paper-scissors." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Rock-Paper-Scissors | In Too Deep</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/posts/keras/rock-paper-scissors/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="../../../assets/javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
<meta content="Cloistered Monkey" name="author">
<link href="/posts/keras/horse-or-human-using-tensorflow-20/" rel="prev" title="Horse Or Human Using TensorFlow 2.0" type="text/html">
<link href="/posts/keras/sign-language-exercise/" rel="next" title="Sign Language Exercise" type="text/html">
<meta content="In Too Deep" property="og:site_name">
<meta content="Rock-Paper-Scissors" property="og:title">
<meta content="https://necromuralist.github.io/In-Too-Deep/posts/keras/rock-paper-scissors/" property="og:url">
<meta content="Classifying hands for rock-paper-scissors." property="og:description">
<meta content="article" property="og:type">
<meta content="2019-08-19T15:16:52-07:00" property="article:published_time">
<meta content="cnn" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="/posts/keras/rock-paper-scissors/index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="/posts/keras/rock-paper-scissors/">Rock-Paper-Scissors</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/rock-paper-scissors/" rel="bookmark"><time class="published dt-published" datetime="2019-08-19T15:16:52-07:00" itemprop="datePublished" title="2019-08-19 15:16">2019-08-19 15:16</time></a></p>
<p class="sourceline"><a class="sourcelink" href="/posts/keras/rock-paper-scissors/index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org8d1bb8c">Beginning</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#orgb4f1d14">Imports</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#orgb58e813">Python</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#org3b99444">PyPi</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#org6b4c78e">graeae</a></li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#orgcd1e499">Set Up</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org7299868">Plotting</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#orgacc65f0">The Timer</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#orgb64b416">The Environment</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#orgb9ae230">Middle</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org0a96d35">The Data</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org41c3d99">Downloading it</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#orgc21640c">Some Examples</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#orgbfb1303">Data Generators</a></li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#org06ab25d">A Four-CNN Model</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#orgb4ccd1e">Definition</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#orge062f25">Compile and Fit</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#orgaacf128">End</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org7e2a8f1">Some Test Images</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org2761ba4">What If we re-train the model, will it get better?</a></li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#org1a4625b">Sources</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org8d1bb8c">
<h2 id="org8d1bb8c">Beginning</h2>
<div class="outline-text-2" id="text-org8d1bb8c"></div>
<div class="outline-3" id="outline-container-orgb4f1d14">
<h3 id="orgb4f1d14">Imports</h3>
<div class="outline-text-3" id="text-orgb4f1d14"></div>
<div class="outline-4" id="outline-container-orgb58e813">
<h4 id="orgb58e813">Python</h4>
<div class="outline-text-4" id="text-orgb58e813">
<div class="highlight">
<pre><span></span>from functools import partial
from pathlib import Path
import hvplot.pandas
import numpy
import pandas
import random
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3b99444">
<h4 id="org3b99444">PyPi</h4>
<div class="outline-text-4" id="text-org3b99444">
<div class="highlight">
<pre><span></span>from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image as keras_image
import holoviews
import matplotlib.pyplot as pyplot
import matplotlib.image as matplotlib_image
import seaborn
import tensorflow
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6b4c78e">
<h4 id="org6b4c78e">graeae</h4>
<div class="outline-text-4" id="text-org6b4c78e">
<div class="highlight">
<pre><span></span>from graeae import EmbedHoloviews, SubPathLoader, Timer, ZipDownloader
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgcd1e499">
<h3 id="orgcd1e499">Set Up</h3>
<div class="outline-text-3" id="text-orgcd1e499"></div>
<div class="outline-4" id="outline-container-org7299868">
<h4 id="org7299868">Plotting</h4>
<div class="outline-text-4" id="text-org7299868">
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=1)
FIGURE_SIZE = (12, 10)

Embed = partial(EmbedHoloviews,
                folder_path="../../files/posts/keras/rock-paper-scissors/")
holoviews.extension("bokeh")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgacc65f0">
<h4 id="orgacc65f0">The Timer</h4>
<div class="outline-text-4" id="text-orgacc65f0">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb64b416">
<h4 id="orgb64b416">The Environment</h4>
<div class="outline-text-4" id="text-orgb64b416">
<div class="highlight">
<pre><span></span>ENVIRONMENT = SubPathLoader("DATASETS")
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb9ae230">
<h2 id="orgb9ae230">Middle</h2>
<div class="outline-text-2" id="text-orgb9ae230"></div>
<div class="outline-3" id="outline-container-org0a96d35">
<h3 id="org0a96d35">The Data</h3>
<div class="outline-text-3" id="text-org0a96d35"></div>
<div class="outline-4" id="outline-container-org41c3d99">
<h4 id="org41c3d99">Downloading it</h4>
<div class="outline-text-4" id="text-org41c3d99">
<div class="highlight">
<pre><span></span>TRAINING_URL = "https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip"
TEST_URL = "https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip"
OUT_PATH = Path(ENVIRONMENT["ROCK_PAPER_SCISSORS"]).expanduser()
download_train = ZipDownloader(TRAINING_URL, OUT_PATH/"train")
download_test = ZipDownloader(TEST_URL, OUT_PATH/"test")
download_train()
download_test()
</pre></div>
<pre class="example">
I0825 15:06:11.577721 139626236733248 environment.py:35] Environment Path: /home/athena/.env
I0825 15:06:11.578975 139626236733248 environment.py:90] Environment Path: /home/athena/.config/datasets/env
Files exist, not downloading
Files exist, not downloading

</pre>
<p>The data structure for the folders is fairly deep, so I'll make some shortcuts.</p>
<div class="highlight">
<pre><span></span>TRAINING = OUT_PATH/"train/rps"
rocks = TRAINING/"rock"
papers = TRAINING/"paper"
scissors = TRAINING/"scissors"
assert papers.is_dir()
assert rocks.is_dir()
assert scissors.is_dir()
</pre></div>
<div class="highlight">
<pre><span></span>rock_images = list(rocks.iterdir())
paper_images = list(papers.iterdir())
scissors_images = list(scissors.iterdir())
print(f"Rocks: {len(rock_images):,}")
print(f"Papers: {len(paper_images):,}")
print(f"Scissors: {len(scissors_images):,}")
</pre></div>
<pre class="example">
Rocks: 840
Papers: 840
Scissors: 840

</pre></div>
</div>
<div class="outline-4" id="outline-container-orgc21640c">
<h4 id="orgc21640c">Some Examples</h4>
<div class="outline-text-4" id="text-orgc21640c">
<div class="highlight">
<pre><span></span>count=1
rock_sample = random.choice(rock_images[:count])

image = matplotlib_image.imread(str(rock_sample))
pyplot.title("Rock")
pyplot.imshow(image)
pyplot.axis('Off')
pyplot.show()
</pre></div>
<div class="figure">
<p><img alt="rock.png" src="/posts/keras/rock-paper-scissors/rock.png"></p>
</div>
<div class="highlight">
<pre><span></span>paper_sample = random.choice(paper_images)
image = matplotlib_image.imread(str(paper_sample))
pyplot.title("Paper")
pyplot.imshow(image)
pyplot.axis('Off')
pyplot.show()
</pre></div>
<div class="figure">
<p><img alt="paper.png" src="/posts/keras/rock-paper-scissors/paper.png"></p>
</div>
<div class="highlight">
<pre><span></span>scissors_sample = random.choice(scissors_images)
image = matplotlib_image.imread(str(scissors_sample))
pyplot.title("Scissors")
pyplot.imshow(image)
pyplot.axis('Off')
pyplot.show()
</pre></div>
<div class="figure">
<p><img alt="scissors.png" src="/posts/keras/rock-paper-scissors/scissors.png"></p>
</div>
</div>
</div>
<div class="outline-4" id="outline-container-orgbfb1303">
<h4 id="orgbfb1303">Data Generators</h4>
<div class="outline-text-4" id="text-orgbfb1303">
<p><b>Note:</b> I was originally using <code>keras_preprocessing.image.ImageDataGenerator</code> and getting</p>
<div class="highlight">
<pre><span></span><span class="ne">AttributeError</span><span class="p">:</span> <span class="s1">'DirectoryIterator'</span> <span class="nb">object</span> <span class="n">has</span> <span class="n">no</span> <span class="n">attribute</span> <span class="s1">'shape'</span>
</pre></div>
<p>Make sure to use <code>tensorflow.keras.preprocessing.image.ImageDataGenerator</code> instead.</p>
<div class="highlight">
<pre><span></span>VALIDATION = OUT_PATH/"test/rps-test-set"
training_data_generator = ImageDataGenerator(
      rescale = 1./255,
          rotation_range=40,
      width_shift_range=0.2,
      height_shift_range=0.2,
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')

validation_data_generator = ImageDataGenerator(rescale = 1./255)

train_generator = training_data_generator.flow_from_directory(
        TRAINING,
        target_size=(150,150),
        class_mode='categorical'
)

validation_generator = validation_data_generator.flow_from_directory(
        VALIDATION,
        target_size=(150,150),
        class_mode='categorical'
)
</pre></div>
<pre class="example">
Found 2520 images belonging to 3 classes.
Found 372 images belonging to 3 classes.

</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org06ab25d">
<h3 id="org06ab25d">A Four-CNN Model</h3>
<div class="outline-text-3" id="text-org06ab25d"></div>
<div class="outline-4" id="outline-container-orgb4ccd1e">
<h4 id="orgb4ccd1e">Definition</h4>
<div class="outline-text-4" id="text-orgb4ccd1e">
<p>This is a hand-crafted, relatively shallow Convolutional Neural Network. The input shape matches our <code>target_size</code> arguments for the data-generators. There are four convolutional layers with a filter size of 3 x 3 each follewd by a max-pooling layer. The first two layers have 64 nodes while the two following those have 128 nodes. The convolution layers are followed by a layer to flatten the input and add dropout before reaching our fully connected and output layer which uses softmax to predict the most likely category. Since we have three categories (rock, paper, or scissors) the final layer has three nodes.</p>
<div class="highlight">
<pre><span></span>model = tensorflow.keras.models.Sequential([
    # Input Layer/convolution
    tensorflow.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tensorflow.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    tensorflow.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tensorflow.keras.layers.MaxPooling2D(2,2),
    # The third convolution
    tensorflow.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tensorflow.keras.layers.MaxPooling2D(2,2),
    # The fourth convolution
    tensorflow.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tensorflow.keras.layers.MaxPooling2D(2,2),
    # Flatten
    tensorflow.keras.layers.Flatten(),
    tensorflow.keras.layers.Dropout(0.5),
    # Fully-connected and output layers
    tensorflow.keras.layers.Dense(512, activation='relu'),
    tensorflow.keras.layers.Dense(3, activation='softmax')
])
</pre></div>
<p>Here's a summary of the layers.</p>
<div class="highlight">
<pre><span></span>model.summary()
</pre></div>
<pre class="example">
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_16 (Conv2D)           (None, 148, 148, 64)      1792      
_________________________________________________________________
max_pooling2d_16 (MaxPooling (None, 74, 74, 64)        0         
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 72, 72, 64)        36928     
_________________________________________________________________
max_pooling2d_17 (MaxPooling (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_18 (MaxPooling (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_19 (Conv2D)           (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_19 (MaxPooling (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_8 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_9 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 3,473,475
Trainable params: 3,473,475
Non-trainable params: 0
_________________________________________________________________
</pre>
<p>You can see that the convolutional layers lose two pixels on output, so the filters are stopping when their edges match the image (so the 3 x 3 filter stops with the center one pixel away from the edge of the image). Additionally, our max-pooling layers are cutting the size of the convolutional layers' output in half, so as we progress through the network the inputs are getting smaller and smaller before reaching the fully-connected layers.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orge062f25">
<h4 id="orge062f25">Compile and Fit</h4>
<div class="outline-text-4" id="text-orge062f25">
<p>Now we need to compile and train the model.</p>
<p><b>Note:</b> The metrics can change with your settings - make sure the <code>monitor=</code> parameter is pointing to a key in the history. If you see this in the output:</p>
<pre class="example">
Can save best model only with val_acc available, skipping.
</pre>
<p>You might have the wrong name for your metric (it isn't <code>val_acc</code>).</p>
<div class="highlight">
<pre><span></span>model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
MODELS = Path("~/models/rock-paper-scissors/").expanduser()
best_model = MODELS/"four-layer-cnn.hdf5"
checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(
    str(best_model), monitor="val_accuracy", verbose=1, 
    save_best_only=True)

with TIMER:
    model.fit_generator(generator=train_generator,
                        epochs=25,
                        callbacks=[checkpoint],
                        validation_data = validation_generator,
                        verbose=2)
</pre></div>
<pre class="example">
2019-08-25 15:06:17,145 graeae.timers.timer start: Started: 2019-08-25 15:06:17.145536
I0825 15:06:17.145575 139626236733248 timer.py:70] Started: 2019-08-25 15:06:17.145536
Epoch 1/25

Epoch 00001: val_accuracy improved from -inf to 0.61559, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 15s - loss: 1.1174 - accuracy: 0.3996 - val_loss: 0.8997 - val_accuracy: 0.6156
Epoch 2/25

Epoch 00002: val_accuracy improved from 0.61559 to 0.93817, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.8115 - accuracy: 0.6381 - val_loss: 0.2403 - val_accuracy: 0.9382
Epoch 3/25

Epoch 00003: val_accuracy improved from 0.93817 to 0.97043, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.5604 - accuracy: 0.7750 - val_loss: 0.2333 - val_accuracy: 0.9704
Epoch 4/25

Epoch 00004: val_accuracy improved from 0.97043 to 0.98387, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.3926 - accuracy: 0.8496 - val_loss: 0.0681 - val_accuracy: 0.9839
Epoch 5/25

Epoch 00005: val_accuracy improved from 0.98387 to 0.99194, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.2746 - accuracy: 0.8925 - val_loss: 0.0395 - val_accuracy: 0.9919
Epoch 6/25

Epoch 00006: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.2018 - accuracy: 0.9246 - val_loss: 0.1427 - val_accuracy: 0.9328
Epoch 7/25

Epoch 00007: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.2052 - accuracy: 0.9238 - val_loss: 0.4212 - val_accuracy: 0.8253
Epoch 8/25

Epoch 00008: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1649 - accuracy: 0.9460 - val_loss: 0.1079 - val_accuracy: 0.9597
Epoch 9/25

Epoch 00009: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1678 - accuracy: 0.9452 - val_loss: 0.0782 - val_accuracy: 0.9597
Epoch 10/25

Epoch 00010: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1388 - accuracy: 0.9508 - val_loss: 0.0425 - val_accuracy: 0.9731
Epoch 11/25

Epoch 00011: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1207 - accuracy: 0.9611 - val_loss: 0.0758 - val_accuracy: 0.9570
Epoch 12/25

Epoch 00012: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1195 - accuracy: 0.9639 - val_loss: 0.1392 - val_accuracy: 0.9489
Epoch 13/25

Epoch 00013: val_accuracy improved from 0.99194 to 1.00000, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.1182 - accuracy: 0.9583 - val_loss: 0.0147 - val_accuracy: 1.0000
Epoch 14/25

Epoch 00014: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0959 - accuracy: 0.9722 - val_loss: 0.1264 - val_accuracy: 0.9543
Epoch 15/25

Epoch 00015: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.1225 - accuracy: 0.9643 - val_loss: 0.1124 - val_accuracy: 0.9677
Epoch 16/25

Epoch 00016: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0959 - accuracy: 0.9706 - val_loss: 0.0773 - val_accuracy: 0.9677
Epoch 17/25

Epoch 00017: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0817 - accuracy: 0.9687 - val_loss: 0.0120 - val_accuracy: 1.0000
Epoch 18/25

Epoch 00018: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.1308 - accuracy: 0.9627 - val_loss: 0.1058 - val_accuracy: 0.9758
Epoch 19/25

Epoch 00019: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0967 - accuracy: 0.9675 - val_loss: 0.0356 - val_accuracy: 0.9866
Epoch 20/25

Epoch 00020: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0785 - accuracy: 0.9726 - val_loss: 0.0474 - val_accuracy: 0.9704
Epoch 21/25

Epoch 00021: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0962 - accuracy: 0.9710 - val_loss: 0.0774 - val_accuracy: 0.9677
Epoch 22/25

Epoch 00022: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0802 - accuracy: 0.9754 - val_loss: 0.1592 - val_accuracy: 0.9516
Epoch 23/25

Epoch 00023: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0909 - accuracy: 0.9714 - val_loss: 0.1123 - val_accuracy: 0.9382
Epoch 24/25

Epoch 00024: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0573 - accuracy: 0.9782 - val_loss: 0.0609 - val_accuracy: 0.9785
Epoch 25/25

Epoch 00025: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0860 - accuracy: 0.9778 - val_loss: 0.1106 - val_accuracy: 0.9677
2019-08-25 15:12:11,360 graeae.timers.timer end: Ended: 2019-08-25 15:12:11.360361
I0825 15:12:11.360388 139626236733248 timer.py:77] Ended: 2019-08-25 15:12:11.360361
2019-08-25 15:12:11,361 graeae.timers.timer end: Elapsed: 0:05:54.214825
I0825 15:12:11.361701 139626236733248 timer.py:78] Elapsed: 0:05:54.214825
</pre>
<p>That did surprisingly well… is it really that easy a problem?</p>
<div class="highlight">
<pre><span></span>predictor = load_model(best_model)
</pre></div>
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(model.history.history)
plot = data.hvplot().opts(title="Rock, Paper, Scissors Training and Validation", width=1000, height=800)
Embed(plot=plot, file_name="training")()
</pre></div>
<object data="/posts/keras/rock-paper-scissors/training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Looking at the validation accuracy it appears that it starts to overfit at the end. Strangely, the validation loss, up until the overfitting, is lower than the training loss, and the validation accuracy is better almost throughout - perhaps this is because the image augmentation for the training set is too hard.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgaacf128">
<h2 id="orgaacf128">End</h2>
<div class="outline-text-2" id="text-orgaacf128"></div>
<div class="outline-3" id="outline-container-org7e2a8f1">
<h3 id="org7e2a8f1">Some Test Images</h3>
<div class="outline-text-3" id="text-org7e2a8f1">
<div class="highlight">
<pre><span></span>base = Path("~/test_images").expanduser()
paper = base/"Rock-paper-scissors_(paper).png"

image_ = matplotlib_image.imread(str(paper))
pyplot.title("Paper Test Case")
pyplot.imshow(image)
pyplot.axis('Off')
pyplot.show()
</pre></div>
<div class="figure">
<p><img alt="test_paper.png" src="/posts/keras/rock-paper-scissors/test_paper.png"></p>
</div>
<div class="highlight">
<pre><span></span>classifications = dict(zip(range(3), ("Paper", "Rock", "Scissors")))
image_ = keras_image.load_img(str(paper), target_size=(150, 150))
x = keras_image.img_to_array(image_)
x = numpy.expand_dims(x, axis=0)
images = numpy.vstack([x])
classes = predictor.predict(images, batch_size=10)
print(classifications[classes.argmax()])
</pre></div>
<pre class="example">
Paper

</pre>
<div class="highlight">
<pre><span></span>base = Path("~/test_images").expanduser()
rock = base/"Rock-paper-scissors_(rock).png"

image = matplotlib_image.imread(str(rock))
pyplot.title("Rock Test Case")
pyplot.imshow(image)
pyplot.axis('Off')
pyplot.show()
</pre></div>
<div class="figure">
<p><img alt="test_rock.png" src="/posts/keras/rock-paper-scissors/test_rock.png"></p>
</div>
<div class="highlight">
<pre><span></span>base = Path("~/test_images").expanduser()
rock = base/"Rock-paper-scissors_(rock).png"
image_ = keras_image.load_img(str(rock), target_size=(150, 150))
x = keras_image.img_to_array(image_)
x = numpy.expand_dims(x, axis=0)
images = numpy.vstack([x])
classes = predictor.predict(images, batch_size=10)
print(classifications[classes.argmax()])
</pre></div>
<pre class="example">
Rock

</pre>
<div class="highlight">
<pre><span></span>base = Path("~/test_images").expanduser()
scissors = base/"Rock-paper-scissors_(scissors).png"

image = matplotlib_image.imread(str(scissors))
pyplot.title("Scissors Test Case")
pyplot.imshow(image)
pyplot.axis('Off')
pyplot.show()
</pre></div>
<div class="figure">
<p><img alt="test_scissors.png" src="/posts/keras/rock-paper-scissors/test_scissors.png"></p>
</div>
<div class="highlight">
<pre><span></span>image_ = keras_image.load_img(str(scissors), target_size=(150, 150))
x = keras_image.img_to_array(image_)
x = numpy.expand_dims(x, axis=0)
images = numpy.vstack([x])
classes = predictor.predict(images, batch_size=10)
print(classifications[classes.argmax()])
</pre></div>
<pre class="example">
Paper

</pre></div>
<div class="outline-4" id="outline-container-org2761ba4">
<h4 id="org2761ba4">What If we re-train the model, will it get better?</h4>
<div class="outline-text-4" id="text-org2761ba4">
<div class="highlight">
<pre><span></span>with TIMER:
    model.fit_generator(generator=train_generator,
                        epochs=25,
                        callbacks=[checkpoint],
                        validation_data = validation_generator,
                        verbose=2)
</pre></div>
<pre class="example">
2019-08-25 15:21:37,706 graeae.timers.timer start: Started: 2019-08-25 15:21:37.706175
I0825 15:21:37.706199 139626236733248 timer.py:70] Started: 2019-08-25 15:21:37.706175
Epoch 1/25

Epoch 00001: val_accuracy did not improve from 1.00000
79/79 - 15s - loss: 0.0792 - accuracy: 0.9798 - val_loss: 0.1101 - val_accuracy: 0.9543
Epoch 2/25

Epoch 00002: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0691 - accuracy: 0.9798 - val_loss: 0.1004 - val_accuracy: 0.9570
Epoch 3/25

Epoch 00003: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0850 - accuracy: 0.9762 - val_loss: 0.0098 - val_accuracy: 1.0000
Epoch 4/25

Epoch 00004: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0799 - accuracy: 0.9730 - val_loss: 0.1022 - val_accuracy: 0.9409
Epoch 5/25

Epoch 00005: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0767 - accuracy: 0.9758 - val_loss: 0.1134 - val_accuracy: 0.9328
Epoch 6/25

Epoch 00006: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0747 - accuracy: 0.9833 - val_loss: 0.0815 - val_accuracy: 0.9731
Epoch 7/25

Epoch 00007: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0680 - accuracy: 0.9817 - val_loss: 0.1476 - val_accuracy: 0.9059
Epoch 8/25

Epoch 00008: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0669 - accuracy: 0.9821 - val_loss: 0.0202 - val_accuracy: 0.9866
Epoch 9/25

Epoch 00009: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0809 - accuracy: 0.9774 - val_loss: 0.3860 - val_accuracy: 0.8844
Epoch 10/25

Epoch 00010: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0583 - accuracy: 0.9817 - val_loss: 0.0504 - val_accuracy: 0.9812
Epoch 11/25

Epoch 00011: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0691 - accuracy: 0.9806 - val_loss: 0.0979 - val_accuracy: 0.9624
Epoch 12/25

Epoch 00012: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0459 - accuracy: 0.9881 - val_loss: 0.1776 - val_accuracy: 0.9167
Epoch 13/25

Epoch 00013: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0648 - accuracy: 0.9821 - val_loss: 0.0770 - val_accuracy: 0.9435
Epoch 14/25

Epoch 00014: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0549 - accuracy: 0.9825 - val_loss: 0.0075 - val_accuracy: 1.0000
Epoch 15/25

Epoch 00015: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0575 - accuracy: 0.9829 - val_loss: 0.1787 - val_accuracy: 0.9167
Epoch 16/25

Epoch 00016: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0665 - accuracy: 0.9778 - val_loss: 0.0230 - val_accuracy: 0.9866
Epoch 17/25

Epoch 00017: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0557 - accuracy: 0.9825 - val_loss: 0.0431 - val_accuracy: 0.9785
Epoch 18/25

Epoch 00018: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0628 - accuracy: 0.9817 - val_loss: 0.2121 - val_accuracy: 0.8952
Epoch 19/25

Epoch 00019: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0580 - accuracy: 0.9841 - val_loss: 0.0705 - val_accuracy: 0.9651
Epoch 20/25

Epoch 00020: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0578 - accuracy: 0.9810 - val_loss: 0.3318 - val_accuracy: 0.8925
Epoch 21/25

Epoch 00021: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0500 - accuracy: 0.9821 - val_loss: 0.2106 - val_accuracy: 0.8925
Epoch 22/25

Epoch 00022: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0520 - accuracy: 0.9829 - val_loss: 0.1040 - val_accuracy: 0.9382
Epoch 23/25

Epoch 00023: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0693 - accuracy: 0.9853 - val_loss: 0.6132 - val_accuracy: 0.8575
Epoch 24/25

Epoch 00024: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0553 - accuracy: 0.9849 - val_loss: 0.3048 - val_accuracy: 0.8817
Epoch 25/25

Epoch 00025: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0376 - accuracy: 0.9877 - val_loss: 0.0121 - val_accuracy: 1.0000
2019-08-25 15:27:32,278 graeae.timers.timer end: Ended: 2019-08-25 15:27:32.278250
I0825 15:27:32.278276 139626236733248 timer.py:77] Ended: 2019-08-25 15:27:32.278250
2019-08-25 15:27:32,279 graeae.timers.timer end: Elapsed: 0:05:54.572075
I0825 15:27:32.279404 139626236733248 timer.py:78] Elapsed: 0:05:54.572075
</pre>
<p>So, your validation went up to 100%, is it a super-classifier?</p>
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(model.history.history)
plot = data.hvplot().opts(title="Rock, Paper, Scissors Re-Training and Re-Validation", width=1000, height=800)
Embed(plot=plot, file_name="re_training")()
</pre></div>
<object data="/posts/keras/rock-paper-scissors/re_training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<div class="highlight">
<pre><span></span>predictor = load_model(best_model)
</pre></div>
<div class="highlight">
<pre><span></span>classifications = dict(zip(range(3), ("Paper", "Rock", "Scissors")))
image_ = keras_image.load_img(str(paper), target_size=(150, 150))
x = keras_image.img_to_array(image_)
x = numpy.expand_dims(x, axis=0)
images = numpy.vstack([x])
classes = model.predict(images, batch_size=10)
print(classifications[classes.argmax()])
</pre></div>
<pre class="example">
Rock

</pre>
<div class="highlight">
<pre><span></span>classifications = dict(zip(range(3), ("Paper", "Rock", "Scissors")))
image_ = keras_image.load_img(str(rock), target_size=(150, 150))
x = keras_image.img_to_array(image_)
x = numpy.expand_dims(x, axis=0)
images = numpy.vstack([x])
classes = model.predict(images, batch_size=10)
print(classifications[classes.argmax()])
</pre></div>
<pre class="example">
Rock

</pre>
<div class="highlight">
<pre><span></span>classifications = dict(zip(range(3), ("Paper", "Rock", "Scissors")))
image_ = keras_image.load_img(str(scissors), target_size=(150, 150))
x = keras_image.img_to_array(image_)
x = numpy.expand_dims(x, axis=0)
images = numpy.vstack([x])
classes = model.predict(images, batch_size=10)
print(classifications[classes.argmax()])
</pre></div>
<pre class="example">
Paper

</pre>
<p>I don't have a large test set, but just from these three it seems like the model got worse.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org1a4625b">
<h3 id="org1a4625b">Sources</h3>
<div class="outline-text-3" id="text-org1a4625b">
<ul class="org-ul">
<li>The <a href="http://www.laurencemoroney.com/rock-paper-scissors-dataset/">Rock-Paper-Scissors</a> dataset was created by Laurence Moroney (lmoroney@gmail.com / laurencemoroney.com).</li>
<li>The test images came from the Wikipedia article on the <a href="https://en.wikipedia.org/wiki/Rock%E2%80%93paper%E2%80%93scissors?oldformat=true">Rock-paper-scissors game</a>.</li>
</ul>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="/categories/cnn/" rel="tag">cnn</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="/posts/keras/horse-or-human-using-tensorflow-20/" rel="prev" title="Horse Or Human Using TensorFlow 2.0">Previous post</a></li>
<li class="next"><a href="/posts/keras/sign-language-exercise/" rel="next" title="Sign Language Exercise">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Contents © 2019 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
