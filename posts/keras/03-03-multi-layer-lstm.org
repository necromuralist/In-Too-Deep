#+BEGIN_COMMENT
.. title: Multi-Layer LSTM
.. slug: multi-layer-lstm
.. date: 2019-09-19 16:07:27 UTC-07:00
.. tags: lstm,nlp
.. category: NLP
.. link: 
.. description: Using a multi-layer LSTM model to classify the IMDB reviews.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
* Beginning
** Imports
*** Python
#+begin_src ipython :session lstm :results none
from functools import partial
#+end_src
*** PyPi
#+begin_src ipython :session lstm :results none
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
#+end_src
*** Others
#+begin_src ipython :session lstm :results none
from graeae import EmbedHoloviews
#+end_src
** Set Up
*** Plotting
#+begin_src ipython :session lstm :results none
Embed = partial(EmbedHoloviews,
                folder_path="../../files/posts/keras/multi-layer-lstm/")
#+end_src
*** The Dataset
    This once again uses the [[https://www.tensorflow.org/datasets/catalog/imdb_reviews][IMDB dataset]] with 50,000 reviews. It has already been converted from strings to integers - each word is encoded as its own integer. Adding ~with_info=True~ returns an object that contains the dictionary with the word to integer mapping. Passing in =imdb_reviews/subwords8k= limits the vocabulary to 8,000 words.
#+begin_src ipython :session lstm :results none
split = (tensorflow_datasets.Split.TRAIN.subsplit([6, 4]), tensorflow_datasets.Split.TEST)
dataset, info = tensorflow_datasets.load("imdb_reviews/subwords8k",
                                         with_info=True
                                         as_supervised=True
                                         split=split)
#+end_src
* Middle
** Set Up the Datasets
#+begin_src ipython :session lstm :results none
train_dataset, test_dataset = dataset['train'], dataset['test']
tokenizer = info.features['text'].encoder
#+end_src

Now we're going to shuffle and padd the data. The =BUFFER_SIZE= argument sets the size of the data to sample from. In this case 10,000 entries in the training set will be selected to be put in the buffer and then the "shuffle" is created by randomly selecting items from the buffer, replacing each item as it's selected until all the data has been through the buffer. The =padded_batch= method creates batches of consecutive data and pads them so that they are all the same shape.

#+begin_src ipython :session lstm :results none
BUFFER_SIZE = 10000
BATCH_SIZE = 64

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)
#+end_src
** The Model
   The previous model had one Bidirectional layer, this will add a second one.

*** Embedding
    The [[https://www.tensorflow.org/guide/embedding][Embedding layer]] converts our inputs of integers and converts them to vectors of real-numbers, which is a better input for a neural network.
*** Bidirectional
    The [[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional][Bidirectional layer]] is a wrapper for Recurrent Neural Networks.
*** LSTM
    The [[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM][LSTM layer]] implements Long-Short-Term Memory. The first argument is the size of the outputs
#+begin_src ipython :session lstm :results none
model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.LSTM(64, return_sequences=True)),
    tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.LSTM(32)),
    tensorflow.keras.layers.Dense(64, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
#+end_src

#+begin_src ipython :session lstm :results output :exports both
print(model.summary())
#+end_src
*** Compile It
#+begin_src ipython :session lstm :results none
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#+end_src
** Train the Model
#+begin_src ipython :session lstm :results none
NUM_EPOCHS = 10
history = model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data=test_dataset)
#+end_src
** Looking at the Performance
#+begin_src ipython :session lstm :results none
data = pandas.DataFrame(history)
plot = data.hvplot().opts(title="Two-Layer LSTM Model", width=1000, height=800)
Embed(plot=plot, filename="lstm_training")()
#+end_src
* End
