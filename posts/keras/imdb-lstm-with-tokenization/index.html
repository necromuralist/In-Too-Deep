<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Building a GRU model for the IMDB reviews using a Tokenizer." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>IMDB GRU With Tokenization | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../he-used-sarcasm/" rel="prev" title="He Used Sarcasm" type="text/html">
<link href="../embeddings-from-scratch/" rel="next" title="Embeddings from Scratch" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="IMDB GRU With Tokenization" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/" property="og:url">
<meta content="Building a GRU model for the IMDB reviews using a Tokenizer." property="og:description">
<meta content="article" property="og:type">
<meta content="2019-09-23T14:14:04-07:00" property="article:published_time">
<meta content="gru" property="article:tag">
<meta content="nlp" property="article:tag">
<meta content="tokenization" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">IMDB GRU With Tokenization</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2019-09-23T14:14:04-07:00" itemprop="datePublished" title="2019-09-23 14:14">2019-09-23 14:14</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org8d39b7c">Beginning</a>
<ul>
<li><a href="#orga263ded">Imports</a>
<ul>
<li><a href="#org20237f9">Python</a></li>
<li><a href="#orgc8e15ee">PyPi</a></li>
<li><a href="#org1c02a15">Other</a></li>
</ul>
</li>
<li><a href="#orgca63888">Set Up</a>
<ul>
<li><a href="#orgeba9e82">The Timer</a></li>
<li><a href="#org87d9b75">Plotting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1e0b891">Middle</a>
<ul>
<li><a href="#org0d3e147">Set Up the Data</a></li>
<li><a href="#org7221d6d">Building Up the Tokenizer</a>
<ul>
<li><a href="#org952e944">Split Up the Sentences and Their Labels</a></li>
<li><a href="#org8d7d2de">Some Constants</a></li>
</ul>
</li>
<li><a href="#orga808ef0">Build the Tokenizer</a></li>
<li><a href="#orgc95e825">Decoder Ring</a></li>
<li><a href="#org723fbfb">Build the Model</a></li>
<li><a href="#orgdb7b5d9">Train it</a></li>
<li><a href="#org982950d">Plot It</a></li>
</ul>
</li>
<li><a href="#orgb4dd8bc">Raw</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org8d39b7c">
<h2 id="org8d39b7c">Beginning</h2>
<div class="outline-text-2" id="text-org8d39b7c">
<p>This is another version of the RNN model to classify the IMDB reviews, but this time we're going to tokenize it ourselves and use a GRU, instead of using the tensorflow-datasets version.</p>
</div>
<div class="outline-3" id="outline-container-orga263ded">
<h3 id="orga263ded">Imports</h3>
<div class="outline-text-3" id="text-orga263ded"></div>
<div class="outline-4" id="outline-container-org20237f9">
<h4 id="org20237f9">Python</h4>
<div class="outline-text-4" id="text-org20237f9">
<div class="highlight">
<pre><span></span>from argparse import Namespace
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc8e15ee">
<h4 id="orgc8e15ee">PyPi</h4>
<div class="outline-text-4" id="text-orgc8e15ee">
<div class="highlight">
<pre><span></span>from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import hvplot.pandas
import numpy
import pandas
import tensorflow
import tensorflow_datasets
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1c02a15">
<h4 id="org1c02a15">Other</h4>
<div class="outline-text-4" id="text-org1c02a15">
<div class="highlight">
<pre><span></span>from graeae import Timer, EmbedHoloviews
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgca63888">
<h3 id="orgca63888">Set Up</h3>
<div class="outline-text-3" id="text-orgca63888"></div>
<div class="outline-4" id="outline-container-orgeba9e82">
<h4 id="orgeba9e82">The Timer</h4>
<div class="outline-text-4" id="text-orgeba9e82">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org87d9b75">
<h4 id="org87d9b75">Plotting</h4>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1e0b891">
<h2 id="org1e0b891">Middle</h2>
<div class="outline-text-2" id="text-org1e0b891"></div>
<div class="outline-3" id="outline-container-org0d3e147">
<h3 id="org0d3e147">Set Up the Data</h3>
<div class="outline-text-3" id="text-org0d3e147">
<div class="highlight">
<pre><span></span>imdb, info = tensorflow_datasets.load("imdb_reviews",
                                      with_info=True,
                                      as_supervised=True)
</pre></div>
<pre class="example">
WARNING: Logging before flag parsing goes to stderr.
W0924 21:52:10.158111 139862640383808 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.
</pre>
<div class="highlight">
<pre><span></span>training, testing = imdb["train"], imdb["test"]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7221d6d">
<h3 id="org7221d6d">Building Up the Tokenizer</h3>
<div class="outline-text-3" id="text-org7221d6d">
<p>Since we didn't pass in a specifier for the configuration we wanted (e.g. <code>imdb/subwords8k</code>) it defaulted to giving us the plain text reviews (and their labels) so we have to build the tokenizer ourselves.</p>
</div>
<div class="outline-4" id="outline-container-org952e944">
<h4 id="org952e944">Split Up the Sentences and Their Labels</h4>
<div class="outline-text-4" id="text-org952e944">
<p>As you might recall, the data set consists of 50,000 IMDB movie reviews categorized as positive or negative. To build the tokenize we first have to split the sentences from their labels</p>
<div class="highlight">
<pre><span></span>training_sentences = []
training_labels = []
testing_sentences = []
testing_labels = []
</pre></div>
<div class="highlight">
<pre><span></span>with TIMER:
    for sentence, label in training:
        training_sentences.append(str(sentence.numpy()))
        training_labels.append(str(label.numpy()))


    for sentence, label in testing:
        testing_sentences.append(str(sentence.numpy))
        testing_labels.append(str(label.numpy()))
</pre></div>
<pre class="example">
2019-09-24 21:52:11,396 graeae.timers.timer start: Started: 2019-09-24 21:52:11.395126
I0924 21:52:11.396310 139862640383808 timer.py:70] Started: 2019-09-24 21:52:11.395126
2019-09-24 21:52:18,667 graeae.timers.timer end: Ended: 2019-09-24 21:52:18.667789
I0924 21:52:18.667830 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:18.667789
2019-09-24 21:52:18,670 graeae.timers.timer end: Elapsed: 0:00:07.272663
I0924 21:52:18.670069 139862640383808 timer.py:78] Elapsed: 0:00:07.272663
</pre>
<div class="highlight">
<pre><span></span>training_labels_final = numpy.array(training_labels)
testing_labels_final = numpy.array(testing_labels)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8d7d2de">
<h4 id="org8d7d2de">Some Constants</h4>
<div class="outline-text-4" id="text-org8d7d2de">
<div class="highlight">
<pre><span></span>Text = Namespace(
    vocab_size = 10000,
    embedding_dim = 16,
    max_length = 120,
    trunc_type='post',
    oov_token = "&lt;OOV&gt;",
)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orga808ef0">
<h3 id="orga808ef0">Build the Tokenizer</h3>
<div class="outline-text-3" id="text-orga808ef0">
<div class="highlight">
<pre><span></span>tokenizer = Tokenizer(num_words=Text.vocab_size, oov_token=Text.oov_token)
with TIMER:
    tokenizer.fit_on_texts(training_sentences)

    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(training_sentences)
    padded = pad_sequences(sequences, maxlen=Text.max_length, truncating=Text.trunc_type)

    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
    testing_padded = pad_sequences(testing_sequences, maxlen=Text.max_length)
</pre></div>
<pre class="example">
2019-09-24 21:52:21,705 graeae.timers.timer start: Started: 2019-09-24 21:52:21.705287
I0924 21:52:21.705317 139862640383808 timer.py:70] Started: 2019-09-24 21:52:21.705287
2019-09-24 21:52:32,152 graeae.timers.timer end: Ended: 2019-09-24 21:52:32.152267
I0924 21:52:32.152314 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:32.152267
2019-09-24 21:52:32,154 graeae.timers.timer end: Elapsed: 0:00:10.446980
I0924 21:52:32.154620 139862640383808 timer.py:78] Elapsed: 0:00:10.446980
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgc95e825">
<h3 id="orgc95e825">Decoder Ring</h3>
<div class="outline-text-3" id="text-orgc95e825">
<div class="highlight">
<pre><span></span>index_to_word = {value: key for key, value in word_index.items()}

def decode_review(text: numpy.array) -&gt; str:
    return " ".join([index_to_word.get(item, "&lt;?&gt;") for item in text])
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org723fbfb">
<h3 id="org723fbfb">Build the Model</h3>
<div class="outline-text-3" id="text-org723fbfb">
<p>This time we're going to build a four-layer model with one Bidirectional layer that uses a <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU">GRU</a> (<a href="https://www.wikiwand.com/en/Gated_recurrent_unit">Gated Recurrent Unit</a>) instead of a LSTM.</p>
<div class="highlight">
<pre><span></span>model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(Text.vocab_size, Text.embedding_dim, input_length=Text.max_length),
    tensorflow.keras.layers.Bidirectional(tensorflow.compat.v2.keras.layers.GRU(32)),
    tensorflow.keras.layers.Dense(6, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
</pre></div>
<div class="highlight">
<pre><span></span>print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
bidirectional (Bidirectional (None, 64)                9600      
_________________________________________________________________
dense (Dense)                (None, 6)                 390       
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 169,997
Trainable params: 169,997
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgdb7b5d9">
<h3 id="orgdb7b5d9">Train it</h3>
<div class="outline-text-3" id="text-orgdb7b5d9">
<div class="highlight">
<pre><span></span>EPOCHS = 50
ONCE_PER_EPOCH = 2
batch_size = 8
history = model.fit(padded, training_labels_final,
                    epochs=EPOCHS,
                    batch_size=batch_size,
                    validation_data=(testing_padded, testing_labels_final),
                    verbose=ONCE_PER_EPOCH)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org982950d">
<h3 id="org982950d">Plot It</h3>
<div class="outline-text-3" id="text-org982950d">
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="GRU Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="gru_training")()
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb4dd8bc">
<h2 id="orgb4dd8bc">Raw</h2>
<div class="outline-text-2" id="text-orgb4dd8bc"></div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/gru/" rel="tag">gru</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/tokenization/" rel="tag">tokenization</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../he-used-sarcasm/" rel="prev" title="He Used Sarcasm">Previous post</a></li>
<li class="next"><a href="../embeddings-from-scratch/" rel="next" title="Embeddings from Scratch">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
