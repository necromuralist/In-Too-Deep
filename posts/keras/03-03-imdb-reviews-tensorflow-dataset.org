#+BEGIN_COMMENT
.. title: IMDB Reviews Tensorflow Dataset
.. slug: imdb-reviews-tensorflow-dataset
.. date: 2019-09-09 16:24:46 UTC-07:00
.. tags: nlp,sentiment,tensorflow
.. category: NLP
.. link: 
.. description: Using the Tensorflow IMDB Reviews data-set to train a Single-Layer LSTM Model.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
* Beginning
** Imports
*** Python
#+begin_src ipython :session imdb :results none
from functools import partial
#+end_src
*** PyPi
#+begin_src ipython :session imdb :results none
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
#+end_src
*** Graeae
#+begin_src ipython :session imdb :results none
from graeae import EmbedHoloviews, Timer
#+end_src
** Set Up
*** Plotting
#+begin_src ipython :session imdb :results none
SLUG = "imdb-reviews-tensorflow-dataset"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{SLUG}")
#+end_src
*** Timer
#+begin_src ipython :session imdb :results none
TIMER = Timer()
#+end_src
* Middle
** Get the Dataset
*** Load It
#+begin_src ipython :session imdb :results none
with TIMER:
    dataset, info = tensorflow_datasets.load('imdb_reviews/subwords8k',
                                             with_info=True,
                                             as_supervised=True)
#+end_src
*** Split It
#+begin_src ipython :session imdb :results none
train_dataset, test_dataset = dataset['train'], dataset['test']
#+end_src
*** The Tokenizer
#+begin_src ipython :session imdb :results none
tokenizer = info.features['text'].encoder
#+end_src
*** Set Up Data
#+begin_src ipython :session imdb :results none
BUFFER_SIZE = 20000
BATCH_SIZE = 64
train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)
#+end_src
** The Model
#+begin_src ipython :session imdb :results none
model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.LSTM(64)),
    tensorflow.keras.layers.Dense(64, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
#+end_src

#+begin_src ipython :session imdb :results output :exports both
model.summary()
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 64)          523840    
_________________________________________________________________
bidirectional_1 (Bidirection (None, 128)               66048     
_________________________________________________________________
dense_2 (Dense)              (None, 64)                8256      
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 65        
=================================================================
Total params: 598,209
Trainable params: 598,209
Non-trainable params: 0
_________________________________________________________________
#+end_example

*** Compile It
#+begin_src ipython :session imdb :results none
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#+end_src

*** Train It
#+begin_src ipython :session imdb :results output :exports both
EPOCHS = 10
history = model.fit(train_dataset, epochs=EPOCHS, validation_data=test_dataset)
#+end_src
*** Plot the Performance
#+begin_src ipython :session imdb :results output raw :exports both
data = pandas.DataFrame(history)
plot = data.hvplot().opts(title="LSTM IMDB Performance", width=1000, height=800)
Embed(plot=plot, file_name="model_performance")()
#+end_src
* End
