#+BEGIN_COMMENT
.. title: Hello There
.. slug: hello-there
.. date: 2019-06-25 06:59:52 UTC-07:00
.. tags: keras,notes
.. category: Notes
.. link: 
.. description: A Keras Hello World.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2
#+BEGIN_SRC ipython :session hello :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* Beginning
  This 'Hello World' takes data created by a simple linear model and trains a neural network to model it. The actual model will take this form:

\[
y = mx + b
\]
** Imports
*** Python
#+begin_src ipython :session hello :results none
from argparse import Namespace
from functools import partial
from pathlib import Path
import random
#+end_src
*** PyPi
#+begin_src ipython :session hello :results none
from tensorflow import keras
import holoviews
import numpy
import pandas
import tensorflow
#+end_src
*** My Stuff
#+begin_src ipython :session hello :results none
from graeae.visualization.embed import EmbedHoloview as EmbedHoloviews
Embed = partial(EmbedHoloviews, 
                folder_path=Path("../../files/posts/keras/hello-there/"))
#+end_src

** Set Up
#+begin_src ipython :session hello :results none
Plot = Namespace(
    height=800,
    width=1000,
)
#+end_src
* Middle
*** The Neural Network
    Our model will be a fully-connected network with one layer with one neuron that takes one input.

 - [[https://www.tensorflow.org/api_docs/python/tf/keras/Sequential][Sequential]] : A linear stack of layers
 - [[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense][Dense]]: A densely connected neural network layer

#+begin_src ipython :session hello :results none
model = keras.Sequential()
model.add(keras.layers.Dense(units=1, input_shape=[1]))
#+end_src

*Note:* The original notebook passed the Dense layer into the constructor, but this gives a warning that you should pass in the dtype instead. Adding it to the constructed object seems to be the way they prefer to do it currently.

** Compile the Model
   "Compiling" in this case means telling the model what optimizer and loss methods to use. In this case it will be [[https://en.wikipedia.org/wiki/Stochastic_gradient_descent?oldformat=true][Stochastic Gradient Descent]] and [[https://en.wikipedia.org/wiki/Mean_squared_error?oldformat=true][Mean Squared Error]]

#+begin_src ipython :session hello :results none
model.compile(optimizer='sgd', loss='mean_squared_error')
#+end_src

** The Data

#+begin_src ipython :session hello :results output :exports both
X = 20 * numpy.random.random_sample((10,)) - 10
slope = random.randrange(2, 10)
intercept = random.randrange(200)
Y = slope * X + intercept
print(X)
print(Y)
data = pandas.DataFrame(dict(X=X,
                             Y=Y))
#+end_src

#+RESULTS:
: [ 5.10568132 -1.46051504 -0.17702021 -0.21922121  5.70757972  9.14936613
:  -9.57982056  6.86277341  4.88901193  6.41590785]
: [233.95113185 174.85536467 186.40681812 186.02700912 239.3682175
:  270.3442952  101.781615   249.76496066 232.00110736 245.74317065]

#+begin_src ipython :session hello :results output raw :exports both
plot = holoviews.Scatter(dict(x=data.X, y=data.Y)).opts(
    height=Plot.height,
    width=Plot.width,
)
Embed(plot=plot, file_name="data_scatter")()
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="data_scatter.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

* End
 - Taken from [[https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb][The Hello World of Deep Learning With Neural Networks]]


# ## Providing the Data
# 
# Next up we'll feed in some data. In this case we are taking 6 xs and 6ys. You can see that the relationship between these is that y=2x-1, so where x = -1, y=-3 etc. etc. 
# 
# A python library called 'Numpy' provides lots of array type data structures that are a defacto standard way of doing it. We declare that we want to use these by specifying the values as an np.array[]

# In[ ]:


xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)


# # Training the Neural Network

# The process of training the neural network, where it 'learns' the relationship between the Xs and Ys is in the **model.fit**  call. This is where it will go through the loop we spoke about above, making a guess, measuring how good or bad it is (aka the loss), using the opimizer to make another guess etc. It will do it for the number of epochs you specify. When you run this code, you'll see the loss on the right hand side.

# In[ ]:


model.fit(xs, ys, epochs=500)


# Ok, now you have a model that has been trained to learn the relationshop between X and Y. You can use the **model.predict** method to have it figure out the Y for a previously unknown X. So, for example, if X = 10, what do you think Y will be? Take a guess before you run this code:

# In[ ]:


print(model.predict([10.0]))


# You might have thought 19, right? But it ended up being a little under. Why do you think that is? 
# 
# Remember that neural networks deal with probabilities, so given the data that we fed the NN with, it calculated that there is a very high probability that the relationship between X and Y is Y=2X-1, but with only 6 data points we can't know for sure. As a result, the result for 10 is very close to 19, but not necessarily 19. 
# 
# As you work with neural networks, you'll see this pattern recurring. You will almost always deal with probabilities, not certainties, and will do a little bit of coding to figure out what the result is based on the probabilities, particularly when it comes to classification.
# 

