#+BEGIN_COMMENT
.. title: Convolutional Neural Networks and Fashion MNIST
.. slug: convolutional-neural-networks-and-fashion-mnist
.. date: 2019-06-30 16:26:01 UTC-07:00
.. tags: cnn,keras
.. category: CNN
.. link: 
.. description: Using a CNN to classify the Fashion MNIST data set.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2
#+begin_src ipython :session cnn :results none :exports none
%load_ext autoreload
%autoreload 2
#+end_src
* Beginning
  The goal of this exercise is to create a model that can classify the Fashion MNIST data better than our previous single hidden-layer model.
** Imports 
*** PyPi
#+begin_src ipython :session cnn :results none
import matplotlib.pyplot as pyplot
import seaborn
import tensorflow
#+end_src
*** My Stuff
#+begin_src ipython :session cnn :results none
from graeae.timers import Timer
#+end_src
** Set Up
*** The Timer
#+begin_src ipython :session cnn :results none
TIMER = Timer()
#+end_src
*** The Data
#+begin_src ipython :session cnn :results none
(training_images, training_labels), (testing_images, testing_labels) = (
    tensorflow.keras.datasets.fashion_mnist.load_data())

training_images = training_images / 255
testing_images = testing_images / 255
#+end_src
* Middle
** Some Exploratory Work  
*** A Baseline Model
   Our baseline that we want to beat is a model with a single dense hidden layer with 128 nodes.

#+begin_src ipython :session cnn :results output :exports both
model = tensorflow.keras.models.Sequential()
model.add(tensorflow.keras.layers.Flatten())
model.add(tensorflow.keras.layers.Dense(128, activation=tensorflow.nn.relu))
model.add(tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.softmax))

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", 
              metrics=["accuracy"])
with TIMER:
    model.fit(training_images, training_labels, epochs=10, verbose=2)
loss, accuracy = model.evaluate(testing_images, testing_labels, verbose=0)
print(f"Testing Loss: {loss:.2f} Testing Accuracy: {accuracy: .2f}")
#+end_src

#+RESULTS:
#+begin_example
2019-07-02 02:32:39,099 graeae.timers.timer start: Started: 2019-07-02 02:32:39.099077
I0702 02:32:39.099112 140189922453312 timer.py:70] Started: 2019-07-02 02:32:39.099077
Epoch 1/10
60000/60000 - 2s - loss: 0.5005 - acc: 0.8246
Epoch 2/10
60000/60000 - 2s - loss: 0.3780 - acc: 0.8645
Epoch 3/10
60000/60000 - 2s - loss: 0.3370 - acc: 0.8779
Epoch 4/10
60000/60000 - 2s - loss: 0.3134 - acc: 0.8863
Epoch 5/10
60000/60000 - 2s - loss: 0.2950 - acc: 0.8916
Epoch 6/10
60000/60000 - 2s - loss: 0.2800 - acc: 0.8971
Epoch 7/10
60000/60000 - 2s - loss: 0.2654 - acc: 0.9022
Epoch 8/10
60000/60000 - 2s - loss: 0.2557 - acc: 0.9050
Epoch 9/10
60000/60000 - 2s - loss: 0.2472 - acc: 0.9087
Epoch 10/10
60000/60000 - 2s - loss: 0.2391 - acc: 0.9111
2019-07-02 02:33:00,397 graeae.timers.timer end: Ended: 2019-07-02 02:33:00.397259
I0702 02:33:00.397287 140189922453312 timer.py:77] Ended: 2019-07-02 02:33:00.397259
2019-07-02 02:33:00,397 graeae.timers.timer end: Elapsed: 0:00:21.298182
I0702 02:33:00.397914 140189922453312 timer.py:78] Elapsed: 0:00:21.298182
Testing Loss: 0.33 Testing Accuracy:  0.89
#+end_example
*** A Convolutional Neural Network
   The convolutional layer expects a single tensor instead of a feed of many of them so you need to reshape the input to make it work.
#+begin_src ipython :session cnn :results none
training_images = training_images.reshape(60000, 28, 28, 1)
testing_images = testing_images.reshape(10000, 28, 28, 1)
#+end_src

Our model starts with a [[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D][Conv2D layer]]. The arguments we're using are:

 - =filters=: the dimensionality of the output space (the number of output filters in the convolution)
 - =kerner_size=: The height and width of the convolution window
 - =activation=:  The activation function for the output
 - =input_shape=: If this is the first layer in the model you have to tell it what the input shape is

The output of the convolutional layers go to a [[https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D][MaxPool2D]] layer. The only argument we're passing in is =pool_size=, the factors by which to downsize the input. Using =(2, 2)= will reduce the size in half. After the convolutions and pooling are applied, the output is sent through a version of the fully-connected network that we were using before (see the baseline model above).
**** A Model Builder
     Something to make it a little easier to re-use things. Note that in the original notebook the first example has 64 filters in the CNN, but later it says that it's better to start with 32 (and the exercises expect that you used 32) so I'm using that as the default value.

#+begin_src ipython :session cnn :results none
class ModelBuilder:
    """Builds, trains, and tests our model

    Args:
     training_images: images to train on
     training_labels: labels for the training data
     testing_images: images to test the trained model with
     testing_labels: labels for the testing data
     additional_convolutions: convolutions besides the input convolution
     epochs: number of times to repeat training
     filters: number of filters in the output of the convolutional layers
     callbacks: something to stop training
    """
    def __init__(self, training_images=training_images, 
                 training_labels=training_labels, 
                 testing_images=testing_images, 
                 testing_labels=testing_labels,
                 additional_convolutions=1, epochs=10, filters=32,
                 callbacks=None) -> None:
        self.training_images = training_images
        self.training_labels = training_labels
        self.testing_images = testing_images
        self.testing_labels = testing_labels
        
        self.additional_convolutions = additional_convolutions
        self.epochs = epochs
        self.filters = filters
        self.callbacks = callbacks
        self._model = None
        return

    @property
    def model(self) -> tensorflow.keras.models.Sequential:
        """Our CNN Model"""
        if self._model is None:
            self._model = tensorflow.keras.models.Sequential()
            self._model.add(tensorflow.keras.layers.Conv2D(
                self.filters, (3, 3), 
                activation="relu", 
                input_shape=(28, 28, 1)))
            self._model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
            
            for convolution in range(self.additional_convolutions):
                self._model.add(tensorflow.keras.layers.Conv2D(self.filters, (3, 3), 
                                                               activation="relu"))
                self._model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
            self._model.add(tensorflow.keras.layers.Flatten())
            self._model.add(tensorflow.keras.layers.Dense(128, activation="relu"))
            self._model.add(tensorflow.keras.layers.Dense(10, activation="softmax"))
            self._model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", 
                                metrics=["accuracy"])
        return self._model
    
    def print_summary(self):
        """Print out the summary for the model"""
        print(self.model.summary)
        return
    
    def fit(self):
        """
        Fit the model to the training data
        """
        callbacks = [self.callbacks] if self.callbacks is not None else []
        self.model.fit(self.training_images, self.training_labels, 
                       epochs=self.epochs, verbose=2, 
                       callbacks=callbacks)
        return

    def test(self) -> tuple:
        """Check the loss and accuracy of the model against the testing set

        Returns:
         (loss, accuracy): the output of the evaluation of the testing data
        """
        return model.evaluate(self.testing_images, self.testing_labels, verbose=0)
    
    def __call__(self):
        """Builds and tests the model"""
        self.model.fit()
        loss, accuracy = self.test()
        print(f"Testing Loss: {loss:.2f}  Testing Accuracy: {accuracy:.2f}")
        return
#+end_src


#+begin_src ipython :session cnn :results none
def create_model(filters=64):
    model = tensorflow.keras.models.Sequential()
    model.add(tensorflow.keras.layers.Conv2D(64, (3, 3), activation="relu", 
                                             input_shape=(28, 28, 1)))
    model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
    model.add(tensorflow.keras.layers.Conv2D(64, (3, 3), activation="relu"))
    model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
    model.add(tensorflow.keras.layers.Flatten())
    model.add(tensorflow.keras.layers.Dense(128, activation="relu"))
    model.add(tensorflow.keras.layers.Dense(10, activation="softmax"))
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", 
                  metrics=["accuracy"])
    return model
#+end_src

#+begin_src ipython :session cnn :results output :exports both
# model = create_model()
builder = ModelBuilder()
builder.print_summary()
#+end_src

#+RESULTS:
: <bound method Network.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f7ff83f4b38>>

*** Layer By Layer
    - Our input is a set of 28 x 28 images.
    - Because we didn't pad the images, the convolutional layer "trims" off one row and column on each side (the center cell can't reach the outermost cells) so we get a 26 x 26 grid with 64 filters (which is what we set up in the definition).
    - The Max Pooling layer the halves the image so we have 13 x 13 grid with 64 filters
    - The next convolution layer once again trims off one row on each side so we have a 11 x 11 grid with 64 filters
    - Then the Max Pooling halves the grid once again so we have a 5 x 5 grid with 64 filters
    - The Flatten layer outputs a vector with 1,600 cells (/5 x 5 x 64 = 1,600/).
    - The first Dense layer has 128 neurons in it so that's the size of the output
    - And the final Dense layer converts it to 10 outputs to match the number of labels we have

#+begin_src ipython :session cnn :results none
def fit_model(model, epochs=5, callbacks=None):
    callbacks = [callbacks] if callbacks is not None else []
    model.fit(training_images, training_labels, epochs=epochs, verbose=2, 
              callbacks=callbacks)
    return model.evaluate(testing_images, testing_labels, verbose=0)
#+end_src

#+begin_src ipython :session cnn :results output :exports both
# test_loss = fit_model(model)
builder.fit()

loss, accuracy = builder.test()
print(f"Test Loss: {loss:.2f} Test Accuracy: {accuracy:.2f}")
#+end_src

#+results:
#+begin_example
Epoch 1/10
60000/60000 - 4s - loss: 0.4791 - acc: 0.8272
Epoch 2/10
60000/60000 - 4s - loss: 0.3221 - acc: 0.8823
Epoch 3/10
60000/60000 - 4s - loss: 0.2765 - acc: 0.8986
Epoch 4/10
60000/60000 - 4s - loss: 0.2460 - acc: 0.9093
Epoch 5/10
60000/60000 - 4s - loss: 0.2230 - acc: 0.9167
Epoch 6/10
60000/60000 - 4s - loss: 0.2001 - acc: 0.9256
Epoch 7/10
60000/60000 - 4s - loss: 0.1845 - acc: 0.9309
Epoch 8/10
60000/60000 - 4s - loss: 0.1664 - acc: 0.9373
Epoch 9/10
60000/60000 - 4s - loss: 0.1521 - acc: 0.9432
Epoch 10/10
60000/60000 - 4s - loss: 0.1397 - acc: 0.9474
Test Loss: 0.33 Test Accuracy: 0.89
#+end_example

Using the Convolutional Neural Network we've gone from 88% to 91% accuracy.

** 10 Epochs
   Using five epochs it appears that the loss is still going down while the accuracy is going up. What happens with ten epochs?
#+begin_src ipython :session cnn :results output :exports both
print(fit_model(model, epochs=10))
#+end_src

#+results:
#+begin_example
Epoch 1/10
60000/60000 - 2s - loss: 0.2289 - acc: 0.9145
Epoch 2/10
60000/60000 - 2s - loss: 0.2243 - acc: 0.9159
Epoch 3/10
60000/60000 - 2s - loss: 0.2158 - acc: 0.9194
Epoch 4/10
60000/60000 - 2s - loss: 0.2098 - acc: 0.9218
Epoch 5/10
60000/60000 - 2s - loss: 0.2023 - acc: 0.9243
Epoch 6/10
60000/60000 - 2s - loss: 0.1979 - acc: 0.9255
Epoch 7/10
60000/60000 - 2s - loss: 0.1907 - acc: 0.9287
Epoch 8/10
60000/60000 - 2s - loss: 0.1871 - acc: 0.9294
Epoch 9/10
60000/60000 - 2s - loss: 0.1808 - acc: 0.9323
Epoch 10/10
60000/60000 - 2s - loss: 0.1792 - acc: 0.9330
[0.38613701688647273, 0.8768]
#+end_example

It looks like it's still learning.
** 20 Epochs
#+begin_src ipython :session cnn :results output :exports both
print(fit_model(model, epochs=20))
#+end_src

#+results:
#+begin_example
Epoch 1/20
60000/60000 - 2s - loss: 0.1732 - acc: 0.9355
Epoch 2/20
60000/60000 - 2s - loss: 0.1706 - acc: 0.9361
Epoch 3/20
60000/60000 - 2s - loss: 0.1659 - acc: 0.9378
Epoch 4/20
60000/60000 - 2s - loss: 0.1616 - acc: 0.9384
Epoch 5/20
60000/60000 - 2s - loss: 0.1597 - acc: 0.9399
Epoch 6/20
60000/60000 - 2s - loss: 0.1563 - acc: 0.9420
Epoch 7/20
60000/60000 - 2s - loss: 0.1513 - acc: 0.9427
Epoch 8/20
60000/60000 - 2s - loss: 0.1499 - acc: 0.9439
Epoch 9/20
60000/60000 - 2s - loss: 0.1446 - acc: 0.9451
Epoch 10/20
60000/60000 - 2s - loss: 0.1455 - acc: 0.9455
Epoch 11/20
60000/60000 - 2s - loss: 0.1384 - acc: 0.9475
Epoch 12/20
60000/60000 - 2s - loss: 0.1375 - acc: 0.9488
Epoch 13/20
60000/60000 - 2s - loss: 0.1342 - acc: 0.9501
Epoch 14/20
60000/60000 - 2s - loss: 0.1325 - acc: 0.9505
Epoch 15/20
60000/60000 - 2s - loss: 0.1288 - acc: 0.9513
Epoch 16/20
60000/60000 - 2s - loss: 0.1248 - acc: 0.9534
Epoch 17/20
60000/60000 - 2s - loss: 0.1258 - acc: 0.9524
Epoch 18/20
60000/60000 - 2s - loss: 0.1225 - acc: 0.9535
Epoch 19/20
60000/60000 - 2s - loss: 0.1204 - acc: 0.9545
Epoch 20/20
60000/60000 - 2s - loss: 0.1174 - acc: 0.9559
[0.4491727818608284, 0.8902]
#+end_example

It looks like it is still improving.
** Try a Loss Callback
#+begin_src ipython :session cnn :results none
class Stop(tensorflow.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get("loss") < 0.02):
            print(f"Stopping point reached at epoch {epoch}")
            self.model.stop_training = True
#+end_src

Rather than continuously incrementing the epochs, maybe we can just go for a good loss.
** Visualizing the Convolutions and Pooling
#+begin_src ipython :session cnn :results output :exports both
print(testing_labels[:100])
#+end_src

#+RESULTS:
: [9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7
:  5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6
:  2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]

#+begin_src ipython :session cnn : results output raw :exports both
f, axarr = pyplot.subplots(3,4)
FIRST_IMAGE=0
SECOND_IMAGE=7
THIRD_IMAGE=26
CONVOLUTION_NUMBER = 1

layer_outputs = [layer.output for layer in model.layers]

activation_model = tensorflow.keras.models.Model(inputs = model.input, outputs = layer_outputs)

for x in range(0,4):
  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[0,x].grid(False)
  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[1,x].grid(False)
  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[2,x].grid(False)
#+end_src


** Exercises
 
*** 1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.
#+begin_src ipython :session cnn : results output :exports both
builder = ModelBuilder(filters=16)
with TIMER:
    builder()
#+end_src

#+begin_src ipython :session cnn : results output :exports both
builder = ModelBuilder(filters=64)
with TIMER:
    builder()
#+end_src
 
*** 2. Remove the final Convolution. What impact will this have on accuracy or training time?

#+begin_src ipython :session cnn : results output :exports both
builder = ModelBuilder(additional_convolutions=0)
with TIMER:
    builder()
#+end_src
 
*** 3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.

#+begin_src ipython :session cnn : results output :exports both
builder = ModelBuilder(additional_convolutions = 2)
with TIMER:
    builder()
#+end_src
  
*** 4. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!
#+begin_src ipython :session cnn : results output :exports both
builder = ModelBuilder(callbacks=Stop, epochs=100)
#+end_src
* End
** Source
   - This is a redo of the [[https://github.com/lmoroney/dlaicourse/blob/master/course%201%20-%20part%206%20-%20lesson%202%20-%20notebook.ipynb][Improving Computer Vision Accuracy Using Convolutions]] notebook.

