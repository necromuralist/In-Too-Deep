#+BEGIN_COMMENT
.. title: Convolutional Neural Networks and Fashion MNIST
.. slug: convolutional-neural-networks-and-fashion-mnist
.. date: 2019-06-30 16:26:01 UTC-07:00
.. tags: cnn,keras
.. category: CNN
.. link: 
.. description: Using a CNN to classify the Fashion MNIST data set.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2
#+begin_src ipython :session cnn :results none :exports none
%load_ext autoreload
%autoreload 2
#+end_src
* Beginning
  The goal of this exercise is to create a model that can classify the Fashion MNIST data better than our previous single hidden-layer model.
** Imports 
*** PyPi
#+begin_src ipython :session cnn :results none
import matplotlib.pyplot as pyplot
import seaborn
import tensorflow
#+end_src
*** My Stuff
#+begin_src ipython :session cnn :results none
from graeae.timers import Timer
#+end_src
** Set Up
*** The Timer
#+begin_src ipython :session cnn :results none
TIMER = Timer()
#+end_src
*** The Data
#+begin_src ipython :session cnn :results none
(training_images, training_labels), (testing_images, testing_labels) = (
    tensorflow.keras.datasets.fashion_mnist.load_data())

training_images = training_images / 255
testing_images = testing_images / 255
#+end_src
*** Plotting
#+begin_src ipython :session cnn :results none :exports none
get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=1)
#+end_src
* Middle
** Some Exploratory Work  
*** A Baseline Model
   Our baseline that we want to beat is a model with a single dense hidden layer with 128 nodes.

#+begin_src ipython :session cnn :results output :exports both
model = tensorflow.keras.models.Sequential()
model.add(tensorflow.keras.layers.Flatten())
model.add(tensorflow.keras.layers.Dense(128, activation=tensorflow.nn.relu))
model.add(tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.softmax))

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", 
              metrics=["accuracy"])
with TIMER:
    model.fit(training_images, training_labels, epochs=10, verbose=2)
loss, accuracy = model.evaluate(testing_images, testing_labels, verbose=0)
print(f"Testing Loss: {loss:.2f} Testing Accuracy: {accuracy: .2f}")
#+end_src

#+RESULTS:
#+begin_example
WARNING: Logging before flag parsing goes to stderr.
W0702 11:43:44.131074 140322301069120 deprecation.py:506] From /home/brunhilde/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2019-07-02 11:43:44,133 graeae.timers.timer start: Started: 2019-07-02 11:43:44.133513
I0702 11:43:44.133839 140322301069120 timer.py:70] Started: 2019-07-02 11:43:44.133513
Epoch 1/10
60000/60000 - 3s - loss: 0.5045 - acc: 0.8224
Epoch 2/10
60000/60000 - 3s - loss: 0.3806 - acc: 0.8630
Epoch 3/10
60000/60000 - 3s - loss: 0.3407 - acc: 0.8764
Epoch 4/10
60000/60000 - 3s - loss: 0.3131 - acc: 0.8851
Epoch 5/10
60000/60000 - 3s - loss: 0.2962 - acc: 0.8902
Epoch 6/10
60000/60000 - 3s - loss: 0.2821 - acc: 0.8955
Epoch 7/10
60000/60000 - 3s - loss: 0.2685 - acc: 0.9009
Epoch 8/10
60000/60000 - 3s - loss: 0.2599 - acc: 0.9033
Epoch 9/10
60000/60000 - 3s - loss: 0.2499 - acc: 0.9075
Epoch 10/10
60000/60000 - 3s - loss: 0.2386 - acc: 0.9096
2019-07-02 11:44:17,515 graeae.timers.timer end: Ended: 2019-07-02 11:44:17.514999
I0702 11:44:17.515156 140322301069120 timer.py:77] Ended: 2019-07-02 11:44:17.514999
2019-07-02 11:44:17,516 graeae.timers.timer end: Elapsed: 0:00:33.381486
I0702 11:44:17.516870 140322301069120 timer.py:78] Elapsed: 0:00:33.381486
Testing Loss: 0.33 Testing Accuracy:  0.88
#+end_example
*** A Convolutional Neural Network
   The convolutional layer expects a single tensor instead of a feed of many of them so you need to reshape the input to make it work.
#+begin_src ipython :session cnn :results none
training_images = training_images.reshape(60000, 28, 28, 1)
testing_images = testing_images.reshape(10000, 28, 28, 1)
#+end_src

Our model starts with a [[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D][Conv2D layer]]. The arguments we're using are:

 - =filters=: the dimensionality of the output space (the number of output filters in the convolution)
 - =kerner_size=: The height and width of the convolution window
 - =activation=:  The activation function for the output
 - =input_shape=: If this is the first layer in the model you have to tell it what the input shape is

The output of the convolutional layers go to a [[https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D][MaxPool2D]] layer. The only argument we're passing in is =pool_size=, the factors by which to downsize the input. Using =(2, 2)= will reduce the size in half. After the convolutions and pooling are applied, the output is sent through a version of the fully-connected network that we were using before (see the baseline model above).
**** A Model Builder
     Something to make it a little easier to re-use things. Note that in the original notebook the first example has 64 filters in the CNN, but later it says that it's better to start with 32 (and the exercises expect that you used 32) so I'm using that as the default value.

#+begin_src ipython :session cnn :results none
class ModelBuilder:
    """Builds, trains, and tests our model

    Args:
     training_images: images to train on
     training_labels: labels for the training data
     testing_images: images to test the trained model with
     testing_labels: labels for the testing data
     additional_convolutions: convolutions besides the input convolution
     epochs: number of times to repeat training
     filters: number of filters in the output of the convolutional layers
     callbacks: something to stop training
    """
    def __init__(self, training_images=training_images, 
                 training_labels=training_labels, 
                 testing_images=testing_images, 
                 testing_labels=testing_labels,
                 additional_convolutions=1, epochs=10, filters=32,
                 callbacks=None) -> None:
        self.training_images = training_images
        self.training_labels = training_labels
        self.testing_images = testing_images
        self.testing_labels = testing_labels
        
        self.additional_convolutions = additional_convolutions
        self.epochs = epochs
        self.filters = filters
        self.callbacks = callbacks
        self._model = None
        return

    @property
    def model(self) -> tensorflow.keras.models.Sequential:
        """Our CNN Model"""
        if self._model is None:
            self._model = tensorflow.keras.models.Sequential()
            self._model.add(tensorflow.keras.layers.Conv2D(
                self.filters, (3, 3), 
                activation="relu", 
                input_shape=(28, 28, 1)))
            self._model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
            
            for convolution in range(self.additional_convolutions):
                self._model.add(tensorflow.keras.layers.Conv2D(self.filters, (3, 3), 
                                                               activation="relu"))
                self._model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
            self._model.add(tensorflow.keras.layers.Flatten())
            self._model.add(tensorflow.keras.layers.Dense(128, activation="relu"))
            self._model.add(tensorflow.keras.layers.Dense(10, activation="softmax"))
            self._model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", 
                                metrics=["accuracy"])
        return self._model
    
    def print_summary(self):
        """Print out the summary for the model"""
        print(self.model.summary())
        return
    
    def fit(self):
        """
        Fit the model to the training data
        """
        callbacks = [self.callbacks] if self.callbacks is not None else []
        self.model.fit(self.training_images, self.training_labels, 
                       epochs=self.epochs, verbose=2, 
                       callbacks=callbacks)
        return

    def test(self) -> tuple:
        """Check the loss and accuracy of the model against the testing set

        Returns:
         (loss, accuracy): the output of the evaluation of the testing data
        """
        return self.model.evaluate(self.testing_images, self.testing_labels, verbose=0)
    
    def __call__(self):
        """Builds and tests the model"""
        self.fit()
        loss, accuracy = self.test()
        print(f"Testing Loss: {loss:.2f}  Testing Accuracy: {accuracy:.2f}")
        return
#+end_src



#+begin_src ipython :session cnn :results output :exports both
# model = create_model()
builder = ModelBuilder()
builder.print_summary()
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_2 (Conv2D)            (None, 26, 26, 32)        320       
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 11, 11, 32)        9248      
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 5, 5, 32)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 800)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 128)               102528    
_________________________________________________________________
dense_5 (Dense)              (None, 10)                1290      
=================================================================
Total params: 113,386
Trainable params: 113,386
Non-trainable params: 0
_________________________________________________________________
None
#+end_example

*** Layer By Layer
    - Our input is a set of 28 x 28 images.
    - Because we didn't pad the images, the convolutional layer "trims" off one row and column on each side (the center cell can't reach the outermost cells) so we get a 26 x 26 grid with 64 filters (which is what we set up in the definition).
    - The Max Pooling layer the halves the image so we have 13 x 13 grid with 64 filters
    - The next convolution layer once again trims off one row on each side so we have a 11 x 11 grid with 64 filters
    - Then the Max Pooling halves the grid once again so we have a 5 x 5 grid with 64 filters
    - The Flatten layer outputs a vector with 1,600 cells (/5 x 5 x 64 = 1,600/).
    - The first Dense layer has 128 neurons in it so that's the size of the output
    - And the final Dense layer converts it to 10 outputs to match the number of labels we have


#+begin_src ipython :session cnn :results output :exports both
builder.fit()

loss, accuracy = builder.test()
print(f"Test Loss: {loss:.2f} Test Accuracy: {accuracy:.2f}")
#+end_src

#+results:
#+begin_example
Epoch 1/10
60000/60000 - 13s - loss: 0.4731 - acc: 0.8298
Epoch 2/10
60000/60000 - 13s - loss: 0.3195 - acc: 0.8826
Epoch 3/10
60000/60000 - 12s - loss: 0.2744 - acc: 0.8979
Epoch 4/10
60000/60000 - 12s - loss: 0.2443 - acc: 0.9087
Epoch 5/10
60000/60000 - 12s - loss: 0.2204 - acc: 0.9167
Epoch 6/10
60000/60000 - 12s - loss: 0.2013 - acc: 0.9247
Epoch 7/10
60000/60000 - 12s - loss: 0.1834 - acc: 0.9316
Epoch 8/10
60000/60000 - 12s - loss: 0.1662 - acc: 0.9379
Epoch 9/10
60000/60000 - 12s - loss: 0.1528 - acc: 0.9424
Epoch 10/10
60000/60000 - 12s - loss: 0.1407 - acc: 0.9478
Test Loss: 0.44 Test Accuracy: 0.89
#+end_example

Using the Convolutional Neural Network we've gone from 88% to 91% accuracy.

** 10 Epochs
   Using five epochs it appears that the loss is still going down while the accuracy is going up. What happens with ten epochs?
#+begin_src ipython :session cnn :results output :exports both
builder_10 = ModelBuilder(epochs=10)
builder_10()
#+end_src

#+results:
#+begin_example
Epoch 1/10
60000/60000 - 16s - loss: 0.4814 - acc: 0.8249
Epoch 2/10
60000/60000 - 16s - loss: 0.3171 - acc: 0.8857
Epoch 3/10
60000/60000 - 17s - loss: 0.2725 - acc: 0.9001
Epoch 4/10
60000/60000 - 16s - loss: 0.2430 - acc: 0.9105
Epoch 5/10
60000/60000 - 16s - loss: 0.2189 - acc: 0.9186
Epoch 6/10
60000/60000 - 17s - loss: 0.1996 - acc: 0.9249
Epoch 7/10
60000/60000 - 16s - loss: 0.1819 - acc: 0.9319
Epoch 8/10
60000/60000 - 16s - loss: 0.1669 - acc: 0.9378
Epoch 9/10
60000/60000 - 16s - loss: 0.1520 - acc: 0.9427
Epoch 10/10
60000/60000 - 16s - loss: 0.1409 - acc: 0.9471
Testing Loss: 0.26  Testing Accuracy: 0.91
#+end_example

It looks like it's still learning.
** 15 Epochs

#+begin_src ipython :session cnn :results output :exports both
builder_15 = ModelBuilder(epochs=15)
builder_15()
#+end_src

#+results:
#+begin_example
Epoch 1/15
60000/60000 - 16s - loss: 0.4766 - acc: 0.8276
Epoch 2/15
60000/60000 - 15s - loss: 0.3226 - acc: 0.8810
Epoch 3/15
60000/60000 - 16s - loss: 0.2752 - acc: 0.8980
Epoch 4/15
60000/60000 - 16s - loss: 0.2452 - acc: 0.9092
Epoch 5/15
60000/60000 - 15s - loss: 0.2211 - acc: 0.9185
Epoch 6/15
60000/60000 - 15s - loss: 0.2006 - acc: 0.9253
Epoch 7/15
60000/60000 - 15s - loss: 0.1800 - acc: 0.9324
Epoch 8/15
60000/60000 - 15s - loss: 0.1660 - acc: 0.9378
Epoch 9/15
60000/60000 - 14s - loss: 0.1497 - acc: 0.9441
Epoch 10/15
60000/60000 - 15s - loss: 0.1376 - acc: 0.9483
Epoch 11/15
60000/60000 - 15s - loss: 0.1257 - acc: 0.9523
Epoch 12/15
60000/60000 - 15s - loss: 0.1130 - acc: 0.9564
Epoch 13/15
60000/60000 - 15s - loss: 0.1031 - acc: 0.9612
Epoch 14/15
60000/60000 - 16s - loss: 0.0932 - acc: 0.9645
Epoch 15/15
60000/60000 - 16s - loss: 0.0844 - acc: 0.9682
Testing Loss: 0.36  Testing Accuracy: 0.91
#+end_example

It looks like it's started to overfit, the accuracy is okay, but the loss is a little worse.

** 20 Epochs
#+begin_src ipython :session cnn :results output :exports both
builder = ModelBuilder(epochs=20)
builder()
#+end_src

#+results:
#+begin_example
Epoch 1/20
60000/60000 - 15s - loss: 0.4677 - acc: 0.8315
Epoch 2/20
60000/60000 - 15s - loss: 0.3155 - acc: 0.8856
Epoch 3/20
60000/60000 - 14s - loss: 0.2710 - acc: 0.9004
Epoch 4/20
60000/60000 - 15s - loss: 0.2413 - acc: 0.9105
Epoch 5/20
60000/60000 - 15s - loss: 0.2186 - acc: 0.9195
Epoch 6/20
60000/60000 - 15s - loss: 0.1981 - acc: 0.9262
Epoch 7/20
60000/60000 - 15s - loss: 0.1822 - acc: 0.9310
Epoch 8/20
60000/60000 - 15s - loss: 0.1662 - acc: 0.9380
Epoch 9/20
60000/60000 - 15s - loss: 0.1518 - acc: 0.9412
Epoch 10/20
60000/60000 - 15s - loss: 0.1394 - acc: 0.9467
Epoch 11/20
60000/60000 - 15s - loss: 0.1283 - acc: 0.9515
Epoch 12/20
60000/60000 - 15s - loss: 0.1173 - acc: 0.9556
Epoch 13/20
60000/60000 - 15s - loss: 0.1065 - acc: 0.9590
Epoch 14/20
60000/60000 - 14s - loss: 0.0960 - acc: 0.9640
Epoch 15/20
60000/60000 - 15s - loss: 0.0891 - acc: 0.9660
Epoch 16/20
60000/60000 - 15s - loss: 0.0809 - acc: 0.9696
Epoch 17/20
60000/60000 - 16s - loss: 0.0758 - acc: 0.9706
Epoch 18/20
60000/60000 - 15s - loss: 0.0693 - acc: 0.9738
Epoch 19/20
60000/60000 - 15s - loss: 0.0633 - acc: 0.9762
Epoch 20/20
60000/60000 - 15s - loss: 0.0584 - acc: 0.9776
Testing Loss: 0.43  Testing Accuracy: 0.90
#+end_example

It looks like it might be overfitting - both the loss and the accuracy went down a little.
** Try a Loss Callback
#+begin_src ipython :session cnn :results none
class Stop(tensorflow.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if (logs.get("loss") < 0.02):
            print(f"Stopping point reached at epoch {epoch}")
            self.model.stop_training = True
#+end_src

Rather than continuously incrementing the epochs, maybe we can just go for a good loss.
** Visualizing the Convolutions and Pooling
#+begin_src ipython :session cnn :results output :exports both
print(testing_labels[:100])
#+end_src

#+RESULTS:
: [9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7
:  5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6
:  2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]

#+begin_src ipython :session cnn :results raw drawer :ipyfile ../../files/posts/keras/convolutional-neural-networks-and-fashion-mnist/layer_visualization.png
model = builder_10.model
figure, axis_array = pyplot.subplots(3,4)
FIRST_IMAGE=0
SECOND_IMAGE=7
THIRD_IMAGE=26
CONVOLUTION_NUMBER = 1

layer_outputs = [layer.output for layer in model.layers]

activation_model = tensorflow.keras.models.Model(inputs = model.input, outputs = layer_outputs)

for x in range(0,4):
  f1 = activation_model.predict(testing_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]
  axis_array[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axis_array[0,x].grid(False)
  f2 = activation_model.predict(testing_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]
  axis_array[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axis_array[1,x].grid(False)
  f3 = activation_model.predict(testing_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]
  axis_array[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axis_array[2,x].grid(False)
#+end_src

#+RESULTS:
:results:
# Out[37]:
[[file:../../files/posts/keras/convolutional-neural-networks-and-fashion-mnist/layer_visualization.png]]
:end:


** Exercises
 
*** 1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.
**** 16 Nodes
#+begin_src ipython :session cnn :results output :exports both
builder = ModelBuilder(filters=16)
with TIMER:
    builder()
#+end_src

#+RESULTS:
: 2019-07-02 18:01:48,610 graeae.timers.timer start: Started: 2019-07-02 18:01:48.610472
: I0702 18:01:48.610538 140322301069120 timer.py:70] Started: 2019-07-02 18:01:48.610472
: Epoch 1/10
: 60000/60000 - 17s - loss: 0.5022 - acc: 0.8180
: Epoch 2/10
: 60000/60000 - 16s - loss: 0.3436 - acc: 0.8742
: Epoch 3/10
: 60000/60000 - 16s - loss: 0.3017 - acc: 0.8889
: Epoch 4/10
: 60000/60000 - 16s - loss: 0.2719 - acc: 0.8997
: Epoch 5/10
: 60000/60000 - 16s - loss: 0.2518 - acc: 0.9077
: Epoch 6/10
: 60000/60000 - 16s - loss: 0.2350 - acc: 0.9123
: Epoch 7/10
: 60000/60000 - 16s - loss: 0.2209 - acc: 0.9170
: Epoch 8/10
: 60000/60000 - 16s - loss: 0.2068 - acc: 0.9222
: Epoch 9/10
: 60000/60000 - 16s - loss: 0.1978 - acc: 0.9253
: Epoch 10/10
: 60000/60000 - 16s - loss: 0.1863 - acc: 0.9297
: 2019-07-02 18:04:32,138 graeae.timers.timer end: Ended: 2019-07-02 18:04:32.137851
: I0702 18:04:32.138059 140322301069120 timer.py:77] Ended: 2019-07-02 18:04:32.137851
: 2019-07-02 18:04:32,140 graeae.timers.timer end: Elapsed: 0:02:43.527379
: I0702 18:04:32.140508 140322301069120 timer.py:78] Elapsed: 0:02:43.527379
: Testing Loss: 0.28  Testing Accuracy: 0.90

The smaller model had slightly more loss than the 32 node model as well as a little less accuracy.
**** 64 Nodes
#+begin_src ipython :session cnn :results output :exports both
builder = ModelBuilder(filters=64)
with TIMER:
    builder()
#+end_src

#+RESULTS:
#+begin_example
2019-07-02 18:08:30,896 graeae.timers.timer start: Started: 2019-07-02 18:08:30.896062
I0702 18:08:30.896127 140322301069120 timer.py:70] Started: 2019-07-02 18:08:30.896062
Epoch 1/10
60000/60000 - 19s - loss: 0.4433 - acc: 0.8397
Epoch 2/10
60000/60000 - 18s - loss: 0.2991 - acc: 0.8903
Epoch 3/10
60000/60000 - 18s - loss: 0.2496 - acc: 0.9074
Epoch 4/10
60000/60000 - 18s - loss: 0.2169 - acc: 0.9196
Epoch 5/10
60000/60000 - 18s - loss: 0.1913 - acc: 0.9279
Epoch 6/10
60000/60000 - 364s - loss: 0.1671 - acc: 0.9370
Epoch 7/10
60000/60000 - 19s - loss: 0.1475 - acc: 0.9446
Epoch 8/10
60000/60000 - 19s - loss: 0.1310 - acc: 0.9505
Epoch 9/10
60000/60000 - 19s - loss: 0.1148 - acc: 0.9562
Epoch 10/10
60000/60000 - 19s - loss: 0.1023 - acc: 0.9612
2019-07-02 18:17:28,167 graeae.timers.timer end: Ended: 2019-07-02 18:17:28.167601
I0702 18:17:28.167735 140322301069120 timer.py:77] Ended: 2019-07-02 18:17:28.167601
2019-07-02 18:17:28,169 graeae.timers.timer end: Elapsed: 0:08:57.271539
I0702 18:17:28.169652 140322301069120 timer.py:78] Elapsed: 0:08:57.271539
Testing Loss: 0.29  Testing Accuracy: 0.91
#+end_example

This has the same accuracy as the 32 node model but with a slight increase in the loss.
 
*** 2. Remove the final Convolution. What impact will this have on accuracy or training time?

#+begin_src ipython :session cnn :results output :exports both
builder = ModelBuilder(additional_convolutions=0)
with TIMER:
    builder()
#+end_src

#+RESULTS:
: 2019-07-02 18:22:53,077 graeae.timers.timer start: Started: 2019-07-02 18:22:53.076964
: I0702 18:22:53.077012 140322301069120 timer.py:70] Started: 2019-07-02 18:22:53.076964
: Epoch 1/10
: 60000/60000 - 14s - loss: 0.3835 - acc: 0.8644
: Epoch 2/10
: 60000/60000 - 13s - loss: 0.2614 - acc: 0.9045
: Epoch 3/10
: 60000/60000 - 13s - loss: 0.2154 - acc: 0.9209
: Epoch 4/10
: 60000/60000 - 14s - loss: 0.1857 - acc: 0.9305
: Epoch 5/10
: 60000/60000 - 13s - loss: 0.1581 - acc: 0.9414
: Epoch 6/10
: 60000/60000 - 13s - loss: 0.1367 - acc: 0.9496
: Epoch 7/10
: 60000/60000 - 13s - loss: 0.1166 - acc: 0.9567
: Epoch 8/10
: 60000/60000 - 13s - loss: 0.0993 - acc: 0.9630
: Epoch 9/10
: 60000/60000 - 14s - loss: 0.0830 - acc: 0.9686
: Epoch 10/10
: 60000/60000 - 14s - loss: 0.0704 - acc: 0.9741
: 2019-07-02 18:25:09,531 graeae.timers.timer end: Ended: 2019-07-02 18:25:09.531189
: I0702 18:25:09.531369 140322301069120 timer.py:77] Ended: 2019-07-02 18:25:09.531189
: 2019-07-02 18:25:09,534 graeae.timers.timer end: Elapsed: 0:02:16.454225
: I0702 18:25:09.534272 140322301069120 timer.py:78] Elapsed: 0:02:16.454225
: Testing Loss: 0.31  Testing Accuracy: 0.92

 
Once again the accuracy is a little better than the 32 node model but the testing loss is also a little higher. We probably need more data.
*** 3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.

#+begin_src ipython :session cnn : results output :exports both
builder = ModelBuilder(additional_convolutions = 2)
with TIMER:
    builder()
#+end_src
  
*** 4. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!
#+begin_src ipython :session cnn : results output :exports both
builder = ModelBuilder(callbacks=Stop, epochs=100)
#+end_src
* End
** Source
   - This is a redo of the [[https://github.com/lmoroney/dlaicourse/blob/master/course%201%20-%20part%206%20-%20lesson%202%20-%20notebook.ipynb][Improving Computer Vision Accuracy Using Convolutions]] notebook.

