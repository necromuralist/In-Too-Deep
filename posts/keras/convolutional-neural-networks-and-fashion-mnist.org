#+BEGIN_COMMENT
.. title: Convolutional Neural Networks and Fashion MNIST
.. slug: convolutional-neural-networks-and-fashion-mnist
.. date: 2019-06-30 16:26:01 UTC-07:00
.. tags: cnn,keras
.. category: CNN
.. link: 
.. description: Using a CNN to classify the Fashion MNIST data set.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 2
#+begin_src ipython :session cnn :results none :exports none
%load_ext autoreload
%autoreload 2
#+end_src
* Beginning
  The goal of this exercise is to create a model that can classify the Fashion MNIST data better than our previous single hidden-layer model.
** Imports 
*** PyPi
#+begin_src ipython :session cnn :results none
import tensorflow
#+end_src
** The Data
#+begin_src ipython :session cnn :results none
(training_images, training_labels), (testing_images, testing_labels) = (
    tensorflow.keras.datasets.fashion_mnist.load_data())

training_images = training_images / 255
testing_images = testing_images / 255
#+end_src
* Middle
  
** A Baseline Model
   Our baseline that we want to beat is a model with a single dense hidden layer with 512 nodes.

#+begin_src ipython :session cnn :results output :exports both
model = tensorflow.keras.models.Sequential()
model.add(tensorflow.keras.layers.Flatten())
model.add(tensorflow.keras.layers.Dense(512, activation=tensorflow.nn.relu))
model.add(tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.softmax))

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", 
              metrics=["accuracy"])
model.fit(training_images, training_labels, epochs=5, verbose=2)
loss, accuracy = model.evaluate(testing_images, testing_labels, verbose=0)
print(f"Testing Loss: {loss:.2f} Testing Accuracy: {accuracy: .2f}")
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/5
60000/60000 - 3s - loss: 0.4709 - acc: 0.8330
Epoch 2/5
60000/60000 - 3s - loss: 0.3583 - acc: 0.8684
Epoch 3/5
60000/60000 - 2s - loss: 0.3218 - acc: 0.8809
Epoch 4/5
60000/60000 - 2s - loss: 0.2981 - acc: 0.8896
Epoch 5/5
60000/60000 - 2s - loss: 0.2835 - acc: 0.8943
Testing Loss: 0.34 Testing Accuracy:  0.88
#+end_example
** A Convolutional Neural Network
   The convolutional layer expects a single tensor instead of a feed of many of them so you need to reshape the input to make it work.
#+begin_src ipython :session cnn :results none
training_images = training_images.reshape(60000, 28, 28, 1)
testing_images = testing_images.reshape(10000, 28, 28, 1)
#+end_src

#+begin_src ipython :session cnn :results none
def create_model():
    model = tensorflow.keras.models.Sequential()
    model.add(tensorflow.keras.layers.Conv2D(64, (3, 3), activation="relu", 
                                             input_shape=(28, 28, 1)))
    model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
    model.add(tensorflow.keras.layers.Conv2D(64, (3, 3), activation="relu", 
                                             input_shape=(28, 28, 1)))
    model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
    model.add(tensorflow.keras.layers.Flatten())
    model.add(tensorflow.keras.layers.Dense(128, activation="relu"))
    model.add(tensorflow.keras.layers.Dense(10, activation="softmax"))
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", 
                  metrics=["accuracy"])
    return model
#+end_src

#+begin_src ipython :session cnn :results output :exports both
model = create_model()
print(model.summary())
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_8 (Conv2D)            (None, 26, 26, 64)        640       
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 13, 13, 64)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 11, 11, 64)        36928     
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
flatten_7 (Flatten)          (None, 1600)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 128)               204928    
_________________________________________________________________
dense_14 (Dense)             (None, 10)                1290      
=================================================================
Total params: 243,786
Trainable params: 243,786
Non-trainable params: 0
_________________________________________________________________
None
#+end_example

*** Layer By Layer
    - Our input is a set of 28 x 28 images.
    - The convolutional layer

#+begin_src ipython :session cnn :results none
def fit_model(model, epochs=5):
    model.fit(training_images, training_labels, epochs=epochs, verbose=2)
    return model.evaluate(testing_images, testing_labels, verbose=0)
#+end_src

#+begin_src ipython :session cnn :results output :exports both
test_loss = fit_model(model)
print(test_loss)
#+end_src

#+results:
#+begin_example
epoch 1/5
60000/60000 - 7s - loss: 0.4384 - acc: 0.8417
epoch 2/5
60000/60000 - 7s - loss: 0.2941 - acc: 0.8919
epoch 3/5
60000/60000 - 7s - loss: 0.2485 - acc: 0.9093
epoch 4/5
60000/60000 - 6s - loss: 0.2182 - acc: 0.9186
epoch 5/5
60000/60000 - 6s - loss: 0.1912 - acc: 0.9273
[0.25359962485432624, 0.9082]
#+end_example

we've gone from 88% to 91% accuracy

** 10 epochs
#+begin_src ipython :session cnn :results output :exports both
print(fit_model(model, epochs=10))
#+end_src

#+results:
#+begin_example
epoch 1/10
60000/60000 - 7s - loss: 0.1677 - acc: 0.9369
epoch 2/10
60000/60000 - 6s - loss: 0.1468 - acc: 0.9441
epoch 3/10
60000/60000 - 7s - loss: 0.1315 - acc: 0.9514
epoch 4/10
60000/60000 - 7s - loss: 0.1155 - acc: 0.9565
epoch 5/10
60000/60000 - 7s - loss: 0.1033 - acc: 0.9610
epoch 6/10
60000/60000 - 8s - loss: 0.0881 - acc: 0.9665
epoch 7/10
60000/60000 - 7s - loss: 0.0796 - acc: 0.9697
epoch 8/10
60000/60000 - 7s - loss: 0.0726 - acc: 0.9730
epoch 9/10
60000/60000 - 7s - loss: 0.0618 - acc: 0.9766
epoch 10/10
60000/60000 - 7s - loss: 0.0577 - acc: 0.9789
[0.4223233294188976, 0.9062]
#+end_example

#+begin_src ipython :session cnn :results output :exports both
print(fit_model(model, epochs=20))
#+end_src

#+results:
#+begin_example
epoch 1/20
60000/60000 - 8s - loss: 0.0533 - acc: 0.9804
epoch 2/20
60000/60000 - 7s - loss: 0.0454 - acc: 0.9827
epoch 3/20
60000/60000 - 7s - loss: 0.0455 - acc: 0.9830
epoch 4/20
60000/60000 - 7s - loss: 0.0420 - acc: 0.9844
epoch 5/20
60000/60000 - 7s - loss: 0.0380 - acc: 0.9858
epoch 6/20
60000/60000 - 8s - loss: 0.0363 - acc: 0.9861
epoch 7/20
60000/60000 - 7s - loss: 0.0342 - acc: 0.9871
epoch 8/20
60000/60000 - 7s - loss: 0.0332 - acc: 0.9874
epoch 9/20
60000/60000 - 6s - loss: 0.0306 - acc: 0.9885
epoch 10/20
60000/60000 - 7s - loss: 0.0301 - acc: 0.9893
epoch 11/20
60000/60000 - 7s - loss: 0.0286 - acc: 0.9901
epoch 12/20
60000/60000 - 7s - loss: 0.0268 - acc: 0.9901
epoch 13/20
60000/60000 - 8s - loss: 0.0260 - acc: 0.9907
epoch 14/20
60000/60000 - 7s - loss: 0.0280 - acc: 0.9906
epoch 15/20
60000/60000 - 7s - loss: 0.0266 - acc: 0.9906
epoch 16/20
60000/60000 - 7s - loss: 0.0204 - acc: 0.9925
epoch 17/20
60000/60000 - 7s - loss: 0.0234 - acc: 0.9919
epoch 18/20
60000/60000 - 7s - loss: 0.0242 - acc: 0.9916
epoch 19/20
60000/60000 - 7s - loss: 0.0235 - acc: 0.9915
epoch 20/20
60000/60000 - 8s - loss: 0.0228 - acc: 0.9927
[0.762037175056804, 0.9086]
#+end_example

the accuracy didn't get much better, but the loss got much worse, suggesting it's overfitting.
* end
** source
   - this is a redo of the [[https://github.com/lmoroney/dlaicourse/blob/master/course%201%20-%20part%206%20-%20lesson%202%20-%20notebook.ipynb][improving computer vision accuracy using convolutions]].
* raw
# 
# ```
# model = tf.keras.models.Sequential([
#   tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
#   tf.keras.layers.MaxPooling2D(2, 2),
# ```
# 
# 

# Add another convolution
# 
# 
# 
# ```
#   tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
#   tf.keras.layers.MaxPooling2D(2,2)
# ```
# 
# 

# Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version
# 
# ```
#   tf.keras.layers.Flatten(),
# ```
# 
# 

# The same 128 dense layers, and 10 output layers as in the pre-convolution example:
# 
# 
# 
# ```
#   tf.keras.layers.Dense(128, activation='relu'),
#   tf.keras.layers.Dense(10, activation='softmax')
# ])
# ```
# 
# 

# Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.
# 
# 
# 
# ```
# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
# model.fit(training_images, training_labels, epochs=5)
# test_loss, test_acc = model.evaluate(test_images, test_labels)
# print(test_acc)
# ```
# 
# 
# 

# # Visualizing the Convolutions and Pooling
# 
# This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination.

# In[ ]:


print(test_labels[:100])


# In[ ]:


import matplotlib.pyplot as plt
f, axarr = plt.subplots(3,4)
FIRST_IMAGE=0
SECOND_IMAGE=7
THIRD_IMAGE=26
CONVOLUTION_NUMBER = 1
from tensorflow.keras import models
layer_outputs = [layer.output for layer in model.layers]
activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)
for x in range(0,4):
  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[0,x].grid(False)
  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[1,x].grid(False)
  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]
  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')
  axarr[2,x].grid(False)


# EXERCISES
# 
# 1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.
# 
# 2. Remove the final Convolution. What impact will this have on accuracy or training time?
# 
# 3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.
# 
# 4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. 
# 
# 5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!

# In[7]:


import tensorflow as tf
print(tf.__version__)
mnist = tf.keras.datasets.mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
training_images=training_images.reshape(60000, 28, 28, 1)
training_images=training_images / 255.0
test_images = test_images.reshape(10000, 28, 28, 1)
test_images=test_images/255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
  tf.keras.layers.MaxPooling2D(2, 2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=10)
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(test_acc)

