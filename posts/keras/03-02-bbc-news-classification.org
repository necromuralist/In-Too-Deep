#+BEGIN_COMMENT
.. title: BBC News Classification
.. slug: bbc-news-classification
.. date: 2019-08-26 15:28:56 UTC-07:00
.. tags: nlp
.. category: NLP 
.. link: 
.. description: Building a classifier for the BBC news.
.. type: text

#+END_COMMENT
#+TOC: headlines 3
#+begin_src ipython :session bbc :results none :exports none
%load_ext autoreload
%autoreload 2
#+end_src
* Beginning
** Imports
*** Python
#+begin_src ipython :session bbc :results none
from functools import partial
from pathlib import Path
import csv
#+end_src

*** PyPi
#+begin_src ipython :session bbc :results none
from tensorflow.keras.preprocessing.text import Tokenizer
import hvplot.pandas
import pandas
import spacy
#+end_src
*** Graeae
#+begin_src ipython :session bbc :results none
from graeae import EmbedHoloviews, SubPathLoader, Timer
#+end_src
** Setup
*** The Timer
#+begin_src ipython :session bbc :results none
TIMER = Timer()
#+end_src
*** The Environment
#+begin_src ipython :session bbc :results none
ENVIRONMENT = SubPathLoader('DATASETS')
#+end_src
*** Spacy
#+begin_src ipython :session bbc :results none
nlp = spacy.load("en_core_web_lg")
#+end_src
*** Plotting
#+begin_src ipython :session bbc :results none
SLUG = "bbc-news-classification"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{SLUG}")
#+end_src
* Middle
** Load the Datasets
#+begin_src ipython :session bbc :results output :exports both
path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()

with TIMER:
    with path.open() as csvfile:
        lines = csv.DictReader(csvfile)
        labels = [line["category"] for line in lines]
    
    with path.open() as csvfile:
        lines = csv.DictReader(csvfile)
        texts = [nlp(line["text"]) for line in lines]
#+end_src

#+RESULTS:
: WARNING: Logging before flag parsing goes to stderr.
: I0829 17:52:41.605380 140355150067520 environment.py:35] Environment Path: /home/brunhilde/.env
: I0829 17:52:41.607632 140355150067520 environment.py:90] Environment Path: /home/brunhilde/.config/datasets/env
: 2019-08-29 17:52:41,609 graeae.timers.timer start: Started: 2019-08-29 17:52:41.609407
: I0829 17:52:41.609555 140355150067520 timer.py:70] Started: 2019-08-29 17:52:41.609407
: 2019-08-29 17:55:29,802 graeae.timers.timer end: Ended: 2019-08-29 17:55:29.801999
: I0829 17:55:29.802120 140355150067520 timer.py:77] Ended: 2019-08-29 17:55:29.801999
: 2019-08-29 17:55:29,805 graeae.timers.timer end: Elapsed: 0:02:48.192592
: I0829 17:55:29.805274 140355150067520 timer.py:78] Elapsed: 0:02:48.192592

#+begin_src ipython :session bbc :results output :exports both
print(texts[1])
#+end_src

#+RESULTS:
: worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (Â£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.

#+begin_src ipython :session bbc :results output :exports both
print(f"Rows: {len(labels):,}")
print(f"Unique Labels: {len(set(labels)):,}")
#+end_src

#+RESULTS:
: Rows: 2,225
: Unique Labels: 5

Since there's only five maybe we should plot it.

#+begin_src ipython :session bbc :results output raw :exports both
labels_frame = pandas.DataFrame({"label": labels})
counts = labels_frame.label.value_counts().reset_index().rename(
    columns={"index": "Category", "label": "Articles"})
plot = counts.hvplot.bar("Category", "Articles").opts(
    title="Count of BBC News Articles by Category",
    height=800, width=1000)
Embed(plot=plot, file_name="bbc_category_counts")()
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="bbc_category_counts.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

It looks like the categories are somewhat unevenly distributed. Now to normalize the tokens.

#+begin_src ipython :session bbc :results output :exports both
with TIMER:
    cleaned = [[token.lemma_ for token in text if not any((token.is_stop, token.is_space, token.is_punct))]
               for text in texts]
#+end_src

#+RESULTS:
: 2019-08-29 17:55:37,748 graeae.timers.timer start: Started: 2019-08-29 17:55:37.748600
: I0829 17:55:37.748631 140355150067520 timer.py:70] Started: 2019-08-29 17:55:37.748600
: 2019-08-29 17:55:38,704 graeae.timers.timer end: Ended: 2019-08-29 17:55:38.704025
: I0829 17:55:38.704106 140355150067520 timer.py:77] Ended: 2019-08-29 17:55:38.704025
: 2019-08-29 17:55:38,706 graeae.timers.timer end: Elapsed: 0:00:00.955425
: I0829 17:55:38.706015 140355150067520 timer.py:78] Elapsed: 0:00:00.955425

** The Tokenizer
   Even though I've already tokenized the text, we need a one-hot-encoded version of it. So I'll use the [[https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer][tensorflow keras Tokenizer]].

#+begin_src ipython :session bbc :results none
tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>")
#+end_src

The =num_words= is the total amount of words that will be kept in the word index - I don't know why a thousand, I just found that in the "answer" notebook. The =oov_token= is what's used when a word is encountered outside of the words we're building into our word-index.

#+begin_src ipython :session bbc :results output :exports both
with TIMER:
    tokenizer.fit_on_texts(cleaned)
#+end_src

#+RESULTS:
: 2019-08-29 17:55:42,420 graeae.timers.timer start: Started: 2019-08-29 17:55:42.419971
: I0829 17:55:42.420001 140355150067520 timer.py:70] Started: 2019-08-29 17:55:42.419971
: 2019-08-29 17:55:42,728 graeae.timers.timer end: Ended: 2019-08-29 17:55:42.728127
: I0829 17:55:42.728209 140355150067520 timer.py:77] Ended: 2019-08-29 17:55:42.728127
: 2019-08-29 17:55:42,729 graeae.timers.timer end: Elapsed: 0:00:00.308156
: I0829 17:55:42.729554 140355150067520 timer.py:78] Elapsed: 0:00:00.308156

That was fast. Maybe because I limited it to 1,000.

We should now have a dictionary named =word_index= that holds the words:index pairs for all the tokens found (it only uses the =num_words= when you call tokenizer's methods according to [[https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work][Stack Overflow]])

#+begin_src ipython :session bbc :results output :exports both
print(f"{len(tokenizer.word_index):,}")
#+end_src

#+RESULTS:
: 24,339

* End
* Raw
#+begin_comment
# In[ ]:


train_size = # YOUR CODE HERE

train_sentences = # YOUR CODE HERE
train_labels = # YOUR CODE HERE

validation_sentences = # YOUR CODE HERE
validation_labels = # YOUR CODE HERE

print(train_size)
print(len(train_sentences))
print(len(train_labels))
print(len(validation_sentences))
print(len(validation_labels))

# Expected output (if training_portion=.8)
# 1780
# 1780
# 1780
# 445
# 445


# In[ ]:


tokenizer = # YOUR CODE HERE
tokenizer.fit_on_texts(# YOUR CODE HERE)
word_index = # YOUR CODE HERE

train_sequences = # YOUR CODE HERE
train_padded = # YOUR CODE HERE

print(len(train_sequences[0]))
print(len(train_padded[0]))

print(len(train_sequences[1]))
print(len(train_padded[1]))

print(len(train_sequences[10]))
print(len(train_padded[10]))

# Expected Ouput
# 449
# 120
# 200
# 120
# 192
# 120


# In[ ]:


validation_sequences = # YOUR CODE HERE
validation_padded = # YOUR CODE HERE

print(len(validation_sequences))
print(validation_padded.shape)

# Expected output
# 445
# (445, 120)


# In[ ]:


label_tokenizer = # YOUR CODE HERE
label_tokenizer.fit_on_texts(# YOUR CODE HERE)

training_label_seq = # YOUR CODE HERE
validation_label_seq = # YOUR CODE HERE

print(training_label_seq[0])
print(training_label_seq[1])
print(training_label_seq[2])
print(training_label_seq.shape)

print(validation_label_seq[0])
print(validation_label_seq[1])
print(validation_label_seq[2])
print(validation_label_seq.shape)

# Expected output
# [4]
# [2]
# [1]
# (1780, 1)
# [5]
# [4]
# [3]
# (445, 1)


# In[ ]:


model = tf.keras.Sequential([
# YOUR CODE HERE
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

# Expected Output
# Layer (type)                 Output Shape              Param #   
# =================================================================
# embedding (Embedding)        (None, 120, 16)           16000     
# _________________________________________________________________
# global_average_pooling1d (Gl (None, 16)                0         
# _________________________________________________________________
# dense (Dense)                (None, 24)                408       
# _________________________________________________________________
# dense_1 (Dense)              (None, 6)                 150       
# =================================================================
# Total params: 16,558
# Trainable params: 16,558
# Non-trainable params: 0


# In[ ]:


num_epochs = 30
history = model.fit(# YOUR CODE HERE)


# In[ ]:


import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "acc")
plot_graphs(history, "loss")


# In[ ]:


reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_sentence(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])


# In[ ]:


e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)

# Expected output
# (1000, 16)


# In[ ]:


import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()


# In[ ]:


try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')
#+end_comment
