#+BEGIN_COMMENT
.. title: BBC News Classification
.. slug: bbc-news-classification
.. date: 2019-08-26 15:28:56 UTC-07:00
.. tags: nlp
.. category: NLP 
.. link: 
.. description: Building a classifier for the BBC news.
.. type: text

#+END_COMMENT
#+TOC: headlines 3
#+begin_src jupyter-python :session bbc :results none :exports none
%load_ext autoreload
%autoreload 2
#+end_src
* Beginning
** Imports
*** Python
#+begin_src jupyter-python :session bbc :results none
from functools import partial
from pathlib import Path
import csv
#+end_src

*** PyPi
#+begin_src jupyter-python :session bbc :results none
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import hvplot.pandas
import pandas
import spacy
#+end_src
*** Graeae
#+begin_src jupyter-python :session bbc :results none
from graeae import EmbedHoloviews, SubPathLoader, Timer
#+end_src
** Setup
*** The Timer
#+begin_src jupyter-python :session bbc :results none
TIMER = Timer()
#+end_src
*** The Environment
#+begin_src jupyter-python :session bbc :results none
ENVIRONMENT = SubPathLoader('DATASETS')
#+end_src
*** Spacy
#+begin_src jupyter-python :session bbc :results none
nlp = spacy.load("en_core_web_lg")
#+end_src
*** Plotting
#+begin_src jupyter-python :session bbc :results none
SLUG = "bbc-news-classification"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{SLUG}")
#+end_src
* Middle
** Load the Datasets
#+begin_src jupyter-python :session bbc :results output :exports both
path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()

texts = []
labels = []
with TIMER:
    with path.open() as csvfile:
        lines = csv.DictReader(csvfile)
        for line in lines:
            labels.append(line["category"])
            texts.append(nlp(line["text"]))
#+end_src

#+RESULTS:
: WARNING: Logging before flag parsing goes to stderr.
: I0905 15:48:03.509695 140023457298240 environment.py:35] Environment Path: /home/brunhilde/.env
: I0905 15:48:03.613945 140023457298240 environment.py:90] Environment Path: /home/brunhilde/.config/datasets/env
: 2019-09-05 15:48:03,666 graeae.timers.timer start: Started: 2019-09-05 15:48:03.666158
: I0905 15:48:03.666611 140023457298240 timer.py:70] Started: 2019-09-05 15:48:03.666158
: 2019-09-05 15:50:53,872 graeae.timers.timer end: Ended: 2019-09-05 15:50:53.871899
: I0905 15:50:53.872024 140023457298240 timer.py:77] Ended: 2019-09-05 15:50:53.871899
: 2019-09-05 15:50:53,874 graeae.timers.timer end: Elapsed: 0:02:50.205741
: I0905 15:50:53.874814 140023457298240 timer.py:78] Elapsed: 0:02:50.205741

#+begin_src jupyter-python :session bbc :results output :exports both
print(texts[1])
#+end_src

#+RESULTS:
: worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (Â£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.

#+begin_src jupyter-python :session bbc :results output :exports both
print(f"Rows: {len(labels):,}")
print(f"Unique Labels: {len(set(labels)):,}")
#+end_src

#+RESULTS:
: Rows: 2,225
: Unique Labels: 5

Since there's only five maybe we should plot it.

#+begin_src jupyter-python :session bbc :results output raw :exports both
labels_frame = pandas.DataFrame({"label": labels})
counts = labels_frame.label.value_counts().reset_index().rename(
    columns={"index": "Category", "label": "Articles"})
plot = counts.hvplot.bar("Category", "Articles").opts(
    title="Count of BBC News Articles by Category",
    height=800, width=1000)
Embed(plot=plot, file_name="bbc_category_counts")()
#+end_src

#+RESULTS:
: #+begin_export html
: <object type="text/html" data="bbc_category_counts.html" style="width:100%" height=800>
:   <p>Figure Missing</p>
: </object>
: #+end_export
11 - 21dc1ad2-9c56-4027-aa31-4e1fad9e63c5
11 - 777ca893-a6e3-48ab-ac57-5cebe1e1c9bb

It looks like the categories are somewhat unevenly distributed. Now to normalize the tokens.

#+begin_src jupyter-python :session bbc :results output :exports both
with TIMER:
    cleaned = [[token.lemma_ for token in text if not any((token.is_stop, token.is_space, token.is_punct))]
               for text in texts]
#+end_src

#+RESULTS:
: 2019-09-05 15:50:56,542 graeae.timers.timer start: Started: 2019-09-05 15:50:56.542750
: I0905 15:50:56.542788 140023457298240 timer.py:70] Started: 2019-09-05 15:50:56.542750
: 2019-09-05 15:50:57,539 graeae.timers.timer end: Ended: 2019-09-05 15:50:57.539612
: I0905 15:50:57.539690 140023457298240 timer.py:77] Ended: 2019-09-05 15:50:57.539612
: 2019-09-05 15:50:57,541 graeae.timers.timer end: Elapsed: 0:00:00.996862
: I0905 15:50:57.541328 140023457298240 timer.py:78] Elapsed: 0:00:00.996862

** The Tokenizer
   Even though I've already tokenized the text, we need a one-hot-encoded version of it. So I'll use the [[https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer][tensorflow keras Tokenizer]].

#+begin_src jupyter-python :session bbc :results none
tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>")
#+end_src

The =num_words= is the total amount of words that will be kept in the word index - I don't know why a thousand, I just found that in the "answer" notebook. The =oov_token= is what's used when a word is encountered outside of the words we're building into our word-index.

#+begin_src jupyter-python :session bbc :results output :exports both
with TIMER:
    tokenizer.fit_on_texts(cleaned)
#+end_src

#+RESULTS:
: 2019-09-05 15:50:58,801 graeae.timers.timer start: Started: 2019-09-05 15:50:58.801139
: I0905 15:50:58.801204 140023457298240 timer.py:70] Started: 2019-09-05 15:50:58.801139
: 2019-09-05 15:50:59,112 graeae.timers.timer end: Ended: 2019-09-05 15:50:59.112249
: I0905 15:50:59.112313 140023457298240 timer.py:77] Ended: 2019-09-05 15:50:59.112249
: 2019-09-05 15:50:59,113 graeae.timers.timer end: Elapsed: 0:00:00.311110
: I0905 15:50:59.113864 140023457298240 timer.py:78] Elapsed: 0:00:00.311110

That was fast. Maybe because I limited it to 1,000.

We should now have a dictionary named =word_index= that holds the words:index pairs for all the tokens found (it only uses the =num_words= when you call tokenizer's methods according to [[https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work][Stack Overflow]])

#+begin_src jupyter-python :session bbc :results output :exports both
print(f"{len(tokenizer.word_index):,}")
#+end_src

#+RESULTS:
: 24,339
** Make training and testing sets

#+begin_src jupyter-python :session bbc :results none
sequences = tokenizer.texts_to_sequences(cleaned)
#+end_src

#+begin_src jupyter-python :session bbc :results output :exports both
TESTING = 0.2
x_train, x_test, y_train, y_test = train_test_split(sequences, labels, test_size=TESTING)
x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=TESTING)

print(f"Training: {len(x_train):,}")
print(f"Validation: {len(x_validation):,}")
print(f"Testing: {len(x_test):,}")
#+end_src

#+RESULTS:
: Training: 1,424
: Validation: 356
: Testing: 445
** The Model
#+begin_src jupyter-python :session bbc :results none
model = tensorflow.keras.Sequential([
])
#+end_src

* End
* Raw
#+begin_comment
# In[ ]:






model = tf.keras.Sequential([
# YOUR CODE HERE
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

# Expected Output
# Layer (type)                 Output Shape              Param #   
# =================================================================
# embedding (Embedding)        (None, 120, 16)           16000     
# _________________________________________________________________
# global_average_pooling1d (Gl (None, 16)                0         
# _________________________________________________________________
# dense (Dense)                (None, 24)                408       
# _________________________________________________________________
# dense_1 (Dense)              (None, 6)                 150       
# =================================================================
# Total params: 16,558
# Trainable params: 16,558
# Non-trainable params: 0


# In[ ]:


num_epochs = 30
history = model.fit(# YOUR CODE HERE)


# In[ ]:


import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "acc")
plot_graphs(history, "loss")


# In[ ]:


reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_sentence(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])


# In[ ]:


e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)

# Expected output
# (1000, 16)


# In[ ]:


import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()


# In[ ]:


try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')
#+end_comment
