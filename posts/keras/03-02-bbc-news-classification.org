#+BEGIN_COMMENT
.. title: BBC News Classification
.. slug: bbc-news-classification
.. date: 2019-08-26 15:28:56 UTC-07:00
.. tags: nlp
.. category: NLP 
.. link: 
.. description: Building a classifier for the BBC news.
.. type: text

#+END_COMMENT
#+TOC: headlines 3
#+begin_src jupyter-python :session bbc :results none :exports none
%load_ext autoreload
%autoreload 2
#+end_src
* Beginning
** Imports
*** Python
#+begin_src jupyter-python :session bbc :results none
from functools import partial
from pathlib import Path
import csv
#+end_src

*** PyPi
#+begin_src jupyter-python :session bbc :results none
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import hvplot.pandas
import pandas
import spacy
import tensorflow
#+end_src
*** Graeae
#+begin_src jupyter-python :session bbc :results none
from graeae import EmbedHoloviews, SubPathLoader, Timer
#+end_src
** Setup
*** The Timer
#+begin_src jupyter-python :session bbc :results none
TIMER = Timer()
#+end_src
*** The Environment
#+begin_src jupyter-python :session bbc :results none
ENVIRONMENT = SubPathLoader('DATASETS')
#+end_src
*** Spacy
#+begin_src jupyter-python :session bbc :results none
spacy.prefer_gpu()
nlp = spacy.load("en_core_web_lg")
#+end_src
*** Plotting
#+begin_src jupyter-python :session bbc :results none
SLUG = "bbc-news-classification"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{SLUG}")
#+end_src
* Middle
** Load the Datasets
#+begin_src jupyter-python :session bbc :results output :exports both
path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()

texts = []
labels = []
with TIMER:
    with path.open() as csvfile:
        lines = csv.DictReader(csvfile)
        for line in lines:
            labels.append(line["category"])
            texts.append(nlp(line["text"]))
#+end_src

#+RESULTS:
: WARNING: Logging before flag parsing goes to stderr.
: I0831 11:53:24.805988 140324198664000 environment.py:35] Environment Path: /home/athena/.env
: I0831 11:53:24.807426 140324198664000 environment.py:90] Environment Path: /home/athena/.config/datasets/env
: 2019-08-31 11:53:24,808 graeae.timers.timer start: Started: 2019-08-31 11:53:24.808391
: I0831 11:53:24.808531 140324198664000 timer.py:70] Started: 2019-08-31 11:53:24.808391
: 2019-08-31 11:54:48,792 graeae.timers.timer end: Ended: 2019-08-31 11:54:48.792346
: I0831 11:54:48.792379 140324198664000 timer.py:77] Ended: 2019-08-31 11:54:48.792346
: 2019-08-31 11:54:48,793 graeae.timers.timer end: Elapsed: 0:01:23.983955
: I0831 11:54:48.793155 140324198664000 timer.py:78] Elapsed: 0:01:23.983955

#+begin_src jupyter-python :session bbc :results output :exports both
print(texts[1])
#+end_src

#+RESULTS:
: worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (Â£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.

#+begin_src jupyter-python :session bbc :results output :exports both
print(f"Rows: {len(labels):,}")
print(f"Unique Labels: {len(set(labels)):,}")
#+end_src

#+RESULTS:
: Rows: 2,225
: Unique Labels: 5

Since there's only five maybe we should plot it.

#+begin_src jupyter-python :session bbc :results output raw :exports both
labels_frame = pandas.DataFrame({"label": labels})
counts = labels_frame.label.value_counts().reset_index().rename(
    columns={"index": "Category", "label": "Articles"})
plot = counts.hvplot.bar("Category", "Articles").opts(
    title="Count of BBC News Articles by Category",
    height=800, width=1000)
Embed(plot=plot, file_name="bbc_category_counts")()
#+end_src

#+RESULTS:
#+begin_export html
<object type="text/html" data="bbc_category_counts.html" style="width:100%" height=800>
  <p>Figure Missing</p>
</object>
#+end_export

It looks like the categories are somewhat unevenly distributed. Now to normalize the tokens.

#+begin_src jupyter-python :session bbc :results output :exports both
with TIMER:
    cleaned = [[token.lemma_ for token in text if not any((token.is_stop, token.is_space, token.is_punct))]
               for text in texts]
#+end_src

#+RESULTS:
: 2019-08-31 11:54:50,873 graeae.timers.timer start: Started: 2019-08-31 11:54:50.873810
: I0831 11:54:50.873836 140324198664000 timer.py:70] Started: 2019-08-31 11:54:50.873810
: 2019-08-31 11:54:51,487 graeae.timers.timer end: Ended: 2019-08-31 11:54:51.487066
: I0831 11:54:51.487107 140324198664000 timer.py:77] Ended: 2019-08-31 11:54:51.487066
: 2019-08-31 11:54:51,487 graeae.timers.timer end: Elapsed: 0:00:00.613256
: I0831 11:54:51.487945 140324198664000 timer.py:78] Elapsed: 0:00:00.613256

** The Tokenizers
   Even though I've already tokenized the texts, we need to eventually one-hot-encode, them so I'll use the [[https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer][tensorflow keras Tokenizer]].

#+begin_src jupyter-python :session bbc :results none
tokenizer = Tokenizer(num_words=1000, oov_token="<OOV>")
labels_tokenizer = Tokenizer(oov_token="<OOV>")
labels_tokenizer.fit_on_texts(labels)
#+end_src

The =num_words= is the total amount of words that will be kept in the word index - I don't know why a thousand, I just found that in the "answer" notebook. The =oov_token= is what's used when a word is encountered outside of the words we're building into our word-index. Now I'll create the word-index by fitting the tokenizer to the text.

#+begin_src jupyter-python :session bbc :results output :exports both
with TIMER:
    tokenizer.fit_on_texts(cleaned)
#+end_src

#+RESULTS:
: 2019-08-31 13:56:48,962 graeae.timers.timer start: Started: 2019-08-31 13:56:48.962142
: I0831 13:56:48.962174 140324198664000 timer.py:70] Started: 2019-08-31 13:56:48.962142
: 2019-08-31 13:56:49,162 graeae.timers.timer end: Ended: 2019-08-31 13:56:49.162026
: I0831 13:56:49.162072 140324198664000 timer.py:77] Ended: 2019-08-31 13:56:49.162026
: 2019-08-31 13:56:49,162 graeae.timers.timer end: Elapsed: 0:00:00.199884
: I0831 13:56:49.162937 140324198664000 timer.py:78] Elapsed: 0:00:00.199884

The tokenizer now has a dictionary named =word_index= that holds the words:index pairs for all the tokens found (it only uses the =num_words= when you call tokenizer's methods according to [[https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work][Stack Overflow]]).

#+begin_src jupyter-python :session bbc :results output :exports both
print(f"{len(tokenizer.word_index):,}")
#+end_src

#+RESULTS:
: 24,339
** Making the Sequences
I've trained the Tokenizer so that it has a word-index, but now we have to one hot encode our texts and pad them so they're all the same length.
** Make training and testing sets

#+begin_src jupyter-python :session bbc :results none
MAX_LENGTH = 120
sequences = tokenizer.texts_to_sequences(cleaned)
padded = pad_sequences(sequences, padding="post", maxlen=MAX_LENGTH)
labels_sequenced = labels_tokenizer.texts_to_sequences(labels)
#+end_src

#+begin_src jupyter-python :session bbc :results output :exports both
TESTING = 0.2
x_train, x_test, y_train, y_test = train_test_split(
    padded, labels_sequenced,
    test_size=TESTING)
x_train, x_validation, y_train, y_validation = train_test_split(
    x_train, y_train, test_size=TESTING)

print(f"Training: {len(x_train):,}")
print(f"Validation: {len(x_validation):,}")
print(f"Testing: {len(x_test):,}")
#+end_src

#+RESULTS:
: Training: 1,424
: Validation: 356
: Testing: 445

*Note:* I originally forgot to pass the =TESTING= variable with the keyword =test_size= and got an error that I couldn't use a Singleton array - don't forget the keywords when you pass in anything other than the data to =train_test_split=.
** The Model
#+begin_src jupyter-python :session bbc :results output :exports both
vocabulary_size = 1000
embedding_dimension = 16
max_length=120

model = tensorflow.keras.Sequential([
    layers.Embedding(vocabulary_size, embedding_dimension,
                     input_length=max_length),
    layers.GlobalAveragePooling1D(),
    layers.Dense(24, activation="relu"),
    layers.Dense(6, activation="softmax"),
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 120, 16)           16000     
_________________________________________________________________
global_average_pooling1d_1 ( (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 24)                408       
_________________________________________________________________
dense_3 (Dense)              (None, 6)                 150       
=================================================================
Total params: 16,558
Trainable params: 16,558
Non-trainable params: 0
_________________________________________________________________
None
#+end_example

#+begin_src jupyter-python :session bbc :results output :exports both
model.fit(x_train, y_train, epochs=30,
          validation_data=(x_validation, y_validation), verbose=2)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example

ValueErrorTraceback (most recent call last)
<ipython-input-81-e69b3cd977e4> in <module>
      1 model.fit(x_train, y_train, epochs=30,
----> 2           validation_data=(x_validation, y_validation), verbose=2)

~/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    641         max_queue_size=max_queue_size,
    642         workers=workers,
--> 643         use_multiprocessing=use_multiprocessing)
    644 
    645   def evaluate(self,

~/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
    630         steps=steps_per_epoch,
    631         validation_split=validation_split,
--> 632         shuffle=shuffle)
    633 
    634     if validation_data:

~/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2448           shapes=None,
   2449           check_batch_axis=False,  # Don't enforce the batch size.
-> 2450           exception_prefix='target')
   2451 
   2452       # Generate sample-wise weight values given the `sample_weight` and

~/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    477                        'Expected to see ' + str(len(names)) + ' array(s), '
    478                        'but instead got the following list of ' +
--> 479                        str(len(data)) + ' arrays: ' + str(data)[:200] + '...')
    480     elif len(names) > 1:
    481       raise ValueError('Error when checking model ' + exception_prefix +

ValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 1424 arrays: [array([[2]]), array([[6]]), array([[2]]), array([[2]]), array([[2]]), array([[3]]), array([[4]]), array([[3]]), array([[6]]), array([[6]]), array([[2]]), array([[6]]), array([[2]]), array([[6]]), arr...
#+end_example
:END:

* End
* Raw
#+begin_comment
model = tf.keras.Sequential([
# YOUR CODE HERE
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

# Expected Output
# Layer (type)                 Output Shape              Param #   
# =================================================================
# embedding (Embedding)        (None, 120, 16)           16000     
# _________________________________________________________________
# global_average_pooling1d (Gl (None, 16)                0         
# _________________________________________________________________
# dense (Dense)                (None, 24)                408       
# _________________________________________________________________
# dense_1 (Dense)              (None, 6)                 150       
# =================================================================
# Total params: 16,558
# Trainable params: 16,558
# Non-trainable params: 0


# In[ ]:


num_epochs = 30
history = model.fit(# YOUR CODE HERE)


# In[ ]:


import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "acc")
plot_graphs(history, "loss")


# In[ ]:


reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_sentence(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])


# In[ ]:


e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)

# Expected output
# (1000, 16)


# In[ ]:


import io

out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = weights[word_num]
  out_m.write(word + "\n")
  out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()


# In[ ]:


try:
  from google.colab import files
except ImportError:
  pass
else:
  files.download('vecs.tsv')
  files.download('meta.tsv')
#+end_comment
