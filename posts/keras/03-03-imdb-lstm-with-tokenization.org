#+BEGIN_COMMENT
.. title: IMDB LSTM With Tokenization
.. slug: imdb-lstm-with-tokenization
.. date: 2019-09-23 14:14:04 UTC-07:00
.. tags: nlp,lstm,tokenization
.. category: NLP
.. link: 
.. description: Building a LSTM model for the IMDB reviews using a Tokenizer.
.. type: text

#+END_COMMENT
* Beginning
  This is another version of the LSTM model to classify the IMDB reviews using two LSTM layers, but this time we're going to tokenize it ourselves, instead of using the tensorflow-datasets version.
** Imports
*** Python
#+begin_src ipython :session lstm :results none
from argparse import Namespace
#+end_src
*** PyPi
#+begin_src ipython :session lstm :results none
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import hvplot.pandas
import numpy
import pandas
import tensorflow
import tensorflow_datasets
#+end_src
*** Other
#+begin_src ipython :session lstm :results none
from graeae import Timer, EmbedHoloviews
#+end_src
** Set Up
*** The Timer
#+begin_src ipython :session lstm :results none
TIMER = Timer()
#+end_src
*** Plotting

* Middle
** Set Up the Data
#+begin_src ipython :session lstm :results output :exports both
imdb, info = tensorflow_datasets.load("imdb_reviews",
                                     with_info=True,
                                     as_supervised=True)
#+end_src

#+RESULTS:
: 0.1.0...
: 
: 
: 
: 
: -packages/tensorflow_datasets/core/file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
: 
: 
: 
: ent calls will reuse this data.
: shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.


#+begin_src ipython :session lstm :results none
training, testing = imdb["train"], imdb["test"]
#+end_src
** Building Up the Tokenizer
   Since we didn't pass in a specifier for the configuration we wanted (e.g. =imdb/subwords8k=) it defaulted to giving us the plain text reviews (and their labels) so we have to build the tokenizer ourselves.
*** Split Up the Sentences and Their Labels
    As you might recall, the data set consists of 50,000 IMDB movie reviews categorized as positive or negative. To build the tokenize we first have to split the sentences from their labels
#+begin_src ipython :session lstm :results none
training_sentences = []
training_labels = []
testing_sentences = []
testing_labels = []
#+end_src

#+begin_src ipython :session lstm :results output :exports both
with TIMER:
    for sentence, label in training_data:
        training_sentences.append(str(sentence.numpy()))
        training_labels.append(str(label.numpy()))
    
    
    for sentence, label in testing_data:
        testing_sentences.append(str(sentence.numpy))
        testing_labels.append(str(label.numpy()))

    training_labels_final = np.array(training_labels)
    testing_labels_final = np.array(testing_labels)
#+end_src
*** Some Constants
#+begin_src ipython :session lstm :results none
Text = Namespace(
    vocab_size = 10000,
    embedding_dim = 16,
    max_length = 120,
    trunc_type='post',
    oov_token = "<OOV>",
)
#+end_src
** Build the Tokenizer
#+begin_src ipython :session lstm :results output :exports both
tokenizer = Tokenizer(num_words=Text.vocab_size, oov_token=Text.oov_token)
with TIMER:
    tokenizer.fit_on_texts(training_sequences)

    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(training_sequences)
    padded = pad_sequences(sequences, maxlen=Text.max_length, truncating=Text.trunc_type)

    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
    testing_padded = pad_sequences(testing_sequences, maxlen=Text.max_length)
#+end_src

** Decoder Ring
#+begin_src ipython :session lstm :results none

,#+begin_src ipython :session lstm :results none
index_to_word = {value: key for key, value in word_index.items()}

def decode_review(text: array) -> str:
    return " ".join([index_to_word.get(item, "<?>") for item in text])
#+end_src

** Build the Model
   This time we're going to build a four-layer model with one Bidirectional layer that uses a [[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU][GRU]] ([[https://www.wikiwand.com/en/Gated_recurrent_unit][Gated Recurrent Unit]]) instead of a LSTM.

#+begin_src ipython :session lstm :results none
model = Sequential([
    tensorflow.keras.layers.Embedding(Text.vocab_size, Text.embedding_dim, input_length=Text.max_length),
    tensorflow.keras.layers.Bidirectional(tensorflow.keras.layers.GRU(32)),
    tensorflow.keras.layers.Dense(6, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
#+end_src

#+begin_src ipython :session lstm :results output :exports both
print(model.summary())
#+end_src
** Train it

#+begin_src ipython :session lstm :results output :exports both
EPOCHS = 50
ONCE_PER_EPOCH = 2
history = model.fit(padded, training_labels_final,
                    epochs=EPOCHS,
                    validation_data=(testing_padded, testing_labels_final),
                    verbosity=ONCE_PER_EPOCH)
#+end_src

** Plot It
#+begin_src ipython :session lstm :results output raw :exports both
data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="GRU Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="gru_training")()
#+end_src
* Raw
#+begin_comment
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)


# In[ ]:


reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])

print(decode_review(padded[1]))
print(training_sentences[1])


# In[ ]:


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()


# In[ ]:


num_epochs = 50
history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))


# In[ ]:


import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history, 'accuracy')
plot_graphs(history, 'loss')


# In[ ]:


# Model Definition with LSTM
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()


# In[ ]:


# Model Definition with Conv1D
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()


#+end_comment
