#+BEGIN_COMMENT
.. title: IMDB GRU With Tokenization
.. slug: imdb-lstm-with-tokenization
.. date: 2019-09-23 14:14:04 UTC-07:00
.. tags: nlp,gru,tokenization
.. category: NLP
.. link: 
.. description: Building a GRU model for the IMDB reviews using a Tokenizer.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3

* Beginning
  This is another version of the RNN model to classify the IMDB reviews, but this time we're going to tokenize it ourselves and use a GRU, instead of using the tensorflow-datasets version.
** Imports
*** Python
#+begin_src ipython :session kernel-755-ssh.json :results none
from argparse import Namespace
#+end_src
*** PyPi
#+begin_src ipython :session kernel-755-ssh.json :results none
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import hvplot.pandas
import numpy
import pandas
import tensorflow
import tensorflow_datasets
#+end_src
*** Other
#+begin_src ipython :session kernel-755-ssh.json :results none
from graeae import Timer, EmbedHoloviews
#+end_src
** Set Up
*** The Timer
#+begin_src ipython :session kernel-755-ssh.json :results none
TIMER = Timer()
#+end_src
*** Plotting

* Middle
** Set Up the Data
#+begin_src ipython :session kernel-755-ssh.json :results output :exports both
imdb, info = tensorflow_datasets.load("imdb_reviews",
                                      with_info=True,
                                      as_supervised=True)
#+end_src

#+RESULTS:
: WARNING: Logging before flag parsing goes to stderr.
: W0924 21:52:10.158111 139862640383808 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.


#+begin_src ipython :session kernel-755-ssh.json :results none
training, testing = imdb["train"], imdb["test"]
#+end_src
** Building Up the Tokenizer
   Since we didn't pass in a specifier for the configuration we wanted (e.g. =imdb/subwords8k=) it defaulted to giving us the plain text reviews (and their labels) so we have to build the tokenizer ourselves.
*** Split Up the Sentences and Their Labels
    As you might recall, the data set consists of 50,000 IMDB movie reviews categorized as positive or negative. To build the tokenize we first have to split the sentences from their labels
#+begin_src ipython :session kernel-755-ssh.json :results none
training_sentences = []
training_labels = []
testing_sentences = []
testing_labels = []
#+end_src

#+begin_src ipython :session kernel-755-ssh.json :results output :exports both
with TIMER:
    for sentence, label in training:
        training_sentences.append(str(sentence.numpy()))
        training_labels.append(str(label.numpy()))
    
    
    for sentence, label in testing:
        testing_sentences.append(str(sentence.numpy))
        testing_labels.append(str(label.numpy()))
#+end_src

#+RESULTS:
: 2019-09-24 21:52:11,396 graeae.timers.timer start: Started: 2019-09-24 21:52:11.395126
: I0924 21:52:11.396310 139862640383808 timer.py:70] Started: 2019-09-24 21:52:11.395126
: 2019-09-24 21:52:18,667 graeae.timers.timer end: Ended: 2019-09-24 21:52:18.667789
: I0924 21:52:18.667830 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:18.667789
: 2019-09-24 21:52:18,670 graeae.timers.timer end: Elapsed: 0:00:07.272663
: I0924 21:52:18.670069 139862640383808 timer.py:78] Elapsed: 0:00:07.272663

#+begin_src ipython :session kernel-755-ssh.json :results none
training_labels_final = numpy.array(training_labels)
testing_labels_final = numpy.array(testing_labels)
#+end_src
*** Some Constants
#+begin_src ipython :session kernel-755-ssh.json :results none
Text = Namespace(
    vocab_size = 10000,
    embedding_dim = 16,
    max_length = 120,
    trunc_type='post',
    oov_token = "<OOV>",
)
#+end_src
** Build the Tokenizer
#+begin_src ipython :session kernel-755-ssh.json :results output :exports both
tokenizer = Tokenizer(num_words=Text.vocab_size, oov_token=Text.oov_token)
with TIMER:
    tokenizer.fit_on_texts(training_sentences)

    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(training_sentences)
    padded = pad_sequences(sequences, maxlen=Text.max_length, truncating=Text.trunc_type)

    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
    testing_padded = pad_sequences(testing_sequences, maxlen=Text.max_length)
#+end_src

#+RESULTS:
: 2019-09-24 21:52:21,705 graeae.timers.timer start: Started: 2019-09-24 21:52:21.705287
: I0924 21:52:21.705317 139862640383808 timer.py:70] Started: 2019-09-24 21:52:21.705287
: 2019-09-24 21:52:32,152 graeae.timers.timer end: Ended: 2019-09-24 21:52:32.152267
: I0924 21:52:32.152314 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:32.152267
: 2019-09-24 21:52:32,154 graeae.timers.timer end: Elapsed: 0:00:10.446980
: I0924 21:52:32.154620 139862640383808 timer.py:78] Elapsed: 0:00:10.446980

** Decoder Ring
#+begin_src ipython :session kernel-755-ssh.json :results none
index_to_word = {value: key for key, value in word_index.items()}

def decode_review(text: numpy.array) -> str:
    return " ".join([index_to_word.get(item, "<?>") for item in text])
#+end_src

** Build the Model
   This time we're going to build a four-layer model with one Bidirectional layer that uses a [[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU][GRU]] ([[https://www.wikiwand.com/en/Gated_recurrent_unit][Gated Recurrent Unit]]) instead of a LSTM.

#+begin_src ipython :session kernel-755-ssh.json :results none
model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(Text.vocab_size, Text.embedding_dim, input_length=Text.max_length),
    tensorflow.keras.layers.Bidirectional(tensorflow.compat.v2.keras.layers.GRU(32)),
    tensorflow.keras.layers.Dense(6, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
#+end_src

#+begin_src ipython :session kernel-755-ssh.json :results output :exports both
print(model.summary())
#+end_src

#+RESULTS:
#+begin_example
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
bidirectional (Bidirectional (None, 64)                9600      
_________________________________________________________________
dense (Dense)                (None, 6)                 390       
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 169,997
Trainable params: 169,997
Non-trainable params: 0
_________________________________________________________________
None
#+end_example

** Train it

#+begin_src ipython :session kernel-755-ssh.json :results output :exports both
EPOCHS = 50
ONCE_PER_EPOCH = 2
batch_size = 8
history = model.fit(padded, training_labels_final,
                    epochs=EPOCHS,
                    batch_size=batch_size,
                    validation_data=(testing_padded, testing_labels_final),
                    verbose=ONCE_PER_EPOCH)
#+end_src

** Plot It
#+begin_src ipython :session kernel-755-ssh.json :results output raw :exports both
data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="GRU Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="gru_training")()
#+end_src
* Raw
#+begin_comment
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)


# In[ ]:


reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_review(text):
    return ' '.join([reverse_word_index.get(i, '?') for i in text])

print(decode_review(padded[1]))
print(training_sentences[1])


# In[ ]:


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()


# In[ ]:


num_epochs = 50
history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))


# In[ ]:


import matplotlib.pyplot as plt


def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history, 'accuracy')
plot_graphs(history, 'loss')


# In[ ]:


# Model Definition with LSTM
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()


# In[ ]:


# Model Definition with Conv1D
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Conv1D(128, 5, activation='relu'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()


#+end_comment
