<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Using a convolutional neural network to identify horses and humans." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Horses And Humans | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="../../../assets/javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
<meta content="Cloistered Monkey" name="author">
<link href="/posts/keras/convolution-exploration/" rel="prev" title="Convolution Exploration" type="text/html">
<link href="/posts/keras/adding-automatic-validation/" rel="next" title="Adding Automatic Validation" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Horses And Humans" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/keras/horses-and-humans/" property="og:url">
<meta content="Using a convolutional neural network to identify horses and humans." property="og:description">
<meta content="article" property="og:type">
<meta content="2019-07-04T16:36:16-07:00" property="article:published_time">
<meta content="cnn" property="article:tag">
<meta content="exercise" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/Neurotic-Networking/"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="/posts/keras/horses-and-humans/index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="/posts/keras/horses-and-humans/">Horses And Humans</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/horses-and-humans/" rel="bookmark"><time class="published dt-published" datetime="2019-07-04T16:36:16-07:00" itemprop="datePublished" title="2019-07-04 16:36">2019-07-04 16:36</time></a></p>
<p class="sourceline"><a class="sourcelink" href="/posts/keras/horses-and-humans/index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/horses-and-humans/#org7db1211">Beginning</a>
<ul>
<li><a href="/posts/keras/horses-and-humans/#org4ad492b">Imports</a></li>
</ul>
</li>
<li><a href="/posts/keras/horses-and-humans/#org8c7fcb7">Middle</a>
<ul>
<li><a href="/posts/keras/horses-and-humans/#orgc598d18">The Data Set</a></li>
<li><a href="/posts/keras/horses-and-humans/#orge3b6b1a">A Model</a></li>
<li><a href="/posts/keras/horses-and-humans/#org0330b82">Compile the Model</a></li>
<li><a href="/posts/keras/horses-and-humans/#org2d52c46">Transform the Data</a></li>
<li><a href="/posts/keras/horses-and-humans/#org55493dc">Training the Model</a></li>
<li><a href="/posts/keras/horses-and-humans/#orgefbf120">Looking At Some Predictions</a></li>
<li><a href="/posts/keras/horses-and-humans/#org11ac72a">Visualizing The Layer Outputs</a></li>
</ul>
</li>
<li><a href="/posts/keras/horses-and-humans/#orgb49dc6c">End</a>
<ul>
<li><a href="/posts/keras/horses-and-humans/#org4924e56">Source</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org7db1211">
<h2 id="org7db1211">Beginning</h2>
<div class="outline-text-2" id="text-org7db1211"></div>
<div class="outline-3" id="outline-container-org4ad492b">
<h3 id="org4ad492b">Imports</h3>
<div class="outline-text-3" id="text-org4ad492b"></div>
<div class="outline-4" id="outline-container-org2af956a">
<h4 id="org2af956a">Python</h4>
<div class="outline-text-4" id="text-org2af956a">
<div class="highlight">
<pre><span></span>from functools import partial
from pathlib import Path
import random
import zipfile
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8389982">
<h4 id="org8389982">PyPi</h4>
<div class="outline-text-4" id="text-org8389982">
<div class="highlight">
<pre><span></span>from expects import (
    be_true,
    expect,
)
from holoviews.operation.datashader import datashade
from keras import backend
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import (ImageDataGenerator,
                                                  img_to_array, load_img)
import cv2
import holoviews
import matplotlib.pyplot as pyplot
import numpy
import requests
import tensorflow
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org0841aad">
<h4 id="org0841aad">My Stuff</h4>
<div class="outline-text-4" id="text-org0841aad">
<div class="highlight">
<pre><span></span>from graeae import EmbedHoloviews
Embed = partial(EmbedHoloviews, 
                folder_path="../../files/posts/keras/horses-and-humans/")
holoviews.extension("bokeh")
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org8c7fcb7">
<h2 id="org8c7fcb7">Middle</h2>
<div class="outline-text-2" id="text-org8c7fcb7"></div>
<div class="outline-3" id="outline-container-orgc598d18">
<h3 id="orgc598d18">The Data Set</h3>
<div class="outline-text-3" id="text-orgc598d18">
<div class="highlight">
<pre><span></span>OUTPUT = "~/data/datasets/images/horse-or-human/training/"
output_path = Path(OUTPUT).expanduser()
if not output_path.is_dir():
    print("Downloading the images")
    URL = ("https://storage.googleapis.com/"
           "laurencemoroney-blog.appspot.com/"
           "horse-or-human.zip")
    response = requests.get(URL)
    ZIP = "/tmp/horse-or-human.zip"
    with open(ZIP, "wb") as writer:
        writer.write(response.content)
    print(f"Downloaded zip to {ZIP}")
    with zipfile.ZipFile(ZIP, "r") as unzipper:
        unzipper.extractall(output_path)
else:
    print("Files exist, not downloading")
expect(output_path.is_dir()).to(be_true)
for thing in output_path.iterdir():
    print(thing)
data_path = output_path
</pre></div>
<pre class="example">
Files exist, not downloading
/home/athena/data/datasets/images/horse-or-human/training/horses
/home/athena/data/datasets/images/horse-or-human/training/humans

</pre>
<p>The convention for training models for computer vision appears to be that you use the folder names to label the contents of the images within them. In this case we have <code>horses</code> and <code>humans</code>.</p>
<p>Here's what some of the files themselves are named.</p>
<div class="highlight">
<pre><span></span>horses_path = output_path/"horses"
humans_path = output_path/"humans"

for path in (horses_path, humans_path):
    print(path.name)
    for index, image in enumerate(path.iterdir()):
        print(f"File: {image.name}")
        if index == 9:
            break
    print()
</pre></div>
<pre class="example">
horses
File: horse48-5.png
File: horse45-8.png
File: horse13-5.png
File: horse34-4.png
File: horse46-5.png
File: horse02-3.png
File: horse06-3.png
File: horse32-1.png
File: horse25-3.png
File: horse04-3.png

humans
File: human01-07.png
File: human02-11.png
File: human13-07.png
File: human10-10.png
File: human15-06.png
File: human05-15.png
File: human06-18.png
File: human16-28.png
File: human02-24.png
File: human10-05.png

</pre>
<p>So, in this case you can tell what they are from the file-names as well. How many images are there?</p>
<div class="highlight">
<pre><span></span>horse_files = list(horses_path.iterdir())
human_files = list(humans_path.iterdir())
print(f"Horse Images: {len(horse_files)}")
print(f"Human Images: {len(human_files)})")
print(f"Image Shape: {pyplot.imread(str(horse_files[0])).shape}")
</pre></div>
<pre class="example">
Horse Images: 500
Human Images: 527)
Image Shape: (300, 300, 4)

</pre>
<p>This is sort of a small data-set, and it's odd that there are more humans than horses. Let's see what some of them look like. I'm assuming all the files have the same shape. In this case it looks like they are 300 x 300 with four channels (RGB and alpha?).</p>
<div class="highlight">
<pre><span></span>height = width = 300
count = 4
columns = 2
horse_plots = [datashade(holoviews.RGB.load_image(str(horse)).opts(
    height=height,
    width=width,
))
               for horse in horse_files[:count]]
human_plots = [datashade(holoviews.RGB.load_image(str(human))).opts(
    height=height,
    width=width,
)
               for human in human_files[:count]]

plot = holoviews.Layout(horse_plots + human_plots).cols(2).opts(
    title="Horses and Humans")
Embed(plot=plot, file_name="horses_and_humans", 
      height_in_pixels=900)()
</pre></div>
<object data="/posts/keras/horses-and-humans/horses_and_humans.html" height="900" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>As you can see, the people in the images aren't really humans (and it may not be so obvious, but they aren't horses either), these are computer-generated images.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orge3b6b1a">
<h3 id="orge3b6b1a">A Model</h3>
<div class="outline-text-3" id="text-orge3b6b1a">
<p>As before, the model will be a sequential model with convolutional layers. In this case we'll have five convolutional layers before passing the convolved images to the fully-connected layer. Although my inspection showed that the images have 4 channels, the model in the example only uses 3.</p>
<p>Also, in this case we are doing a binary classification (it's either a human or a horse, so instead of the softmax activation function on the output layer we're using a <a href="https://en.wikipedia.org/wiki/Sigmoid_function?oldformat=true">Sigmoid function</a> (<a href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid">documentation link</a>).</p>
<div class="highlight">
<pre><span></span>model = tensorflow.keras.models.Sequential()
</pre></div>
</div>
<div class="outline-4" id="outline-container-org420dc85">
<h4 id="org420dc85">The Input Layer</h4>
<div class="outline-text-4" id="text-org420dc85">
<p>The input layer is a Convolutional layer with 16 layers and a 3 x 3 filter (all the convolutions use the same filter shape). All the convolutional layers are also followed by a max-pooling layer that halves their size.</p>
<div class="highlight">
<pre><span></span>model.add(tensorflow.keras.layers.Conv2D(16, (3,3), 
                                         activation='relu', 
                                         input_shape=(300, 300, 3)))
model.add(tensorflow.keras.layers.MaxPooling2D(2, 2))
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org099fcea">
<h4 id="org099fcea">The Rest Of The Convolutional Layers</h4>
<div class="outline-text-4" id="text-org099fcea">
<p>The remaining convolutional layers increase the depth by doubling until they reach 64.</p>
<div class="highlight">
<pre><span></span># The second convolution
model.add(tensorflow.keras.layers.Conv2D(32, (3,3), 
                                         activation='relu'))
model.add(tensorflow.keras.layers.MaxPooling2D(2,2))

# The third convolution
model.add(tensorflow.keras.layers.Conv2D(64, (3,3), 
                                         activation='relu'))
model.add(tensorflow.keras.layers.MaxPooling2D(2,2))

# The fourth convolution
model.add(tensorflow.keras.layers.Conv2D(64, (3,3), 
                                         activation='relu'))
model.add(tensorflow.keras.layers.MaxPooling2D(2,2))

# The fifth convolution
model.add(tensorflow.keras.layers.Conv2D(64, (3,3), 
                                         activation='relu'))
model.add(tensorflow.keras.layers.MaxPooling2D(2,2))
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga73aafc">
<h4 id="orga73aafc">The Fully Connected Layer</h4>
<div class="outline-text-4" id="text-orga73aafc">
<p>Once we have the convolved version of our image, we feed it into the fully-connected layer to get a classification.</p>
<p>First we flatten the input into a vector.</p>
<div class="highlight">
<pre><span></span>model.add(tensorflow.keras.layers.Flatten())
</pre></div>
<p>Then we feed the input into a 512 neuron fully-connected layer.</p>
<div class="highlight">
<pre><span></span>model.add(tensorflow.keras.layers.Dense(512, activation='relu'))
</pre></div>
<p>And now we get to our output layer which makes the prediction of whether the image is a human or a horse.</p>
<div class="highlight">
<pre><span></span>model.add(tensorflow.keras.layers.Dense(1, activation='sigmoid'))
</pre></div>
<p>One thing that's not so obvious is what the output means - is it predicting that it's a human or that it's a horse? There isn't really anything to indicate which is which. Presumably, like the case with the MNIST and Fashion MNIST, the alphabetical ordering of the folders is what determines what we're predicting.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgdf6dc34">
<h4 id="orgdf6dc34">A Summary of the Model.</h4>
<div class="outline-text-4" id="text-orgdf6dc34">
<div class="highlight">
<pre><span></span>print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_10 (Conv2D)           (None, 298, 298, 16)      448       
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 149, 149, 16)      0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 147, 147, 32)      4640      
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 73, 73, 32)        0         
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 71, 71, 64)        18496     
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 35, 35, 64)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 33, 33, 64)        36928     
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 14, 14, 64)        36928     
_________________________________________________________________
max_pooling2d_14 (MaxPooling (None, 7, 7, 64)          0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 3136)              0         
_________________________________________________________________
dense_5 (Dense)              (None, 512)               1606144   
_________________________________________________________________
dense_6 (Dense)              (None, 1)                 513       
=================================================================
Total params: 1,704,097
Trainable params: 1,704,097
Non-trainable params: 0
_________________________________________________________________
None
</pre>
<p>That's a lot of parameters… It's interesting to note that by the time the data gets fed into the <code>Flatten</code> layer it has been reduced to a 7 x 7 x 64 matrix.</p>
<div class="highlight">
<pre><span></span>print(f"300 x 300 x 3 = {300 * 300 * 3:,}")
</pre></div>
<pre class="example">
300 x 300 x 3 = 270,000

</pre>
<p>So the original input has been reduced form 270,000 pixels to 3,136 when it gets to the fully-connected layer.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org0330b82">
<h3 id="org0330b82">Compile the Model</h3>
<div class="outline-text-3" id="text-org0330b82">
<p>The optimizer we're going to use is the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop">RMSprop</a> optimizer, which, unlike SGD, tunes the learning rate as it progresses. Also, since we only have two categories, the loss function will be <a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/binary_crossentropy">binary crossentropy</a>. Our metric will once again be <i>accuracy</i>.</p>
<div class="highlight">
<pre><span></span>model.compile(loss='binary_crossentropy',
              optimizer=RMSprop(lr=0.001),
              metrics=['acc'])
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2d52c46">
<h3 id="org2d52c46">Transform the Data</h3>
<div class="outline-text-3" id="text-org2d52c46">
<p>We're going to use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator">ImageDataGenerator</a> to preprocess the images to get them to normalized and batched. This class also supports transforming the images to create more variety in the training set.</p>
<div class="highlight">
<pre><span></span>training_data_generator = ImageDataGenerator(rescale=1/255)
</pre></div>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory">flow_from_directory</a> method takes a path to the directory of images and generates batches of augmented data.</p>
<div class="highlight">
<pre><span></span>training_batches = training_data_generator.flow_from_directory(
    data_path, 
    target_size=(300, 300),
    batch_size=128,
    class_mode='binary')
</pre></div>
<pre class="example">
Found 1027 images belonging to 2 classes.

</pre></div>
</div>
<div class="outline-3" id="outline-container-org55493dc">
<h3 id="org55493dc">Training the Model</h3>
<div class="outline-text-3" id="text-org55493dc">
<div class="highlight">
<pre><span></span>history = model.fit_generator(
    training_batches,
    steps_per_epoch=8,  
    epochs=15,
    verbose=2)
</pre></div>
<pre class="example">
Epoch 1/15
8/8 - 5s - loss: 0.7879 - acc: 0.5732
Epoch 2/15
8/8 - 4s - loss: 0.7427 - acc: 0.6615
Epoch 3/15
8/8 - 4s - loss: 0.8984 - acc: 0.6897
Epoch 4/15
8/8 - 4s - loss: 0.3973 - acc: 0.8165
Epoch 5/15
8/8 - 4s - loss: 0.2011 - acc: 0.9188
Epoch 6/15
8/8 - 5s - loss: 1.2254 - acc: 0.7373
Epoch 7/15
8/8 - 4s - loss: 0.2228 - acc: 0.8902
Epoch 8/15
8/8 - 4s - loss: 0.1798 - acc: 0.9333
Epoch 9/15
8/8 - 5s - loss: 0.2079 - acc: 0.9287
Epoch 10/15
8/8 - 4s - loss: 0.3128 - acc: 0.8999
Epoch 11/15
8/8 - 4s - loss: 0.0782 - acc: 0.9722
Epoch 12/15
8/8 - 4s - loss: 0.0683 - acc: 0.9711
Epoch 13/15
8/8 - 4s - loss: 0.1263 - acc: 0.9789
Epoch 14/15
8/8 - 5s - loss: 0.6828 - acc: 0.8574
Epoch 15/15
8/8 - 4s - loss: 0.0453 - acc: 0.9855
</pre>
<p>The training loss is very low and we seem to have reached 100% accuracy.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgefbf120">
<h3 id="orgefbf120">Looking At Some Predictions</h3>
<div class="outline-text-3" id="text-orgefbf120">
<div class="highlight">
<pre><span></span>test_path = Path("~/test_images/").expanduser()
</pre></div>
<div class="highlight">
<pre><span></span>height = width = 400
plots = [datashade(holoviews.RGB.load_image(str(path))).opts(
    title=f"{path.name}",
    height=height,
    width=width
) for path in test_path.iterdir()]
plot = holoviews.Layout(plots).cols(2).opts(title="Test Images")
Embed(plot=plot, file_name="test_images", height_in_pixels=900)()
</pre></div>
<object data="/posts/keras/horses-and-humans/test_images.html" height="900" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
<div class="outline-4" id="outline-container-org124d739">
<h4 id="org124d739">Horse</h4>
<div class="outline-text-4" id="text-org124d739">
<div class="highlight">
<pre><span></span>target_size = (300, 300)

images = (("horse.jpg", "Horse"), 
          ("centaur.jpg", "Centaur"), 
          ("tomb_figure.jpg", "Statue of a Man Riding a Horse"),
          ("rembrandt.jpg", "Woman"))
for filename, label in images:
    loaded = cv2.imread(str(test_path/filename))
    x = cv2.resize(loaded, target_size)
    x = numpy.reshape(x, (1, 300, 300, 3))
    prediction = model.predict(x)
    predicted = "human" if prediction[0] &gt; 0.5 else "horse"
    print(f"The {label} is a {predicted}.")
</pre></div>
<pre class="example">
The Horse is a horse.
The Centaur is a horse.
The Statue of a Man Riding a Horse is a human.
The Woman is a horse.

</pre>
<p>Strangely, the model predicted the woman was a horse.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org11ac72a">
<h3 id="org11ac72a">Visualizing The Layer Outputs</h3>
<div class="outline-text-3" id="text-org11ac72a">
<div class="highlight">
<pre><span></span>outputs = [layer.output for layer in model.layers[1:]]
new_model = Model(inputs=model.input, outputs=outputs)
image_path = random.choice(horse_files + human_files)
image = load_img(image_path, target_size=target_size)
x = img_to_array(image)
x = x.reshape((1,) + x.shape)

x /= 255.

predictions = new_model.predict(x)
layer_names = [layer.name for layer in model.layers]
for layer_name, feature_map in zip(layer_names, predictions):
  if len(feature_map.shape) == 4:
    # Just do this for the conv / maxpool layers, not the fully-connected layers
    n_features = feature_map.shape[-1]  # number of features in feature map
    # The feature map has shape (1, size, size, n_features)
    size = feature_map.shape[1]
    # We will tile our images in this matrix
    display_grid = numpy.zeros((size, size * n_features))
    for i in range(n_features):
      # Postprocess the feature to make it visually palatable
      x = feature_map[0, :, :, i]
      x -= x.mean()
      x /= x.std()
      x *= 64
      x += 128
      x = numpy.clip(x, 0, 255).astype('uint8')
      # We'll tile each filter into this big horizontal grid
      display_grid[:, i * size : (i + 1) * size] = x
    # Display the grid
    scale = 20. / n_features
    pyplot.figure(figsize=(scale * n_features, scale))
    pyplot.title(layer_name)
    pyplot.grid(False)
    pyplot.imshow(display_grid, aspect='auto', cmap='viridis')
</pre></div>
<div class="figure">
<p><img alt="layer_visualization.png" src="/posts/keras/horses-and-humans/layer_visualization.png"></p>
</div>
<p>Some of the images seem blank (or nearly so). It's hard to really interpret what's going on here.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb49dc6c">
<h2 id="orgb49dc6c">End</h2>
<div class="outline-text-2" id="text-orgb49dc6c"></div>
<div class="outline-3" id="outline-container-org4924e56">
<h3 id="org4924e56">Source</h3>
<div class="outline-text-3" id="text-org4924e56">
<p>This is a walk-through of the <a href="https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%208%20-%20Lesson%202%20-%20Notebook.ipynb">Course 1 - Part 8 - Lesson 2 - Notebook.ipynb</a> on github.</p>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="/categories/cnn/" rel="tag">cnn</a></li>
<li><a class="tag p-category" href="/categories/exercise/" rel="tag">exercise</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="/posts/keras/convolution-exploration/" rel="prev" title="Convolution Exploration">Previous post</a></li>
<li class="next"><a href="/posts/keras/adding-automatic-validation/" rel="next" title="Adding Automatic Validation">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Contents © 2020 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
