<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>In Too Deep (old posts, page 7) | In Too Deep</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/index-7.html" rel="canonical">
<link href="." rel="prev" type="text/html">
<link href="index-6.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/style-transfer/style-transfer/">Style Transfer</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/style-transfer/style-transfer/" rel="bookmark"><time class="published dt-published" datetime="2018-12-22T14:44:24-08:00" itemprop="datePublished" title="2018-12-22 14:44">2018-12-22 14:44</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/style-transfer/style-transfer/#orgece9d70">Introduction</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#org6879f6a">Set Up</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#orgbbd59fc">The VGG 19 Network</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#orga57fa98">Load in Content and Style Images</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#org5f23b95">VGG19 Layers</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#orga3e70b3">Content and Style Features</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#orgb537f5e">Gram Matrix</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#org1832144">Putting it all Together</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#org00493b2">Loss and Weights</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#orgcc71458">Updating the Target & Calculating Losses</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#org501abf2">Content Loss</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#orgc220390">Style Loss</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#org246d5a0">Total Loss</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#orgbb67beb">Display the Target Image</a></li>
<li><a href="posts/nano/style-transfer/style-transfer/#orgc4ecbea">A Holhwein Transfer</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgece9d70">
<h2 id="orgece9d70">Introduction</h2>
<div class="outline-text-2" id="text-orgece9d70">
<p>In this notebook, weâ€™ll <b>recreate</b> a style transfer method that is outlined in the paper, <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf">Image Style Transfer Using Convolutional Neural Networks, by Gatys</a> in PyTorch.</p>
<p>In this paper, style transfer uses the features found in the 19-layer VGG Network, which is comprised of a series of convolutional and pooling layers, and a few fully-connected layers. In the image below, the convolutional layers are named by stack and their order in the stack. Conv_1_1 is the first convolutional layer that an image is passed through, in the first stack. Conv_2_1 is the first convolutional layer in the <b>second</b> stack. The deepest convolutional layer in the network is conv_5_4.</p>
</div>
<div class="outline-3" id="outline-container-org24e8784">
<h3 id="org24e8784">Separating Style and Content</h3>
<div class="outline-text-3" id="text-org24e8784">
<p>Style transfer relies on separating the content and style of an image. Given one content image and one style image, we aim to create a new, <i>target</i> image which should contain our desired content and style components:</p>
<ul class="org-ul">
<li>objects and their arrangement are similar to that of the <b>content image</b></li>
<li>style, colors, and textures are similar to that of the <b>style image</b></li>
</ul>
<p>In this notebook, we'll use a pre-trained VGG19 Net to extract content or style features from a passed in image. We'll then formalize the idea of content and style <i>losses</i> and use those to iteratively update our target image until we get a result that we want. You are encouraged to use a style and content image of your own and share your work on Twitter with @udacity; we'd love to see what you come up with!</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org6879f6a">
<h2 id="org6879f6a">Set Up</h2>
<div class="outline-text-2" id="text-org6879f6a"></div>
<div class="outline-3" id="outline-container-org8bc1875">
<h3 id="org8bc1875">Imports</h3>
<div class="outline-text-3" id="text-org8bc1875"></div>
<div class="outline-4" id="outline-container-orgd3df06a">
<h4 id="orgd3df06a">Python Standard Library</h4>
<div class="outline-text-4" id="text-orgd3df06a">
<div class="highlight">
<pre><span></span>from datetime import datetime
import pathlib
from typing import Union
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org9c845d9">
<h4 id="org9c845d9">From PyPi</h4>
<div class="outline-text-4" id="text-org9c845d9">
<div class="highlight">
<pre><span></span>start = datetime.now()
from dotenv import load_dotenv
from PIL import Image
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.optim as optim
import torch.nn.functional as F
from torchvision import transforms, models
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Elapsed: 0:00:03.711236

</pre></div>
</div>
<div class="outline-4" id="outline-container-org9530e8c">
<h4 id="org9530e8c">This Project</h4>
<div class="outline-text-4" id="text-org9530e8c">
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgdfe006b">
<h3 id="orgdfe006b">Plotting</h3>
<div class="outline-text-3" id="text-orgdfe006b">
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "font.size": 12,
                "xtick.labelsize": 10,
                "ytick.labelsize": 10,
                "axes.titlesize": 12,
                "figure.figsize": (8, 6),
            },
            font_scale=3)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd46c743">
<h3 id="orgd46c743">Typing</h3>
<div class="outline-text-3" id="text-orgd46c743">
<div class="highlight">
<pre><span></span>PathType = Union[pathlib.Path, str]
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgbbd59fc">
<h2 id="orgbbd59fc">The VGG 19 Network</h2>
<div class="outline-text-2" id="text-orgbbd59fc"></div>
<div class="outline-3" id="outline-container-org68d2d7c">
<h3 id="org68d2d7c">Load in VGG19 (features)</h3>
<div class="outline-text-3" id="text-org68d2d7c">
<p>VGG19 is split into two portions:</p>
<ul class="org-ul">
<li><code>vgg19.features</code>, which are all the convolutional and pooling layers</li>
<li><code>vgg19.classifier</code>, which are the three linear, classifier layers at the end</li>
</ul>
<p>We only need the <code>features</code> portion, which we're going to load in and "freeze" the weights of, below.</p>
<p>Get the "features" portion of VGG19 (we will not need the "classifier" portion).</p>
<div class="highlight">
<pre><span></span>start = datetime.now()
vgg = models.vgg19(pretrained=True).features
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Elapsed: 0:00:03.197737

</pre>
<p>Freeze all VGG parameters since we're only optimizing the target image.</p>
<div class="highlight">
<pre><span></span>for param in vgg.parameters():
    param.requires_grad_(False)
</pre></div>
<p>move the model to GPU, if available</p>
<div class="highlight">
<pre><span></span>start = datetime.now()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vgg.to(device)
print("Using: {}".format(device))
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Using: cuda
Elapsed: 0:00:04.951571

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orga57fa98">
<h2 id="orga57fa98">Load in Content and Style Images</h2>
<div class="outline-text-2" id="text-orga57fa98">
<p>You can load in any images you want! Below, we've provided a helper function for loading in any type and size of image. The <code>load_image</code> function also converts images to normalized Tensors.</p>
<p>Additionally, it will be easier to have smaller images and to squish the content and style images so that they are of the same size.</p>
<div class="highlight">
<pre><span></span>def load_image(img_path: PathType, max_size: int=400, shape=None):
    ''' Load in and transform an image, making sure the image
       is &lt;= max_size pixels in the x-y dims.'''

    image = Image.open(img_path).convert('RGB')

    # large images will slow down processing
    if max(image.size) &gt; max_size:
        size = max_size
    else:
        size = max(image.size)

    if shape is not None:
        size = shape

    in_transform = transforms.Compose([
                        transforms.Resize(size),
                        transforms.ToTensor(),
                        transforms.Normalize((0.485, 0.456, 0.406), 
                                             (0.229, 0.224, 0.225))])

    # discard the transparent, alpha channel (that's the :3) and add the batch dimension
    image = in_transform(image)[:3,:,:].unsqueeze(0)

    return image
</pre></div>
<p>Next, I'm loading in images by file name and forcing the style image to be the same size as the content image.</p>
<p>Load in content and style image.</p>
<div class="highlight">
<pre><span></span>load_dotenv()
max_size = 400 if torch.cuda.is_available() else 128
path = DataPathTwo(folder_key="IMAGES", filename_key="RAVEN")
content = load_image(path.from_folder, max_size=max_size).to(device)
</pre></div>
<p>Resize style to match content, makes code easier</p>
<div class="highlight">
<pre><span></span>style_path = DataPathTwo(filename_key="VERMEER", folder_key="IMAGES")
style = load_image(style_path.from_folder, shape=content.shape[-2:]).to(device)
</pre></div>
<p>A helper function for un-normalizing an image and converting it from a Tensor image to a NumPy image for display.</p>
<div class="highlight">
<pre><span></span>def im_convert(tensor: torch.Tensor) -&gt; numpy.ndarray:
    """ Display a tensor as an image.

    Args:
     tensor: tensor with image

    Returns:
     numpy image from tensor
    """

    image = tensor.to("cpu").clone().detach()
    image = image.numpy().squeeze()
    image = image.transpose(1,2,0)
    image = image * numpy.array((0.229, 0.224, 0.225)) + numpy.array((0.485, 0.456, 0.406))
    image = image.clip(0, 1)
    return image
</pre></div>
<p>Display the images.</p>
<div class="highlight">
<pre><span></span>figure, (ax1, ax2) = pyplot.subplots(1, 2)
figure.suptitle("Content and Style Images Side-By-Side", weight="bold", y=0.75)
ax1.set_title("Raven (content)")
ax2.set_title("Girl With a Pearl Earring (style)")
ax1.imshow(im_convert(content))
image = ax2.imshow(im_convert(style))
</pre></div>
<div class="figure">
<p><img alt="images.png" src="posts/nano/style-transfer/style-transfer/images.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5f23b95">
<h2 id="org5f23b95">VGG19 Layers</h2>
<div class="outline-text-2" id="text-org5f23b95">
<p>To get the content and style representations of an image, we have to pass an image forward through the VGG19 network until we get to the desired layer(s) and then get the output from that layer.</p>
<p>Print out VGG19 structure so you can see the names of various layers.</p>
<div class="highlight">
<pre><span></span>print(vgg)
</pre></div>
<pre class="example">
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace)
  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU(inplace)
  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (6): ReLU(inplace)
  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (8): ReLU(inplace)
  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (11): ReLU(inplace)
  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (13): ReLU(inplace)
  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (15): ReLU(inplace)
  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (17): ReLU(inplace)
  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (20): ReLU(inplace)
  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (22): ReLU(inplace)
  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (24): ReLU(inplace)
  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (26): ReLU(inplace)
  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (29): ReLU(inplace)
  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (31): ReLU(inplace)
  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (33): ReLU(inplace)
  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (35): ReLU(inplace)
  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
</pre></div>
</div>
<div class="outline-2" id="outline-container-orga3e70b3">
<h2 id="orga3e70b3">Content and Style Features</h2>
<div class="outline-text-2" id="text-orga3e70b3">
<div class="highlight">
<pre><span></span>def get_features(image, model, layers=None):
    """ Run an image forward through a model and get the features for 
        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)
    """
    if layers is None:
        layers = {'0': 'conv1_1',
                  '5': 'conv2_1',
                  '10': 'conv3_1', 
                  '19': 'conv4_1',
                  '21': 'conv4_2',  ## content representation
                  '28': 'conv5_1'}


    ## -- do not need to change the code below this line -- ##
    features = {}
    x = image
    # model._modules is a dictionary holding each module in the model
    for name, layer in model._modules.items():
        x = layer(x)
        if name in layers:
            features[layers[name]] = x            
    return features
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb537f5e">
<h2 id="orgb537f5e">Gram Matrix</h2>
<div class="outline-text-2" id="text-orgb537f5e">
<p>The output of every convolutional layer is a Tensor with dimensions associated with the <code>batch_size</code>, a depth, <code>d</code> and some height and width (<code>h</code>, <code>w</code>). The Gram matrix of a convolutional layer can be calculated as follows:</p>
<ul class="org-ul">
<li>Get the depth, height, and width of a tensor using <code>batch_size, d, h, w = tensor.size</code></li>
<li>Reshape that tensor so that the spatial dimensions are flattened</li>
<li>Calculate the gram matrix by multiplying the reshaped tensor by it's transpose</li>
</ul>
<p><b>Note: You can multiply two matrices using <code>torch.mm(matrix1, matrix2)</code>.</b></p>
<div class="highlight">
<pre><span></span>def gram_matrix(tensor: torch.Tensor) -&gt; torch.Tensor:
    """ Calculate the Gram Matrix of a given tensor 
        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix
    """
    batch_size, depth, height, width = tensor.size()
    tensor = tensor.view(batch_size * depth, height * width)
    gram = torch.mm(tensor, tensor.t())
    return gram 
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org1832144">
<h2 id="org1832144">Putting it all Together</h2>
<div class="outline-text-2" id="text-org1832144">
<p>Now that we've written functions for extracting features and computing the gram matrix of a given convolutional layer; let's put all these pieces together! We'll extract our features from our images and calculate the gram matrices for each layer in our style representation.</p>
<p>Get content and style features only once before forming the target image.</p>
<div class="highlight">
<pre><span></span>content_features = get_features(content, vgg)
style_features = get_features(style, vgg)
</pre></div>
<p>calculate the gram matrices for each layer of our style representation</p>
<div class="highlight">
<pre><span></span>style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}
</pre></div>
<p>Create a third "target" image and prep it for change. It is a good idea to start off with the target as a copy of our <b>content</b> image then iteratively change its style.</p>
<div class="highlight">
<pre><span></span>target = content.clone().requires_grad_(True).to(device)
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org00493b2">
<h2 id="org00493b2">Loss and Weights</h2>
<div class="outline-text-2" id="text-org00493b2"></div>
<div class="outline-3" id="outline-container-orgefc35f2">
<h3 id="orgefc35f2">Individual Layer Style Weights</h3>
<div class="outline-text-3" id="text-orgefc35f2">
<p>Below, you are given the option to weight the style representation at each relevant layer. It's suggested that you use a range between 0-1 to weight these layers. By weighting earlier layers (<code>conv1_1</code> and <code>conv2_1</code>) more, you can expect to get <i>larger</i> style artifacts in your resulting, target image. Should you choose to weight later layers, you'll get more emphasis on smaller features. This is because each layer is a different size and together they create a multi-scale style representation!</p>
</div>
</div>
<div class="outline-3" id="outline-container-org08f2691">
<h3 id="org08f2691">Content and Style Weight</h3>
<div class="outline-text-3" id="text-org08f2691">
<p>Just like in the paper, we define an alpha (<code>content_weight</code>) and a beta (<code>style_weight</code>). This ratio will affect how <i>stylized</i> your final image is. It's recommended that you leave the content_weight = 1 and set the style_weight to achieve the ratio you want.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org4a25aa1">
<h3 id="org4a25aa1">Weights For Each Style Layer</h3>
<div class="outline-text-3" id="text-org4a25aa1">
<p>Weighting earlier layers more will result in <b>larger</b> style artifacts. Notice we are excluding <code>conv4_2</code> our content representation.</p>
<div class="highlight">
<pre><span></span>style_weights = {'conv1_1': 1.,
                 'conv2_1': 0.8,
                 'conv3_1': 0.6,
                 'conv4_1': 0.4,
                 'conv5_1': 0.2}
</pre></div>
<div class="highlight">
<pre><span></span>content_weight = 1  # alpha
style_weight = 1e6  # beta
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgcc71458">
<h2 id="orgcc71458">Updating the Target & Calculating Losses</h2>
<div class="outline-text-2" id="text-orgcc71458">
<p>You'll decide on a number of steps for which to update your image, this is similar to the training loop that you've seen before, only we are changing our <span class="underline">target</span> image and nothing else about VGG19 or any other image. Therefore, the number of steps is really up to you to set! <b>I recommend using at least 2000 steps for good results.</b> But, you may want to start out with fewer steps if you are just testing out different weight values or experimenting with different images.</p>
<p>Inside the iteration loop, you'll calculate the content and style losses and update your target image, accordingly.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org501abf2">
<h2 id="org501abf2">Content Loss</h2>
<div class="outline-text-2" id="text-org501abf2">
<p>The content loss will be the mean squared difference between the target and content features at layer <code>conv4_2</code>. This can be calculated as follows:</p>
<div class="highlight">
<pre><span></span><span class="n">content_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">target_features</span><span class="p">[</span><span class="s1">'conv4_2'</span><span class="p">]</span> <span class="o">-</span> <span class="n">content_features</span><span class="p">[</span><span class="s1">'conv4_2'</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc220390">
<h2 id="orgc220390">Style Loss</h2>
<div class="outline-text-2" id="text-orgc220390">
<p>The style loss is calculated in a similar way, only you have to iterate through a number of layers, specified by name in our dictionary <code>style_weights</code>.</p>
<ul class="org-ul">
<li>You'll calculate the gram matrix for the target image, <code>target_gram</code> and style image <code>style_gram</code> at each of these layers and compare those gram matrices, calculating the <code>layer_style_loss</code>.</li>
<li>Later, you'll see that this value is normalized by the size of the layer.</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org246d5a0">
<h2 id="org246d5a0">Total Loss</h2>
<div class="outline-text-2" id="text-org246d5a0">
<p>Finally, you'll create the total loss by adding up the style and content losses and weighting them with your specified alpha and beta!</p>
<p>Intermittently, we'll print out this loss; don't be alarmed if the loss is very large. It takes some time for an image's style to change and you should focus on the appearance of your target image rather than any loss value. Still, you should see that this loss decreases over some number of iterations.</p>
<div class="highlight">
<pre><span></span>show_every = 400

# iteration hyperparameters
optimizer = optim.Adam([target], lr=0.003)
steps = 2000  # decide how many iterations to update your image (5000)
CONTENT_LAYER = "conv4_2"
start = datetime.now()
for repetition in range(1, steps+1):
    target_features = get_features(target, vgg)
    content_loss = F.mse_loss(target_features[CONTENT_LAYER],
                              content_features[CONTENT_LAYER])

    # the style loss
    # initialize the style loss to 0
    style_loss = 0
    # iterate through each style layer and add to the style loss
    for layer in style_weights:
        # get the "target" style representation for the layer
        target_feature = target_features[layer]
        _, d, h, w = target_feature.shape

        target_gram = gram_matrix(target_feature)

        style_gram = style_grams[layer]

        layer_style_loss = style_weights[layer] * F.mse_loss(target_gram,
                                                             style_gram)
        # add to the style loss
        style_loss += layer_style_loss / (d * h * w)

    total_loss = content_weight * content_loss + style_weight * style_loss

    ## -- do not need to change code, below -- ##
    # update your target image
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    # display intermediate images and print the loss
    if  repetition % show_every == 0:
        print('({}) Total loss: {}'.format(repetition, total_loss.item()))
        #plt.imshow(im_convert(target))
        #plt.show()
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
(400) Total loss: 26489776.0
(800) Total loss: 12765434.0
(1200) Total loss: 8439541.0
(1600) Total loss: 6268045.0
(2000) Total loss: 4820489.5
Elapsed: 0:08:03.885520

</pre></div>
</div>
<div class="outline-2" id="outline-container-orgbb67beb">
<h2 id="orgbb67beb">Display the Target Image</h2>
<div class="outline-text-2" id="text-orgbb67beb">
<div class="highlight">
<pre><span></span>figure, (ax1, ax2) = pyplot.subplots(1, 2)
figure.suptitle("Vermeer Raven", weight="bold", y=0.75)
ax1.imshow(im_convert(content))
image = ax2.imshow(im_convert(target))
</pre></div>
<div class="figure">
<p><img alt="raven_vermeer.png" src="posts/nano/style-transfer/style-transfer/raven_vermeer.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc4ecbea">
<h2 id="orgc4ecbea">A Holhwein Transfer</h2>
<div class="outline-text-2" id="text-orgc4ecbea">
<div class="highlight">
<pre><span></span>max_size = 400 if torch.cuda.is_available() else 128
path = DataPathTwo(folder_key="IMAGES", filename_key="RAVEN")
content = load_image(path.from_folder, max_size=max_size).to(device)

style_path = DataPathTwo(filename_key="HOHLWEIN", folder_key="IMAGES")
style = load_image(style_path.from_folder, shape=content.shape[-2:]).to(device)

content_features = get_features(content, vgg)
target = content.clone().requires_grad_(True).to(device)
content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)
style_features = get_features(style, vgg)
style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}
</pre></div>
<div class="highlight">
<pre><span></span>show_every = 400
vgg = models.vgg19(pretrained=True).features
for param in vgg.parameters():
    param.requires_grad_(False)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vgg.to(device)
# iteration hyperparameters
optimizer = optim.Adam([target], lr=0.003)
steps = 2000  # decide how many iterations to update your image (5000)
CONTENT_LAYER = "conv4_2"
start = datetime.now()
for repetition in range(1, steps+1):
    target_features = get_features(target, vgg)
    content_loss = F.mse_loss(target_features[CONTENT_LAYER],
                              content_features[CONTENT_LAYER])

    # the style loss
    # initialize the style loss to 0
    style_loss = 0
    # iterate through each style layer and add to the style loss
    for layer in style_weights:
        # get the "target" style representation for the layer
        target_feature = target_features[layer]
        _, d, h, w = target_feature.shape

        target_gram = gram_matrix(target_feature)

        style_gram = style_grams[layer]

        layer_style_loss = style_weights[layer] * F.mse_loss(target_gram,
                                                             style_gram)
        # add to the style loss
        style_loss += layer_style_loss / (d * h * w)

    total_loss = content_weight * content_loss + style_weight * style_loss

    ## -- do not need to change code, below -- ##
    # update your target image
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    # display intermediate images and print the loss
    if  repetition % show_every == 0:
        print('({}) Total loss: {}'.format(repetition, total_loss.item()))
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
(400) Total loss: 38191616.0
(800) Total loss: 19276114.0
(1200) Total loss: 12646590.0
(1600) Total loss: 9095670.0
(2000) Total loss: 6934397.0
Elapsed: 0:08:09.517655

</pre>
<div class="highlight">
<pre><span></span>figure, (ax1, ax2) = pyplot.subplots(1, 2)
figure.suptitle("Hohlwein Raven", weight="bold", y=.8)
ax1.imshow(im_convert(content))
image = ax2.imshow(im_convert(target))
</pre></div>
<div class="figure">
<p><img alt="hohlwein_raven.png" src="posts/nano/style-transfer/style-transfer/hohlwein_raven.png"></p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/autoencoders/denoising-autoencoder/">Denoising Autoencoder</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/autoencoders/denoising-autoencoder/" rel="bookmark"><time class="published dt-published" datetime="2018-12-21T18:07:29-08:00" itemprop="datePublished" title="2018-12-21 18:07">2018-12-21 18:07</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#orga02c71a">Set Up</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#orga3b3ac6">Visualize the Data</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#orgc6ca2af">Denoising</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#org38667c4">Define the NN Architecture</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#org5586d28">Initialize The NN</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#org8af3518">Training</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#org14c882d">Checking out the results</a></li>
</ul>
</div>
</div>
<p>Sticking with the MNIST dataset, let's add noise to our data and see if we can define and train an autoencoder to <i>de</i>-noise the images.</p>
<div class="outline-2" id="outline-container-orga02c71a">
<h2 id="orga02c71a">Set Up</h2>
<div class="outline-text-2" id="text-orga02c71a"></div>
<div class="outline-3" id="outline-container-org78543d0">
<h3 id="org78543d0">Imports</h3>
<div class="outline-text-3" id="text-org78543d0"></div>
<div class="outline-4" id="outline-container-org825edb0">
<h4 id="org825edb0">Python</h4>
<div class="outline-text-4" id="text-org825edb0">
<div class="highlight">
<pre><span></span>from collections import namedtuple
from datetime import datetime
from pathlib import Path
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org2be66a9">
<h4 id="org2be66a9">PyPi</h4>
<div class="outline-text-4" id="text-org2be66a9">
<div class="highlight">
<pre><span></span>from torchvision import datasets
from graphviz import Graph
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org33fe326">
<h3 id="org33fe326">The Plotting</h3>
<div class="outline-text-3" id="text-org33fe326">
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=3)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8210e73">
<h3 id="org8210e73">The Data</h3>
<div class="outline-text-3" id="text-org8210e73"></div>
<div class="outline-4" id="outline-container-org9c76b8c">
<h4 id="org9c76b8c">The Transform</h4>
<div class="outline-text-4" id="text-org9c76b8c">
<div class="highlight">
<pre><span></span>transform = transforms.ToTensor()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org348c94e">
<h4 id="org348c94e">Load the Training and Test Datasets</h4>
<div class="outline-text-4" id="text-org348c94e">
<div class="highlight">
<pre><span></span>path = Path("~/datasets/MNIST/").expanduser()
print(path.is_dir())
</pre></div>
<pre class="example">
True

</pre>
<div class="highlight">
<pre><span></span>train_data = datasets.MNIST(root=path, train=True,
                            download=True, transform=transform)
test_data = datasets.MNIST(root=path, train=False,
                           download=True, transform=transform)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org41740f9">
<h4 id="org41740f9">Create training and test dataloaders</h4>
<div class="outline-text-4" id="text-org41740f9">
<div class="highlight">
<pre><span></span>NUM_WORKERS = 0
BATCH_SIZE = 20
</pre></div>
<div class="highlight">
<pre><span></span>train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
                                           num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,
                                          num_workers=NUM_WORKERS)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgad2bffd">
<h3 id="orgad2bffd">Test for <a href="http://pytorch.org/docs/stable/cuda.html">CUDA</a></h3>
<div class="outline-text-3" id="text-orgad2bffd">
<div class="highlight">
<pre><span></span>device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using: {}".format(device))
</pre></div>
<pre class="example">
Using: cuda:0

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orga3b3ac6">
<h2 id="orga3b3ac6">Visualize the Data</h2>
<div class="outline-text-2" id="text-orga3b3ac6"></div>
<div class="outline-3" id="outline-container-org37e974d">
<h3 id="org37e974d">Obtain One Batch of Training Images</h3>
<div class="outline-text-3" id="text-org37e974d">
<div class="highlight">
<pre><span></span>dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd556e8e">
<h3 id="orgd556e8e">Get One Image From the Batch</h3>
<div class="outline-text-3" id="text-orgd556e8e">
<div class="highlight">
<pre><span></span>img = numpy.squeeze(images[0])
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org31c63e8">
<h3 id="org31c63e8">Plot</h3>
<div class="outline-text-3" id="text-org31c63e8">
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
figure.suptitle("Sample Image", weight="bold")
image = axe.imshow(img, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="first_image.png" src="posts/nano/autoencoders/denoising-autoencoder/first_image.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc6ca2af">
<h2 id="orgc6ca2af">Denoising</h2>
<div class="outline-text-2" id="text-orgc6ca2af">
<p>As I've mentioned before, autoencoders like the ones you've built so far aren't too useful in practive. However, they can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1.</p>
<p><b>We'll use noisy images as input and the original, clean images as targets.</b></p>
<p>Since this is a harder problem for the network, we'll want to use <i>deeper</i> convolutional layers here; layers with more feature maps. You might also consider adding additional layers. I suggest starting with a depth of 32 for the convolutional layers in the encoder, and the same depths going backward through the decoder.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org38667c4">
<h2 id="org38667c4">Define the NN Architecture</h2>
<div class="outline-text-2" id="text-org38667c4">
<div class="highlight">
<pre><span></span>graph = Graph(format="png")

# Input layer
graph.node("a", "28x28x1 Input")

# the Encoder
graph.node("b", "28x28x32 Convolution")
graph.node("c", "14x14x32 MaxPool")
graph.node("d", "14x14x16 Convolution")
graph.node("e", "7x7x16 MaxPool")
graph.node("f", "7x7x8 Convolution")
graph.node("g", "3x3x8 MaxPool")

# The Decoder
graph.node("h", "7x7x8 Transpose Convolution")
graph.node("i", "14x14x16 Transpose Convolution")
graph.node("j", "28x28x32 Transpose Convolution")
graph.node("k", "28x28x1 Convolution")

# The Output
graph.node("l", "28x28x1 Output")

edges = "abcdefghijkl"
graph.edges([edges[edge] + edges[edge+1] for edge in range(len(edges) - 1)])

graph.render("graphs/network.dot")
graph
</pre></div>
<div class="figure">
<p><img alt="network.dot.png" src="posts/nano/autoencoders/denoising-autoencoder/network.dot.png"></p>
</div>
<div class="highlight">
<pre><span></span>Layer = namedtuple("Layer", "kernel stride in_depth out_depth padding".split())
Layer.__new__.__defaults__= (0,)
def output_size(input_size: int, layer: Layer, expected: int) -&gt; int:
    """Calculates the output size of the layer

    Args:
     input_size: the size of the input to the layer
     layer: named tuple with values for the layer
     expected: the value you are expecting

    Returns:
     the size of the output

    Raises:
     AssertionError: the calculated value wasn't the expected one
    """
    size = 1 + int(
        (input_size - layer.kernel + 2 * layer.padding)/layer.stride)
    print(layer)
    print("Layer Output: {0} x {0} x {1}".format(size, layer.out_depth))
    assert size == expected, size
    return size
</pre></div>
</div>
<div class="outline-3" id="outline-container-org285f6fb">
<h3 id="org285f6fb">The Encoder Layers</h3>
<div class="outline-text-3" id="text-org285f6fb"></div>
<div class="outline-4" id="outline-container-org37da205">
<h4 id="org37da205">Layer One</h4>
<div class="outline-text-4" id="text-org37da205">
<div class="highlight">
<pre><span></span> INPUT_DEPTH = 1
 convolution_one = Layer(kernel = 3,
                         padding = 1,
                         stride = 1,
                         in_depth=INPUT_DEPTH,
                         out_depth = 32)
 INPUT_ONE = 28
 OUTPUT_ONE = output_size(INPUT_ONE, convolution_one, INPUT_ONE)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=1, out_depth=32, padding=1)
Layer Output: 28 x 28 x 32

</pre></div>
</div>
<div class="outline-4" id="outline-container-orge468868">
<h4 id="orge468868">Layer Two</h4>
<div class="outline-text-4" id="text-orge468868">
<p>The second layer is a MaxPool layer that will keep the depth of thirty-two but will halve the size to fourteen. According to the <a href="https://cs231n.github.io/convolutional-networks/">CS 231 n</a> page on Convolutional Networks, there are only two values for the kernel size that are usually used - 2 and 3, and the stride is usually just 2, with a kernel size of 2 being more common, and as it turns out, a kernel size of 2 and a stride of 2 will reduce our input dimensions by a half, which is what we want.</p>
\begin{align} W &amp;= \frac{28 - 2}{2} + 1\\ &amp;= 14\\ \end{align}
<div class="highlight">
<pre><span></span> max_pool_one = Layer(kernel=2, stride=2,
                      in_depth=convolution_one.out_depth,
                      out_depth=convolution_one.out_depth)
 OUTPUT_TWO = output_size(OUTPUT_ONE, max_pool_one, 14)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=32, out_depth=32, padding=0)
Layer Output: 14 x 14 x 32

</pre></div>
</div>
<div class="outline-4" id="outline-container-orgf29e1be">
<h4 id="orgf29e1be">Layer Three</h4>
<div class="outline-text-4" id="text-orgf29e1be">
<p>Our third layer is another convolutional layer that preserves the input width and height but this time the output will have a depth of 16.</p>
<div class="highlight">
<pre><span></span>convolution_two = Layer(kernel=3, stride=1, in_depth=max_pool_one.out_depth,
                        out_depth=16, padding=1)
OUTPUT_THREE = output_size(OUTPUT_TWO, convolution_two, OUTPUT_TWO)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=32, out_depth=16, padding=1)
Layer Output: 14 x 14 x 16

</pre></div>
</div>
<div class="outline-4" id="outline-container-org5bea6d9">
<h4 id="org5bea6d9">Layer Four</h4>
<div class="outline-text-4" id="text-org5bea6d9">
<p>The fourth layer is another max-pool layer that will halve the dimensions.</p>
<div class="highlight">
<pre><span></span>max_pool_two = Layer(kernel=2, stride=2, in_depth=convolution_two.out_depth,
                        out_depth=convolution_two.out_depth)
OUTPUT_FOUR = output_size(OUTPUT_THREE, max_pool_two, 7)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=16, out_depth=16, padding=0)
Layer Output: 7 x 7 x 16

</pre></div>
</div>
<div class="outline-4" id="outline-container-org057ece3">
<h4 id="org057ece3">Layer Five</h4>
<div class="outline-text-4" id="text-org057ece3">
<p>The fifth layer is another convolutional layer that will reduce the depth to eight.</p>
<div class="highlight">
<pre><span></span>convolution_three = Layer(kernel=3, stride=1,
                          in_depth=max_pool_two.out_depth, out_depth=8,
                          padding=1)
OUTPUT_FIVE = output_size(OUTPUT_FOUR, convolution_three, 7)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=16, out_depth=8, padding=1)
Layer Output: 7 x 7 x 8

</pre></div>
</div>
<div class="outline-4" id="outline-container-orgf723c94">
<h4 id="orgf723c94">Layer Six</h4>
<div class="outline-text-4" id="text-orgf723c94">
<p>The last layer in the encoder is a max pool layer that reduces the previous layer by half (to dimensions of 3) while preserving the depth.</p>
<div class="highlight">
<pre><span></span>max_pool_three = Layer(kernel=2, stride=2,
                       in_depth=convolution_three.out_depth,
                       out_depth=convolution_three.out_depth)
OUTPUT_SIX = output_size(OUTPUT_FIVE, max_pool_three, 3)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=8, out_depth=8, padding=0)
Layer Output: 3 x 3 x 8

</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga6ff16e">
<h3 id="orga6ff16e">Decoders</h3>
<div class="outline-text-3" id="text-orga6ff16e"></div>
<div class="outline-4" id="outline-container-orgfce7a76">
<h4 id="orgfce7a76">Layer Six</h4>
<div class="outline-text-4" id="text-orgfce7a76">
<p>This is a transpose convolution layer to (more than) double the size of the image. The image put out by the encoder is 3x3, but we want a 7x7 output, not a 6x6, so the kernel has to be upped to 3.</p>
<div class="highlight">
<pre><span></span>transpose_one = Layer(kernel=3, stride=2, out_depth=8,
                      in_depth=max_pool_three.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge22e8f5">
<h4 id="orge22e8f5">Layer Seven</h4>
<div class="outline-text-4" id="text-orge22e8f5">
<p>This will double the size again (to 14x14) and increase the depth to 16.</p>
<div class="highlight">
<pre><span></span>transpose_two = Layer(kernel=2, stride=2, out_depth=16,
                      in_depth=transpose_one.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgcda854f">
<h4 id="orgcda854f">Layer Eight</h4>
<div class="outline-text-4" id="text-orgcda854f">
<p>This will double the size to 28x28 and up the depth back again to 32, the size of our original encoding convolution.</p>
<div class="highlight">
<pre><span></span>transpose_three = Layer(kernel=2, stride=2, out_depth=32,
                        in_depth=transpose_two.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org725cc47">
<h4 id="org725cc47">Layer Nine</h4>
<div class="outline-text-4" id="text-org725cc47">
<p>This is a convolution layer to bring the depth back to one.</p>
<div class="highlight">
<pre><span></span>convolution_out = Layer(kernel=3, stride=1, in_depth=transpose_three.out_depth,
                        out_depth=1, padding=1)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org595e8e8">
<h3 id="org595e8e8">The Implementation</h3>
<div class="outline-text-3" id="text-org595e8e8">
<div class="highlight">
<pre><span></span>class ConvDenoiser(nn.Module):
    def __init__(self):
        super().__init__()
        ## encoder layers ##
        self.convolution_1 =  nn.Conv2d(in_channels=convolution_one.in_depth,
                                       out_channels=convolution_one.out_depth,
                                       kernel_size=convolution_one.kernel,
                                       padding=convolution_one.padding)

        self.convolution_2 = nn.Conv2d(in_channels=convolution_two.in_depth,
                                       out_channels=convolution_two.out_depth,
                                       kernel_size=convolution_two.kernel,
                                       padding=convolution_two.padding)

        self.convolution_3 = nn.Conv2d(in_channels=convolution_three.in_depth,
                                       out_channels=convolution_three.out_depth,
                                       kernel_size=convolution_three.kernel,
                                       padding=convolution_three.padding)

        self.max_pool = nn.MaxPool2d(kernel_size=max_pool_one.kernel,
                                     stride=max_pool_one.stride)

        ## decoder layers ##
        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2
        self.transpose_convolution_1 = nn.ConvTranspose2d(
            in_channels=transpose_one.in_depth,
            out_channels=transpose_one.out_depth,
            kernel_size=transpose_one.kernel,
            stride=transpose_one.stride)

        self.transpose_convolution_2 = nn.ConvTranspose2d(
            in_channels=transpose_two.in_depth, 
            out_channels=transpose_two.out_depth,
            kernel_size=transpose_two.kernel,
            stride=transpose_two.stride)

        self.transpose_convolution_3 = nn.ConvTranspose2d(
            in_channels=transpose_three.in_depth,
            out_channels=transpose_three.out_depth,
            kernel_size=transpose_three.kernel,
            stride=transpose_three.stride)

        self.convolution_out = nn.Conv2d(in_channels=convolution_out.in_depth,
                                         out_channels=convolution_out.out_depth,
                                         kernel_size=convolution_out.kernel,
                                         padding=convolution_out.padding)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        return


    def forward(self, x):
        ## encode ##
        x = self.max_pool(self.relu(self.convolution_1(x)))
        x = self.max_pool(self.relu(self.convolution_2(x)))
        x = self.max_pool(self.relu(self.convolution_3(x)))

        ## decode ##
        x = self.relu(self.transpose_convolution_1(x))
        x = self.relu(self.transpose_convolution_2(x))
        x = self.relu(self.transpose_convolution_3(x))
        return self.sigmoid(self.convolution_out(x))
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5586d28">
<h2 id="org5586d28">Initialize The NN</h2>
<div class="outline-text-2" id="text-org5586d28">
<div class="highlight">
<pre><span></span>model = ConvDenoiser()
print(model)
</pre></div>
<pre class="example">
ConvDenoiser(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (t_conv1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2))
  (t_conv2): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))
  (t_conv3): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))
  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
</pre>
<div class="highlight">
<pre><span></span>test = ConvDenoiser()
dataiter = iter(train_loader)
images, labels = dataiter.next()
x = test.convolution_1(images)
assert x.shape == torch.Size([BATCH_SIZE, 32, 28, 28])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 32, 14, 14])
print(x.shape)

x = test.convolution_2(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 14, 14])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 7, 7])
print(x.shape)

x = test.convolution_3(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 7, 7])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 3, 3]), x.shape

x = test.transpose_convolution_1(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 7, 7]), x.shape
print(x.shape)

x = test.transpose_convolution_2(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 14, 14])
print(x.shape)

x = test.transpose_convolution_3(x)
assert x.shape == torch.Size([BATCH_SIZE, 32, 28, 28])
print(x.shape)

x = test.convolution_out(x)
assert x.shape == torch.Size([BATCH_SIZE, 1, 28, 28])
print(x.shape)
</pre></div>
<pre class="example">
torch.Size([20, 32, 28, 28])
torch.Size([20, 32, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 7, 7])
torch.Size([20, 8, 7, 7])
torch.Size([20, 8, 7, 7])
torch.Size([20, 16, 14, 14])
torch.Size([20, 32, 28, 28])
torch.Size([20, 1, 28, 28])

</pre></div>
</div>
<div class="outline-2" id="outline-container-org8af3518">
<h2 id="org8af3518">Training</h2>
<div class="outline-text-2" id="text-org8af3518">
<p>We are only concerned with the training images, which we can get from the <code>train_loader</code>.</p>
<p>In this case, we are actually <b>adding some noise</b> to these images and we'll feed these <code>noisy_imgs</code> to our model. The model will produce reconstructed images based on the noisy input. But, we want it to produce <span class="underline">normal</span> un-noisy images, and so, when we calculate the loss, we will still compare the reconstructed outputs to the original images!</p>
<p>Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use <code>MSELoss</code>. And compare output images and input images as follows:</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
</pre></div>
<p><b>Warning:</b> I spent an unreasonable amount of time trying to de-bug this thing because I was passing in the model's parameters to the optimizer before passing it to the GPU. I don't know why it didn't throw an error, but it didn't, it just never learned and gave me really high losses. I think it's because the style of these notebooks is to create the parts all over the place so there might have been another 'model' variable in the namespace. In any case, move away from this style and start putting everything into functions and classes - especially the stuff that comes from udacity.</p>
<div class="highlight">
<pre><span></span>class Trainer:
    """Trains our model

    Args:
     data: data-iterator for training
     epochs: number of times to train on the data
     noise: factor for the amount of noise to add
     learning_rate: rate for the optimizer
    """
    def __init__(self, data: torch.utils.data.DataLoader, epochs: int=30,
                 noise:float=0.5,
                 learning_rate:float=0.001) -&gt; None:
        self.data = data
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.noise = noise
        self._criterion = None
        self._model = None
        self._device = None
        self._optimizer = None
        return

    @property
    def device(self) -&gt; torch.device:
        """CUDA or CPU"""
        if self._device is None:
            self._device = torch.device(
                "cuda:0" if torch.cuda.is_available() else "cpu")
        return self._device

    @property
    def criterion(self) -&gt; nn.MSELoss:
        """Loss-calculator"""
        if self._criterion is None:
            self._criterion = nn.MSELoss()
        return self._criterion

    @property
    def model(self) -&gt; ConvDenoiser:
        """Our model"""
        if self._model is None:
            self._model = ConvDenoiser()
            self.model.to(self.device)
        return self._model

    @property
    def optimizer(self) -&gt; torch.optim.Adam:
        """The gradient descent optimizer"""
        if self._optimizer is None:
            self._optimizer = torch.optim.Adam(self.model.parameters(),
                                               lr=self.learning_rate)
        return self._optimizer

    def __call__(self) -&gt; None:
        """Trains the model on the data"""
        self.model.train()
        started = datetime.now()
        for epoch in range(1, self.epochs + 1):
            train_loss = 0.0
            for batch in self.data:
                images, _ = batch
                images = images.to(self.device)
                ## add random noise to the input images
                noisy_imgs = (images
                              + self.noise
                              * torch.randn(*images.shape).to(self.device))
                # Clip the images to be between 0 and 1
                noisy_imgs = numpy.clip(noisy_imgs, 0., 1.).to(self.device)

                # clear the gradients of all optimized variables
                self.optimizer.zero_grad()
                ## forward pass: compute predicted outputs by passing *noisy* images to the model
                outputs = self.model(noisy_imgs)
                # calculate the loss
                # the "target" is still the original, not-noisy images
                loss = self.criterion(outputs, images)
                # backward pass: compute gradient of the loss with respect to model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                self.optimizer.step()
                # update running training loss
                train_loss += loss.item() * images.size(0)

            # print avg training statistics 
            train_loss = train_loss/len(train_loader)
            print('Epoch: {} \tTraining Loss: {:.6f}'.format(
                epoch, 
                train_loss
                ))
        ended = datetime.now()
        print("Ended: {}".format(ended))
        print("Elapsed: {}".format(ended - started))
        return
</pre></div>
<div class="highlight">
<pre><span></span>train_the_model = Trainer(train_loader)
train_the_model()
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 0.952294
Epoch: 2        Training Loss: 0.686571
Epoch: 3        Training Loss: 0.647284
Epoch: 4        Training Loss: 0.628790
Epoch: 5        Training Loss: 0.615522
Epoch: 6        Training Loss: 0.604566
Epoch: 7        Training Loss: 0.595838
Epoch: 8        Training Loss: 0.585816
Epoch: 9        Training Loss: 0.578257
Epoch: 10       Training Loss: 0.572502
Epoch: 11       Training Loss: 0.566983
Epoch: 12       Training Loss: 0.562720
Epoch: 13       Training Loss: 0.558449
Epoch: 14       Training Loss: 0.554410
Epoch: 15       Training Loss: 0.550995
Epoch: 16       Training Loss: 0.546916
Epoch: 17       Training Loss: 0.543798
Epoch: 18       Training Loss: 0.541859
Epoch: 19       Training Loss: 0.539242
Epoch: 20       Training Loss: 0.536748
Epoch: 21       Training Loss: 0.534675
Epoch: 22       Training Loss: 0.532690
Epoch: 23       Training Loss: 0.531692
Epoch: 24       Training Loss: 0.529910
Epoch: 25       Training Loss: 0.528826
Epoch: 26       Training Loss: 0.526354
Epoch: 27       Training Loss: 0.526260
Epoch: 28       Training Loss: 0.525294
Epoch: 29       Training Loss: 0.524029
Epoch: 30       Training Loss: 0.523341
Epoch: 31       Training Loss: 0.522387
Epoch: 32       Training Loss: 0.521689
Ended: 2018-12-22 14:10:08.869789
Elapsed: 0:14:14.036518
</pre></div>
</div>
<div class="outline-2" id="outline-container-org14c882d">
<h2 id="org14c882d">Checking out the results</h2>
<div class="outline-text-2" id="text-org14c882d">
<p>Here I'm adding noise to the test images and passing them through the autoencoder. It does a suprising great job of removing the noise, even though it's sometimes difficult to tell what the original number is.</p>
<div class="highlight">
<pre><span></span># obtain one batch of test images
dataiter = iter(test_loader)
images, labels = dataiter.next()

# add noise to the test images
noisy_imgs = images + noise_factor * torch.randn(*images.shape)
noisy_imgs = numpy.clip(noisy_imgs, 0., 1.)

# get sample outputs
noisy_imgs = noisy_imgs.to(train_the_model.device)
output = train_the_model.model(noisy_imgs)
# prep images for display
noisy_imgs = noisy_imgs.cpu().numpy()

# output is resized into a batch of iages
output = output.view(BATCH_SIZE, 1, 28, 28)
# use detach when it's an output that requires_grad
output = output.detach().cpu().numpy()
</pre></div>
<div class="highlight">
<pre><span></span># plot the first ten input images and then reconstructed images
fig, axes = pyplot.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))

# input images on top row, reconstructions on bottom
for noisy_imgs, row in zip([noisy_imgs, output], axes):
    for img, ax in zip(noisy_imgs, row):
        ax.imshow(numpy.squeeze(img), cmap='gray')
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
</pre></div>
<div class="figure">
<p><img alt="de-noised.png" src="posts/nano/autoencoders/denoising-autoencoder/de-noised.png"></p>
</div>
<p>That did surprisingly well.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/autoencoders/convolutional-autoencoder/">Convolutional Autoencoder</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/autoencoders/convolutional-autoencoder/" rel="bookmark"><time class="published dt-published" datetime="2018-12-19T12:15:02-08:00" itemprop="datePublished" title="2018-12-19 12:15">2018-12-19 12:15</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org516a880">Introduction</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org7cde952">Compressed Representation</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#orgf876962">Set Up</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org03e6828">The Data</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#orgfea10b8">Visualize the Data</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org686c768">Convolutional Autoencoder</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org263586d">Transpose Convolutions, Decoder</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#orgefd7bfa">Initialize The NN</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#orga3d850f">Training</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org9655eee">Checking out the results</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org516a880">
<h2 id="org516a880">Introduction</h2>
<div class="outline-text-2" id="text-org516a880">
<p>Sticking with the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset, let's improve our autoencoder's performance using convolutional layers. We'll build a convolutional autoencoder to compress the MNIST dataset.</p>
<ul class="org-ul">
<li>The encoder portion will be made of convolutional and pooling layers and the decoder will be made of <b>transpose convolutional layers</b> that learn to "upsample" a compressed representation.</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org7cde952">
<h2 id="org7cde952">Compressed Representation</h2>
<div class="outline-text-2" id="text-org7cde952">
<p>A compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or other kinds of reconstruction and transformation!</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgf876962">
<h2 id="orgf876962">Set Up</h2>
<div class="outline-text-2" id="text-orgf876962"></div>
<div class="outline-3" id="outline-container-org246be18">
<h3 id="org246be18">Imports</h3>
<div class="outline-text-3" id="text-org246be18"></div>
<div class="outline-4" id="outline-container-orgb3ddf20">
<h4 id="orgb3ddf20">Python Standard Library</h4>
<div class="outline-text-4" id="text-orgb3ddf20">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb87572f">
<h4 id="orgb87572f">From PyPi</h4>
<div class="outline-text-4" id="text-orgb87572f">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Graph</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org65f53ef">
<h3 id="org65f53ef">Plotting</h3>
<div class="outline-text-3" id="text-org65f53ef">
<div class="highlight">
<pre><span></span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Open Sans"</span><span class="p">,</span> <span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0694208">
<h3 id="org0694208">Test for <a href="http://pytorch.org/docs/stable/cuda.html">CUDA</a></h3>
<div class="outline-text-3" id="text-org0694208">
<p>The test-code uses the check later on so I'll save it to the <code>train_on_gpu</code> variable.</p>
<div class="highlight">
<pre><span></span><span class="n">train_on_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda:0"</span> <span class="k">if</span> <span class="n">train_on_gpu</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Using: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>
<pre class="example">
Using: cuda:0

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org03e6828">
<h2 id="org03e6828">The Data</h2>
<div class="outline-text-2" id="text-org03e6828"></div>
<div class="outline-3" id="outline-container-orgb7eb9b5">
<h3 id="orgb7eb9b5">Setup the Data Transform</h3>
<div class="outline-text-3" id="text-orgb7eb9b5">
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org34914d6">
<h3 id="org34914d6">Load the Training and Test Datasets</h3>
<div class="outline-text-3" id="text-org34914d6">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/datasets/MNIST/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">is_dir</span><span class="p">())</span>
</pre></div>
<pre class="example">
/home/hades/datasets/MNIST
True

</pre>
<div class="highlight">
<pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                            <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                           <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org57154b2">
<h3 id="org57154b2">Create training and test dataloaders</h3>
<div class="outline-text-3" id="text-org57154b2">
<div class="highlight">
<pre><span></span><span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># how many samples per batch to load</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org164bf01">
<h3 id="org164bf01">Prepare Data Loaders</h3>
<div class="outline-text-3" id="text-org164bf01">
<div class="highlight">
<pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                           <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgfea10b8">
<h2 id="orgfea10b8">Visualize the Data</h2>
<div class="outline-text-2" id="text-orgfea10b8"></div>
<div class="outline-3" id="outline-container-org4e2e530">
<h3 id="org4e2e530">Obtain One Batch of Training Images</h3>
<div class="outline-text-3" id="text-org4e2e530">
<div class="highlight">
<pre><span></span><span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb176b0a">
<h3 id="orgb176b0a">Get One Image From the Batch</h3>
<div class="outline-text-3" id="text-orgb176b0a">
<div class="highlight">
<pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf24252a">
<h3 id="orgf24252a">Plot</h3>
<div class="outline-text-3" id="text-orgf24252a">
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"First Image"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="first_image.png" src="posts/nano/autoencoders/convolutional-autoencoder/first_image.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org686c768">
<h2 id="org686c768">Convolutional Autoencoder</h2>
<div class="outline-text-2" id="text-org686c768"></div>
<div class="outline-3" id="outline-container-org68c3d45">
<h3 id="org68c3d45">Encoder</h3>
<div class="outline-text-3" id="text-org68c3d45">
<p>The encoder part of the network will be a typical convolutional pyramid. Each convolutional layer will be followed by a max-pooling layer to reduce the dimensions of the layers.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgb0f1c15">
<h3 id="orgb0f1c15">Decoder</h3>
<div class="outline-text-3" id="text-orgb0f1c15">
<p>The decoder, though, might be something new to you. The decoder needs to convert from a narrow representation to a wide, reconstructed image. For example, the representation could be a 7x7x4 max-pool layer. This is the output of the encoder, but also the input to the decoder. We want to get a 28x28x1 image out from the decoder so we need to work our way back up from the compressed representation. A schematic of the network is shown below.</p>
<div class="highlight">
<pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="n">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span>

<span class="c1"># Input layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"a"</span><span class="p">,</span> <span class="s2">"28x28x1 Input"</span><span class="p">)</span>

<span class="c1"># the Encoder</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"b"</span><span class="p">,</span> <span class="s2">"28x28x16 Convolution"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"c"</span><span class="p">,</span> <span class="s2">"14x14x16 MaxPool"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"d"</span><span class="p">,</span> <span class="s2">"14x14x4 Convolution"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"e"</span><span class="p">,</span> <span class="s2">"7x7x4 MaxPool"</span><span class="p">)</span>

<span class="c1"># The Decoder</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"f"</span><span class="p">,</span> <span class="s2">"14x14x16 Transpose Convolution"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="s2">"28x28x1 Transpose Convolution"</span><span class="p">)</span>

<span class="c1"># The Output</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"h"</span><span class="p">,</span> <span class="s2">"28x28x1 Output"</span><span class="p">)</span>

<span class="n">edges</span> <span class="o">=</span> <span class="s2">"abcdefgh"</span>
<span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="n">edges</span><span class="p">[</span><span class="n">edge</span><span class="p">]</span> <span class="o">+</span> <span class="n">edges</span><span class="p">[</span><span class="n">edge</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>

<span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">"graphs/network_graph.dot"</span><span class="p">)</span>
<span class="n">graph</span>
</pre></div>
<div class="figure">
<p><img alt="network_graph.dot.png" src="posts/files/posts/nano/autoencoders/convolutional-autoencoder/network_graph.dot.png"></p>
</div>
<div class="figure">
<p><img alt="network_graph.dot.png" src="posts/nano/autoencoders/convolutional-autoencoder/network_graph.dot.png"></p>
</div>
<p>Here our final encoder layer has size 7x7x4 = 196. The original images have size 28x28 = 784, so the encoded vector is 25% the size of the original image. These are just suggested sizes for each of the layers. Feel free to change the depths and sizes, in fact, you're encouraged to add additional layers to make this representation even smaller! Remember our goal here is to find a small representation of the input data.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org263586d">
<h2 id="org263586d">Transpose Convolutions, Decoder</h2>
<div class="outline-text-2" id="text-org263586d">
<p>This decoder uses <b>transposed convolutional</b> layers to increase the width and height of the input layers. They work almost exactly the same as convolutional layers, but in reverse. A stride in the input layer results in a larger stride in the transposed convolution layer. For example, if you have a 3x3 kernel, a 3x3 patch in the input layer will be reduced to one unit in a convolutional layer. Comparatively, one unit in the input layer will be expanded to a 3x3 path in a transposed convolution layer. PyTorch provides us with an easy way to create the layers, <a href="https://pytorch.org/docs/stable/nn.html#convtranspose2d"><code>nn.ConvTranspose2d</code></a>.</p>
<p>It is important to note that transpose convolution layers can lead to artifacts in the final images, such as checkerboard patterns. This is due to overlap in the kernels which can be avoided by setting the stride and kernel size equal. In <a href="http://distill.pub/2016/deconv-checkerboard/">this Distill article</a> from Augustus Odena, <b>et al</b>, the authors show that these checkerboard artifacts can be avoided by resizing the layers using nearest neighbor or bilinear interpolation (upsampling) followed by a convolutional layer.</p>
<p>We'll show this approach in another notebook, so you can experiment with it and see the difference.</p>
<ul class="org-ul">
<li>Build the encoder out of a series of convolutional and pooling layers.</li>
<li>When building the decoder, recall that transpose convolutional layers can upsample an input by a factor of 2 using a stride and kernel_size of 2.</li>
</ul>
<p>See:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d">Conv2d</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html?highlight=maxpool#torch.nn.MaxPool2d">MaxPool2d</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html#relu">ReLU</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html#sigmoid">Sigmoid</a></li>
</ul>
<p>To get the output size of our Convolutional Layers you use the formula:</p>
<p>\[ o = \frac{W - F + 2P}{S} + 1 \]</p>
<p>Where <i>W</i> is the input size (28 here), <i>F</i> is the filter size, <i>P</i> is the zero-padding, and <i>S</i> is the stride. For our first layer we want to keep the output the same size as the input.</p>
<p>The output for a maxpool layer uses a similar set of equations.</p>
\begin{align} W_2 &amp;= \frac{W_1 - F}{S} + 1\\ H_2 &amp;= \frac{H_Y - F}{S} + 1\\ D_2 = D_1\\ \end{align}
<p>Where <i>W</i> is the width, <i>H</i> is the height, and <i>D</i> is the depth.</p>
<div class="highlight">
<pre><span></span><span class="n">Layer</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Layer"</span><span class="p">,</span> <span class="s2">"kernel stride depth padding"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="n">Layer</span><span class="o">.</span><span class="fm">__new__</span><span class="o">.</span><span class="vm">__defaults__</span><span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
<span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">expected</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Calculates the output size of the layer</span>

<span class="sd">    Args:</span>
<span class="sd">     input_size: the size of the input to the layer</span>
<span class="sd">     layer: named tuple with values for the layer</span>
<span class="sd">     expected: the value you are expecting</span>

<span class="sd">    Returns:</span>
<span class="sd">     the size of the output</span>

<span class="sd">    Raises:</span>
<span class="sd">     AssertionError: the calculated value wasn't the expected one</span>
<span class="sd">    """</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">((</span><span class="n">input_size</span> <span class="o">-</span> <span class="n">layer</span><span class="o">.</span><span class="n">kernel</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">layer</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span><span class="o">/</span><span class="n">layer</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Layer Output Size: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    <span class="k">assert</span> <span class="n">size</span> <span class="o">==</span> <span class="n">expected</span>
    <span class="k">return</span> <span class="n">size</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-org0b2ed79">
<h3 id="org0b2ed79">The Encoder Layers</h3>
<div class="outline-text-3" id="text-org0b2ed79"></div>
<div class="outline-4" id="outline-container-org4ab7acf">
<h4 id="org4ab7acf">Layer One</h4>
<div class="outline-text-4" id="text-org4ab7acf">
<p>The first layer is a Convolutional Layer that we want to have the same size output as the input but with a depth of sixteen. The <a href="https://cs231n.github.io/convolutional-networks/">CS 231</a> page notes that to keep the size of the output the same as the input you should set the stride to one and once you have decided on your kernle size (<i>F</i>) then you can find your padding using this equation:</p>
<p>\[ P = \frac{F - 1}{2} \]</p>
<p>In this case I'm going to use a filter size of three so our padding will be:</p>
\begin{align} P &amp;= \frac{3 - 1}{2}\\ &amp;= 1\\ \end{align}
<p>We can double-check this by plugging the values back intoo the equation for output size.</p>
\begin{align} W' &amp;= \frac{W - F + 2P}{S} + 1\\ &amp;= \frac{28 - 3 + 2(1)}{1} + 1\\ &amp;= 28\\ \end{align}
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Variable</th>
<th class="org-left" scope="col">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><i>W</i></td>
<td class="org-left">One dimension of the input</td>
</tr>
<tr>
<td class="org-left"><i>F</i></td>
<td class="org-left">One dimension of the Kernel (filter)</td>
</tr>
<tr>
<td class="org-left"><i>S</i></td>
<td class="org-left">Stride</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span> <span class="n">layer_one</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                   <span class="n">padding</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                   <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                   <span class="n">depth</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>

 <span class="n">INPUT_ONE</span> <span class="o">=</span> <span class="mi">28</span>
 <span class="n">OUTPUT_ONE</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">(</span><span class="n">INPUT_ONE</span><span class="p">,</span> <span class="n">layer_one</span><span class="p">,</span> <span class="n">INPUT_ONE</span><span class="p">)</span>
 <span class="n">INPUT_DEPTH</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, depth=16, padding=1)
Layer Output Size: 28.0

</pre></div>
</div>
<div class="outline-4" id="outline-container-org3554142">
<h4 id="org3554142">Layer Two</h4>
<div class="outline-text-4" id="text-org3554142">
<p>The second layer is a MaxPool layer that will keep the depth of six but will halve the size to fourteen. According to the <a href="https://cs231n.github.io/convolutional-networks/">CS 231 n</a> page on Convolutional Networks, there are only two values for the kernel size that are usually used - 2 and 3, and the stride is usually just 2, with a kernel size of 2 being more common, and as it turns out, a kernel size of 2 and a stride of 2 will reduce our input dimensions by a half, which is what we want.</p>
\begin{align} W &amp;= \frac{28 - 2}{2} + 1\\ &amp;= 14\\ \end{align}
<div class="highlight">
<pre><span></span> <span class="n">layer_two</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">depth</span><span class="p">)</span>
 <span class="n">OUTPUT_TWO</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">(</span><span class="n">OUTPUT_ONE</span><span class="p">,</span> <span class="n">layer_two</span><span class="p">,</span> <span class="mi">14</span><span class="p">)</span>
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, depth=16, padding=0)
Layer Output Size: 14.0

</pre></div>
</div>
<div class="outline-4" id="outline-container-org9aab4b8">
<h4 id="org9aab4b8">Layer Three</h4>
<div class="outline-text-4" id="text-org9aab4b8">
<p>Our third layer is another convolutional layer that preserves the input width and height but this time the output will have a depth of 4.</p>
<div class="highlight">
<pre><span></span><span class="n">layer_three</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">OUTPUT_THREE</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">(</span><span class="n">OUTPUT_TWO</span><span class="p">,</span> <span class="n">layer_three</span><span class="p">,</span> <span class="n">OUTPUT_TWO</span><span class="p">)</span>
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, depth=4, padding=1)
Layer Output Size: 14.0

</pre></div>
</div>
<div class="outline-4" id="outline-container-orgde4a64e">
<h4 id="orgde4a64e">Layer Four</h4>
<div class="outline-text-4" id="text-orgde4a64e">
<p>The last layer in the encoder is a max pool layer that reduces the previous layer by half (to dimensions of 7) while preserving the depth.</p>
<div class="highlight">
<pre><span></span><span class="n">layer_four</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">depth</span><span class="p">)</span>
<span class="n">OUTPUT_FOUR</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">(</span><span class="n">OUTPUT_THREE</span><span class="p">,</span> <span class="n">layer_four</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, depth=4, padding=0)
Layer Output Size: 7.0

</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc1211bb">
<h3 id="orgc1211bb">Decoders</h3>
<div class="outline-text-3" id="text-orgc1211bb"></div>
<div class="outline-4" id="outline-container-org47ebcf9">
<h4 id="org47ebcf9">Layer Five</h4>
<div class="outline-text-4" id="text-org47ebcf9">
<p>We want an output of 14 x 14 x 16 from an input of 7 x 7 x 4. The comments given with this exercise say that using a kernel of 2 and stride of 2 will double the dimensions, much as those same values halve the dimensions with Max-Pooling.</p>
<div class="highlight">
<pre><span></span><span class="n">layer_five</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org2913e16">
<h4 id="org2913e16">Layer Six</h4>
<div class="outline-text-4" id="text-org2913e16">
<p>This layer will expand the image back to its original size of 28 x 28 x 1</p>
<div class="highlight">
<pre><span></span><span class="n">layer_six</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orga4ca7f5">
<h3 id="orga4ca7f5">Define the NN Architecture</h3>
<div class="outline-text-3" id="text-orga4ca7f5">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">ConvAutoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""A CNN AutoEncoder-Decoder"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1">## encoder layers ##</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">INPUT_DEPTH</span><span class="p">,</span>
                                       <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
                                       <span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> 
                                       <span class="n">stride</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                       <span class="n">padding</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_two</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                                       <span class="n">stride</span><span class="o">=</span><span class="n">layer_two</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">layer_two</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
                                       <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
                                       <span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                                       <span class="n">stride</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                       <span class="n">padding</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>

        <span class="c1">## decoder layers ##</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose_convolution_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_four</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span> 
            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_five</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_five</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">layer_five</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transpose_convolution_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_five</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span> 
            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_six</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_six</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">layer_six</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1">## encode ##</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convolution_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convolution_2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="c1">## decode ##</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transpose_convolution_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transpose_convolution_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="n">ConvAutoencoder</span><span class="p">()</span>
<span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">convolution_1</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">max_pool_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">convolution_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">max_pool_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">transpose_convolution_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">transpose_convolution_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</pre></div>
<pre class="example">
torch.Size([20, 16, 28, 28])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 4, 14, 14])
torch.Size([20, 4, 7, 7])
torch.Size([20, 4, 7, 7])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 1, 28, 28])

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgefd7bfa">
<h2 id="orgefd7bfa">Initialize The NN</h2>
<div class="outline-text-2" id="text-orgefd7bfa">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ConvAutoencoder</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
<pre class="example">
ConvAutoencoder(
  (convolution_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (convolution_2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (transpose_convolution_1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
  (transpose_convolution_2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))
  (relu): ReLU()
  (sigmoid): Sigmoid()
)

</pre></div>
</div>
<div class="outline-2" id="outline-container-orga3d850f">
<h2 id="orga3d850f">Training</h2>
<div class="outline-text-2" id="text-orga3d850f">
<p>Here I'll write a bit of code to train the network. I'm not too interested in validation here, so I'll just monitor the training loss and the test loss afterwards.</p>
<p>We are not concerned with labels in this case, just images, which we can get from the <code>train_loader</code>. Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use <a href="https://pytorch.org/docs/stable/nn.html?highlight=mseloss#torch.nn.MSELoss">MSELoss</a>. And compare output images and input images as follows:</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
</pre></div>
<p>Otherwise, this is pretty straightfoward training with PyTorch. Since this is a convolutional autoencoder, our images <i>do not</i> need to be flattened before being passed in an input to our model.</p>
</div>
<div class="outline-3" id="outline-container-org48afbc0">
<h3 id="org48afbc0">Train the Model</h3>
<div class="outline-text-3" id="text-org48afbc0">
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">started</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># monitor training loss</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1">###################</span>
    <span class="c1"># train the model #</span>
    <span class="c1">###################</span>

    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="c1"># _ stands in for labels, here</span>
        <span class="c1"># no need to flatten images</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># clear the gradients of all optimized variables</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># forward pass: compute predicted outputs by passing inputs to the model</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="c1"># calculate the loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
        <span class="c1"># backward pass: compute gradient of the loss with respect to model parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># perform a single optimization step (parameter update)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># update running training loss</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">*</span><span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># print avg training statistics </span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Epoch: {} </span><span class="se">\t</span><span class="s1">Training Loss: {:.6f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">epoch</span><span class="p">,</span> 
        <span class="n">train_loss</span>
        <span class="p">))</span>
<span class="n">ended</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Ended: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ended</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Elapsed: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ended</span> <span class="o">-</span> <span class="n">started</span><span class="p">))</span>
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 0.259976
Epoch: 2        Training Loss: 0.244956
Epoch: 3        Training Loss: 0.235354
Epoch: 4        Training Loss: 0.226544
Epoch: 5        Training Loss: 0.216255
Epoch: 6        Training Loss: 0.207204
Epoch: 7        Training Loss: 0.200490
Epoch: 8        Training Loss: 0.195582
Epoch: 9        Training Loss: 0.191870
Epoch: 10       Training Loss: 0.189247
Epoch: 11       Training Loss: 0.187027
Epoch: 12       Training Loss: 0.185084
Epoch: 13       Training Loss: 0.183055
Epoch: 14       Training Loss: 0.181224
Epoch: 15       Training Loss: 0.179749
Epoch: 16       Training Loss: 0.178564
Epoch: 17       Training Loss: 0.177572
Epoch: 18       Training Loss: 0.176735
Epoch: 19       Training Loss: 0.176076
Epoch: 20       Training Loss: 0.175518
Epoch: 21       Training Loss: 0.175040
Epoch: 22       Training Loss: 0.174629
Epoch: 23       Training Loss: 0.174230
Epoch: 24       Training Loss: 0.173856
Epoch: 25       Training Loss: 0.173497
Epoch: 26       Training Loss: 0.173166
Epoch: 27       Training Loss: 0.172838
Epoch: 28       Training Loss: 0.172520
Epoch: 29       Training Loss: 0.172212
Epoch: 30       Training Loss: 0.171920
Ended: 2018-12-21 17:41:26.461977
Elapsed: 0:07:50.942721
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org9655eee">
<h2 id="org9655eee">Checking out the results</h2>
<div class="outline-text-2" id="text-org9655eee">
<p>Below I've plotted some of the test images along with their reconstructions. These look a little rough around the edges, likely due to the checkerboard effect we mentioned above that tends to happen with transpose layers.</p>
</div>
<div class="outline-3" id="outline-container-orgda66156">
<h3 id="orgda66156">Obtain One Batch Of Test Images</h3>
<div class="outline-text-3" id="text-orgda66156">
<div class="highlight">
<pre><span></span><span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org66e880e">
<h3 id="org66e880e">Get Sample Outputs</h3>
<div class="outline-text-3" id="text-org66e880e">
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org54236c8">
<h3 id="org54236c8">Prep Images for Display</h3>
<div class="outline-text-3" id="text-org54236c8">
<div class="highlight">
<pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org6ebbda9">
<h3 id="org6ebbda9">Output Is Resized Into a Batch Of Images</h3>
<div class="outline-text-3" id="text-org6ebbda9">
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga11bf9e">
<h3 id="orga11bf9e">Use Detach When It's An Output That Requires Grad</h3>
<div class="outline-text-3" id="text-orga11bf9e">
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc8338c8">
<h3 id="orgc8338c8">plot the first ten input images and then reconstructed images</h3>
<div class="outline-text-3" id="text-orgc8338c8">
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Auto-Encoded/Decoded Images"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="c1"># input images on top row, reconstructions on bottom</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">images</span><span class="p">,</span> <span class="n">output</span><span class="p">],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="reconstructed.png" src="posts/nano/autoencoders/convolutional-autoencoder/reconstructed.png"></p>
</div>
<p>That is better than I would have thought it would be.</p>
</div>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="." rel="prev">Newer posts</a></li>
<li class="next"><a href="index-6.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer">Contents Â© 2019 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js">
</script>
<script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
