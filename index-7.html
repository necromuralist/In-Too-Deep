<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>In Too Deep (old posts, page 7) | In Too Deep</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/index-7.html" rel="canonical">
<link href="." rel="prev" type="text/html">
<link href="index-6.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/autoencoders/denoising-autoencoder/">Denoising Autoencoder</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/autoencoders/denoising-autoencoder/" rel="bookmark"><time class="published dt-published" datetime="2018-12-21T18:07:29-08:00" itemprop="datePublished" title="2018-12-21 18:07">2018-12-21 18:07</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#org68d039e">Set Up</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#orge9e914d">Visualize the Data</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#org4308c4a">Denoising</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#orgc495de6">Define the NN Architecture</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#orgb37a9d7">Initialize The NN</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#org0aa8488">Training</a></li>
<li><a href="posts/nano/autoencoders/denoising-autoencoder/#org3366460">Checking out the results</a></li>
</ul>
</div>
</div>
<p>Sticking with the MNIST dataset, let's add noise to our data and see if we can define and train an autoencoder to <i>de</i>-noise the images.</p>
<div class="outline-2" id="outline-container-org68d039e">
<h2 id="org68d039e">Set Up</h2>
<div class="outline-text-2" id="text-org68d039e"></div>
<div class="outline-3" id="outline-container-org04e536b">
<h3 id="org04e536b">Imports</h3>
<div class="outline-text-3" id="text-org04e536b"></div>
<div class="outline-4" id="outline-container-org8f2c389">
<h4 id="org8f2c389">Python</h4>
<div class="outline-text-4" id="text-org8f2c389">
<div class="highlight">
<pre><span></span>from collections import namedtuple
from datetime import datetime
from pathlib import Path
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5bba724">
<h4 id="org5bba724">PyPi</h4>
<div class="outline-text-4" id="text-org5bba724">
<div class="highlight">
<pre><span></span>from torchvision import datasets
from graphviz import Graph
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org1f5406f">
<h3 id="org1f5406f">The Plotting</h3>
<div class="outline-text-3" id="text-org1f5406f">
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=3)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga99673c">
<h3 id="orga99673c">The Data</h3>
<div class="outline-text-3" id="text-orga99673c"></div>
<div class="outline-4" id="outline-container-org64bc85b">
<h4 id="org64bc85b">The Transform</h4>
<div class="outline-text-4" id="text-org64bc85b">
<div class="highlight">
<pre><span></span>transform = transforms.ToTensor()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga55223f">
<h4 id="orga55223f">Load the Training and Test Datasets</h4>
<div class="outline-text-4" id="text-orga55223f">
<div class="highlight">
<pre><span></span>path = Path("~/datasets/MNIST/").expanduser()
print(path.is_dir())
</pre></div>
<pre class="example">
True

</pre>
<div class="highlight">
<pre><span></span>train_data = datasets.MNIST(root=path, train=True,
                            download=True, transform=transform)
test_data = datasets.MNIST(root=path, train=False,
                           download=True, transform=transform)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org32caa67">
<h4 id="org32caa67">Create training and test dataloaders</h4>
<div class="outline-text-4" id="text-org32caa67">
<div class="highlight">
<pre><span></span>NUM_WORKERS = 0
BATCH_SIZE = 20
</pre></div>
<div class="highlight">
<pre><span></span>train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
                                           num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,
                                          num_workers=NUM_WORKERS)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org9a1dd8a">
<h3 id="org9a1dd8a">Test for <a href="http://pytorch.org/docs/stable/cuda.html">CUDA</a></h3>
<div class="outline-text-3" id="text-org9a1dd8a">
<div class="highlight">
<pre><span></span>device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Using: {}".format(device))
</pre></div>
<pre class="example">
Using: cuda:0

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orge9e914d">
<h2 id="orge9e914d">Visualize the Data</h2>
<div class="outline-text-2" id="text-orge9e914d"></div>
<div class="outline-3" id="outline-container-orgebebc49">
<h3 id="orgebebc49">Obtain One Batch of Training Images</h3>
<div class="outline-text-3" id="text-orgebebc49">
<div class="highlight">
<pre><span></span>dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0bf273b">
<h3 id="org0bf273b">Get One Image From the Batch</h3>
<div class="outline-text-3" id="text-org0bf273b">
<div class="highlight">
<pre><span></span>img = numpy.squeeze(images[0])
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgbf512a1">
<h3 id="orgbf512a1">Plot</h3>
<div class="outline-text-3" id="text-orgbf512a1">
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
figure.suptitle("Sample Image", weight="bold")
image = axe.imshow(img, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="first_image.png" src="posts/nano/autoencoders/denoising-autoencoder/first_image.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4308c4a">
<h2 id="org4308c4a">Denoising</h2>
<div class="outline-text-2" id="text-org4308c4a">
<p>As I've mentioned before, autoencoders like the ones you've built so far aren't too useful in practive. However, they can be used to denoise images quite successfully just by training the network on noisy images. We can create the noisy images ourselves by adding Gaussian noise to the training images, then clipping the values to be between 0 and 1.</p>
<p><b>We'll use noisy images as input and the original, clean images as targets.</b></p>
<p>Since this is a harder problem for the network, we'll want to use <i>deeper</i> convolutional layers here; layers with more feature maps. You might also consider adding additional layers. I suggest starting with a depth of 32 for the convolutional layers in the encoder, and the same depths going backward through the decoder.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgc495de6">
<h2 id="orgc495de6">Define the NN Architecture</h2>
<div class="outline-text-2" id="text-orgc495de6">
<div class="highlight">
<pre><span></span>graph = Graph(format="png")

# Input layer
graph.node("a", "28x28x1 Input")

# the Encoder
graph.node("b", "28x28x32 Convolution")
graph.node("c", "14x14x32 MaxPool")
graph.node("d", "14x14x16 Convolution")
graph.node("e", "7x7x16 MaxPool")
graph.node("f", "7x7x8 Convolution")
graph.node("g", "3x3x8 MaxPool")

# The Decoder
graph.node("h", "7x7x8 Transpose Convolution")
graph.node("i", "14x14x16 Transpose Convolution")
graph.node("j", "28x28x32 Transpose Convolution")
graph.node("k", "28x28x1 Convolution")

# The Output
graph.node("l", "28x28x1 Output")

edges = "abcdefghijkl"
graph.edges([edges[edge] + edges[edge+1] for edge in range(len(edges) - 1)])

graph.render("graphs/network.dot")
graph
</pre></div>
<div class="figure">
<p><img alt="network.dot.png" src="posts/nano/autoencoders/denoising-autoencoder/network.dot.png"></p>
</div>
<div class="highlight">
<pre><span></span>Layer = namedtuple("Layer", "kernel stride in_depth out_depth padding".split())
Layer.__new__.__defaults__= (0,)
def output_size(input_size: int, layer: Layer, expected: int) -&gt; int:
    """Calculates the output size of the layer

    Args:
     input_size: the size of the input to the layer
     layer: named tuple with values for the layer
     expected: the value you are expecting

    Returns:
     the size of the output

    Raises:
     AssertionError: the calculated value wasn't the expected one
    """
    size = 1 + int(
        (input_size - layer.kernel + 2 * layer.padding)/layer.stride)
    print(layer)
    print("Layer Output: {0} x {0} x {1}".format(size, layer.out_depth))
    assert size == expected, size
    return size
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgb240467">
<h3 id="orgb240467">The Encoder Layers</h3>
<div class="outline-text-3" id="text-orgb240467"></div>
<div class="outline-4" id="outline-container-orgc5fe7a6">
<h4 id="orgc5fe7a6">Layer One</h4>
<div class="outline-text-4" id="text-orgc5fe7a6">
<div class="highlight">
<pre><span></span> INPUT_DEPTH = 1
 convolution_one = Layer(kernel = 3,
                         padding = 1,
                         stride = 1,
                         in_depth=INPUT_DEPTH,
                         out_depth = 32)
 INPUT_ONE = 28
 OUTPUT_ONE = output_size(INPUT_ONE, convolution_one, INPUT_ONE)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=1, out_depth=32, padding=1)
Layer Output: 28 x 28 x 32

</pre></div>
</div>
<div class="outline-4" id="outline-container-org955c596">
<h4 id="org955c596">Layer Two</h4>
<div class="outline-text-4" id="text-org955c596">
<p>The second layer is a MaxPool layer that will keep the depth of thirty-two but will halve the size to fourteen. According to the <a href="https://cs231n.github.io/convolutional-networks/">CS 231 n</a> page on Convolutional Networks, there are only two values for the kernel size that are usually used - 2 and 3, and the stride is usually just 2, with a kernel size of 2 being more common, and as it turns out, a kernel size of 2 and a stride of 2 will reduce our input dimensions by a half, which is what we want.</p>
\begin{align} W &amp;= \frac{28 - 2}{2} + 1\\ &amp;= 14\\ \end{align}
<div class="highlight">
<pre><span></span> max_pool_one = Layer(kernel=2, stride=2,
                      in_depth=convolution_one.out_depth,
                      out_depth=convolution_one.out_depth)
 OUTPUT_TWO = output_size(OUTPUT_ONE, max_pool_one, 14)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=32, out_depth=32, padding=0)
Layer Output: 14 x 14 x 32

</pre></div>
</div>
<div class="outline-4" id="outline-container-org3c0a912">
<h4 id="org3c0a912">Layer Three</h4>
<div class="outline-text-4" id="text-org3c0a912">
<p>Our third layer is another convolutional layer that preserves the input width and height but this time the output will have a depth of 16.</p>
<div class="highlight">
<pre><span></span>convolution_two = Layer(kernel=3, stride=1, in_depth=max_pool_one.out_depth,
                        out_depth=16, padding=1)
OUTPUT_THREE = output_size(OUTPUT_TWO, convolution_two, OUTPUT_TWO)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=32, out_depth=16, padding=1)
Layer Output: 14 x 14 x 16

</pre></div>
</div>
<div class="outline-4" id="outline-container-org7f7d96f">
<h4 id="org7f7d96f">Layer Four</h4>
<div class="outline-text-4" id="text-org7f7d96f">
<p>The fourth layer is another max-pool layer that will halve the dimensions.</p>
<div class="highlight">
<pre><span></span>max_pool_two = Layer(kernel=2, stride=2, in_depth=convolution_two.out_depth,
                        out_depth=convolution_two.out_depth)
OUTPUT_FOUR = output_size(OUTPUT_THREE, max_pool_two, 7)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=16, out_depth=16, padding=0)
Layer Output: 7 x 7 x 16

</pre></div>
</div>
<div class="outline-4" id="outline-container-org6d48985">
<h4 id="org6d48985">Layer Five</h4>
<div class="outline-text-4" id="text-org6d48985">
<p>The fifth layer is another convolutional layer that will reduce the depth to eight.</p>
<div class="highlight">
<pre><span></span>convolution_three = Layer(kernel=3, stride=1,
                          in_depth=max_pool_two.out_depth, out_depth=8,
                          padding=1)
OUTPUT_FIVE = output_size(OUTPUT_FOUR, convolution_three, 7)
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, in_depth=16, out_depth=8, padding=1)
Layer Output: 7 x 7 x 8

</pre></div>
</div>
<div class="outline-4" id="outline-container-org3a89be1">
<h4 id="org3a89be1">Layer Six</h4>
<div class="outline-text-4" id="text-org3a89be1">
<p>The last layer in the encoder is a max pool layer that reduces the previous layer by half (to dimensions of 3) while preserving the depth.</p>
<div class="highlight">
<pre><span></span>max_pool_three = Layer(kernel=2, stride=2,
                       in_depth=convolution_three.out_depth,
                       out_depth=convolution_three.out_depth)
OUTPUT_SIX = output_size(OUTPUT_FIVE, max_pool_three, 3)
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, in_depth=8, out_depth=8, padding=0)
Layer Output: 3 x 3 x 8

</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org25b25e8">
<h3 id="org25b25e8">Decoders</h3>
<div class="outline-text-3" id="text-org25b25e8"></div>
<div class="outline-4" id="outline-container-org0130a69">
<h4 id="org0130a69">Layer Six</h4>
<div class="outline-text-4" id="text-org0130a69">
<p>This is a transpose convolution layer to (more than) double the size of the image. The image put out by the encoder is 3x3, but we want a 7x7 output, not a 6x6, so the kernel has to be upped to 3.</p>
<div class="highlight">
<pre><span></span>transpose_one = Layer(kernel=3, stride=2, out_depth=8,
                      in_depth=max_pool_three.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1913b20">
<h4 id="org1913b20">Layer Seven</h4>
<div class="outline-text-4" id="text-org1913b20">
<p>This will double the size again (to 14x14) and increase the depth to 16.</p>
<div class="highlight">
<pre><span></span>transpose_two = Layer(kernel=2, stride=2, out_depth=16,
                      in_depth=transpose_one.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4c4dc2f">
<h4 id="org4c4dc2f">Layer Eight</h4>
<div class="outline-text-4" id="text-org4c4dc2f">
<p>This will double the size to 28x28 and up the depth back again to 32, the size of our original encoding convolution.</p>
<div class="highlight">
<pre><span></span>transpose_three = Layer(kernel=2, stride=2, out_depth=32,
                        in_depth=transpose_two.out_depth)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3c7c14b">
<h4 id="org3c7c14b">Layer Nine</h4>
<div class="outline-text-4" id="text-org3c7c14b">
<p>This is a convolution layer to bring the depth back to one.</p>
<div class="highlight">
<pre><span></span>convolution_out = Layer(kernel=3, stride=1, in_depth=transpose_three.out_depth,
                        out_depth=1, padding=1)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7560fd0">
<h3 id="org7560fd0">The Implementation</h3>
<div class="outline-text-3" id="text-org7560fd0">
<div class="highlight">
<pre><span></span>class ConvDenoiser(nn.Module):
    def __init__(self):
        super().__init__()
        ## encoder layers ##
        self.convolution_1 =  nn.Conv2d(in_channels=convolution_one.in_depth,
                                       out_channels=convolution_one.out_depth,
                                       kernel_size=convolution_one.kernel,
                                       padding=convolution_one.padding)

        self.convolution_2 = nn.Conv2d(in_channels=convolution_two.in_depth,
                                       out_channels=convolution_two.out_depth,
                                       kernel_size=convolution_two.kernel,
                                       padding=convolution_two.padding)

        self.convolution_3 = nn.Conv2d(in_channels=convolution_three.in_depth,
                                       out_channels=convolution_three.out_depth,
                                       kernel_size=convolution_three.kernel,
                                       padding=convolution_three.padding)

        self.max_pool = nn.MaxPool2d(kernel_size=max_pool_one.kernel,
                                     stride=max_pool_one.stride)

        ## decoder layers ##
        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2
        self.transpose_convolution_1 = nn.ConvTranspose2d(
            in_channels=transpose_one.in_depth,
            out_channels=transpose_one.out_depth,
            kernel_size=transpose_one.kernel,
            stride=transpose_one.stride)

        self.transpose_convolution_2 = nn.ConvTranspose2d(
            in_channels=transpose_two.in_depth, 
            out_channels=transpose_two.out_depth,
            kernel_size=transpose_two.kernel,
            stride=transpose_two.stride)

        self.transpose_convolution_3 = nn.ConvTranspose2d(
            in_channels=transpose_three.in_depth,
            out_channels=transpose_three.out_depth,
            kernel_size=transpose_three.kernel,
            stride=transpose_three.stride)

        self.convolution_out = nn.Conv2d(in_channels=convolution_out.in_depth,
                                         out_channels=convolution_out.out_depth,
                                         kernel_size=convolution_out.kernel,
                                         padding=convolution_out.padding)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        return


    def forward(self, x):
        ## encode ##
        x = self.max_pool(self.relu(self.convolution_1(x)))
        x = self.max_pool(self.relu(self.convolution_2(x)))
        x = self.max_pool(self.relu(self.convolution_3(x)))

        ## decode ##
        x = self.relu(self.transpose_convolution_1(x))
        x = self.relu(self.transpose_convolution_2(x))
        x = self.relu(self.transpose_convolution_3(x))
        return self.sigmoid(self.convolution_out(x))
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb37a9d7">
<h2 id="orgb37a9d7">Initialize The NN</h2>
<div class="outline-text-2" id="text-orgb37a9d7">
<div class="highlight">
<pre><span></span>model = ConvDenoiser()
print(model)
</pre></div>
<pre class="example">
ConvDenoiser(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (t_conv1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2))
  (t_conv2): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))
  (t_conv3): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))
  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
</pre>
<div class="highlight">
<pre><span></span>test = ConvDenoiser()
dataiter = iter(train_loader)
images, labels = dataiter.next()
x = test.convolution_1(images)
assert x.shape == torch.Size([BATCH_SIZE, 32, 28, 28])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 32, 14, 14])
print(x.shape)

x = test.convolution_2(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 14, 14])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 7, 7])
print(x.shape)

x = test.convolution_3(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 7, 7])
print(x.shape)

x = test.max_pool(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 3, 3]), x.shape

x = test.transpose_convolution_1(x)
assert x.shape == torch.Size([BATCH_SIZE, 8, 7, 7]), x.shape
print(x.shape)

x = test.transpose_convolution_2(x)
assert x.shape == torch.Size([BATCH_SIZE, 16, 14, 14])
print(x.shape)

x = test.transpose_convolution_3(x)
assert x.shape == torch.Size([BATCH_SIZE, 32, 28, 28])
print(x.shape)

x = test.convolution_out(x)
assert x.shape == torch.Size([BATCH_SIZE, 1, 28, 28])
print(x.shape)
</pre></div>
<pre class="example">
torch.Size([20, 32, 28, 28])
torch.Size([20, 32, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 7, 7])
torch.Size([20, 8, 7, 7])
torch.Size([20, 8, 7, 7])
torch.Size([20, 16, 14, 14])
torch.Size([20, 32, 28, 28])
torch.Size([20, 1, 28, 28])

</pre></div>
</div>
<div class="outline-2" id="outline-container-org0aa8488">
<h2 id="org0aa8488">Training</h2>
<div class="outline-text-2" id="text-org0aa8488">
<p>We are only concerned with the training images, which we can get from the <code>train_loader</code>.</p>
<p>In this case, we are actually <b>adding some noise</b> to these images and we'll feed these <code>noisy_imgs</code> to our model. The model will produce reconstructed images based on the noisy input. But, we want it to produce <span class="underline">normal</span> un-noisy images, and so, when we calculate the loss, we will still compare the reconstructed outputs to the original images!</p>
<p>Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use <code>MSELoss</code>. And compare output images and input images as follows:</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
</pre></div>
<p><b>Warning:</b> I spent an unreasonable amount of time trying to de-bug this thing because I was passing in the model's parameters to the optimizer before passing it to the GPU. I don't know why it didn't throw an error, but it didn't, it just never learned and gave me really high losses. I think it's because the style of these notebooks is to create the parts all over the place so there might have been another 'model' variable in the namespace. In any case, move away from this style and start putting everything into functions and classes - especially the stuff that comes from udacity.</p>
<div class="highlight">
<pre><span></span>class Trainer:
    """Trains our model

    Args:
     data: data-iterator for training
     epochs: number of times to train on the data
     noise: factor for the amount of noise to add
     learning_rate: rate for the optimizer
    """
    def __init__(self, data: torch.utils.data.DataLoader, epochs: int=30,
                 noise:float=0.5,
                 learning_rate:float=0.001) -&gt; None:
        self.data = data
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.noise = noise
        self._criterion = None
        self._model = None
        self._device = None
        self._optimizer = None
        return

    @property
    def device(self) -&gt; torch.device:
        """CUDA or CPU"""
        if self._device is None:
            self._device = torch.device(
                "cuda:0" if torch.cuda.is_available() else "cpu")
        return self._device

    @property
    def criterion(self) -&gt; nn.MSELoss:
        """Loss-calculator"""
        if self._criterion is None:
            self._criterion = nn.MSELoss()
        return self._criterion

    @property
    def model(self) -&gt; ConvDenoiser:
        """Our model"""
        if self._model is None:
            self._model = ConvDenoiser()
            self.model.to(self.device)
        return self._model

    @property
    def optimizer(self) -&gt; torch.optim.Adam:
        """The gradient descent optimizer"""
        if self._optimizer is None:
            self._optimizer = torch.optim.Adam(self.model.parameters(),
                                               lr=self.learning_rate)
        return self._optimizer

    def __call__(self) -&gt; None:
        """Trains the model on the data"""
        self.model.train()
        started = datetime.now()
        for epoch in range(1, self.epochs + 1):
            train_loss = 0.0
            for batch in self.data:
                images, _ = batch
                images = images.to(self.device)
                ## add random noise to the input images
                noisy_imgs = (images
                              + self.noise
                              * torch.randn(*images.shape).to(self.device))
                # Clip the images to be between 0 and 1
                noisy_imgs = numpy.clip(noisy_imgs, 0., 1.).to(self.device)

                # clear the gradients of all optimized variables
                self.optimizer.zero_grad()
                ## forward pass: compute predicted outputs by passing *noisy* images to the model
                outputs = self.model(noisy_imgs)
                # calculate the loss
                # the "target" is still the original, not-noisy images
                loss = self.criterion(outputs, images)
                # backward pass: compute gradient of the loss with respect to model parameters
                loss.backward()
                # perform a single optimization step (parameter update)
                self.optimizer.step()
                # update running training loss
                train_loss += loss.item() * images.size(0)

            # print avg training statistics 
            train_loss = train_loss/len(train_loader)
            print('Epoch: {} \tTraining Loss: {:.6f}'.format(
                epoch, 
                train_loss
                ))
        ended = datetime.now()
        print("Ended: {}".format(ended))
        print("Elapsed: {}".format(ended - started))
        return
</pre></div>
<div class="highlight">
<pre><span></span>train_the_model = Trainer(train_loader)
train_the_model()
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 0.952294
Epoch: 2        Training Loss: 0.686571
Epoch: 3        Training Loss: 0.647284
Epoch: 4        Training Loss: 0.628790
Epoch: 5        Training Loss: 0.615522
Epoch: 6        Training Loss: 0.604566
Epoch: 7        Training Loss: 0.595838
Epoch: 8        Training Loss: 0.585816
Epoch: 9        Training Loss: 0.578257
Epoch: 10       Training Loss: 0.572502
Epoch: 11       Training Loss: 0.566983
Epoch: 12       Training Loss: 0.562720
Epoch: 13       Training Loss: 0.558449
Epoch: 14       Training Loss: 0.554410
Epoch: 15       Training Loss: 0.550995
Epoch: 16       Training Loss: 0.546916
Epoch: 17       Training Loss: 0.543798
Epoch: 18       Training Loss: 0.541859
Epoch: 19       Training Loss: 0.539242
Epoch: 20       Training Loss: 0.536748
Epoch: 21       Training Loss: 0.534675
Epoch: 22       Training Loss: 0.532690
Epoch: 23       Training Loss: 0.531692
Epoch: 24       Training Loss: 0.529910
Epoch: 25       Training Loss: 0.528826
Epoch: 26       Training Loss: 0.526354
Epoch: 27       Training Loss: 0.526260
Epoch: 28       Training Loss: 0.525294
Epoch: 29       Training Loss: 0.524029
Epoch: 30       Training Loss: 0.523341
Epoch: 31       Training Loss: 0.522387
Epoch: 32       Training Loss: 0.521689
Ended: 2018-12-22 14:10:08.869789
Elapsed: 0:14:14.036518
</pre></div>
</div>
<div class="outline-2" id="outline-container-org3366460">
<h2 id="org3366460">Checking out the results</h2>
<div class="outline-text-2" id="text-org3366460">
<p>Here I'm adding noise to the test images and passing them through the autoencoder. It does a suprising great job of removing the noise, even though it's sometimes difficult to tell what the original number is.</p>
<div class="highlight">
<pre><span></span># obtain one batch of test images
dataiter = iter(test_loader)
images, labels = dataiter.next()

# add noise to the test images
noisy_imgs = images + noise_factor * torch.randn(*images.shape)
noisy_imgs = numpy.clip(noisy_imgs, 0., 1.)

# get sample outputs
noisy_imgs = noisy_imgs.to(train_the_model.device)
output = train_the_model.model(noisy_imgs)
# prep images for display
noisy_imgs = noisy_imgs.cpu().numpy()

# output is resized into a batch of iages
output = output.view(BATCH_SIZE, 1, 28, 28)
# use detach when it's an output that requires_grad
output = output.detach().cpu().numpy()
</pre></div>
<div class="highlight">
<pre><span></span># plot the first ten input images and then reconstructed images
fig, axes = pyplot.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))

# input images on top row, reconstructions on bottom
for noisy_imgs, row in zip([noisy_imgs, output], axes):
    for img, ax in zip(noisy_imgs, row):
        ax.imshow(numpy.squeeze(img), cmap='gray')
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
</pre></div>
<div class="figure">
<p><img alt="de-noised.png" src="posts/nano/autoencoders/denoising-autoencoder/de-noised.png"></p>
</div>
<p>That did surprisingly well.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/autoencoders/convolutional-autoencoder/">Convolutional Autoencoder</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/autoencoders/convolutional-autoencoder/" rel="bookmark"><time class="published dt-published" datetime="2018-12-19T12:15:02-08:00" itemprop="datePublished" title="2018-12-19 12:15">2018-12-19 12:15</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org80ca7b3">Introduction</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org3cd04bc">Compressed Representation</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#orgf25eca8">Set Up</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org04fdaec">The Data</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org1e31c3b">Visualize the Data</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org91b1263">Convolutional Autoencoder</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org5635b1e">Transpose Convolutions, Decoder</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#org064d769">Initialize The NN</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#orgd43d271">Training</a></li>
<li><a href="posts/nano/autoencoders/convolutional-autoencoder/#orgb7a7622">Checking out the results</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org80ca7b3">
<h2 id="org80ca7b3">Introduction</h2>
<div class="outline-text-2" id="text-org80ca7b3">
<p>Sticking with the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset, let's improve our autoencoder's performance using convolutional layers. We'll build a convolutional autoencoder to compress the MNIST dataset.</p>
<ul class="org-ul">
<li>The encoder portion will be made of convolutional and pooling layers and the decoder will be made of <b>transpose convolutional layers</b> that learn to "upsample" a compressed representation.</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org3cd04bc">
<h2 id="org3cd04bc">Compressed Representation</h2>
<div class="outline-text-2" id="text-org3cd04bc">
<p>A compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or other kinds of reconstruction and transformation!</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgf25eca8">
<h2 id="orgf25eca8">Set Up</h2>
<div class="outline-text-2" id="text-orgf25eca8"></div>
<div class="outline-3" id="outline-container-orgdb2f9da">
<h3 id="orgdb2f9da">Imports</h3>
<div class="outline-text-3" id="text-orgdb2f9da"></div>
<div class="outline-4" id="outline-container-org8e2e9c5">
<h4 id="org8e2e9c5">Python Standard Library</h4>
<div class="outline-text-4" id="text-org8e2e9c5">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgffb2a4f">
<h4 id="orgffb2a4f">From PyPi</h4>
<div class="outline-text-4" id="text-orgffb2a4f">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Graph</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgdcaf183">
<h3 id="orgdcaf183">Plotting</h3>
<div class="outline-text-3" id="text-orgdcaf183">
<div class="highlight">
<pre><span></span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Open Sans"</span><span class="p">,</span> <span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgeddc6a9">
<h3 id="orgeddc6a9">Test for <a href="http://pytorch.org/docs/stable/cuda.html">CUDA</a></h3>
<div class="outline-text-3" id="text-orgeddc6a9">
<p>The test-code uses the check later on so I'll save it to the <code>train_on_gpu</code> variable.</p>
<div class="highlight">
<pre><span></span><span class="n">train_on_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda:0"</span> <span class="k">if</span> <span class="n">train_on_gpu</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Using: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
</pre></div>
<pre class="example">
Using: cuda:0

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org04fdaec">
<h2 id="org04fdaec">The Data</h2>
<div class="outline-text-2" id="text-org04fdaec"></div>
<div class="outline-3" id="outline-container-orgd4ec668">
<h3 id="orgd4ec668">Setup the Data Transform</h3>
<div class="outline-text-3" id="text-orgd4ec668">
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgaf63edb">
<h3 id="orgaf63edb">Load the Training and Test Datasets</h3>
<div class="outline-text-3" id="text-orgaf63edb">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/datasets/MNIST/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">is_dir</span><span class="p">())</span>
</pre></div>
<pre class="example">
/home/hades/datasets/MNIST
True

</pre>
<div class="highlight">
<pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                            <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                           <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfa982c1">
<h3 id="orgfa982c1">Create training and test dataloaders</h3>
<div class="outline-text-3" id="text-orgfa982c1">
<div class="highlight">
<pre><span></span><span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># how many samples per batch to load</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org681f735">
<h3 id="org681f735">Prepare Data Loaders</h3>
<div class="outline-text-3" id="text-org681f735">
<div class="highlight">
<pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> 
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                           <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1e31c3b">
<h2 id="org1e31c3b">Visualize the Data</h2>
<div class="outline-text-2" id="text-org1e31c3b"></div>
<div class="outline-3" id="outline-container-orgf1348ab">
<h3 id="orgf1348ab">Obtain One Batch of Training Images</h3>
<div class="outline-text-3" id="text-orgf1348ab">
<div class="highlight">
<pre><span></span><span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0f78314">
<h3 id="org0f78314">Get One Image From the Batch</h3>
<div class="outline-text-3" id="text-org0f78314">
<div class="highlight">
<pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8c91f61">
<h3 id="org8c91f61">Plot</h3>
<div class="outline-text-3" id="text-org8c91f61">
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"First Image"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="first_image.png" src="posts/nano/autoencoders/convolutional-autoencoder/first_image.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org91b1263">
<h2 id="org91b1263">Convolutional Autoencoder</h2>
<div class="outline-text-2" id="text-org91b1263"></div>
<div class="outline-3" id="outline-container-org18b31dd">
<h3 id="org18b31dd">Encoder</h3>
<div class="outline-text-3" id="text-org18b31dd">
<p>The encoder part of the network will be a typical convolutional pyramid. Each convolutional layer will be followed by a max-pooling layer to reduce the dimensions of the layers.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org58d4160">
<h3 id="org58d4160">Decoder</h3>
<div class="outline-text-3" id="text-org58d4160">
<p>The decoder, though, might be something new to you. The decoder needs to convert from a narrow representation to a wide, reconstructed image. For example, the representation could be a 7x7x4 max-pool layer. This is the output of the encoder, but also the input to the decoder. We want to get a 28x28x1 image out from the decoder so we need to work our way back up from the compressed representation. A schematic of the network is shown below.</p>
<div class="highlight">
<pre><span></span><span class="n">graph</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="n">format</span><span class="o">=</span><span class="s2">"png"</span><span class="p">)</span>

<span class="c1"># Input layer</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"a"</span><span class="p">,</span> <span class="s2">"28x28x1 Input"</span><span class="p">)</span>

<span class="c1"># the Encoder</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"b"</span><span class="p">,</span> <span class="s2">"28x28x16 Convolution"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"c"</span><span class="p">,</span> <span class="s2">"14x14x16 MaxPool"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"d"</span><span class="p">,</span> <span class="s2">"14x14x4 Convolution"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"e"</span><span class="p">,</span> <span class="s2">"7x7x4 MaxPool"</span><span class="p">)</span>

<span class="c1"># The Decoder</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"f"</span><span class="p">,</span> <span class="s2">"14x14x16 Transpose Convolution"</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"g"</span><span class="p">,</span> <span class="s2">"28x28x1 Transpose Convolution"</span><span class="p">)</span>

<span class="c1"># The Output</span>
<span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s2">"h"</span><span class="p">,</span> <span class="s2">"28x28x1 Output"</span><span class="p">)</span>

<span class="n">edges</span> <span class="o">=</span> <span class="s2">"abcdefgh"</span>
<span class="n">graph</span><span class="o">.</span><span class="n">edges</span><span class="p">([</span><span class="n">edges</span><span class="p">[</span><span class="n">edge</span><span class="p">]</span> <span class="o">+</span> <span class="n">edges</span><span class="p">[</span><span class="n">edge</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">edge</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>

<span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">"graphs/network_graph.dot"</span><span class="p">)</span>
<span class="n">graph</span>
</pre></div>
<div class="figure">
<p><img alt="network_graph.dot.png" src="posts/files/posts/nano/autoencoders/convolutional-autoencoder/network_graph.dot.png"></p>
</div>
<div class="figure">
<p><img alt="network_graph.dot.png" src="posts/nano/autoencoders/convolutional-autoencoder/network_graph.dot.png"></p>
</div>
<p>Here our final encoder layer has size 7x7x4 = 196. The original images have size 28x28 = 784, so the encoded vector is 25% the size of the original image. These are just suggested sizes for each of the layers. Feel free to change the depths and sizes, in fact, you're encouraged to add additional layers to make this representation even smaller! Remember our goal here is to find a small representation of the input data.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5635b1e">
<h2 id="org5635b1e">Transpose Convolutions, Decoder</h2>
<div class="outline-text-2" id="text-org5635b1e">
<p>This decoder uses <b>transposed convolutional</b> layers to increase the width and height of the input layers. They work almost exactly the same as convolutional layers, but in reverse. A stride in the input layer results in a larger stride in the transposed convolution layer. For example, if you have a 3x3 kernel, a 3x3 patch in the input layer will be reduced to one unit in a convolutional layer. Comparatively, one unit in the input layer will be expanded to a 3x3 path in a transposed convolution layer. PyTorch provides us with an easy way to create the layers, <a href="https://pytorch.org/docs/stable/nn.html#convtranspose2d"><code>nn.ConvTranspose2d</code></a>.</p>
<p>It is important to note that transpose convolution layers can lead to artifacts in the final images, such as checkerboard patterns. This is due to overlap in the kernels which can be avoided by setting the stride and kernel size equal. In <a href="http://distill.pub/2016/deconv-checkerboard/">this Distill article</a> from Augustus Odena, <b>et al</b>, the authors show that these checkerboard artifacts can be avoided by resizing the layers using nearest neighbor or bilinear interpolation (upsampling) followed by a convolutional layer.</p>
<p>We'll show this approach in another notebook, so you can experiment with it and see the difference.</p>
<ul class="org-ul">
<li>Build the encoder out of a series of convolutional and pooling layers.</li>
<li>When building the decoder, recall that transpose convolutional layers can upsample an input by a factor of 2 using a stride and kernel_size of 2.</li>
</ul>
<p>See:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d">Conv2d</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html?highlight=maxpool#torch.nn.MaxPool2d">MaxPool2d</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html#relu">ReLU</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html#sigmoid">Sigmoid</a></li>
</ul>
<p>To get the output size of our Convolutional Layers you use the formula:</p>
<p>\[ o = \frac{W - F + 2P}{S} + 1 \]</p>
<p>Where <i>W</i> is the input size (28 here), <i>F</i> is the filter size, <i>P</i> is the zero-padding, and <i>S</i> is the stride. For our first layer we want to keep the output the same size as the input.</p>
<p>The output for a maxpool layer uses a similar set of equations.</p>
\begin{align} W_2 &amp;= \frac{W_1 - F}{S} + 1\\ H_2 &amp;= \frac{H_Y - F}{S} + 1\\ D_2 = D_1\\ \end{align}
<p>Where <i>W</i> is the width, <i>H</i> is the height, and <i>D</i> is the depth.</p>
<div class="highlight">
<pre><span></span><span class="n">Layer</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Layer"</span><span class="p">,</span> <span class="s2">"kernel stride depth padding"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="n">Layer</span><span class="o">.</span><span class="fm">__new__</span><span class="o">.</span><span class="vm">__defaults__</span><span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
<span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">layer</span><span class="p">:</span> <span class="n">Layer</span><span class="p">,</span> <span class="n">expected</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Calculates the output size of the layer</span>

<span class="sd">    Args:</span>
<span class="sd">     input_size: the size of the input to the layer</span>
<span class="sd">     layer: named tuple with values for the layer</span>
<span class="sd">     expected: the value you are expecting</span>

<span class="sd">    Returns:</span>
<span class="sd">     the size of the output</span>

<span class="sd">    Raises:</span>
<span class="sd">     AssertionError: the calculated value wasn't the expected one</span>
<span class="sd">    """</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">((</span><span class="n">input_size</span> <span class="o">-</span> <span class="n">layer</span><span class="o">.</span><span class="n">kernel</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">layer</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span><span class="o">/</span><span class="n">layer</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Layer Output Size: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    <span class="k">assert</span> <span class="n">size</span> <span class="o">==</span> <span class="n">expected</span>
    <span class="k">return</span> <span class="n">size</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orge263071">
<h3 id="orge263071">The Encoder Layers</h3>
<div class="outline-text-3" id="text-orge263071"></div>
<div class="outline-4" id="outline-container-org1276ec1">
<h4 id="org1276ec1">Layer One</h4>
<div class="outline-text-4" id="text-org1276ec1">
<p>The first layer is a Convolutional Layer that we want to have the same size output as the input but with a depth of sixteen. The <a href="https://cs231n.github.io/convolutional-networks/">CS 231</a> page notes that to keep the size of the output the same as the input you should set the stride to one and once you have decided on your kernle size (<i>F</i>) then you can find your padding using this equation:</p>
<p>\[ P = \frac{F - 1}{2} \]</p>
<p>In this case I'm going to use a filter size of three so our padding will be:</p>
\begin{align} P &amp;= \frac{3 - 1}{2}\\ &amp;= 1\\ \end{align}
<p>We can double-check this by plugging the values back intoo the equation for output size.</p>
\begin{align} W' &amp;= \frac{W - F + 2P}{S} + 1\\ &amp;= \frac{28 - 3 + 2(1)}{1} + 1\\ &amp;= 28\\ \end{align}
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-left"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Variable</th>
<th class="org-left" scope="col">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><i>W</i></td>
<td class="org-left">One dimension of the input</td>
</tr>
<tr>
<td class="org-left"><i>F</i></td>
<td class="org-left">One dimension of the Kernel (filter)</td>
</tr>
<tr>
<td class="org-left"><i>S</i></td>
<td class="org-left">Stride</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span> <span class="n">layer_one</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                   <span class="n">padding</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                   <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
                   <span class="n">depth</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>

 <span class="n">INPUT_ONE</span> <span class="o">=</span> <span class="mi">28</span>
 <span class="n">OUTPUT_ONE</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">(</span><span class="n">INPUT_ONE</span><span class="p">,</span> <span class="n">layer_one</span><span class="p">,</span> <span class="n">INPUT_ONE</span><span class="p">)</span>
 <span class="n">INPUT_DEPTH</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, depth=16, padding=1)
Layer Output Size: 28.0

</pre></div>
</div>
<div class="outline-4" id="outline-container-org5c4b6b4">
<h4 id="org5c4b6b4">Layer Two</h4>
<div class="outline-text-4" id="text-org5c4b6b4">
<p>The second layer is a MaxPool layer that will keep the depth of six but will halve the size to fourteen. According to the <a href="https://cs231n.github.io/convolutional-networks/">CS 231 n</a> page on Convolutional Networks, there are only two values for the kernel size that are usually used - 2 and 3, and the stride is usually just 2, with a kernel size of 2 being more common, and as it turns out, a kernel size of 2 and a stride of 2 will reduce our input dimensions by a half, which is what we want.</p>
\begin{align} W &amp;= \frac{28 - 2}{2} + 1\\ &amp;= 14\\ \end{align}
<div class="highlight">
<pre><span></span> <span class="n">layer_two</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">depth</span><span class="p">)</span>
 <span class="n">OUTPUT_TWO</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">(</span><span class="n">OUTPUT_ONE</span><span class="p">,</span> <span class="n">layer_two</span><span class="p">,</span> <span class="mi">14</span><span class="p">)</span>
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, depth=16, padding=0)
Layer Output Size: 14.0

</pre></div>
</div>
<div class="outline-4" id="outline-container-org3877e5d">
<h4 id="org3877e5d">Layer Three</h4>
<div class="outline-text-4" id="text-org3877e5d">
<p>Our third layer is another convolutional layer that preserves the input width and height but this time the output will have a depth of 4.</p>
<div class="highlight">
<pre><span></span><span class="n">layer_three</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">OUTPUT_THREE</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">(</span><span class="n">OUTPUT_TWO</span><span class="p">,</span> <span class="n">layer_three</span><span class="p">,</span> <span class="n">OUTPUT_TWO</span><span class="p">)</span>
</pre></div>
<pre class="example">
Layer(kernel=3, stride=1, depth=4, padding=1)
Layer Output Size: 14.0

</pre></div>
</div>
<div class="outline-4" id="outline-container-org2c7ca44">
<h4 id="org2c7ca44">Layer Four</h4>
<div class="outline-text-4" id="text-org2c7ca44">
<p>The last layer in the encoder is a max pool layer that reduces the previous layer by half (to dimensions of 7) while preserving the depth.</p>
<div class="highlight">
<pre><span></span><span class="n">layer_four</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">depth</span><span class="p">)</span>
<span class="n">OUTPUT_FOUR</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">(</span><span class="n">OUTPUT_THREE</span><span class="p">,</span> <span class="n">layer_four</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
<pre class="example">
Layer(kernel=2, stride=2, depth=4, padding=0)
Layer Output Size: 7.0

</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7ee4fb6">
<h3 id="org7ee4fb6">Decoders</h3>
<div class="outline-text-3" id="text-org7ee4fb6"></div>
<div class="outline-4" id="outline-container-org295e39a">
<h4 id="org295e39a">Layer Five</h4>
<div class="outline-text-4" id="text-org295e39a">
<p>We want an output of 14 x 14 x 16 from an input of 7 x 7 x 4. The comments given with this exercise say that using a kernel of 2 and stride of 2 will double the dimensions, much as those same values halve the dimensions with Max-Pooling.</p>
<div class="highlight">
<pre><span></span><span class="n">layer_five</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgce5c4bf">
<h4 id="orgce5c4bf">Layer Six</h4>
<div class="outline-text-4" id="text-orgce5c4bf">
<p>This layer will expand the image back to its original size of 28 x 28 x 1</p>
<div class="highlight">
<pre><span></span><span class="n">layer_six</span> <span class="o">=</span> <span class="n">Layer</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgbfc7fef">
<h3 id="orgbfc7fef">Define the NN Architecture</h3>
<div class="outline-text-3" id="text-orgbfc7fef">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">ConvAutoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""A CNN AutoEncoder-Decoder"""</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1">## encoder layers ##</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">INPUT_DEPTH</span><span class="p">,</span>
                                       <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
                                       <span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> 
                                       <span class="n">stride</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                       <span class="n">padding</span><span class="o">=</span><span class="n">layer_one</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_two</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                                       <span class="n">stride</span><span class="o">=</span><span class="n">layer_two</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">convolution_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">layer_two</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
                                       <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
                                       <span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
                                       <span class="n">stride</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                       <span class="n">padding</span><span class="o">=</span><span class="n">layer_three</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>

        <span class="c1">## decoder layers ##</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose_convolution_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_four</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span> 
            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_five</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_five</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">layer_five</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transpose_convolution_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">layer_five</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span> 
            <span class="n">out_channels</span><span class="o">=</span><span class="n">layer_six</span><span class="o">.</span><span class="n">depth</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">layer_six</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">layer_six</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="c1">## encode ##</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convolution_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convolution_2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="c1">## decode ##</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transpose_convolution_1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transpose_convolution_2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="n">ConvAutoencoder</span><span class="p">()</span>
<span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">convolution_1</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">max_pool_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">convolution_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">max_pool_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">transpose_convolution_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">transpose_convolution_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span>
</pre></div>
<pre class="example">
torch.Size([20, 16, 28, 28])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 4, 14, 14])
torch.Size([20, 4, 7, 7])
torch.Size([20, 4, 7, 7])
torch.Size([20, 16, 14, 14])
torch.Size([20, 16, 14, 14])
torch.Size([20, 1, 28, 28])

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org064d769">
<h2 id="org064d769">Initialize The NN</h2>
<div class="outline-text-2" id="text-org064d769">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ConvAutoencoder</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
<pre class="example">
ConvAutoencoder(
  (convolution_1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (convolution_2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (transpose_convolution_1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
  (transpose_convolution_2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))
  (relu): ReLU()
  (sigmoid): Sigmoid()
)

</pre></div>
</div>
<div class="outline-2" id="outline-container-orgd43d271">
<h2 id="orgd43d271">Training</h2>
<div class="outline-text-2" id="text-orgd43d271">
<p>Here I'll write a bit of code to train the network. I'm not too interested in validation here, so I'll just monitor the training loss and the test loss afterwards.</p>
<p>We are not concerned with labels in this case, just images, which we can get from the <code>train_loader</code>. Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use <a href="https://pytorch.org/docs/stable/nn.html?highlight=mseloss#torch.nn.MSELoss">MSELoss</a>. And compare output images and input images as follows:</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
</pre></div>
<p>Otherwise, this is pretty straightfoward training with PyTorch. Since this is a convolutional autoencoder, our images <i>do not</i> need to be flattened before being passed in an input to our model.</p>
</div>
<div class="outline-3" id="outline-container-org1ac55a3">
<h3 id="org1ac55a3">Train the Model</h3>
<div class="outline-text-3" id="text-org1ac55a3">
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">started</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># monitor training loss</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1">###################</span>
    <span class="c1"># train the model #</span>
    <span class="c1">###################</span>

    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="c1"># _ stands in for labels, here</span>
        <span class="c1"># no need to flatten images</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">data</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># clear the gradients of all optimized variables</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># forward pass: compute predicted outputs by passing inputs to the model</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="c1"># calculate the loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
        <span class="c1"># backward pass: compute gradient of the loss with respect to model parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># perform a single optimization step (parameter update)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># update running training loss</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">*</span><span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># print avg training statistics </span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Epoch: {} </span><span class="se">\t</span><span class="s1">Training Loss: {:.6f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">epoch</span><span class="p">,</span> 
        <span class="n">train_loss</span>
        <span class="p">))</span>
<span class="n">ended</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Ended: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ended</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">"Elapsed: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ended</span> <span class="o">-</span> <span class="n">started</span><span class="p">))</span>
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 0.259976
Epoch: 2        Training Loss: 0.244956
Epoch: 3        Training Loss: 0.235354
Epoch: 4        Training Loss: 0.226544
Epoch: 5        Training Loss: 0.216255
Epoch: 6        Training Loss: 0.207204
Epoch: 7        Training Loss: 0.200490
Epoch: 8        Training Loss: 0.195582
Epoch: 9        Training Loss: 0.191870
Epoch: 10       Training Loss: 0.189247
Epoch: 11       Training Loss: 0.187027
Epoch: 12       Training Loss: 0.185084
Epoch: 13       Training Loss: 0.183055
Epoch: 14       Training Loss: 0.181224
Epoch: 15       Training Loss: 0.179749
Epoch: 16       Training Loss: 0.178564
Epoch: 17       Training Loss: 0.177572
Epoch: 18       Training Loss: 0.176735
Epoch: 19       Training Loss: 0.176076
Epoch: 20       Training Loss: 0.175518
Epoch: 21       Training Loss: 0.175040
Epoch: 22       Training Loss: 0.174629
Epoch: 23       Training Loss: 0.174230
Epoch: 24       Training Loss: 0.173856
Epoch: 25       Training Loss: 0.173497
Epoch: 26       Training Loss: 0.173166
Epoch: 27       Training Loss: 0.172838
Epoch: 28       Training Loss: 0.172520
Epoch: 29       Training Loss: 0.172212
Epoch: 30       Training Loss: 0.171920
Ended: 2018-12-21 17:41:26.461977
Elapsed: 0:07:50.942721
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb7a7622">
<h2 id="orgb7a7622">Checking out the results</h2>
<div class="outline-text-2" id="text-orgb7a7622">
<p>Below I've plotted some of the test images along with their reconstructions. These look a little rough around the edges, likely due to the checkerboard effect we mentioned above that tends to happen with transpose layers.</p>
</div>
<div class="outline-3" id="outline-container-org6422dd3">
<h3 id="org6422dd3">Obtain One Batch Of Test Images</h3>
<div class="outline-text-3" id="text-org6422dd3">
<div class="highlight">
<pre><span></span><span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf9c2c60">
<h3 id="orgf9c2c60">Get Sample Outputs</h3>
<div class="outline-text-3" id="text-orgf9c2c60">
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0579256">
<h3 id="org0579256">Prep Images for Display</h3>
<div class="outline-text-3" id="text-org0579256">
<div class="highlight">
<pre><span></span><span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7ce209d">
<h3 id="org7ce209d">Output Is Resized Into a Batch Of Images</h3>
<div class="outline-text-3" id="text-org7ce209d">
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7392c9b">
<h3 id="org7392c9b">Use Detach When It's An Output That Requires Grad</h3>
<div class="outline-text-3" id="text-org7392c9b">
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org82bc111">
<h3 id="org82bc111">plot the first ten input images and then reconstructed images</h3>
<div class="outline-text-3" id="text-org82bc111">
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Auto-Encoded/Decoded Images"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="c1"># input images on top row, reconstructions on bottom</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">images</span><span class="p">,</span> <span class="n">output</span><span class="p">],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="reconstructed.png" src="posts/nano/autoencoders/convolutional-autoencoder/reconstructed.png"></p>
</div>
<p>That is better than I would have thought it would be.</p>
</div>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="." rel="prev">Newer posts</a></li>
<li class="next"><a href="index-6.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer">Contents © 2019 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script><!-- fancy dates -->
<script>

    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
</script><!-- end fancy dates -->
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
