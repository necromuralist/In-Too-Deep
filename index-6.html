<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>In Too Deep (old posts, page 6) | In Too Deep</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/index-6.html" rel="canonical">
<link href="index-7.html" rel="prev" type="text/html">
<link href="index-5.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript">
</script>
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/autoencoders/simple-autoencoder/">Simple Autoencoder</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/autoencoders/simple-autoencoder/" rel="bookmark"><time class="published dt-published" datetime="2018-12-17T23:30:13-08:00" itemprop="datePublished" title="2018-12-17 23:30">2018-12-17 23:30</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/autoencoders/simple-autoencoder/#orgda965d9">Introduction</a></li>
<li><a href="posts/nano/autoencoders/simple-autoencoder/#orgbc4b7f7">Compressed Representation</a></li>
<li><a href="posts/nano/autoencoders/simple-autoencoder/#org0e89189">Set Up</a></li>
<li><a href="posts/nano/autoencoders/simple-autoencoder/#orgea419cf">Visualize the Data</a></li>
<li><a href="posts/nano/autoencoders/simple-autoencoder/#org7845082">Linear Autoencoder</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgda965d9">
<h2 id="orgda965d9">Introduction</h2>
<div class="outline-text-2" id="text-orgda965d9">
<p>We'll start off by building a simple autoencoder to compress the MNIST dataset. With autoencoders, we pass input data through an encoder that makes a compressed representation of the input. Then, this representation is passed through a decoder to reconstruct the input data. Generally the encoder and decoder will be built with neural networks, then trained on example data.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgbc4b7f7">
<h2 id="orgbc4b7f7">Compressed Representation</h2>
<div class="outline-text-2" id="text-orgbc4b7f7">
<p>A compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or other kinds of reconstruction and transformation!</p>
</div>
</div>
<div class="outline-2" id="outline-container-org0e89189">
<h2 id="org0e89189">Set Up</h2>
<div class="outline-text-2" id="text-org0e89189">
<p>In this notebook, we'll be build a simple network architecture for the encoder and decoder. Let's get started by importing our libraries and getting the dataset.</p>
</div>
<div class="outline-3" id="outline-container-org4003423">
<h3 id="org4003423">Imports</h3>
<div class="outline-text-3" id="text-org4003423"></div>
<div class="outline-4" id="outline-container-org93b7e6e">
<h4 id="org93b7e6e">PyPi</h4>
<div class="outline-text-4" id="text-org93b7e6e">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org9615b90">
<h4 id="org9615b90">This Project</h4>
<div class="outline-text-4" id="text-org9615b90">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPathTwo</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfa39bca">
<h3 id="orgfa39bca">Plotting</h3>
<div class="outline-text-3" id="text-orgfa39bca">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Open Sans"</span><span class="p">,</span> <span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org18cf017">
<h3 id="org18cf017">The Data</h3>
<div class="outline-text-3" id="text-org18cf017"></div>
<div class="outline-4" id="outline-container-org5675c30">
<h4 id="org5675c30">Data Transformer</h4>
<div class="outline-text-4" id="text-org5675c30">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1752430">
<h4 id="org1752430">Load the Data</h4>
<div class="outline-text-4" id="text-org1752430">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"MNIST"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">)</span>
</pre></div>
<pre class="example">
/home/hades/datasets/MNIST

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                            <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                           <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org306769b">
<h4 id="org306769b">Training and Test Batch Loaders</h4>
<div class="outline-text-4" id="text-org306769b"></div>
<ul class="org-ul">
<li><a id="org8692829"></a>Some Constants<br>
<div class="outline-text-5" id="text-org8692829">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="c1"># number of subprocesses to use for data loading</span>
<span class="n">NUM_WORKERS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># how many samples per batch to load</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
<p>Prepare the loaders.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                           <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                          <span class="n">num_workers</span><span class="o">=</span><span class="n">NUM_WORKERS</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgea419cf">
<h2 id="orgea419cf">Visualize the Data</h2>
<div class="outline-text-2" id="text-orgea419cf"></div>
<div class="outline-3" id="outline-container-org880f257">
<h3 id="org880f257">Obtain One Batch of Training Images</h3>
<div class="outline-text-3" id="text-org880f257">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0f44ade">
<h3 id="org0f44ade">Get One Image From the Batch</h3>
<div class="outline-text-3" id="text-org0f44ade">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"First Image"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="first_image.png" src="posts/nano/autoencoders/simple-autoencoder/first_image.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7845082">
<h2 id="org7845082">Linear Autoencoder</h2>
<div class="outline-text-2" id="text-org7845082"></div>
<div class="outline-3" id="outline-container-org279ca33">
<h3 id="org279ca33">Description</h3>
<div class="outline-text-3" id="text-org279ca33">
<p>We'll train an autoencoder with these images by flattening them into 784 length vectors. The images from this dataset are already normalized such that the values are between 0 and 1. Let's start by building a simple autoencoder. The encoder and decoder should be made of <b>one linear layer</b>. The units that connect the encoder and decoder will be the <i>compressed representation</i>.</p>
<p>Since the images are normalized between 0 and 1, we need to use a <b>sigmoid activation on the output layer</b> to get values that match this input value range.</p>
<ul class="org-ul">
<li>The input images will be flattened into 784 length vectors. The targets are the same as the inputs.</li>
<li>The encoder and decoder will be made of two linear layers, each.</li>
<li>The depth dimensions should change as follows: 784 inputs &gt; <b>encoding_dim</b> &gt; 784 outputs.</li>
<li>All layers will have ReLu activations applied except for the final output layer, which has a sigmoid activation.</li>
</ul>
<p><b>The compressed representation should be a vector with dimension <code>encoding_dim=32</code>.</b></p>
</div>
</div>
<div class="outline-3" id="outline-container-org6f9d990">
<h3 id="org6f9d990">Architecture Definition</h3>
<div class="outline-text-3" id="text-org6f9d990">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span>
<span class="n">IMAGE_DIMENSION</span> <span class="o">=</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">columns</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Autoencoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">""""" simple autoencoder-decoder</span>

<span class="sd">    Args:</span>
<span class="sd">     encoding_dim: the dimension of the encoded image</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoding_dim</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">IMAGE_DIMENSION</span><span class="p">,</span> <span class="n">encoding_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_one</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">,</span> <span class="n">IMAGE_DIMENSION</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation_output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="k">return</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Does one feed-forward pass</span>

<span class="sd">       Args:</span>
<span class="sd">        x: flattened MNIST image</span>

<span class="sd">       Returns:</span>
<span class="sd">        the encoded-decoded version of the image</span>
<span class="sd">       """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_one</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation_output</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga8e6158">
<h3 id="orga8e6158">Initialize the Auto-Encoder</h3>
<div class="outline-text-3" id="text-orga8e6158">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">encoding_dim</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Autoencoder</span><span class="p">(</span><span class="n">encoding_dim</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example">
Autoencoder(
  (encoder): Linear(in_features=784, out_features=32, bias=True)
  (activation_one): ReLU()
  (decoder): Linear(in_features=32, out_features=784, bias=True)
  (activation_output): Sigmoid()
)

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgd9879ac">
<h3 id="orgd9879ac">Training</h3>
<div class="outline-text-3" id="text-orgd9879ac">
<p>Here I'll write a bit of code to train the network. I'm not too interested in validation here, so I'll just monitor the training loss and the test loss afterwards.</p>
<p>We are not concerned with labels in this case, just images, which we can get from the <code>train_loader</code>. Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing <i>quantities</i> rather than probabilistic values. So, in this case, I'll use <a href="https://pytorch.org/docs/stable/nn.html?highlight=mseloss#torch.nn.MSELoss"><code>MSELoss</code></a>, which calculates the Mean-Squared Error between the predicted and the actual value, and compare output images and input images as follows:</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
</pre></div>
<p>Otherwise, this is pretty straightfoward training with PyTorch. We flatten our images, pass them into the autoencoder, and record the training loss as we go.</p>
</div>
<div class="outline-4" id="outline-container-org3b3b3c1">
<h4 id="org3b3b3c1">Specify the Loss Function</h4>
<div class="outline-text-4" id="text-org3b3b3c1">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org9b636d2">
<h4 id="org9b636d2">Specifiy the Optimizer</h4>
<div class="outline-text-4" id="text-org9b636d2">
<p>We're going to use the <a href="https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam">Adam</a> optimizer instead of Stochastic Gradient Descent.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc251b05">
<h4 id="orgc251b05">And Now We Train</h4>
<div class="outline-text-4" id="text-orgc251b05">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># monitor training loss</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1">###################</span>
    <span class="c1"># train the model #</span>
    <span class="c1">###################</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="c1"># _ stands in for labels, here</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">data</span>
        <span class="c1"># flatten images</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># clear the gradients of all optimized variables</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="c1"># forward pass: compute predicted outputs by passing inputs to the model</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="c1"># calculate the loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">images</span><span class="p">)</span>
        <span class="c1"># backward pass: compute gradient of the loss with respect to model parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="c1"># perform a single optimization step (parameter update)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="c1"># update running training loss</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">*</span><span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># print avg training statistics </span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Epoch: {} </span><span class="se">\t</span><span class="s1">Training Loss: {:.6f}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">epoch</span><span class="p">,</span> 
        <span class="n">train_loss</span>
        <span class="p">))</span>
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 0.622334
Epoch: 2        Training Loss: 0.297601
Epoch: 3        Training Loss: 0.258895
Epoch: 4        Training Loss: 0.250710
Epoch: 5        Training Loss: 0.247124
Epoch: 6        Training Loss: 0.244808
Epoch: 7        Training Loss: 0.243222
Epoch: 8        Training Loss: 0.242119
Epoch: 9        Training Loss: 0.241254
Epoch: 10       Training Loss: 0.240563
Epoch: 11       Training Loss: 0.239997
Epoch: 12       Training Loss: 0.239529
Epoch: 13       Training Loss: 0.239120
Epoch: 14       Training Loss: 0.238747
Epoch: 15       Training Loss: 0.238395
Epoch: 16       Training Loss: 0.238030
Epoch: 17       Training Loss: 0.237546
Epoch: 18       Training Loss: 0.237213
Epoch: 19       Training Loss: 0.236916
Epoch: 20       Training Loss: 0.236473
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org674ceb0">
<h3 id="org674ceb0">Checking out the results</h3>
<div class="outline-text-3" id="text-org674ceb0">
<p>Below I've plotted some of the test images along with their reconstructions. For the most part these look pretty good except for some blurriness in some parts.</p>
</div>
<div class="outline-4" id="outline-container-org91110e5">
<h4 id="org91110e5">Obtain One Batch Of Test Images</h4>
<div class="outline-text-4" id="text-org91110e5">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">dataiter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">dataiter</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>

<span class="n">images_flatten</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># get sample outputs</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images_flatten</span><span class="p">)</span>
<span class="c1"># prep images for display</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>


<span class="c1"># output is resized into a batch of images</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="c1"># use detach when it's an output that requires_grad</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">figure</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="c1"># input images on top row, reconstructions on bottom</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">images</span><span class="p">,</span> <span class="n">output</span><span class="p">],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_xaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="recomposed.png" src="posts/nano/autoencoders/simple-autoencoder/recomposed.png"></p>
</div>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/cnn/weight-initialization/">Weight Initialization</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/cnn/weight-initialization/" rel="bookmark"><time class="published dt-published" datetime="2018-12-17T13:03:41-08:00" itemprop="datePublished" title="2018-12-17 13:03">2018-12-17 13:03</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/cnn/weight-initialization/#org22828b9">Introduction</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#org4a9a03e">Initial Weights and Observing Training Loss</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#org697d6d7">Dataset and Model</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#org66482df">Import Libraries and Load the Data</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#orga4e6210">Visualize Some Training Data</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#org40f5ee9">Define the Model Architecture</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#org160d8ed">Initialize Weights</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#orgbb4cc13">Compare Model Behavior</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#orgf2058c1">General rule for setting weights</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#org0dd5d23">Normal Distribution</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#org403aeb2">Automatic Initialization</a></li>
<li><a href="posts/nano/cnn/weight-initialization/#org95ad8ba">evaluate the behavior using helpers</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org22828b9">
<h2 id="org22828b9">Introduction</h2>
<div class="outline-text-2" id="text-org22828b9">
<p>In this lesson, you'll learn how to find good initial weights for a neural network. Weight initialization happens once, when a model is created and before it trains. Having good initial weights can place the neural network close to the optimal solution. This allows the neural network to come to the best solution quicker.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org4a9a03e">
<h2 id="org4a9a03e">Initial Weights and Observing Training Loss</h2>
<div class="outline-text-2" id="text-org4a9a03e">
<p>To see how different weights perform, we'll test on the same dataset and neural network. That way, we know that any changes in model behavior are due to the weights and not any changing data or model structure. We'll instantiate at least two of the same models, with <i>different</i> initial weights and see how the training loss decreases over time.</p>
<p>Sometimes the differences in training loss, over time, will be large and other times, certain weights offer only small improvements.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org697d6d7">
<h2 id="org697d6d7">Dataset and Model</h2>
<div class="outline-text-2" id="text-org697d6d7">
<p>We'll train an MLP to classify images from the <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST database</a> to demonstrate the effect of different initial weights. As a reminder, the FashionMNIST dataset contains images of clothing types; <code>classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']</code>. The images are normalized so that their pixel values are in a range [0.0 - 1.0). Run the cell below to download and load the dataset.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org66482df">
<h2 id="org66482df">Import Libraries and Load the <a href="http://pytorch.org/docs/stable/torchvision/datasets.html">Data</a></h2>
<div class="outline-text-2" id="text-org66482df"></div>
<div class="outline-3" id="outline-container-org1805f31">
<h3 id="org1805f31">Imports</h3>
<div class="outline-text-3" id="text-org1805f31">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span># python
from functools import partial
from typing import Collection, Tuple
# from pypi
from dotenv import load_dotenv
from sklearn.model_selection import train_test_split
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision import datasets
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms

# udacity
import nano.helpers as helpers

# this project
from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd68f7a3">
<h3 id="orgd68f7a3">Load the Data</h3>
<div class="outline-text-3" id="text-orgd68f7a3">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span># number of subprocesses to use for data loading
subprocesses = 0
# how many samples per batch to load
batch_size = 100
# percentage of training set to use as validation
VALIDATION_FRACTION = 0.2
</pre></div>
<p>Convert the data to a torch.FloatTensor.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>transform = transforms.ToTensor()
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>load_dotenv()
path = DataPathTwo(folder_key="FASHION")
print(path.folder)
</pre></div>
<pre class="example">
/home/brunhilde/datasets/FASHION

</pre>
<p>Choose the training and test datasets.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_data = datasets.FashionMNIST(root=path.folder, train=True,
                                   download=True, transform=transform)
test_data = datasets.FashionMNIST(root=path.folder, train=False,
                                  download=True, transform=transform)
</pre></div>
<pre class="example">
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
Processing...
Done!
</pre>
<p>Obtain training indices that will be used for validation.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>indices = list(range(len(train_data)))
train_idx, valid_idx = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
</pre></div>
<p>Define samplers for obtaining training and validation batches.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)
</pre></div>
<p>Prepare data loaders (combine dataset and sampler).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,
                                           sampler=train_sampler, num_workers=subprocesses)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, 
                                           sampler=valid_sampler, num_workers=subprocesses)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, 
                                          num_workers=subprocesses)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 
    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga4e6210">
<h2 id="orga4e6210">Visualize Some Training Data</h2>
<div class="outline-text-2" id="text-orga4e6210">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (10, 8)},
            font_scale=1)
</pre></div>
<p>Obtain one batch of training images.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy()
</pre></div>
<p>Plot the images in the batch, along with the corresponding labels.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>fig = pyplot.figure(figsize=(12, 10))
fig.suptitle("Sample FASHION Images", weight="bold")
for idx in np.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    ax.imshow(np.squeeze(images[idx]), cmap='gray')
    ax.set_title(classes[labels[idx]])
</pre></div>
<div class="figure">
<p><img alt="image_one.png" src="posts/nano/cnn/weight-initialization/image_one.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org40f5ee9">
<h2 id="org40f5ee9">Define the Model Architecture</h2>
<div class="outline-text-2" id="text-org40f5ee9">
<p>We've defined the MLP that we'll use for classifying the dataset.</p>
</div>
<div class="outline-3" id="outline-container-orga8c1e66">
<h3 id="orga8c1e66">Neural Network</h3>
<div class="outline-text-3" id="text-orga8c1e66">
<ul class="org-ul">
<li>A 3 layer MLP with hidden dimensions of 256 and 128.</li>
<li>This MLP accepts a flattened image (784-value long vector) as input and produces 10 class scores as output.</li>
</ul>
<p>We'll test the effect of different initial weights on this 3 layer neural network with ReLU activations and an Adam optimizer. The lessons you learn apply to other neural networks, including different activations and optimizers.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org160d8ed">
<h2 id="org160d8ed">Initialize Weights</h2>
<div class="outline-text-2" id="text-org160d8ed">
<p>Let's start looking at some initial weights.</p>
</div>
<div class="outline-3" id="outline-container-org0c5ee26">
<h3 id="org0c5ee26">All Zeros or Ones</h3>
<div class="outline-text-3" id="text-org0c5ee26">
<p>If you follow the principle of <a href="https://en.wikipedia.org/wiki/Occam's_razor">Occam's razor</a>, you might think setting all the weights to 0 or 1 would be the best solution. This is not the case.</p>
<p>With every weight the same, all the neurons at each layer are producing the same output. This makes it hard to decide which weights to adjust.</p>
<p>Let's compare the loss with all ones and all zero weights by defining two models with those constant weights.</p>
<p>Below, we are using PyTorch's <a href="https://pytorch.org/docs/stable/nn.html#torch-nn-init">nn.init</a> to initialize each Linear layer with a constant weight. The init library provides a number of weight initialization functions that give you the ability to initialize the weights of each layer according to layer type.</p>
<p>In the case below, we look at every layer/module in our model. If it is a Linear layer (as all three layers are for this MLP), then we initialize those layer weights to be a <code>constant_weight</code> with <code>bias=0</code> using the following code:</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">constant_weight</span><span class="p">)</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
<p>The <code>constant_weight</code> is a value that you can pass in when you instantiate the model.</p>
</div>
<div class="outline-4" id="outline-container-org393953f">
<h4 id="org393953f">Define the NN architecture</h4>
<div class="outline-text-4" id="text-org393953f">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Net(nn.Module):
    def __init__(self, hidden_1=256, hidden_2=128, constant_weight=None):
        super(Net, self).__init__()
        # linear layer (784 -&gt; hidden_1)
        self.fc1 = nn.Linear(28 * 28, hidden_1)
        # linear layer (hidden_1 -&gt; hidden_2)
        self.fc2 = nn.Linear(hidden_1, hidden_2)
        # linear layer (hidden_2 -&gt; 10)
        self.fc3 = nn.Linear(hidden_2, 10)
        # dropout layer (p=0.2)
        self.dropout = nn.Dropout(0.2)

        # initialize the weights to a specified, constant value
        if(constant_weight is not None):
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.constant_(m.weight, constant_weight)
                    nn.init.constant_(m.bias, 0)


    def forward(self, x):
        # flatten image input
        x = x.view(-1, 28 * 28)
        # add hidden layer, with relu activation function
        x = F.relu(self.fc1(x))
        # add dropout layer
        x = self.dropout(x)
        # add hidden layer, with relu activation function
        x = F.relu(self.fc2(x))
        # add dropout layer
        x = self.dropout(x)
        # add output layer
        x = self.fc3(x)
        return x
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgbb4cc13">
<h2 id="orgbb4cc13">Compare Model Behavior</h2>
<div class="outline-text-2" id="text-orgbb4cc13">
<p>Below, we are using <code>helpers.compare_init_weights</code> to compare the training and validation loss for the two models we defined above, <code>model_0</code> and <code>model_1</code>. This function takes in a list of models (each with different initial weights), the name of the plot to produce, and the training and validation dataset loaders. For each given model, it will plot the training loss for the first 100 batches and print out the validation accuracy after 2 training epochs. <b>Note: if you've used a small batch_size, you may want to increase the number of epochs here to better compare how models behave after seeing a few hundred images.</b></p>
<p>We plot the loss over the first 100 batches to better judge which model weights performed better at the start of training. <b>I recommend that you take a look at the code in <code>helpers.py</code> to look at the details behind how the models are trained, validated, and compared.</b></p>
<p>Run the cell below to see the difference between weights of all zeros against all ones.</p>
<p>Initialize two NN's with 0 and 1 constant weights.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_0 = Net(constant_weight=0)
model_1 = Net(constant_weight=1)
</pre></div>
<p>Put them in list form to compare.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_list = [(model_0, 'All Zeros'),
              (model_1, 'All Ones')]
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>ModelLabel = Tuple[nn.Module, str]
ModelLabels = Collection[ModelLabel]
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def plot_models(title:str, models_labels:ModelLabels):
    """Plots the models

    Args:
     title: the title for the plots
     models_labels: collections of model, plot-label tuples
    """
    figure, axe = pyplot.subplots()
    figure.suptitle(title, weight="bold")    
    axe.set_xlabel("Batches")
    axe.set_ylabel("Loss")

    for model, label in models_labels:
        loss, validation_accuracy = helpers._get_loss_acc(model, train_loader, valid_loader)
        axe.plot(loss[:100], label=label)
    legend = axe.legend()
    return
</pre></div>
<p>Plot the loss over the first 100 batches.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>plot_models("All Zeros vs All Ones",
            ((model_0, "All Zeros"),
             (model_1, "All ones")))
</pre></div>
<div class="figure">
<p><img alt="zeros_ones.png" src="posts/nano/cnn/weight-initialization/zeros_ones.png"></p>
</div>
<pre class="example">
After 2 Epochs:
Validation Accuracy
    9.475% -- All Zeros
   10.175% -- All Ones
Training Loss
    2.304  -- All Zeros
  1914.703  -- All Ones
</pre>
<p>As you can see the accuracy is close to guessing for both zeros and ones, around 10%.</p>
<p>The neural network is having a hard time determining which weights need to be changed, since the neurons have the same output for each layer. To avoid neurons with the same output, let's use unique weights. We can also randomly select these weights to avoid being stuck in a local minimum for each run.</p>
<p>A good solution for getting these random weights is to sample from a uniform distribution.</p>
</div>
<div class="outline-3" id="outline-container-orgb9ba02d">
<h3 id="orgb9ba02d">Uniform Distribution</h3>
<div class="outline-text-3" id="text-orgb9ba02d">
<p>A <a href="https://en.wikipedia.org/wiki/Uniform_distribution">uniform distribution</a> has the equal probability of picking any number from a set of numbers. We'll be picking from a continuous distribution, so the chance of picking the same number is low. We'll use NumPy's <code>np.random.uniform</code> function to pick random numbers from a uniform distribution.</p>
<p><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.uniform.html"><code>np.random_uniform(low=0.0, high=1.0, size=None)</code></a></p>
<p>Outputs random values from a uniform distribution.</p>
<p>The generated values follow a uniform distribution in the range [low, high). The lower bound minval is included in the range, while the upper bound maxval is excluded.</p>
<ul class="org-ul">
<li><b>low:</b> The lower bound on the range of random values to generate. Defaults to 0.</li>
<li><b>high:</b> The upper bound on the range of random values to generate. Defaults to 1.</li>
<li><b>size:</b> An int or tuple of ints that specify the shape of the output array.</li>
</ul>
<p>We can visualize the uniform distribution by using a histogram. Let's map the values from <code>np.random_uniform(-3, 3, [1000])</code> to a histogram using the <code>helper.hist_dist</code> function. This will be <code>1000</code> random float values from <code>-3</code> to <code>3</code>, excluding the value <code>3</code>.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
figure.suptitle("Random Uniform", weight="bold")
data = numpy.random.uniform(-3, 3, [1000])
grid = seaborn.distplot(data)
#helpers.hist_dist('Random Uniform (low=-3, high=3)', )
</pre></div>
<div class="figure">
<p><img alt="uniform_distribution.png" src="posts/nano/cnn/weight-initialization/uniform_distribution.png"></p>
</div>
<p>Now that you understand the uniform function, let's use PyTorch's <code>nn.init</code> to apply it to a model's initial weights.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orga6f5e7f">
<h3 id="orga6f5e7f">Uniform Initialization, Baseline</h3>
<div class="outline-text-3" id="text-orga6f5e7f">
<p>Let's see how well the neural network trains using a uniform weight initialization, where <code>low=0.0</code> and <code>high=1.0</code>. Below, I'll show you another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, you can:</p>
<ol class="org-ol">
<li>Define a function that assigns weights by the type of network layer, <b>then</b></li>
<li>Apply those weights to an initialized model using <code>model.apply(fn)</code>, which applies a function to each model layer.</li>
</ol>
<p>This time, we'll use <code>weight.data.uniform_</code> to initialize the weights of our model, directly.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def weights_init_uniform(m: nn.Module, start=0.0, stop=1.0) -&gt; None:
    """takes in a module and applies the specified weight initialization

    Args:
     m: A model instance
    """
    classname = m.__class__.__name__
    # for every Linear layer in a model..
    if classname.startswith('Linear'):
        # apply a uniform distribution to the weights and a bias=0
        m.weight.data.uniform_(start, stop)
        m.bias.data.fill_(0)
    return
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga083346">
<h4 id="orga083346">Create A New Model With These Weights</h4>
</div>
<div class="outline-4" id="outline-container-org1527353">
<h4 id="org1527353">Evaluate Behavior</h4>
<div class="outline-text-4" id="text-org1527353">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_uniform = Net()
model_uniform.apply(weights_init_uniform)
plot_models("Uniform Baseline", ((model_uniform, "UNIFORM WEIGHTS"),))
</pre></div>
<div class="figure">
<p><img alt="uniform_weights.png" src="posts/nano/cnn/weight-initialization/uniform_weights.png"></p>
</div>
<p>The loss graph is showing the neural network is learning, which it didn't with all zeros or all ones. We're headed in the right direction!</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgf2058c1">
<h2 id="orgf2058c1">General rule for setting weights</h2>
<div class="outline-text-2" id="text-orgf2058c1">
<p>The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. A good practice is to start your weights in the range of \([-y, y]\) where \(y=1/\sqrt{n}\) (\(n\) is the number of inputs to a given neuron).</p>
<p>Let's see if this holds true; let's create a baseline to compare with and center our uniform range over zero by shifting it over by 0.5. This will give us the range [-0.5, 0.5).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>weights_init_uniform_center = partial(weights_init_uniform, -0.5, 0.5)
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgbe3233f">
<h3 id="orgbe3233f">create a new model with these weights</h3>
<div class="outline-text-3" id="text-orgbe3233f">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_centered = Net()
model_centered.apply(weights_init_uniform_center)
</pre></div>
<p>Now let's create a distribution and model that uses the <b>general rule</b> for weight initialization; using the range \([-y, y]\), where \(y=1/\sqrt{n}\) .</p>
<p>And finally, we'll compare the two models.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def weights_init_uniform_rule(m: nn.Module) -&gt; None:
    """takes in a module and applies the specified weight initialization

    Args:
     m: Model instance
    """
    classname = m.__class__.__name__
    # for every Linear layer in a model..
    if classname.find('Linear') != -1:
        # get the number of the inputs
        n = m.in_features
        y = 1.0/numpy.sqrt(n)
        m.weight.data.uniform_(-y, y)
        m.bias.data.fill_(0)
    return
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_rule = Net()
model_rule.apply(weights_init_uniform_rule)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>plot_models("Uniform Centered vs General Rule", (
    (model_centered, 'Centered Weights [-0.5, 0.5)'), 
    (model_rule, 'General Rule [-y, y)'),
))
</pre></div>
<p><img alt="general_rule.png" src="posts/nano/cnn/weight-initialization/general_rule.png"> This behavior is really promising! Not only is the loss decreasing, but it seems to do so very quickly for our uniform weights that follow the general rule; after only two epochs we get a fairly high validation accuracy and this should give you some intuition for why starting out with the right initial weights can really help your training process!</p>
<p>Since the uniform distribution has the same chance to pick <b>any value</b> in a range, what if we used a distribution that had a higher chance of picking numbers closer to 0? Let's look at the normal distribution.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0dd5d23">
<h2 id="org0dd5d23">Normal Distribution</h2>
<div class="outline-text-2" id="text-org0dd5d23">
<p>Unlike the uniform distribution, the <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> has a higher likelihood of picking number close to it's mean. To visualize it, let's plot values from NumPy's <code>np.random.normal</code> function to a histogram.</p>
<p><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html">np.random.normal(loc=0.0, scale=1.0, size=None)</a></p>
<p>Outputs random values from a normal distribution.</p>
<ul class="org-ul">
<li><b>loc:</b> The mean of the normal distribution.</li>
<li><b>scale:</b> The standard deviation of the normal distribution.</li>
<li><b>shape:</b> The shape of the output array.</li>
</ul>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
figure.suptitle("Standard Normal Distribution", weight="bold")
grid = seaborn.distplot(numpy.random.normal(size=[1000]))
</pre></div>
<div class="figure">
<p><img alt="normal_distribution.png" src="posts/nano/cnn/weight-initialization/normal_distribution.png"></p>
</div>
<p>Let's compare the normal distribution against the previous, rule-based, uniform distribution.</p>
<p>The normal distribution should have a mean of 0 and a standard deviation of \(y=1/\sqrt{n}\)</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def weights_init_normal(m: nn.Module) -&gt; None:
    '''Takes in a module and initializes all linear layers with weight
       values taken from a normal distribution.'''

    classname = m.__class__.__name__
    if classname.startswith("Linear"):    
        m.weight.data.normal_(mean=0, std=1/numpy.sqrt(m.in_features))
        m.bias.data.fill_(0)
    return
</pre></div>
<p>create a new model with the rule-based, uniform weights</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_uniform_rule = Net()
model_uniform_rule.apply(weights_init_uniform_rule)
</pre></div>
<p>create a new model with the rule-based, NORMAL weights</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_normal_rule = Net()
model_normal_rule.apply(weights_init_normal)
</pre></div>
<p>compare the two models</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>plot_models('Uniform vs Normal',
            ((model_uniform_rule, 'Uniform Rule [-y, y)'), 
             (model_normal_rule, 'Normal Distribution')))
</pre></div>
<div class="figure">
<p><img alt="normal_vs_uniform.png" src="posts/nano/cnn/weight-initialization/normal_vs_uniform.png"></p>
</div>
<p>The normal distribution gives us pretty similar behavior compared to the uniform distribution, in this case. This is likely because our network is so small; a larger neural network will pick more weight values from each of these distributions, magnifying the effect of both initialization styles. In general, a normal distribution will result in better performance for a model.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org403aeb2">
<h2 id="org403aeb2">Automatic Initialization</h2>
<div class="outline-text-2" id="text-org403aeb2">
<p>Let's quickly take a look at what happens <b>without any explicit weight initialization</b>.</p>
</div>
<div class="outline-3" id="outline-container-org0965736">
<h3 id="org0965736">Instantiate a model with <span class="underline">no</span> explicit weight initialization</h3>
</div>
</div>
<div class="outline-2" id="outline-container-org95ad8ba">
<h2 id="org95ad8ba">evaluate the behavior using helpers</h2>
<div class="outline-text-2" id="text-org95ad8ba">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_normal_rule = Net()
model_normal_rule.apply(weights_init_normal)
model_default = Net()
model_rule = Net()
model_rule.apply(weights_init_uniform_rule)

plot_models("Default vs Normal vs General Rule", (
    (model_default, "Default"),
    (model_normal_rule, "Normal"),
    (model_rule, "General Rule")))
</pre></div>
<div class="figure">
<p><img alt="default.png" src="posts/nano/cnn/weight-initialization/default.png"></p>
</div>
<p>They all sort of look the same at this point.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/cnn/transfer-learning-exercise/">Transfer Learning Exercise</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/cnn/transfer-learning-exercise/" rel="bookmark"><time class="published dt-published" datetime="2018-12-15T14:50:47-08:00" itemprop="datePublished" title="2018-12-15 14:50">2018-12-15 14:50</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#org82898b1">Introduction</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#orgb5b4181">Imports</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#orgdb547c5">Flower power</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#org015c8dd">Download the Data</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#org16f5ba9">Transforming the Data</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#orgd6582ef">DataLoaders and Data Visualization</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#orga98a767">Visualize some sample data</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#org21b592f">Plot The Images In The Batch, Along With The Corresponding Labels</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#orgfc1a8c9">Define the Model</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#org9aa36f1">Final Classifier Layer</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#org56b2b0e">Specify Loss Function and Optimizer</a></li>
<li><a href="posts/nano/cnn/transfer-learning-exercise/#org981b040">Training</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org82898b1">
<h2 id="org82898b1">Introduction</h2>
<div class="outline-text-2" id="text-org82898b1">
<p>Most of the time you won't want to train a whole convolutional network yourself. Modern ConvNets training on huge datasets like ImageNet take weeks on multiple GPUs. Instead, most people use a pretrained network either as a fixed feature extractor, or as an initial network to fine tune.</p>
<p>In this notebook, you'll be using <a href="https://arxiv.org/pdf/1409.1556.pdf">VGGNet</a> trained on the <a href="http://www.image-net.org/">ImageNet dataset</a> as a feature extractor.</p>
<p>VGGNet is great because it's simple and has great performance, coming in second in the ImageNet competition. The idea here is that we keep all the convolutional layers, but <b>replace the final fully-connected layer</b> with our own classifier. This way we can use VGGNet as a <b>fixed feature extractor</b> for our images then easily train a simple classifier on top of that.</p>
<ul class="org-ul">
<li>Use all but the last fully-connected layer as a fixed feature extractor.</li>
<li>Define a new, final classification layer and apply it to a task of our choice!</li>
</ul>
<p>You can read more about transfer learning from <a href="http://cs231n.github.io/transfer-learning/">the CS231n Stanford course notes</a>.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgb5b4181">
<h2 id="orgb5b4181">Imports</h2>
<div class="outline-text-2" id="text-orgb5b4181">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span># python
from collections import OrderedDict
from datetime import datetime
import os

# pypi
from dotenv import load_dotenv
from torch import nn
from sklearn.model_selection import train_test_split
from torch.utils.data.sampler import SubsetRandomSampler

import matplotlib
import numpy
import seaborn
import torch
import torch.optim as optimize
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as pyplot

# this project
from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgcd78a69">
<h3 id="orgcd78a69">Plotting</h3>
<div class="outline-text-3" id="text-orgcd78a69">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.size": 8,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=3)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgdb547c5">
<h2 id="orgdb547c5">Flower power</h2>
<div class="outline-text-2" id="text-orgdb547c5">
<p>Here we'll be using VGGNet to classify images of flowers. We'll start, as usual, by importing our usual resources. And checking if we can train our model on the GPU.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org015c8dd">
<h2 id="org015c8dd">Download the Data</h2>
<div class="outline-text-2" id="text-org015c8dd">
<p>Download the flower data from <a href="https://s3.amazonaws.com/video.udacity-data.com/topher/2018/September/5baa60a0_flower-photos/flower-photos.zip">this link</a>, save it in the home directory of this notebook and extract the zip file to get the directory <code>flower_photos/</code>. <b>Make sure the directory has this exact name for accessing data: flower_photos</b>.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>load_dotenv()
path = DataPathTwo(folder_key="FLOWERS")
print(path.folder)
for target in path.folder.iterdir():
    print(target)
</pre></div>
<pre class="example">
/home/hades/datasets/flower_photos
/home/hades/datasets/flower_photos/.DS_Store
/home/hades/datasets/flower_photos/train
/home/hades/datasets/flower_photos/test
/home/hades/datasets/flower_photos/LICENSE.txt

</pre></div>
<div class="outline-3" id="outline-container-orgdf62783">
<h3 id="orgdf62783">Check If CUDA Is Available</h3>
<div class="outline-text-3" id="text-orgdf62783">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(device)
</pre></div>
<pre class="example">
cuda:0

</pre>
<p>CUDA is running out of memory and crashing so don't use CUDA.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>device = "cpu"
print(device)
</pre></div>
<pre class="example">
cpu

</pre></div>
</div>
<div class="outline-3" id="outline-container-org6dafda1">
<h3 id="org6dafda1">Load and Transform our Data</h3>
<div class="outline-text-3" id="text-org6dafda1">
<p>We'll be using PyTorch's <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder">ImageFolder</a> class which makes is very easy to load data from a directory. For example, the training images are all stored in a directory path that looks like this:</p>
<pre class="example">
root/class_1/xxx.png
root/class_1/xxy.png
root/class_1/xxz.png

root/class_2/123.png
root/class_2/nsdf3.png
root/class_2/asd932_.png
</pre>
<p>Where, in this case, the root folder for training is <code>flower_photos/train/</code> and the classes are the names of flower types.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org5ce0453">
<h3 id="org5ce0453">Define Training and Test Data Directories</h3>
<div class="outline-text-3" id="text-org5ce0453">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_dir = path.folder.joinpath('train/')
test_dir = path.folder.joinpath('test/')
print(train_dir)
print(test_dir)
</pre></div>
<pre class="example">
/home/hades/datasets/flower_photos/train
/home/hades/datasets/flower_photos/test

</pre>
<p><i>Classes</i> are folders in each directory with these names:</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>classes = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']
CLASS_COUNT = len(classes)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org16f5ba9">
<h2 id="org16f5ba9">Transforming the Data</h2>
<div class="outline-text-2" id="text-org16f5ba9">
<p>When we perform transfer learning, we have to shape our input data into the shape that the pre-trained model expects. VGG16 expects `224`-dim square images as input and so, we resize each flower image to fit this mold.</p>
</div>
<div class="outline-3" id="outline-container-org7383380">
<h3 id="org7383380">Load And Transform Data Using ImageFolder</h3>
<div class="outline-text-3" id="text-org7383380">
<p>VGG-16 Takes 224x224 images as input, so we resize all of them.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>data_transform = transforms.Compose([transforms.RandomResizedCrop(224), 
                                      transforms.ToTensor()])

train_data = datasets.ImageFolder(train_dir, transform=data_transform)
test_data = datasets.ImageFolder(test_dir, transform=data_transform)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org61d9125">
<h3 id="org61d9125">Print Out Some Data Stats</h3>
<div class="outline-text-3" id="text-org61d9125">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print('Num training images: ', len(train_data))
print('Num test images: ', len(test_data))
</pre></div>
<pre class="example">
Num training images:  3130
Num test images:  540

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>VALIDATION_FRACTION = 0.2
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>indices = list(range(len(train_data)))
training_indices, validation_indices = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd6582ef">
<h2 id="orgd6582ef">DataLoaders and Data Visualization</h2>
<div class="outline-text-2" id="text-orgd6582ef"></div>
<div class="outline-3" id="outline-container-orgf3ecb89">
<h3 id="orgf3ecb89">Define Dataloader Parameters</h3>
<div class="outline-text-3" id="text-orgf3ecb89">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>BATCH_SIZE = 20
NUM_WORKERS=4
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_sampler = SubsetRandomSampler(training_indices)
valid_sampler = SubsetRandomSampler(validation_indices)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org202b2c8">
<h3 id="org202b2c8">Prepare Data Loaders</h3>
<div class="outline-text-3" id="text-org202b2c8">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, 
                                           sampler=train_sampler,
                                           num_workers=NUM_WORKERS)
valid_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, 
                                           sampler=valid_sampler, num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, 
                                          num_workers=num_workers, shuffle=True)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga98a767">
<h2 id="orga98a767">Visualize some sample data</h2>
<div class="outline-text-2" id="text-orga98a767"></div>
<div class="outline-3" id="outline-container-orge48fe38">
<h3 id="orge48fe38">obtain one batch of training images</h3>
<div class="outline-text-3" id="text-orge48fe38">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy() # convert images to numpy for display
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org21b592f">
<h2 id="org21b592f">Plot The Images In The Batch, Along With The Corresponding Labels</h2>
<div class="outline-text-2" id="text-org21b592f">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>fig = pyplot.figure(figsize=(12, 10))
pyplot.rc("axes", titlesize=10)
for idx in numpy.arange(20):
    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    pyplot.imshow(numpy.transpose(images[idx], (1, 2, 0)))
    ax.set_title(classes[labels[idx]])
</pre></div>
<div class="figure">
<p><img alt="sample_batches.png" src="posts/nano/cnn/transfer-learning-exercise/sample_batches.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgfc1a8c9">
<h2 id="orgfc1a8c9">Define the Model</h2>
<div class="outline-text-2" id="text-orgfc1a8c9">
<p>To define a model for training we'll follow these steps:</p>
<ol class="org-ol">
<li>Load in a pre-trained VGG16 model</li>
<li>"Freeze" all the parameters, so the net acts as a fixed feature extractor</li>
<li>Remove the last layer</li>
<li>Replace the last layer with a linear classifier of our own</li>
</ol>
<p>/Freezing simply means that the parameters in the pre-trained model will <b>not</b> change during training.**</p>
<p>Load the pretrained model from pytorch</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>vgg16 = models.vgg16(pretrained=True)
</pre></div>
<p>Print Out The Model Structure</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print(vgg16)
</pre></div>
<pre class="example">
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace)
    (2): Dropout(p=0.5)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace)
    (5): Dropout(p=0.5)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
</pre>
<p>Since we're only going to change the last (classification) layer, it might be helpful to see how many inputs and outpts it has.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print(vgg16.classifier[6].in_features) 
print(vgg16.classifier[6].out_features) 
</pre></div>
<pre class="example">
4096
1000

</pre>
<p>So, the original model output 1,000 classes - we're going to need to change that to our five classes (eventually).</p>
<p>Freeze training for all "features" layers</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>for param in vgg16.features.parameters():
    param.requires_grad = False
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org9aa36f1">
<h2 id="org9aa36f1">Final Classifier Layer</h2>
<div class="outline-text-2" id="text-org9aa36f1">
<p>Once you have the pre-trained feature extractor, you just need to modify and/or add to the final, fully-connected classifier layers. In this case, we suggest that you replace the last layer in the vgg classifier group of layers.</p>
<p>This layer should see as input the number of features produced by the portion of the network that you are not changing, and produce an appropriate number of outputs for the flower classification task.</p>
<p>You can access any layer in a pretrained network by name and (sometimes) number, i.e. <code>vgg16.classifier[6]</code> is the sixth layer in a group of layers named "classifier".</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>classifier = nn.Sequential(OrderedDict([
    ("Fullly Connected Classifier", nn.Linear(in_features=4096, out_features=CLASS_COUNT, bias=True)),
]))
vgg16.classifier[6] = classifier
</pre></div>
<p>after completing your model, if GPU is available, move the model to GPU</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>vgg16.to(device)
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org56b2b0e">
<h2 id="org56b2b0e">Specify <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">Loss Function</a> and <a href="http://pytorch.org/docs/stable/optim.html">Optimizer</a></h2>
<div class="outline-text-2" id="text-org56b2b0e">
<p>Below we'll use cross-entropy loss and stochastic gradient descent with a small learning rate. Note that the optimizer accepts as input <i>only</i> the trainable parameters <code>vgg.classifier.parameters()</code>.</p>
</div>
<div class="outline-3" id="outline-container-orgcc5b2c5">
<h3 id="orgcc5b2c5">Specify Loss Function (Categorical Cross-Entropy)</h3>
<div class="outline-text-3" id="text-orgcc5b2c5">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>criterion = nn.CrossEntropyLoss()
</pre></div>
<p>specify optimizer (stochastic gradient descent) and learning rate = 0.001</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>optimizer = optimize.SGD(vgg16.classifier.parameters(), lr=0.001)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org981b040">
<h2 id="org981b040">Training</h2>
<div class="outline-text-2" id="text-org981b040">
<p>Here, we'll train the network.</p>
<p><b>Exercise:</b> So far we've been providing the training code for you. Here, I'm going to give you a bit more of a challenge and have you write the code to train the network. Of course, you'll be able to see my solution if you need help.</p>
<p>number of epochs to train the model</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>n_epochs = EPOCHS = 2
def train(model: nn.Module, epochs: int=EPOCHS, model_number: int=0,
          epoch_offset: int=1, print_every: int=10) -&gt; tuple:
    """Train, validate, and save the model
    This trains the model and validates it, saving the best 
    (based on validation loss) as =model_&lt;number&gt;_cifar.pth=

    Args:
     model: the network to train
     epochs: number of times to repeat training
     model_number: an identifier for the saved hyperparameters file
     epoch_offset: amount of epochs that have occurred previously
     print_every: how often to print output
    Returns:
     filename, training-loss, validation-loss, improvements: the outcomes for the training
    """
    optimizer = optimize.SGD(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    output_file = "model_{}_vgg.pth".format(model_number)
    training_losses = []
    validation_losses = []
    improvements = []
    valid_loss_min = numpy.Inf # track change in validation loss
    epoch_start = epoch_offset
    last_epoch = epoch_start + epochs + 1
    for epoch in range(epoch_start, last_epoch):

        # keep track of training and validation loss
        train_loss = 0.0
        valid_loss = 0.0

        model.train()
        for data, target in train_loader:
            # move tensors to GPU if CUDA is available            
            data, target = data.to(device), target.to(device)
            # clear the gradients of all optimized variables
            optimizer.zero_grad()
            # forward pass: compute predicted outputs by passing inputs to the model
            output = model(data)
            # calculate the batch loss
            loss = criterion(output, target)
            # backward pass: compute gradient of the loss with respect to model parameters
            loss.backward()
            # perform a single optimization step (parameter update)
            optimizer.step()
            # update training loss
            train_loss += loss.item() * data.size(0)

        model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            data, target = data.to(device), target.to(device)
            # forward pass: compute predicted outputs by passing inputs to the model
            output = model(data)
            # calculate the batch loss
            loss = criterion(output, target)
            # update total validation loss 
            valid_loss += loss.item() * data.size(0)

        # calculate average losses
        train_loss = train_loss/len(train_loader.dataset)
        valid_loss = valid_loss/len(valid_loader.dataset)

        # print training/validation statistics 
        if not (epoch % print_every):
            print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
                epoch, train_loss, valid_loss))
        training_losses.append(train_loss)
        validation_losses.append(valid_loss)
        # save model if validation loss has decreased
        if valid_loss &lt;= valid_loss_min:
            print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(
            valid_loss_min,
            valid_loss))
            torch.save(model.state_dict(), output_file)
            valid_loss_min = valid_loss
            improvements.append(epoch - 1)
    return output_file, training_losses, validation_losses, improvements
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def test(best_model):
    criterion = nn.CrossEntropyLoss()
    # track test loss
    test_loss = 0.0
    class_correct = list(0. for i in range(10))
    class_total = list(0. for i in range(10))

    best_model.to(device)
    best_model.eval()
    # iterate over test data
    for data, target in test_loader:
        # move tensors to GPU if CUDA is available
        data, target = data.to(device), target.to(device)
        # forward pass: compute predicted outputs by passing inputs to the model
        output = best_model(data)
        # calculate the batch loss
        loss = criterion(output, target)
        # update test loss 
        test_loss += loss.item() * data.size(0)
        # convert output probabilities to predicted class
        _, pred = torch.max(output, 1)    
        # compare predictions to true label
        correct_tensor = pred.eq(target.data.view_as(pred))
        correct = (
            numpy.squeeze(correct_tensor.numpy())
            if not train_on_gpu
            else numpy.squeeze(correct_tensor.cpu().numpy()))
        # calculate test accuracy for each object class
        for i in range(BATCH_SIZE):
            label = target.data[i]
            class_correct[label] += correct[i].item()
            class_total[label] += 1

    # average test loss
    test_loss = test_loss/len(test_loader.dataset)
    print('Test Loss: {:.6f}\n'.format(test_loss))

    for i in range(10):
        if class_total[i] &gt; 0:
            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
                classes[i], 100 * class_correct[i] / class_total[i],
                numpy.sum(class_correct[i]), numpy.sum(class_total[i])))
        else:
            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

    print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
        100. * numpy.sum(class_correct) / numpy.sum(class_total),
        numpy.sum(class_correct), numpy.sum(class_total)))
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>output_file, training_losses, validation_losses, improvements = train(vgg16, print_every=1)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>training_losses = []
validation_losses = []
improvements = []
valid_loss_min = numpy.Inf # track change in validation loss
for epoch in range(1, 3):

    # keep track of training and validation loss
    train_loss = 0.0
    valid_loss = 0.0

    vgg16.train()
    for data, target in train_loader:
        # move tensors to GPU if CUDA is available            
        data, target = data.to(device), target.to(device)
        # clear the gradients of all optimized variables
        optimizer.zero_grad()
        # forward pass: compute predicted outputs by passing inputs to the model
        output = vgg16(data)
        # calculate the batch loss
        loss = criterion(output, target)
        # backward pass: compute gradient of the loss with respect to model parameters
        loss.backward()
        # perform a single optimization step (parameter update)
        optimizer.step()
        # update training loss
        train_loss += loss.item() * data.size(0)

    vgg16.eval()
    for data, target in valid_loader:
        # move tensors to GPU if CUDA is available
        data, target = data.to(device), target.to(device)
        # forward pass: compute predicted outputs by passing inputs to the model
        output = vgg16(data)
        # calculate the batch loss
        loss = criterion(output, target)
        # update total validation loss 
        valid_loss += loss.item() * data.size(0)

    # calculate average losses
    train_loss = train_loss/len(train_loader.dataset)
    valid_loss = valid_loss/len(valid_loader.dataset)

    # print training/validation statistics 
    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
        epoch, train_loss, valid_loss))
    training_losses.append(train_loss)
    validation_losses.append(valid_loss)
    # save model if validation loss has decreased
    if valid_loss &lt;= valid_loss_min:
        print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(
        valid_loss_min,
        valid_loss))
        torch.save(vgg16.state_dict(), output_file)
        valid_loss_min = valid_loss
        improvements.append(epoch - 1)
</pre></div>
<p>test_loss = 0.0 class_correct = list(0. for i in range(5)) class_total = list(0. for i in range(5))</p>
<p>vgg16.eval() # eval mode</p>
<p>for data, target in test_loader:</p>
<p>if train_on_gpu: data, target = data.cuda(), target.cuda()</p>
<p>output = vgg16(data)</p>
<p>loss = criterion(output, target)</p>
<p>test_loss += loss.item()*data.size(0)</p>
<p>_, pred = torch.max(output, 1)</p>
<p>correct_tensor = pred.eq(target.data.view_as(pred)) correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())</p>
<p>for i in range(batch_size): label = target.data[i] class_correct[label] += correct[i].item() class_total[label] += 1</p>
<p>test_loss = test_loss/len(test_loader.dataset) print('Test Loss: {:.6f}\n'.format(test_loss))</p>
<p>for i in range(5): if class_total[i] &gt; 0: print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % ( classes[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i]))) else: print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))</p>
<p>print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (</p>
<ol class="org-ol">
<li>* np.sum(class_correct) / np.sum(class_total),</li>
</ol>
<p>np.sum(class_correct), np.sum(class_total)))</p>
<p>dataiter = iter(test_loader) images, labels = dataiter.next() images.numpy()</p>
<p>if train_on_gpu: images = images.cuda()</p>
<p>output = vgg16(images)</p>
<p>_, preds_tensor = torch.max(output, 1) preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())</p>
<p>fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) plt.imshow(np.transpose(images[idx], (1, 2, 0))) ax.set_title("{} ({})".format(classes[preds[idx]], classes[labels[idx]]), color=("green" if preds[idx]==labels[idx].item() else "red"))</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/cnn/convolutional-layers-in-pytorch/">Convolutional Layers in PyTorch</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/cnn/convolutional-layers-in-pytorch/" rel="bookmark"><time class="published dt-published" datetime="2018-12-06T21:46:04-08:00" itemprop="datePublished" title="2018-12-06 21:46">2018-12-06 21:46</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/cnn/convolutional-layers-in-pytorch/#orgd60e54e">Introduction</a></li>
<li><a href="posts/nano/cnn/convolutional-layers-in-pytorch/#org7206154">Convolutional Layers in PyTorch</a></li>
<li><a href="posts/nano/cnn/convolutional-layers-in-pytorch/#org8416eff">Questions</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgd60e54e">
<h2 id="orgd60e54e">Introduction</h2>
<div class="outline-text-2" id="text-orgd60e54e">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org7206154">
<h2 id="org7206154">Convolutional Layers in PyTorch</h2>
<div class="outline-text-2" id="text-org7206154">
<p>The Convolutional class (Conv2D) is part of the <code>nn</code> module so you have to import that.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org8416eff">
<h2 id="org8416eff">Questions</h2>
<div class="outline-text-2" id="text-org8416eff">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgbe000fb">
<h3 id="orgbe000fb">Question 1</h3>
<div class="outline-text-3" id="text-orgbe000fb">
<p>After going through the four-layer sequence, what is the depth of the final output?</p>
<ul class="org-ul">
<li class="off"><code>[&nbsp;]</code> 1</li>
<li class="off"><code>[&nbsp;]</code> 3</li>
<li class="off"><code>[&nbsp;]</code> 10</li>
<li class="off"><code>[&nbsp;]</code> 20</li>
<li class="off"><code>[&nbsp;]</code> 40</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orgd813969">
<h3 id="orgd813969">Question 2</h3>
<div class="outline-text-3" id="text-orgd813969">
<p>What is the x-y size of the output of the final maxpooling layer?</p>
<ul class="org-ul">
<li class="off"><code>[&nbsp;]</code> 8</li>
<li class="off"><code>[&nbsp;]</code> 15</li>
<li class="off"><code>[&nbsp;]</code> 16</li>
<li class="off"><code>[&nbsp;]</code> 30</li>
<li class="off"><code>[&nbsp;]</code> 32</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orgc5274ca">
<h3 id="orgc5274ca">Question 3</h3>
<div class="outline-text-3" id="text-orgc5274ca">
<p>How many parameters, total, will be left after an image passes through all four of the above layers in sequence?</p>
<ul class="org-ul">
<li class="off"><code>[&nbsp;]</code> 4 x 4 x 20</li>
<li class="off"><code>[&nbsp;]</code> 128 x 20</li>
<li class="off"><code>[&nbsp;]</code> 16 x 16 x 20</li>
<li class="off"><code>[&nbsp;]</code> 32 x 32 x 20</li>
</ul>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/cnn/cifar-10/">CIFAR-10</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/cnn/cifar-10/" rel="bookmark"><time class="published dt-published" datetime="2018-12-04T15:26:15-08:00" itemprop="datePublished" title="2018-12-04 15:26">2018-12-04 15:26</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/cnn/cifar-10/#org45f8aab">Introduction</a></li>
<li><a href="posts/nano/cnn/cifar-10/#orge86fe94">Set Up</a></li>
<li><a href="posts/nano/cnn/cifar-10/#org8e4a666">Visualize a Batch of Training Data</a></li>
<li><a href="posts/nano/cnn/cifar-10/#org5e1356e">Define the Network Architecture</a></li>
<li><a href="posts/nano/cnn/cifar-10/#org6b06a9a">Define a model with multiple convolutional layers, and define the feedforward network behavior.</a></li>
<li><a href="posts/nano/cnn/cifar-10/#orga82b4ea">Output volume for a convolutional layer</a></li>
<li><a href="posts/nano/cnn/cifar-10/#org9bed231">Specify Loss Function and Optimizer</a></li>
<li><a href="posts/nano/cnn/cifar-10/#org5b6ac9f">Train the Network</a></li>
<li><a href="posts/nano/cnn/cifar-10/#org92525ad">Load the Model with the Lowest Validation Loss</a></li>
<li><a href="posts/nano/cnn/cifar-10/#org82d7ed1">Test the Trained Network</a></li>
<li><a href="posts/nano/cnn/cifar-10/#orga0ee2aa">Make it Easier</a></li>
<li><a href="posts/nano/cnn/cifar-10/#orga02a9cd">Take two</a></li>
<li><a href="posts/nano/cnn/cifar-10/#org307af43">Change the Training and Validation Sets</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org45f8aab">
<h2 id="org45f8aab">Introduction</h2>
<div class="outline-text-2" id="text-org45f8aab">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree. This will use a <b>Convolutional Neural Network (CNN)</b> to classify images from the <a href="https://en.wikipedia.org/wiki/CIFAR-10">CIFAR-10</a> data set.</p>
<p>The images in this data set are small color images that fall into one of ten classes:</p>
<ul class="org-ul">
<li>airplane</li>
<li>automobile</li>
<li>bird</li>
<li>cat</li>
<li>deer</li>
<li>dog</li>
<li>frog</li>
<li>horse</li>
<li>ship</li>
<li>truck</li>
</ul>
<p>There is another description of it on the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">University of Toronto's</a> page for it.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orge86fe94">
<h2 id="orge86fe94">Set Up</h2>
<div class="outline-text-2" id="text-orge86fe94"></div>
<div class="outline-3" id="outline-container-orgc362427">
<h3 id="orgc362427">Imports</h3>
<div class="outline-text-3" id="text-orgc362427"></div>
<div class="outline-4" id="outline-container-org6f27046">
<h4 id="org6f27046">From Python</h4>
<div class="outline-text-4" id="text-org6f27046">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from datetime import datetime
from pathlib import Path
from typing import Tuple
import os
import pickle
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org84f2a23">
<h4 id="org84f2a23">From PyPi</h4>
<div class="outline-text-4" id="text-org84f2a23">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from dotenv import load_dotenv
from sklearn.model_selection import train_test_split
from torchvision import datasets
from torch.utils.data.sampler import SubsetRandomSampler
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optimize
import torchvision.transforms as transforms
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga12c96c">
<h4 id="orga12c96c">This Project</h4>
<div class="outline-text-4" id="text-orga12c96c">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org8e25150">
<h3 id="org8e25150">Plotting</h3>
<div class="outline-text-3" id="text-org8e25150">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=3)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfe00428">
<h3 id="orgfe00428">Test for <a href="http://pytorch.org/docs/stable/cuda.html">CUDA</a></h3>
<div class="outline-text-3" id="text-orgfe00428">
<p>The test-code uses the check later on so I'll save it to the <code>train_on_gpu</code> variable.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>if os.environ.get("USER") == "brunhilde":
    train_on_gpu = False
    device = torch.device("cpu")
else:
    train_on_gpu = torch.cuda.is_available()
    device = torch.device("cuda:0" if train_on_gpu else "cpu")
print("Using: {}".format(device))
</pre></div>
<pre class="example">
Using: cuda:0

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgeffc1dd">
<h3 id="orgeffc1dd">Load the <a href="http://pytorch.org/docs/stable/torchvision/datasets.html">Data</a></h3>
<div class="outline-text-3" id="text-orgeffc1dd">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span># subprocesses to use
NUM_WORKERS = 0
# how many samples per batch to load
BATCH_SIZE = 20
# percentage of training set to use as validation
VALIDATION_FRACTION = 0.2

IMAGE_SIZE = 32
</pre></div>
<p>Convert the data to a normalized <code>torch.FloatTensor</code> using a pipeline. I'm also going to introduce some randomness to help the model generalize.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>means = deviations = (0.5, 0.5, 0.5)
train_transform = transforms.Compose([
    transforms.RandomRotation(30),
    transforms.RandomResizedCrop(IMAGE_SIZE),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(means, deviations)
    ])
test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(means,
                         deviations)])
</pre></div>
<p>Choose the training and test datasets.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>load_dotenv()
path = DataPathTwo(folder_key="CIFAR")
print(path.folder)
</pre></div>
<pre class="example">
/home/hades/datasets/CIFAR

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>training_data = datasets.CIFAR10(path.folder, train=True,
                              download=True, transform=train_transform)
test_data = datasets.CIFAR10(path.folder, train=False,
                             download=True, transform=test_transforms)
</pre></div>
<pre class="example">
Files already downloaded and verified
Files already downloaded and verified

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>for item in path.folder.iterdir():
    print(item)
</pre></div>
<pre class="example">
/home/hades/datasets/CIFAR/cifar-10-batches-py
/home/hades/datasets/CIFAR/cifar-10-python.tar.gz

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgf5476e6">
<h3 id="orgf5476e6">Obtain Training Indices For Validation</h3>
<div class="outline-text-3" id="text-orgf5476e6">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>indices = list(range(len(training_data)))
training_indices, validation_indices = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org63dcc02">
<h3 id="org63dcc02">Define Samplers For Training And Validation Batches</h3>
<div class="outline-text-3" id="text-org63dcc02">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_sampler = SubsetRandomSampler(training_indices)
valid_sampler = SubsetRandomSampler(validation_indices)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org48ed052">
<h3 id="org48ed052">Prepare Data Loaders</h3>
<div class="outline-text-3" id="text-org48ed052">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE,
    sampler=train_sampler, num_workers=NUM_WORKERS)
valid_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE, 
    sampler=valid_sampler, num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, 
    num_workers=NUM_WORKERS)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org265e4dd">
<h3 id="org265e4dd">The Image Classes</h3>
<div class="outline-text-3" id="text-org265e4dd">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',
           'dog', 'frog', 'horse', 'ship', 'truck']
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org8e4a666">
<h2 id="org8e4a666">Visualize a Batch of Training Data</h2>
<div class="outline-text-2" id="text-org8e4a666"></div>
<div class="outline-3" id="outline-container-orge9ac810">
<h3 id="orge9ac810">helper function to un-normalize and display an image</h3>
<div class="outline-text-3" id="text-orge9ac810">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def imshow(img):
    img = img / 2 + 0.5  # unnormalize
    pyplot.imshow(numpy.transpose(img, (1, 2, 0)))  # convert from Tensor image
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org748e077">
<h3 id="org748e077">obtain one batch of training images</h3>
<div class="outline-text-3" id="text-org748e077">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dataiter = iter(train_loader)
images, labels = dataiter.next()
images = images.numpy() # convert images to numpy for display
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org13e0fe0">
<h3 id="org13e0fe0">plot the images in the batch, along with the corresponding labels</h3>
<div class="outline-text-3" id="text-org13e0fe0">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure = pyplot.figure(figsize=(25, 4))
# display 20 images
figure.suptitle("Batch Sample", weight="bold")
for idx in numpy.arange(20):
    ax = figure.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])
    imshow(images[idx])
    ax.set_title(classes[labels[idx]])
#pyplot.subplots_adjust(top=0.7)
pyplot.tight_layout(rect=[0, 0.03, 1, 0.95])
</pre></div>
<div class="figure">
<p><img alt="batch.png" src="posts/nano/cnn/cifar-10/batch.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc4a2c38">
<h3 id="orgc4a2c38">View an Image in More Detail</h3>
<div class="outline-text-3" id="text-orgc4a2c38">
<p>Here, we look at the normalized red, green, and blue (RGB) color channels as three separate, grayscale intensity images.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>rgb_img = numpy.squeeze(images[3])
channels = ['red channel', 'green channel', 'blue channel']

fig = pyplot.figure(figsize = (36, 36)) 
for idx in numpy.arange(rgb_img.shape[0]):
    ax = fig.add_subplot(1, 3, idx + 1)
    img = rgb_img[idx]
    ax.imshow(img, cmap='gray')
    ax.set_title(channels[idx])
    width, height = img.shape
    thresh = img.max()/2.5
    for x in range(width):
        for y in range(height):
            val = round(img[x][y],2) if img[x][y] !=0 else 0
            ax.annotate(str(val), xy=(y,x),
                    horizontalalignment='center',
                    verticalalignment='center', size=8,
                    color='white' if img[x][y]&lt;thresh else 'black')
</pre></div>
<div class="figure">
<p><img alt="rgb.png" src="posts/nano/cnn/cifar-10/rgb.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5e1356e">
<h2 id="org5e1356e">Define the Network <a href="http://pytorch.org/docs/stable/nn.html">Architecture</a></h2>
<div class="outline-text-2" id="text-org5e1356e">
<p>This time, you'll define a CNN architecture. Instead of an MLP, which used linear, fully-connected layers, you'll use the following:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/stable/nn.html#conv2d">Convolutional layers</a>, which can be thought of as stack of filtered images.</li>
<li><a href="https://pytorch.org/docs/stable/nn.html#maxpool2d">Maxpooling layers</a>, which reduce the x-y size of an input, keeping only the most <i>active</i> pixels from the previous layer.</li>
<li>The usual Linear + Dropout layers to avoid overfitting and produce a 10-dim output.</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org6b06a9a">
<h2 id="org6b06a9a">Define a model with multiple convolutional layers, and define the feedforward network behavior.</h2>
<div class="outline-text-2" id="text-org6b06a9a">
<p>The more convolutional layers you include, the more complex patterns in color and shape a model can detect. It's suggested that your final model include 2 or 3 convolutional layers as well as linear layers + dropout in between to avoid overfitting.</p>
<p>It's good practice to look at existing research and implementations of related models as a starting point for defining your own models. You may find it useful to look at <a href="https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py">this PyTorch classification example</a> or <a href="https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py">this, more complex Keras example</a> to help decide on a final structure.</p>
<p>This is taken from the <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-an-image-classifier">pytorch tutorial</a>, with padding and dropout added. I also changed the kernel size to 3.</p>
<p>See:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d">Conv2d</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html?highlight=nn%20maxpool#torch.nn.MaxPool2d">MaxPool2d</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html#linear-layers">Linear</a></li>
<li><a href="https://pytorch.org/docs/stable/nn.html#dropout-layers">Dropout</a></li>
<li><a href="https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view">view</a></li>
</ul>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>KERNEL_SIZE = 3
CHANNELS_IN = 3
CHANNELS_OUT_1 = 6
CHANNELS_OUT_2 = 16
CLASSES = 10
PADDING = 1
STRIDE = 1
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>convolutional_1 = nn.Conv2d(CHANNELS_IN, CHANNELS_OUT_1,
                            KERNEL_SIZE, 
                            stride=STRIDE, padding=PADDING)
pool = nn.MaxPool2d(2, 2)
convolutional_2 = nn.Conv2d(CHANNELS_OUT_1, CHANNELS_OUT_2,
                            KERNEL_SIZE,
                            stride=STRIDE, padding=PADDING)

c_no_padding_1 = nn.Conv2d(CHANNELS_IN, CHANNELS_OUT_1, KERNEL_SIZE)
c_no_padding_2 = nn.Conv2d(CHANNELS_OUT_1, CHANNELS_OUT_2, KERNEL_SIZE)
fully_connected_1 = nn.Linear(CHANNELS_OUT_2 * (KERNEL_SIZE + PADDING)**3, 120)
fully_connected_1A = nn.Linear(CHANNELS_OUT_2 * (KERNEL_SIZE)**2, 120)
fully_connected_2 = nn.Linear(120, 84)
fully_connected_3 = nn.Linear(84, CLASSES)
cnn_dropout = nn.Dropout(0.25)
connected_dropout = nn.Dropout(0.5)

dataiter = iter(train_loader)
images, labels = dataiter.next()
input_image = torch.Tensor(images)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print("Input Shape: {}".format(input_image.shape))
x = cnn_dropout(pool(F.relu(convolutional_1(input_image))))
print("Output 1: {}".format(x.shape))
x = cnn_dropout(pool(F.relu(convolutional_2(x))))
print("Output 2: {}".format(x.shape))
x = x.view(x.size()[0], -1)
print("reshaped: {}".format(x.shape))
x = connected_dropout(F.relu(fully_connected_1(x)))
print("Connected Shape: {}".format(x.shape))
x = F.relu(fully_connected_2(x))
print("Connected Shape 2: {}".format(x.shape))
x = fully_connected_3(x)
print("Connected Shape 3: {}".format(x.shape))
</pre></div>
<pre class="example">
Input Shape: torch.Size([20, 3, 32, 32])
Output 1: torch.Size([20, 6, 16, 16])
Output 2: torch.Size([20, 16, 8, 8])
reshaped: torch.Size([20, 1024])
Connected Shape: torch.Size([20, 120])
Connected Shape 2: torch.Size([20, 84])
Connected Shape 3: torch.Size([20, 10])

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print("Input Shape: {}".format(input_image.shape))
x = cnn_dropout(pool(F.relu(c_no_padding_1(input_image))))
print("Output 1: {}".format(x.shape))
x = cnn_dropout(pool(F.relu(c_no_padding_2(x))))
print("Output 2: {}".format(x.shape))
x = x.view(-1, CHANNELS_OUT_2 * (KERNEL_SIZE)**2)
print("reshaped: {}".format(x.shape))
x = connected_dropout(F.relu(fully_connected_1A(x)))
print("Connected Shape: {}".format(x.shape))
x = F.relu(fully_connected_2(x))
print("Connected Shape 2: {}".format(x.shape))
x = fully_connected_3(x)
print("Connected Shape 3: {}".format(x.shape))
</pre></div>
<pre class="example">
Input Shape: torch.Size([20, 3, 32, 32])
Output 1: torch.Size([20, 6, 15, 15])
Output 2: torch.Size([20, 16, 6, 6])
reshaped: torch.Size([80, 144])
Connected Shape: torch.Size([80, 120])
Connected Shape 2: torch.Size([80, 84])
Connected Shape 3: torch.Size([80, 10])

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class CNN(nn.Module):
    """A convolutional neural network for CIFAR-10 images"""
    def __init__(self, filter_size=5) -&gt; None:
        super().__init__()
        self.convolutional_1 = nn.Conv2d(CHANNELS_IN, CHANNELS_OUT_1,
                                         KERNEL_SIZE, 
                                         stride=STRIDE, padding=PADDING)
        self.pool = nn.MaxPool2d(2, 2)
        self.convolutional_2 = nn.Conv2d(CHANNELS_OUT_1, CHANNELS_OUT_2,
                                         KERNEL_SIZE,
                                         stride=STRIDE, padding=PADDING)
        self.fully_connected_1 = nn.Linear(CHANNELS_OUT_2 * (KERNEL_SIZE + PADDING)**3, 120)
        self.fully_connected_2 = nn.Linear(120, 84)
        self.fully_connected_3 = nn.Linear(84, CLASSES)
        self.cnn_dropout = nn.Dropout(0.25)
        self.connected_dropout = nn.Dropout(0.5)
        return

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        """Passes the image through the layers of the network

        Args:
         image: CIFAR image to process
        """
        x = self.cnn_dropout(self.pool(F.relu(self.convolutional_1(x))))
        x = self.cnn_dropout(self.pool(F.relu(self.convolutional_2(x))))
        # flatten to a vector
        x = x.view(x.size()[0], -1)
        x = self.connected_dropout(F.relu(self.fully_connected_1(x)))
        x = F.relu(self.fully_connected_2(x))
        return self.fully_connected_3(x)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model = CNN()
dataiter = iter(train_loader)
images, labels = dataiter.next()
print(images.shape)
print(labels.shape)
output = model(images)
print(output.shape)
</pre></div>
<pre class="example">
torch.Size([20, 3, 32, 32])
torch.Size([20])
torch.Size([20, 10])

</pre></div>
</div>
<div class="outline-2" id="outline-container-orga82b4ea">
<h2 id="orga82b4ea">Output volume for a convolutional layer</h2>
<div class="outline-text-2" id="text-orga82b4ea">
<p>To compute the output size of a given convolutional layer we can perform the following calculation (taken from <a href="http://cs231n.github.io/convolutional-networks/#layers">Stanford's cs231n course</a>):</p>
<p>We can compute the spatial size of the output volume as a function of the input volume size (W), the kernel/filter size (F), the stride with which they are applied (S), and the amount of zero padding used (P) on the border. The correct formula for calculating how many neurons define the output_W is given by <code>(W−F+2P)/S+1</code>.</p>
<p>For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3 output.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org9bed231">
<h2 id="org9bed231">Specify <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">Loss Function</a> and <a href="http://pytorch.org/docs/stable/optim.html">Optimizer</a></h2>
<div class="outline-text-2" id="text-org9bed231">
<p>Decide on a loss and optimization function that is best suited for this classification task. The linked code examples from above, may be a good starting point; <a href="https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py">this PyTorch classification example</a> or <a href="https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py">this, more complex Keras example</a>. Pay close attention to the value for <b>learning rate</b> as this value determines how your model converges to a small error.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>criterion = nn.CrossEntropyLoss()
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org5b6ac9f">
<h2 id="org5b6ac9f">Train the Network</h2>
<div class="outline-text-2" id="text-org5b6ac9f">
<p>Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def train(model: nn.Module, epochs: int=10, model_number: int=0, 
          epoch_offset: int=1, print_every: int=10) -&gt; tuple:
    """Train, validate, and save the model
    This trains the model and validates it, saving the best 
    (based on validation loss) as =model_&lt;number&gt;_cifar.pth=

    Args:
     model: the network to train
     epochs: number of times to repeat training
     model_number: an identifier for the saved hyperparameters file
     epoch_offset: amount of epochs that have occurred previously
     print_every: how often to print output
    Returns:
     filename, training-loss, validation-loss, improvements: the outcomes for the training
    """
    optimizer = optimize.SGD(model.parameters(), lr=0.001, momentum=0.9)
    criterion = nn.CrossEntropyLoss()
    output_file = "model_{}_cifar.pth".format(model_number)
    training_losses = []
    validation_losses = []
    improvements = []
    valid_loss_min = numpy.Inf # track change in validation loss
    epoch_start = epoch_offset
    last_epoch = epoch_start + epochs + 1
    for epoch in range(epoch_start, last_epoch):

        # keep track of training and validation loss
        train_loss = 0.0
        valid_loss = 0.0

        model.train()
        for data, target in train_loader:
            # move tensors to GPU if CUDA is available            
            data, target = data.to(device), target.to(device)
            # clear the gradients of all optimized variables
            optimizer.zero_grad()
            # forward pass: compute predicted outputs by passing inputs to the model
            output = model(data)
            # calculate the batch loss
            loss = criterion(output, target)
            # backward pass: compute gradient of the loss with respect to model parameters
            loss.backward()
            # perform a single optimization step (parameter update)
            optimizer.step()
            # update training loss
            train_loss += loss.item() * data.size(0)

        model.eval()
        for data, target in valid_loader:
            # move tensors to GPU if CUDA is available
            data, target = data.to(device), target.to(device)
            # forward pass: compute predicted outputs by passing inputs to the model
            output = model(data)
            # calculate the batch loss
            loss = criterion(output, target)
            # update total validation loss 
            valid_loss += loss.item() * data.size(0)

        # calculate average losses
        train_loss = train_loss/len(train_loader.dataset)
        valid_loss = valid_loss/len(valid_loader.dataset)

        # print training/validation statistics 
        if not (epoch % print_every):
            print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
                epoch, train_loss, valid_loss))
        training_losses.append(train_loss)
        validation_losses.append(valid_loss)
        # save model if validation loss has decreased
        if valid_loss &lt;= valid_loss_min:
            print('Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'.format(
            valid_loss_min,
            valid_loss))
            torch.save(model.state_dict(), output_file)
            valid_loss_min = valid_loss
            improvements.append(epoch - 1)
    return output_file, training_losses, validation_losses, improvements
</pre></div>
</div>
<div class="outline-3" id="outline-container-org93d7a1f">
<h3 id="org93d7a1f">Pytorch Tutorial Model</h3>
<div class="outline-text-3" id="text-org93d7a1f">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>EPOCHS = 250
</pre></div>
<p>This is only to avoid re-running the initial training and use the saved model. <b>Note:</b> If you use DataParallel you need to save the model using <code>model.module.state_dict()</code> in order to load it later without it. This won't matter if you always use it or never use it, but here I have a model that was trained on a GPU and I'm trying to extend the training with a computer whos GPU is too old for pytorch to use it, so it crashes unless I disable the DataParallel (because I didn't originally save it with <code>model.module.state_dict</code>).</p>
<p><b>Note 2</b>: But if you don't have it in DataParallel then don't use <code>model.module.state_dict</code> because it won't have the <code>module</code> attribute.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def train_and_pickle(model:nn.Module, epochs:int=EPOCHS,
                     model_number:int=2, print_every: int=10) -&gt; dict:
    """Trains and pickles the outcomes of training"""
    path = Path("model_{}_outcomes.pkl".format(model_number))
    existed = False
    epoch_offset = 0
    if path.is_file():
        existed = True
        with path.open("rb") as reader:
            outcomes = pickle.load(reader)
            epoch_offset = len(outcomes["training_loss"])
            model.load_state_dict(torch.load(
                outcome["hyperparameters_file"],
                map_location=device))
    filename, training_loss, validation_loss, improvements  = train(
        model,
        epochs=epochs,
        model_number=model_number,
        epoch_offset=epoch_offset,
        print_every=print_every,
        )

    if existed:
        outcomes["training_loss"] += outcomes["training_loss"]
        outcomes["validation_loss"] += outcomes["validation_loss"]
        outcomes["improvements"] += outcomes["improvements"]
    else:
        outcomes = dict(
            hyperparameters_file=filename,
            outcomes_pickle=path.name,
            training_loss=training_loss,
            validation_loss=validation_loss,
            improvements=improvements,
        )
    with path.open("wb") as writer:
        pickle.dump(outcomes, writer)
    return outcomes
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def update_outcome(outcome: dict, new_outcome: dict) -&gt; dict:
    """Updates the lists in the outcome

    Args:
     outcome: original output of train_and_pickle
     new_outcome: new output of train_and_pickle

    Returns:
     outcome: updated outcome
    """
    for key in ("training_loss", "validation_loss", "improvements"):
        outcome[key] += new_outcome[key]
    return outcome
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga20cda9">
<h3 id="orga20cda9">First Model Training</h3>
<div class="outline-text-3" id="text-orga20cda9">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_2 = CNN()
model_2.to(device)
start = datetime.now()
outcome = train_and_pickle(
    model_2,
    epochs=100,
    model_number=2)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Epoch: 0        Training Loss: 1.834230         Validation Loss: 0.446434
Validation loss decreased (inf --&gt; 0.446434).  Saving model ...
Epoch: 1        Training Loss: 1.685185         Validation Loss: 0.403314
Validation loss decreased (0.446434 --&gt; 0.403314).  Saving model ...
Epoch: 2        Training Loss: 1.602409         Validation Loss: 0.389758
Validation loss decreased (0.403314 --&gt; 0.389758).  Saving model ...
Epoch: 3        Training Loss: 1.551087         Validation Loss: 0.376669
Validation loss decreased (0.389758 --&gt; 0.376669).  Saving model ...
Epoch: 4        Training Loss: 1.524230         Validation Loss: 0.371581
Validation loss decreased (0.376669 --&gt; 0.371581).  Saving model ...
Epoch: 5        Training Loss: 1.496748         Validation Loss: 0.367056
Validation loss decreased (0.371581 --&gt; 0.367056).  Saving model ...
Epoch: 6        Training Loss: 1.479645         Validation Loss: 0.359889
Validation loss decreased (0.367056 --&gt; 0.359889).  Saving model ...
Epoch: 7        Training Loss: 1.462357         Validation Loss: 0.358887
Validation loss decreased (0.359889 --&gt; 0.358887).  Saving model ...
Epoch: 8        Training Loss: 1.454448         Validation Loss: 0.353885
Validation loss decreased (0.358887 --&gt; 0.353885).  Saving model ...
Epoch: 9        Training Loss: 1.442392         Validation Loss: 0.349046
Validation loss decreased (0.353885 --&gt; 0.349046).  Saving model ...
Epoch: 10       Training Loss: 1.435758         Validation Loss: 0.345204
Validation loss decreased (0.349046 --&gt; 0.345204).  Saving model ...
Epoch: 11       Training Loss: 1.428880         Validation Loss: 0.344610
Validation loss decreased (0.345204 --&gt; 0.344610).  Saving model ...
Epoch: 12       Training Loss: 1.420400         Validation Loss: 0.343866
Validation loss decreased (0.344610 --&gt; 0.343866).  Saving model ...
Epoch: 13       Training Loss: 1.409974         Validation Loss: 0.341221
Validation loss decreased (0.343866 --&gt; 0.341221).  Saving model ...
Epoch: 14       Training Loss: 1.400003         Validation Loss: 0.340469
Validation loss decreased (0.341221 --&gt; 0.340469).  Saving model ...
Epoch: 15       Training Loss: 1.396430         Validation Loss: 0.338332
Validation loss decreased (0.340469 --&gt; 0.338332).  Saving model ...
Epoch: 16       Training Loss: 1.396793         Validation Loss: 0.338963
Epoch: 17       Training Loss: 1.391945         Validation Loss: 0.337340
Validation loss decreased (0.338332 --&gt; 0.337340).  Saving model ...
Epoch: 18       Training Loss: 1.383872         Validation Loss: 0.335848
Validation loss decreased (0.337340 --&gt; 0.335848).  Saving model ...
Epoch: 19       Training Loss: 1.371348         Validation Loss: 0.335116
Validation loss decreased (0.335848 --&gt; 0.335116).  Saving model ...
Epoch: 20       Training Loss: 1.374097         Validation Loss: 0.330697
Validation loss decreased (0.335116 --&gt; 0.330697).  Saving model ...
Epoch: 21       Training Loss: 1.373342         Validation Loss: 0.334281
Epoch: 22       Training Loss: 1.366379         Validation Loss: 0.331197
Epoch: 23       Training Loss: 1.366043         Validation Loss: 0.332052
Epoch: 24       Training Loss: 1.359814         Validation Loss: 0.328743
Validation loss decreased (0.330697 --&gt; 0.328743).  Saving model ...
Epoch: 25       Training Loss: 1.359745         Validation Loss: 0.328860
Epoch: 26       Training Loss: 1.353130         Validation Loss: 0.329480
Epoch: 27       Training Loss: 1.352457         Validation Loss: 0.329386
Epoch: 28       Training Loss: 1.348608         Validation Loss: 0.331024
Epoch: 29       Training Loss: 1.346584         Validation Loss: 0.325815
Validation loss decreased (0.328743 --&gt; 0.325815).  Saving model ...
Epoch: 30       Training Loss: 1.341498         Validation Loss: 0.332342
Epoch: 31       Training Loss: 1.339088         Validation Loss: 0.325358
Validation loss decreased (0.325815 --&gt; 0.325358).  Saving model ...
Epoch: 32       Training Loss: 1.347376         Validation Loss: 0.326178
Epoch: 33       Training Loss: 1.342424         Validation Loss: 0.331979
Epoch: 34       Training Loss: 1.339343         Validation Loss: 0.324638
Validation loss decreased (0.325358 --&gt; 0.324638).  Saving model ...
Epoch: 35       Training Loss: 1.332784         Validation Loss: 0.322740
Validation loss decreased (0.324638 --&gt; 0.322740).  Saving model ...
Epoch: 36       Training Loss: 1.335403         Validation Loss: 0.324083
Epoch: 37       Training Loss: 1.332313         Validation Loss: 0.334746
Epoch: 38       Training Loss: 1.329136         Validation Loss: 0.324193
Epoch: 39       Training Loss: 1.327429         Validation Loss: 0.327056
Epoch: 40       Training Loss: 1.328106         Validation Loss: 0.327257
Epoch: 41       Training Loss: 1.330462         Validation Loss: 0.321711
Validation loss decreased (0.322740 --&gt; 0.321711).  Saving model ...
Epoch: 42       Training Loss: 1.326317         Validation Loss: 0.324698
Epoch: 43       Training Loss: 1.325379         Validation Loss: 0.324895
Epoch: 44       Training Loss: 1.322629         Validation Loss: 0.322434
Epoch: 45       Training Loss: 1.320261         Validation Loss: 0.326130
Epoch: 46       Training Loss: 1.316204         Validation Loss: 0.325013
Epoch: 47       Training Loss: 1.315747         Validation Loss: 0.324042
Epoch: 48       Training Loss: 1.313305         Validation Loss: 0.324592
Epoch: 49       Training Loss: 1.313723         Validation Loss: 0.318290
Validation loss decreased (0.321711 --&gt; 0.318290).  Saving model ...
Epoch: 50       Training Loss: 1.313054         Validation Loss: 0.320845
Epoch: 51       Training Loss: 1.316062         Validation Loss: 0.321215
Epoch: 52       Training Loss: 1.316187         Validation Loss: 0.319871
Epoch: 53       Training Loss: 1.312232         Validation Loss: 0.324769
Epoch: 54       Training Loss: 1.315246         Validation Loss: 0.321788
Epoch: 55       Training Loss: 1.307923         Validation Loss: 0.318943
Epoch: 56       Training Loss: 1.316049         Validation Loss: 0.324919
Epoch: 57       Training Loss: 1.310584         Validation Loss: 0.319344
Epoch: 58       Training Loss: 1.305451         Validation Loss: 0.320848
Epoch: 59       Training Loss: 1.309900         Validation Loss: 0.322148
Epoch: 60       Training Loss: 1.306200         Validation Loss: 0.323148
Epoch: 61       Training Loss: 1.303626         Validation Loss: 0.322406
Epoch: 62       Training Loss: 1.304654         Validation Loss: 0.322471
Epoch: 63       Training Loss: 1.302740         Validation Loss: 0.322596
Epoch: 64       Training Loss: 1.306964         Validation Loss: 0.323696
Epoch: 65       Training Loss: 1.301964         Validation Loss: 0.319375
Epoch: 66       Training Loss: 1.302925         Validation Loss: 0.320327
Epoch: 67       Training Loss: 1.302062         Validation Loss: 0.319882
Epoch: 68       Training Loss: 1.299821         Validation Loss: 0.318813
Epoch: 69       Training Loss: 1.298885         Validation Loss: 0.325837
Epoch: 70       Training Loss: 1.303130         Validation Loss: 0.320493
Epoch: 71       Training Loss: 1.301353         Validation Loss: 0.321375
Epoch: 72       Training Loss: 1.294933         Validation Loss: 0.315513
Validation loss decreased (0.318290 --&gt; 0.315513).  Saving model ...
Epoch: 73       Training Loss: 1.303322         Validation Loss: 0.322531
Epoch: 74       Training Loss: 1.298327         Validation Loss: 0.323503
Epoch: 75       Training Loss: 1.298817         Validation Loss: 0.318616
Epoch: 76       Training Loss: 1.296895         Validation Loss: 0.323739
Epoch: 77       Training Loss: 1.301932         Validation Loss: 0.325410
Epoch: 78       Training Loss: 1.291901         Validation Loss: 0.327083
Epoch: 79       Training Loss: 1.295766         Validation Loss: 0.317765
Epoch: 80       Training Loss: 1.295147         Validation Loss: 0.316187
Epoch: 81       Training Loss: 1.294392         Validation Loss: 0.318913
Epoch: 82       Training Loss: 1.290720         Validation Loss: 0.320984
Epoch: 83       Training Loss: 1.296386         Validation Loss: 0.322005
Epoch: 84       Training Loss: 1.294445         Validation Loss: 0.319135
Epoch: 85       Training Loss: 1.288677         Validation Loss: 0.317673
Epoch: 86       Training Loss: 1.292154         Validation Loss: 0.318644
Epoch: 87       Training Loss: 1.292221         Validation Loss: 0.317595
Epoch: 88       Training Loss: 1.295039         Validation Loss: 0.319856
Epoch: 89       Training Loss: 1.289999         Validation Loss: 0.320703
Epoch: 90       Training Loss: 1.290199         Validation Loss: 0.317269
Epoch: 91       Training Loss: 1.289213         Validation Loss: 0.318887
Epoch: 92       Training Loss: 1.284553         Validation Loss: 0.320420
Epoch: 93       Training Loss: 1.292121         Validation Loss: 0.319414
Epoch: 94       Training Loss: 1.281610         Validation Loss: 0.314129
Validation loss decreased (0.315513 --&gt; 0.314129).  Saving model ...
Epoch: 95       Training Loss: 1.292147         Validation Loss: 0.317541
Epoch: 96       Training Loss: 1.288869         Validation Loss: 0.316178
Epoch: 97       Training Loss: 1.284419         Validation Loss: 0.326122
Epoch: 98       Training Loss: 1.292448         Validation Loss: 0.314851
Epoch: 99       Training Loss: 1.287391         Validation Loss: 0.315212
Epoch: 100      Training Loss: 1.285748         Validation Loss: 0.320298
Elapsed: 1:26:31.644031
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>pickle_path = Path("model_2_outcomes.pkl")
with pickle_path.open("rb") as reader:
    outcome = pickle.load(reader)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_2 = CNN()
model_2.to(device)
start = datetime.now()
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
                                   map_location=device))
outcome_2 = train_and_pickle(model_2, epochs=200, model_number=2)
outcome = update_outcome(outcome, outcome_2)
print("Elapsed: {}".format(datetime.now() - start))
</pre></div>
<pre class="example">
Epoch: 101      Training Loss: 1.293572         Validation Loss: 0.323292
Validation loss decreased (inf --&gt; 0.323292).  Saving model ...
Epoch: 102      Training Loss: 1.286175         Validation Loss: 0.316041
Validation loss decreased (0.323292 --&gt; 0.316041).  Saving model ...
Epoch: 103      Training Loss: 1.292286         Validation Loss: 0.318805
Epoch: 104      Training Loss: 1.287122         Validation Loss: 0.318283
Epoch: 105      Training Loss: 1.285004         Validation Loss: 0.316454
Epoch: 106      Training Loss: 1.288655         Validation Loss: 0.328694
Epoch: 107      Training Loss: 1.286483         Validation Loss: 0.311118
Validation loss decreased (0.316041 --&gt; 0.311118).  Saving model ...
Epoch: 108      Training Loss: 1.286722         Validation Loss: 0.322617
Epoch: 109      Training Loss: 1.281688         Validation Loss: 0.317284
Epoch: 110      Training Loss: 1.286374         Validation Loss: 0.316699
Epoch: 111      Training Loss: 1.285399         Validation Loss: 0.315800
Epoch: 112      Training Loss: 1.283735         Validation Loss: 0.321917
Epoch: 113      Training Loss: 1.283596         Validation Loss: 0.311436
Epoch: 114      Training Loss: 1.285218         Validation Loss: 0.314240
Epoch: 115      Training Loss: 1.282439         Validation Loss: 0.315108
Epoch: 116      Training Loss: 1.282893         Validation Loss: 0.317056
Epoch: 117      Training Loss: 1.282942         Validation Loss: 0.313947
Epoch: 118      Training Loss: 1.287284         Validation Loss: 0.316639
Epoch: 119      Training Loss: 1.285622         Validation Loss: 0.321113
Epoch: 120      Training Loss: 1.284308         Validation Loss: 0.319277
Epoch: 121      Training Loss: 1.282111         Validation Loss: 0.314455
Epoch: 122      Training Loss: 1.283129         Validation Loss: 0.313159
Epoch: 123      Training Loss: 1.284335         Validation Loss: 0.322168
Epoch: 124      Training Loss: 1.278320         Validation Loss: 0.318971
Epoch: 125      Training Loss: 1.281218         Validation Loss: 0.313987
Epoch: 126      Training Loss: 1.279132         Validation Loss: 0.328925
Epoch: 127      Training Loss: 1.279555         Validation Loss: 0.316594
Epoch: 128      Training Loss: 1.273169         Validation Loss: 0.315559
Epoch: 129      Training Loss: 1.277613         Validation Loss: 0.319802
Epoch: 130      Training Loss: 1.280081         Validation Loss: 0.322822
Epoch: 131      Training Loss: 1.281299         Validation Loss: 0.317239
Epoch: 132      Training Loss: 1.280862         Validation Loss: 0.317907
Epoch: 133      Training Loss: 1.280196         Validation Loss: 0.323627
Epoch: 134      Training Loss: 1.278056         Validation Loss: 0.315584
Epoch: 135      Training Loss: 1.271644         Validation Loss: 0.317295
Epoch: 136      Training Loss: 1.276935         Validation Loss: 0.325810
Epoch: 137      Training Loss: 1.279832         Validation Loss: 0.320269
Epoch: 138      Training Loss: 1.276127         Validation Loss: 0.320572
Epoch: 139      Training Loss: 1.276283         Validation Loss: 0.319130
Epoch: 140      Training Loss: 1.274293         Validation Loss: 0.324264
Epoch: 141      Training Loss: 1.276226         Validation Loss: 0.318521
Epoch: 142      Training Loss: 1.273648         Validation Loss: 0.317698
Epoch: 143      Training Loss: 1.280384         Validation Loss: 0.318762
Epoch: 144      Training Loss: 1.271613         Validation Loss: 0.321056
Epoch: 145      Training Loss: 1.279159         Validation Loss: 0.319677
Epoch: 146      Training Loss: 1.277133         Validation Loss: 0.313412
Epoch: 147      Training Loss: 1.273115         Validation Loss: 0.316693
Epoch: 148      Training Loss: 1.276824         Validation Loss: 0.324270
Epoch: 149      Training Loss: 1.271500         Validation Loss: 0.317610
Epoch: 150      Training Loss: 1.274339         Validation Loss: 0.319794
Epoch: 151      Training Loss: 1.276326         Validation Loss: 0.316618
Epoch: 152      Training Loss: 1.274265         Validation Loss: 0.317560
Epoch: 153      Training Loss: 1.273693         Validation Loss: 0.315664
Epoch: 154      Training Loss: 1.271308         Validation Loss: 0.314383
Epoch: 155      Training Loss: 1.275785         Validation Loss: 0.311731
Epoch: 156      Training Loss: 1.269926         Validation Loss: 0.317802
Epoch: 157      Training Loss: 1.272163         Validation Loss: 0.326034
Epoch: 158      Training Loss: 1.272792         Validation Loss: 0.323937
Epoch: 159      Training Loss: 1.270623         Validation Loss: 0.314596
Epoch: 160      Training Loss: 1.274752         Validation Loss: 0.318708
Epoch: 161      Training Loss: 1.269636         Validation Loss: 0.315447
Epoch: 162      Training Loss: 1.268630         Validation Loss: 0.318611
Epoch: 163      Training Loss: 1.269201         Validation Loss: 0.321739
Epoch: 164      Training Loss: 1.268440         Validation Loss: 0.318679
Epoch: 165      Training Loss: 1.267896         Validation Loss: 0.317043
Epoch: 166      Training Loss: 1.268580         Validation Loss: 0.319146
Epoch: 167      Training Loss: 1.275538         Validation Loss: 0.317928
Epoch: 168      Training Loss: 1.268560         Validation Loss: 0.323980
Epoch: 169      Training Loss: 1.268632         Validation Loss: 0.313479
Epoch: 170      Training Loss: 1.264794         Validation Loss: 0.318113
Epoch: 171      Training Loss: 1.270822         Validation Loss: 0.313195
Epoch: 172      Training Loss: 1.267813         Validation Loss: 0.317769
Epoch: 173      Training Loss: 1.270347         Validation Loss: 0.315005
Epoch: 174      Training Loss: 1.266662         Validation Loss: 0.314660
Epoch: 175      Training Loss: 1.268849         Validation Loss: 0.319801
Epoch: 176      Training Loss: 1.271820         Validation Loss: 0.320086
Epoch: 177      Training Loss: 1.273374         Validation Loss: 0.318641
Epoch: 178      Training Loss: 1.265961         Validation Loss: 0.314708
Epoch: 179      Training Loss: 1.271811         Validation Loss: 0.322507
Epoch: 180      Training Loss: 1.263662         Validation Loss: 0.323136
Epoch: 181      Training Loss: 1.269750         Validation Loss: 0.314223
Epoch: 182      Training Loss: 1.269853         Validation Loss: 0.321011
Epoch: 183      Training Loss: 1.267138         Validation Loss: 0.313789
Epoch: 184      Training Loss: 1.271545         Validation Loss: 0.321742
Epoch: 185      Training Loss: 1.268025         Validation Loss: 0.316022
Epoch: 186      Training Loss: 1.272954         Validation Loss: 0.324468
Epoch: 187      Training Loss: 1.267895         Validation Loss: 0.314698
Epoch: 188      Training Loss: 1.266716         Validation Loss: 0.318999
Epoch: 189      Training Loss: 1.263130         Validation Loss: 0.319963
Epoch: 190      Training Loss: 1.270730         Validation Loss: 0.319453
Epoch: 191      Training Loss: 1.265955         Validation Loss: 0.314691
Epoch: 192      Training Loss: 1.267399         Validation Loss: 0.321611
Epoch: 193      Training Loss: 1.264792         Validation Loss: 0.320243
Epoch: 194      Training Loss: 1.262446         Validation Loss: 0.314628
Epoch: 195      Training Loss: 1.262605         Validation Loss: 0.312932
Epoch: 196      Training Loss: 1.265456         Validation Loss: 0.313259
Epoch: 197      Training Loss: 1.269357         Validation Loss: 0.311136
Epoch: 198      Training Loss: 1.262179         Validation Loss: 0.312693
Epoch: 199      Training Loss: 1.266902         Validation Loss: 0.313880
Epoch: 200      Training Loss: 1.265160         Validation Loss: 0.312400
Epoch: 201      Training Loss: 1.266844         Validation Loss: 0.316210
Epoch: 202      Training Loss: 1.264941         Validation Loss: 0.317070
Epoch: 203      Training Loss: 1.267308         Validation Loss: 0.321297
Epoch: 204      Training Loss: 1.265302         Validation Loss: 0.318993
Epoch: 205      Training Loss: 1.265829         Validation Loss: 0.313469
Epoch: 206      Training Loss: 1.261570         Validation Loss: 0.321749
Epoch: 207      Training Loss: 1.266412         Validation Loss: 0.310708
Validation loss decreased (0.311118 --&gt; 0.310708).  Saving model ...
Epoch: 208      Training Loss: 1.266944         Validation Loss: 0.318451
Epoch: 209      Training Loss: 1.265850         Validation Loss: 0.315396
Epoch: 210      Training Loss: 1.264065         Validation Loss: 0.315393
Epoch: 211      Training Loss: 1.258434         Validation Loss: 0.315945
Epoch: 212      Training Loss: 1.262104         Validation Loss: 0.317880
Epoch: 213      Training Loss: 1.266053         Validation Loss: 0.326606
Epoch: 214      Training Loss: 1.264815         Validation Loss: 0.317249
Epoch: 215      Training Loss: 1.265139         Validation Loss: 0.319844
Epoch: 216      Training Loss: 1.266425         Validation Loss: 0.320103
Epoch: 217      Training Loss: 1.265218         Validation Loss: 0.313683
Epoch: 218      Training Loss: 1.261013         Validation Loss: 0.316373
Epoch: 219      Training Loss: 1.262247         Validation Loss: 0.313101
Epoch: 220      Training Loss: 1.264393         Validation Loss: 0.314501
Epoch: 221      Training Loss: 1.264149         Validation Loss: 0.315623
Epoch: 222      Training Loss: 1.259319         Validation Loss: 0.318756
Epoch: 223      Training Loss: 1.258570         Validation Loss: 0.319732
Epoch: 224      Training Loss: 1.259029         Validation Loss: 0.311516
Epoch: 225      Training Loss: 1.266348         Validation Loss: 0.314770
Epoch: 226      Training Loss: 1.259851         Validation Loss: 0.321516
Epoch: 227      Training Loss: 1.262397         Validation Loss: 0.314634
Epoch: 228      Training Loss: 1.258319         Validation Loss: 0.314885
Epoch: 229      Training Loss: 1.257705         Validation Loss: 0.313776
Epoch: 230      Training Loss: 1.265772         Validation Loss: 0.317983
Epoch: 231      Training Loss: 1.256625         Validation Loss: 0.315058
Epoch: 232      Training Loss: 1.259640         Validation Loss: 0.315233
Epoch: 233      Training Loss: 1.257951         Validation Loss: 0.312612
Epoch: 234      Training Loss: 1.259246         Validation Loss: 0.318067
Epoch: 235      Training Loss: 1.254118         Validation Loss: 0.319640
Epoch: 236      Training Loss: 1.261764         Validation Loss: 0.323842
Epoch: 237      Training Loss: 1.257337         Validation Loss: 0.312940
Epoch: 238      Training Loss: 1.261468         Validation Loss: 0.312802
Epoch: 239      Training Loss: 1.256006         Validation Loss: 0.317805
Epoch: 240      Training Loss: 1.259415         Validation Loss: 0.313486
Epoch: 241      Training Loss: 1.256178         Validation Loss: 0.314875
Epoch: 242      Training Loss: 1.256519         Validation Loss: 0.313054
Epoch: 243      Training Loss: 1.255753         Validation Loss: 0.310222
Validation loss decreased (0.310708 --&gt; 0.310222).  Saving model ...
Epoch: 244      Training Loss: 1.258942         Validation Loss: 0.329567
Epoch: 245      Training Loss: 1.258942         Validation Loss: 0.311769
Epoch: 246      Training Loss: 1.262446         Validation Loss: 0.313582
Epoch: 247      Training Loss: 1.261230         Validation Loss: 0.318076
Epoch: 248      Training Loss: 1.261161         Validation Loss: 0.314736
Epoch: 249      Training Loss: 1.259770         Validation Loss: 0.313956
Epoch: 250      Training Loss: 1.256420         Validation Loss: 0.312800
Epoch: 251      Training Loss: 1.262006         Validation Loss: 0.316093
Epoch: 252      Training Loss: 1.259628         Validation Loss: 0.314459
Epoch: 253      Training Loss: 1.255323         Validation Loss: 0.320948
Epoch: 254      Training Loss: 1.251152         Validation Loss: 0.312966
Epoch: 255      Training Loss: 1.263651         Validation Loss: 0.324031
Epoch: 256      Training Loss: 1.258022         Validation Loss: 0.317772
Epoch: 257      Training Loss: 1.260936         Validation Loss: 0.316249
Epoch: 258      Training Loss: 1.257661         Validation Loss: 0.318002
Epoch: 259      Training Loss: 1.253739         Validation Loss: 0.317531
Epoch: 260      Training Loss: 1.259165         Validation Loss: 0.318186
Epoch: 261      Training Loss: 1.255523         Validation Loss: 0.315747
Epoch: 262      Training Loss: 1.260258         Validation Loss: 0.323450
Epoch: 263      Training Loss: 1.256247         Validation Loss: 0.315790
Epoch: 264      Training Loss: 1.256523         Validation Loss: 0.322588
Epoch: 265      Training Loss: 1.256251         Validation Loss: 0.316159
Epoch: 266      Training Loss: 1.254540         Validation Loss: 0.317133
Epoch: 267      Training Loss: 1.256788         Validation Loss: 0.320573
Epoch: 268      Training Loss: 1.261198         Validation Loss: 0.326142
Epoch: 269      Training Loss: 1.255286         Validation Loss: 0.311760
Epoch: 270      Training Loss: 1.256038         Validation Loss: 0.320824
Epoch: 271      Training Loss: 1.252561         Validation Loss: 0.313171
Epoch: 272      Training Loss: 1.257770         Validation Loss: 0.318307
Epoch: 273      Training Loss: 1.254161         Validation Loss: 0.309804
Validation loss decreased (0.310222 --&gt; 0.309804).  Saving model ...
Epoch: 274      Training Loss: 1.256829         Validation Loss: 0.318989
Epoch: 275      Training Loss: 1.264886         Validation Loss: 0.317026
Epoch: 276      Training Loss: 1.250972         Validation Loss: 0.315094
Epoch: 277      Training Loss: 1.255500         Validation Loss: 0.324168
Epoch: 278      Training Loss: 1.253158         Validation Loss: 0.321396
Epoch: 279      Training Loss: 1.258170         Validation Loss: 0.320225
Epoch: 280      Training Loss: 1.258867         Validation Loss: 0.318569
Epoch: 281      Training Loss: 1.254345         Validation Loss: 0.316465
Epoch: 282      Training Loss: 1.255778         Validation Loss: 0.314160
Epoch: 283      Training Loss: 1.254325         Validation Loss: 0.313069
Epoch: 284      Training Loss: 1.253357         Validation Loss: 0.328138
Epoch: 285      Training Loss: 1.251017         Validation Loss: 0.316133
Epoch: 286      Training Loss: 1.252227         Validation Loss: 0.316984
Epoch: 287      Training Loss: 1.253182         Validation Loss: 0.313943
Epoch: 288      Training Loss: 1.250671         Validation Loss: 0.318114
Epoch: 289      Training Loss: 1.255845         Validation Loss: 0.316618
Epoch: 290      Training Loss: 1.255237         Validation Loss: 0.312792
Epoch: 291      Training Loss: 1.262059         Validation Loss: 0.314828
Epoch: 292      Training Loss: 1.255877         Validation Loss: 0.318905
Epoch: 293      Training Loss: 1.254416         Validation Loss: 0.314216
Epoch: 294      Training Loss: 1.253497         Validation Loss: 0.314790
Epoch: 295      Training Loss: 1.255368         Validation Loss: 0.321991
Epoch: 296      Training Loss: 1.257793         Validation Loss: 0.317706
Epoch: 297      Training Loss: 1.251250         Validation Loss: 0.316808
Epoch: 298      Training Loss: 1.252172         Validation Loss: 0.315334
Epoch: 299      Training Loss: 1.251001         Validation Loss: 0.314154
Epoch: 300      Training Loss: 1.252786         Validation Loss: 0.320209
Epoch: 301      Training Loss: 1.257268         Validation Loss: 0.319915
Elapsed: 1:15:46.335776
</pre>
<p>It seems to be improving, but really slowly.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>test(model_2)
</pre></div>
<pre class="example">
Test Loss: 1.307058

Test Accuracy of airplane: 57% (572/1000)
Test Accuracy of automobile: 73% (735/1000)
Test Accuracy of  bird: 26% (266/1000)
Test Accuracy of   cat: 35% (357/1000)
Test Accuracy of  deer: 52% (525/1000)
Test Accuracy of   dog: 19% (193/1000)
Test Accuracy of  frog: 79% (798/1000)
Test Accuracy of horse: 59% (598/1000)
Test Accuracy of  ship: 81% (810/1000)
Test Accuracy of truck: 49% (494/1000)

Test Accuracy (Overall): 53% (5348/10000)
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_2 = CNN()
model_2.to(device)
start = datetime.now()
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
                                   map_location=device))
outcome_2 = train_and_pickle(model_2, epochs=200, model_number=2)
outcome = update_outcome(outcome, outcome_2)
print("Elapsed: {}".format(datetime.now() - start))
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
                                   map_location=device))

test(model_2)
</pre></div>
<pre class="example">
Epoch: 202      Training Loss: 1.256388         Validation Loss: 0.313784
Validation loss decreased (inf --&gt; 0.313784).  Saving model ...
Epoch: 203      Training Loss: 1.258825         Validation Loss: 0.317360
Epoch: 204      Training Loss: 1.256599         Validation Loss: 0.316243
Epoch: 205      Training Loss: 1.253339         Validation Loss: 0.322061
Epoch: 206      Training Loss: 1.260164         Validation Loss: 0.319589
Epoch: 207      Training Loss: 1.252303         Validation Loss: 0.318219
Epoch: 208      Training Loss: 1.257676         Validation Loss: 0.326530
Epoch: 209      Training Loss: 1.258256         Validation Loss: 0.322288
Epoch: 210      Training Loss: 1.257436         Validation Loss: 0.316848
Epoch: 211      Training Loss: 1.256364         Validation Loss: 0.313047
Validation loss decreased (0.313784 --&gt; 0.313047).  Saving model ...
Epoch: 212      Training Loss: 1.259785         Validation Loss: 0.321005
Epoch: 213      Training Loss: 1.254453         Validation Loss: 0.307325
Validation loss decreased (0.313047 --&gt; 0.307325).  Saving model ...
Epoch: 214      Training Loss: 1.254806         Validation Loss: 0.320826
Epoch: 215      Training Loss: 1.252779         Validation Loss: 0.320929
Epoch: 216      Training Loss: 1.252038         Validation Loss: 0.320515
Epoch: 217      Training Loss: 1.252444         Validation Loss: 0.317522
Epoch: 218      Training Loss: 1.254665         Validation Loss: 0.313467
Epoch: 219      Training Loss: 1.255900         Validation Loss: 0.315710
Epoch: 220      Training Loss: 1.252430         Validation Loss: 0.321523
Epoch: 221      Training Loss: 1.256561         Validation Loss: 0.310884
Epoch: 222      Training Loss: 1.255160         Validation Loss: 0.309861
Epoch: 223      Training Loss: 1.254754         Validation Loss: 0.319757
Epoch: 224      Training Loss: 1.255497         Validation Loss: 0.318309
Epoch: 225      Training Loss: 1.260697         Validation Loss: 0.314599
Epoch: 226      Training Loss: 1.253136         Validation Loss: 0.318721
Epoch: 227      Training Loss: 1.257839         Validation Loss: 0.312620
Epoch: 228      Training Loss: 1.248965         Validation Loss: 0.320385
Epoch: 229      Training Loss: 1.251453         Validation Loss: 0.318191
Epoch: 230      Training Loss: 1.252814         Validation Loss: 0.324980
Epoch: 231      Training Loss: 1.256732         Validation Loss: 0.318312
Epoch: 232      Training Loss: 1.251452         Validation Loss: 0.319930
Epoch: 233      Training Loss: 1.251726         Validation Loss: 0.311095
Epoch: 234      Training Loss: 1.250112         Validation Loss: 0.318118
Epoch: 235      Training Loss: 1.255064         Validation Loss: 0.311329
Epoch: 236      Training Loss: 1.250156         Validation Loss: 0.322847
Epoch: 237      Training Loss: 1.249897         Validation Loss: 0.310835
Epoch: 238      Training Loss: 1.251495         Validation Loss: 0.322079
Epoch: 239      Training Loss: 1.247715         Validation Loss: 0.321563
Epoch: 240      Training Loss: 1.248373         Validation Loss: 0.328171
Epoch: 241      Training Loss: 1.250492         Validation Loss: 0.321683
Epoch: 242      Training Loss: 1.255231         Validation Loss: 0.313710
Epoch: 243      Training Loss: 1.247742         Validation Loss: 0.318332
Epoch: 244      Training Loss: 1.251414         Validation Loss: 0.315995
Epoch: 245      Training Loss: 1.258454         Validation Loss: 0.317433
Epoch: 246      Training Loss: 1.253335         Validation Loss: 0.317605
Epoch: 247      Training Loss: 1.253148         Validation Loss: 0.316049
Epoch: 248      Training Loss: 1.251510         Validation Loss: 0.312951
Epoch: 249      Training Loss: 1.251977         Validation Loss: 0.321403
Epoch: 250      Training Loss: 1.256146         Validation Loss: 0.320409
Epoch: 251      Training Loss: 1.248189         Validation Loss: 0.317272
Epoch: 252      Training Loss: 1.254679         Validation Loss: 0.317682
Epoch: 253      Training Loss: 1.253137         Validation Loss: 0.317845
Epoch: 254      Training Loss: 1.258417         Validation Loss: 0.317278
Epoch: 255      Training Loss: 1.253359         Validation Loss: 0.319818
Epoch: 256      Training Loss: 1.247390         Validation Loss: 0.320857
Epoch: 257      Training Loss: 1.255359         Validation Loss: 0.317702
Epoch: 258      Training Loss: 1.247608         Validation Loss: 0.316204
Epoch: 259      Training Loss: 1.249561         Validation Loss: 0.312899
Epoch: 260      Training Loss: 1.248591         Validation Loss: 0.322027
Epoch: 261      Training Loss: 1.248232         Validation Loss: 0.316189
Epoch: 262      Training Loss: 1.252761         Validation Loss: 0.317912
Epoch: 263      Training Loss: 1.246621         Validation Loss: 0.317565
Epoch: 264      Training Loss: 1.249730         Validation Loss: 0.321344
Epoch: 265      Training Loss: 1.253313         Validation Loss: 0.317789
Epoch: 266      Training Loss: 1.250943         Validation Loss: 0.319828
Epoch: 267      Training Loss: 1.248345         Validation Loss: 0.319927
Epoch: 268      Training Loss: 1.248811         Validation Loss: 0.316677
Epoch: 269      Training Loss: 1.250617         Validation Loss: 0.311661
Epoch: 270      Training Loss: 1.250927         Validation Loss: 0.324976
Epoch: 271      Training Loss: 1.246129         Validation Loss: 0.321428
Epoch: 272      Training Loss: 1.247270         Validation Loss: 0.313739
Epoch: 273      Training Loss: 1.252439         Validation Loss: 0.314271
Epoch: 274      Training Loss: 1.249031         Validation Loss: 0.315256
Epoch: 275      Training Loss: 1.248926         Validation Loss: 0.318519
Epoch: 276      Training Loss: 1.253851         Validation Loss: 0.317292
Epoch: 277      Training Loss: 1.248241         Validation Loss: 0.312578
Epoch: 278      Training Loss: 1.246958         Validation Loss: 0.317017
Epoch: 279      Training Loss: 1.247038         Validation Loss: 0.317870
Epoch: 280      Training Loss: 1.247711         Validation Loss: 0.320040
Epoch: 281      Training Loss: 1.250939         Validation Loss: 0.319092
Epoch: 282      Training Loss: 1.250168         Validation Loss: 0.318878
Epoch: 283      Training Loss: 1.249140         Validation Loss: 0.323233
Epoch: 284      Training Loss: 1.247192         Validation Loss: 0.320423
Epoch: 285      Training Loss: 1.248637         Validation Loss: 0.321254
Epoch: 286      Training Loss: 1.246468         Validation Loss: 0.322253
Epoch: 287      Training Loss: 1.247990         Validation Loss: 0.316660
Epoch: 288      Training Loss: 1.245704         Validation Loss: 0.327530
Epoch: 289      Training Loss: 1.244317         Validation Loss: 0.316667
Epoch: 290      Training Loss: 1.247457         Validation Loss: 0.316587
Epoch: 291      Training Loss: 1.244423         Validation Loss: 0.323431
Epoch: 292      Training Loss: 1.245140         Validation Loss: 0.319670
Epoch: 293      Training Loss: 1.247903         Validation Loss: 0.315965
Epoch: 294      Training Loss: 1.248071         Validation Loss: 0.314560
Epoch: 295      Training Loss: 1.244779         Validation Loss: 0.321430
Epoch: 296      Training Loss: 1.250301         Validation Loss: 0.314018
Epoch: 297      Training Loss: 1.251302         Validation Loss: 0.316015
Epoch: 298      Training Loss: 1.253560         Validation Loss: 0.315506
Epoch: 299      Training Loss: 1.246812         Validation Loss: 0.323061
Epoch: 300      Training Loss: 1.248937         Validation Loss: 0.315299
Epoch: 301      Training Loss: 1.248918         Validation Loss: 0.318701
Epoch: 302      Training Loss: 1.247325         Validation Loss: 0.315778
Epoch: 303      Training Loss: 1.241974         Validation Loss: 0.315274
Epoch: 304      Training Loss: 1.250347         Validation Loss: 0.315380
Epoch: 305      Training Loss: 1.244912         Validation Loss: 0.316511
Epoch: 306      Training Loss: 1.247815         Validation Loss: 0.317746
Epoch: 307      Training Loss: 1.250566         Validation Loss: 0.314758
Epoch: 308      Training Loss: 1.249454         Validation Loss: 0.317377
Epoch: 309      Training Loss: 1.249325         Validation Loss: 0.316275
Epoch: 310      Training Loss: 1.248658         Validation Loss: 0.319433
Epoch: 311      Training Loss: 1.244979         Validation Loss: 0.312409
Epoch: 312      Training Loss: 1.250389         Validation Loss: 0.319627
Epoch: 313      Training Loss: 1.245450         Validation Loss: 0.318461
Epoch: 314      Training Loss: 1.247308         Validation Loss: 0.318554
Epoch: 315      Training Loss: 1.247195         Validation Loss: 0.316582
Epoch: 316      Training Loss: 1.244136         Validation Loss: 0.318103
Epoch: 317      Training Loss: 1.249054         Validation Loss: 0.319848
Epoch: 318      Training Loss: 1.248777         Validation Loss: 0.323786
Epoch: 319      Training Loss: 1.247198         Validation Loss: 0.315047
Epoch: 320      Training Loss: 1.251294         Validation Loss: 0.318657
Epoch: 321      Training Loss: 1.249177         Validation Loss: 0.337516
Epoch: 322      Training Loss: 1.247499         Validation Loss: 0.326684
Epoch: 323      Training Loss: 1.246539         Validation Loss: 0.319658
Epoch: 324      Training Loss: 1.248925         Validation Loss: 0.313511
Epoch: 325      Training Loss: 1.243196         Validation Loss: 0.315549
Epoch: 326      Training Loss: 1.244999         Validation Loss: 0.321060
Epoch: 327      Training Loss: 1.248777         Validation Loss: 0.317293
Epoch: 328      Training Loss: 1.248694         Validation Loss: 0.317218
Epoch: 329      Training Loss: 1.251560         Validation Loss: 0.317921
Epoch: 330      Training Loss: 1.252284         Validation Loss: 0.317201
Epoch: 331      Training Loss: 1.246083         Validation Loss: 0.321029
Epoch: 332      Training Loss: 1.244893         Validation Loss: 0.316990
Epoch: 333      Training Loss: 1.240543         Validation Loss: 0.317590
Epoch: 334      Training Loss: 1.246393         Validation Loss: 0.325721
Epoch: 335      Training Loss: 1.248191         Validation Loss: 0.320632
Epoch: 336      Training Loss: 1.241560         Validation Loss: 0.324130
Epoch: 337      Training Loss: 1.243119         Validation Loss: 0.318852
Epoch: 338      Training Loss: 1.242660         Validation Loss: 0.319926
Epoch: 339      Training Loss: 1.249028         Validation Loss: 0.315266
Epoch: 340      Training Loss: 1.244741         Validation Loss: 0.324272
Epoch: 341      Training Loss: 1.244523         Validation Loss: 0.318710
Epoch: 342      Training Loss: 1.241070         Validation Loss: 0.319939
Epoch: 343      Training Loss: 1.244101         Validation Loss: 0.321822
Epoch: 344      Training Loss: 1.239239         Validation Loss: 0.315630
Epoch: 345      Training Loss: 1.245509         Validation Loss: 0.318808
Epoch: 346      Training Loss: 1.245012         Validation Loss: 0.320597
Epoch: 347      Training Loss: 1.251397         Validation Loss: 0.318575
Epoch: 348      Training Loss: 1.240546         Validation Loss: 0.313607
Epoch: 349      Training Loss: 1.245582         Validation Loss: 0.317309
Epoch: 350      Training Loss: 1.240588         Validation Loss: 0.319662
Epoch: 351      Training Loss: 1.241194         Validation Loss: 0.316204
Epoch: 352      Training Loss: 1.243081         Validation Loss: 0.321423
Epoch: 353      Training Loss: 1.244287         Validation Loss: 0.316278
Epoch: 354      Training Loss: 1.248997         Validation Loss: 0.322080
Epoch: 355      Training Loss: 1.243133         Validation Loss: 0.314357
Epoch: 356      Training Loss: 1.240463         Validation Loss: 0.317619
Epoch: 357      Training Loss: 1.249085         Validation Loss: 0.317623
Epoch: 358      Training Loss: 1.244508         Validation Loss: 0.316843
Epoch: 359      Training Loss: 1.252762         Validation Loss: 0.317262
Epoch: 360      Training Loss: 1.246585         Validation Loss: 0.321501
Epoch: 361      Training Loss: 1.240622         Validation Loss: 0.318065
Epoch: 362      Training Loss: 1.246144         Validation Loss: 0.317386
Epoch: 363      Training Loss: 1.246127         Validation Loss: 0.314560
Epoch: 364      Training Loss: 1.244285         Validation Loss: 0.318059
Epoch: 365      Training Loss: 1.244826         Validation Loss: 0.317295
Epoch: 366      Training Loss: 1.244527         Validation Loss: 0.313897
Epoch: 367      Training Loss: 1.244683         Validation Loss: 0.325274
Epoch: 368      Training Loss: 1.245969         Validation Loss: 0.325050
Epoch: 369      Training Loss: 1.245889         Validation Loss: 0.317678
Epoch: 370      Training Loss: 1.240173         Validation Loss: 0.321540
Epoch: 371      Training Loss: 1.244970         Validation Loss: 0.318374
Epoch: 372      Training Loss: 1.242400         Validation Loss: 0.322875
Epoch: 373      Training Loss: 1.245613         Validation Loss: 0.319608
Epoch: 374      Training Loss: 1.243773         Validation Loss: 0.322040
Epoch: 375      Training Loss: 1.243070         Validation Loss: 0.320554
Epoch: 376      Training Loss: 1.245695         Validation Loss: 0.321315
Epoch: 377      Training Loss: 1.245310         Validation Loss: 0.321394
Epoch: 378      Training Loss: 1.240203         Validation Loss: 0.316470
Epoch: 379      Training Loss: 1.245251         Validation Loss: 0.317234
Epoch: 380      Training Loss: 1.250027         Validation Loss: 0.330051
Epoch: 381      Training Loss: 1.243686         Validation Loss: 0.322005
Epoch: 382      Training Loss: 1.243251         Validation Loss: 0.315280
Epoch: 383      Training Loss: 1.243953         Validation Loss: 0.326072
Epoch: 384      Training Loss: 1.245808         Validation Loss: 0.316741
Epoch: 385      Training Loss: 1.242827         Validation Loss: 0.315943
Epoch: 386      Training Loss: 1.244012         Validation Loss: 0.310488
Epoch: 387      Training Loss: 1.245015         Validation Loss: 0.314874
Epoch: 388      Training Loss: 1.244292         Validation Loss: 0.317309
Epoch: 389      Training Loss: 1.250823         Validation Loss: 0.313929
Epoch: 390      Training Loss: 1.248937         Validation Loss: 0.314966
Epoch: 391      Training Loss: 1.249134         Validation Loss: 0.321290
Epoch: 392      Training Loss: 1.246164         Validation Loss: 0.316047
Epoch: 393      Training Loss: 1.249995         Validation Loss: 0.318678
Epoch: 394      Training Loss: 1.240377         Validation Loss: 0.327256
Epoch: 395      Training Loss: 1.247659         Validation Loss: 0.317254
Epoch: 396      Training Loss: 1.238285         Validation Loss: 0.314723
Epoch: 397      Training Loss: 1.245013         Validation Loss: 0.324809
Epoch: 398      Training Loss: 1.247650         Validation Loss: 0.330501
Epoch: 399      Training Loss: 1.250368         Validation Loss: 0.318667
Epoch: 400      Training Loss: 1.246211         Validation Loss: 0.323798
Epoch: 401      Training Loss: 1.239634         Validation Loss: 0.322877
Epoch: 402      Training Loss: 1.248236         Validation Loss: 0.321464
Elapsed: 1:17:57.592411
Test Loss: 1.336450

Test Accuracy of airplane: 55% (553/1000)
Test Accuracy of automobile: 58% (583/1000)
Test Accuracy of  bird: 23% (234/1000)
Test Accuracy of   cat: 30% (307/1000)
Test Accuracy of  deer: 36% (365/1000)
Test Accuracy of   dog: 25% (257/1000)
Test Accuracy of  frog: 88% (880/1000)
Test Accuracy of horse: 69% (694/1000)
Test Accuracy of  ship: 76% (766/1000)
Test Accuracy of truck: 61% (611/1000)

Test Accuracy (Overall): 52% (5250/10000)
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_2 = CNN()
model_2.to(device)
start = datetime.now()
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
                                   map_location=device))
outcome_2 = train_and_pickle(model_2, epochs=200, model_number=2)
outcome = update_outcome(outcome, outcome_2)
print("Elapsed: {}".format(datetime.now() - start))
model_2.load_state_dict(torch.load(outcome["hyperparameters_file"],
                                   map_location=device))
test(model_2)
</pre></div>
<pre class="example">
Epoch: 400      Training Loss: 1.085763         Validation Loss: 0.282825
Validation loss decreased (inf --&gt; 0.282825).  Saving model ...
Epoch: 401      Training Loss: 1.094224         Validation Loss: 0.282336
Validation loss decreased (0.282825 --&gt; 0.282336).  Saving model ...
Epoch: 402      Training Loss: 1.090027         Validation Loss: 0.283988
Epoch: 403      Training Loss: 1.088251         Validation Loss: 0.282374
Epoch: 404      Training Loss: 1.088617         Validation Loss: 0.280398
Validation loss decreased (0.282336 --&gt; 0.280398).  Saving model ...
Epoch: 405      Training Loss: 1.092081         Validation Loss: 0.280428
Epoch: 406      Training Loss: 1.091815         Validation Loss: 0.278766
Validation loss decreased (0.280398 --&gt; 0.278766).  Saving model ...
Epoch: 407      Training Loss: 1.088024         Validation Loss: 0.281447
Epoch: 408      Training Loss: 1.094500         Validation Loss: 0.283386
Epoch: 409      Training Loss: 1.089597         Validation Loss: 0.281148
Epoch: 410      Training Loss: 1.091652         Validation Loss: 0.283893
Epoch: 411      Training Loss: 1.087357         Validation Loss: 0.281366
Epoch: 412      Training Loss: 1.091122         Validation Loss: 0.286320
Epoch: 413      Training Loss: 1.089693         Validation Loss: 0.282684
Epoch: 414      Training Loss: 1.088109         Validation Loss: 0.284077
Epoch: 415      Training Loss: 1.087701         Validation Loss: 0.280002
Epoch: 416      Training Loss: 1.085328         Validation Loss: 0.282377
Epoch: 417      Training Loss: 1.089087         Validation Loss: 0.282623
Epoch: 418      Training Loss: 1.086825         Validation Loss: 0.278291
Validation loss decreased (0.278766 --&gt; 0.278291).  Saving model ...
Epoch: 419      Training Loss: 1.086601         Validation Loss: 0.282585
Epoch: 420      Training Loss: 1.082824         Validation Loss: 0.282660
Epoch: 421      Training Loss: 1.089363         Validation Loss: 0.281838
Epoch: 422      Training Loss: 1.087070         Validation Loss: 0.279197
Epoch: 423      Training Loss: 1.084032         Validation Loss: 0.281605
Epoch: 424      Training Loss: 1.087307         Validation Loss: 0.281069
Epoch: 425      Training Loss: 1.090275         Validation Loss: 0.286235
Epoch: 426      Training Loss: 1.084863         Validation Loss: 0.286024
Epoch: 427      Training Loss: 1.086919         Validation Loss: 0.283765
Epoch: 428      Training Loss: 1.087431         Validation Loss: 0.287237
Epoch: 429      Training Loss: 1.084115         Validation Loss: 0.279592
Epoch: 430      Training Loss: 1.093677         Validation Loss: 0.283081
Epoch: 431      Training Loss: 1.090348         Validation Loss: 0.281837
Epoch: 432      Training Loss: 1.088213         Validation Loss: 0.277247
Validation loss decreased (0.278291 --&gt; 0.277247).  Saving model ...
Epoch: 433      Training Loss: 1.089605         Validation Loss: 0.278821
Epoch: 434      Training Loss: 1.085192         Validation Loss: 0.276951
Validation loss decreased (0.277247 --&gt; 0.276951).  Saving model ...
Epoch: 435      Training Loss: 1.085776         Validation Loss: 0.281023
Epoch: 436      Training Loss: 1.086465         Validation Loss: 0.283929
Epoch: 437      Training Loss: 1.087985         Validation Loss: 0.282887
Epoch: 438      Training Loss: 1.086791         Validation Loss: 0.278656
Epoch: 439      Training Loss: 1.087146         Validation Loss: 0.284559
Epoch: 440      Training Loss: 1.086268         Validation Loss: 0.284008
Epoch: 441      Training Loss: 1.074737         Validation Loss: 0.282008
Epoch: 442      Training Loss: 1.090836         Validation Loss: 0.280691
Epoch: 443      Training Loss: 1.086444         Validation Loss: 0.283169
Epoch: 444      Training Loss: 1.083751         Validation Loss: 0.277424
Epoch: 445      Training Loss: 1.084478         Validation Loss: 0.282735
Epoch: 446      Training Loss: 1.087853         Validation Loss: 0.279917
Epoch: 447      Training Loss: 1.087905         Validation Loss: 0.278547
Epoch: 448      Training Loss: 1.083655         Validation Loss: 0.284014
Epoch: 449      Training Loss: 1.085713         Validation Loss: 0.284066
Epoch: 450      Training Loss: 1.082967         Validation Loss: 0.283472
Epoch: 451      Training Loss: 1.087737         Validation Loss: 0.281544
Epoch: 452      Training Loss: 1.084897         Validation Loss: 0.283131
Epoch: 453      Training Loss: 1.085416         Validation Loss: 0.283956
Epoch: 454      Training Loss: 1.079511         Validation Loss: 0.284032
Epoch: 455      Training Loss: 1.081187         Validation Loss: 0.277546
Epoch: 456      Training Loss: 1.081564         Validation Loss: 0.283062
Epoch: 457      Training Loss: 1.090161         Validation Loss: 0.277227
Epoch: 458      Training Loss: 1.082555         Validation Loss: 0.281654
Epoch: 459      Training Loss: 1.084783         Validation Loss: 0.282357
Epoch: 460      Training Loss: 1.086960         Validation Loss: 0.283228
Epoch: 461      Training Loss: 1.088104         Validation Loss: 0.283043
Epoch: 462      Training Loss: 1.079098         Validation Loss: 0.280849
Epoch: 463      Training Loss: 1.077743         Validation Loss: 0.279460
Epoch: 464      Training Loss: 1.080590         Validation Loss: 0.281254
Epoch: 465      Training Loss: 1.083514         Validation Loss: 0.280558
Epoch: 466      Training Loss: 1.089853         Validation Loss: 0.277356
Epoch: 467      Training Loss: 1.080071         Validation Loss: 0.279764
Epoch: 468      Training Loss: 1.083149         Validation Loss: 0.280320
Epoch: 469      Training Loss: 1.086154         Validation Loss: 0.278509
Epoch: 470      Training Loss: 1.075413         Validation Loss: 0.277589
Epoch: 471      Training Loss: 1.090838         Validation Loss: 0.284972
Epoch: 472      Training Loss: 1.083023         Validation Loss: 0.280417
Epoch: 473      Training Loss: 1.078518         Validation Loss: 0.279890
Epoch: 474      Training Loss: 1.081342         Validation Loss: 0.282047
Epoch: 475      Training Loss: 1.082641         Validation Loss: 0.277632
Epoch: 476      Training Loss: 1.077731         Validation Loss: 0.282896
Epoch: 477      Training Loss: 1.074824         Validation Loss: 0.278524
Epoch: 478      Training Loss: 1.081040         Validation Loss: 0.282670
Epoch: 479      Training Loss: 1.078880         Validation Loss: 0.281313
Epoch: 480      Training Loss: 1.077215         Validation Loss: 0.280679
Epoch: 481      Training Loss: 1.081206         Validation Loss: 0.278332
Epoch: 482      Training Loss: 1.084885         Validation Loss: 0.278158
Epoch: 483      Training Loss: 1.075072         Validation Loss: 0.277820
Epoch: 484      Training Loss: 1.081011         Validation Loss: 0.284402
Epoch: 485      Training Loss: 1.081351         Validation Loss: 0.281961
Epoch: 486      Training Loss: 1.083745         Validation Loss: 0.279679
Epoch: 487      Training Loss: 1.081245         Validation Loss: 0.280318
Epoch: 488      Training Loss: 1.075557         Validation Loss: 0.278577
Epoch: 489      Training Loss: 1.079408         Validation Loss: 0.278910
Epoch: 490      Training Loss: 1.082496         Validation Loss: 0.280904
Epoch: 491      Training Loss: 1.078611         Validation Loss: 0.277847
Epoch: 492      Training Loss: 1.087269         Validation Loss: 0.280784
Epoch: 493      Training Loss: 1.080308         Validation Loss: 0.280509
Epoch: 494      Training Loss: 1.079977         Validation Loss: 0.280467
Epoch: 495      Training Loss: 1.071035         Validation Loss: 0.277071
Epoch: 496      Training Loss: 1.081492         Validation Loss: 0.279537
Epoch: 497      Training Loss: 1.076939         Validation Loss: 0.277763
Epoch: 498      Training Loss: 1.076834         Validation Loss: 0.277170
Epoch: 499      Training Loss: 1.077066         Validation Loss: 0.281241
Epoch: 500      Training Loss: 1.078915         Validation Loss: 0.278007
Elapsed: 1:41:06.408824
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>test(model_2)
</pre></div>
<pre class="example">
Test Loss: 1.336450

Test Accuracy of airplane: 55% (553/1000)
Test Accuracy of automobile: 58% (583/1000)
Test Accuracy of  bird: 23% (234/1000)
Test Accuracy of   cat: 30% (307/1000)
Test Accuracy of  deer: 36% (365/1000)
Test Accuracy of   dog: 25% (257/1000)
Test Accuracy of  frog: 88% (880/1000)
Test Accuracy of horse: 69% (694/1000)
Test Accuracy of  ship: 76% (766/1000)
Test Accuracy of truck: 61% (611/1000)

Test Accuracy (Overall): 52% (5250/10000)
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
figure.suptitle("Filter Size 5 Training/Validation Loss", weight="bold")
x = numpy.arange(len(training_loss_2))
axe.plot(x, training_loss_2, label="Training")
axe.plot(x, validation_loss_2, label="Validation")
axe.set_xlabel("Epoch")
axe.set_ylabel("Cross-Entropy Loss")
labeled = False
for improvement in improvements_2:
    label = "_" if labeled else "Model Improved"
    axe.axvline(improvement, color='r', linestyle='--', label=label)
    labeled = True
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="model_2_training.png" src="posts/nano/cnn/cifar-10/model_2_training.png"></p>
</div>
<p>It looks like the model from the Pytorch tutorial starts to overfit after the 15th epoch (by count, not index).</p>
</div>
</div>
<div class="outline-3" id="outline-container-org941e047">
<h3 id="org941e047">Udacity Model</h3>
<div class="outline-text-3" id="text-org941e047">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_1 = CNN(3)
model_1.to(device)
filename_1, training_loss_1, validation_loss_1, improvements_1  = train(model_1, epochs=30, model_number=1)
</pre></div>
<pre class="example">
Epoch: 1        Training Loss: 1.764122         Validation Loss: 0.408952
Validation loss decreased (inf --&gt; 0.408952).  Saving model ...
Epoch: 2        Training Loss: 1.586364         Validation Loss: 0.383241
Validation loss decreased (0.408952 --&gt; 0.383241).  Saving model ...
Epoch: 3        Training Loss: 1.519929         Validation Loss: 0.371740
Validation loss decreased (0.383241 --&gt; 0.371740).  Saving model ...
Epoch: 4        Training Loss: 1.488349         Validation Loss: 0.362653
Validation loss decreased (0.371740 --&gt; 0.362653).  Saving model ...
Epoch: 5        Training Loss: 1.455125         Validation Loss: 0.358624
Validation loss decreased (0.362653 --&gt; 0.358624).  Saving model ...
Epoch: 6        Training Loss: 1.431836         Validation Loss: 0.353852
Validation loss decreased (0.358624 --&gt; 0.353852).  Saving model ...
Epoch: 7        Training Loss: 1.406383         Validation Loss: 0.351643
Validation loss decreased (0.353852 --&gt; 0.351643).  Saving model ...
Epoch: 8        Training Loss: 1.396167         Validation Loss: 0.342488
Validation loss decreased (0.351643 --&gt; 0.342488).  Saving model ...
Epoch: 9        Training Loss: 1.374800         Validation Loss: 0.344513
Epoch: 10       Training Loss: 1.365321         Validation Loss: 0.339705
Validation loss decreased (0.342488 --&gt; 0.339705).  Saving model ...
Epoch: 11       Training Loss: 1.350646         Validation Loss: 0.334100
Validation loss decreased (0.339705 --&gt; 0.334100).  Saving model ...
Epoch: 12       Training Loss: 1.336463         Validation Loss: 0.342720
Epoch: 13       Training Loss: 1.327740         Validation Loss: 0.329569
Validation loss decreased (0.334100 --&gt; 0.329569).  Saving model ...
Epoch: 14       Training Loss: 1.318054         Validation Loss: 0.330011
Epoch: 15       Training Loss: 1.318000         Validation Loss: 0.331113
Epoch: 16       Training Loss: 1.307698         Validation Loss: 0.325177
Validation loss decreased (0.329569 --&gt; 0.325177).  Saving model ...
Epoch: 17       Training Loss: 1.300564         Validation Loss: 0.324221
Validation loss decreased (0.325177 --&gt; 0.324221).  Saving model ...
Epoch: 18       Training Loss: 1.298909         Validation Loss: 0.323380
Validation loss decreased (0.324221 --&gt; 0.323380).  Saving model ...
Epoch: 19       Training Loss: 1.284629         Validation Loss: 0.317989
Validation loss decreased (0.323380 --&gt; 0.317989).  Saving model ...
Epoch: 20       Training Loss: 1.284566         Validation Loss: 0.316856
Validation loss decreased (0.317989 --&gt; 0.316856).  Saving model ...
Epoch: 21       Training Loss: 1.276280         Validation Loss: 0.320113
Epoch: 22       Training Loss: 1.274713         Validation Loss: 0.320777
Epoch: 23       Training Loss: 1.267952         Validation Loss: 0.317876
Epoch: 24       Training Loss: 1.270328         Validation Loss: 0.311076
Validation loss decreased (0.316856 --&gt; 0.311076).  Saving model ...
Epoch: 25       Training Loss: 1.258179         Validation Loss: 0.313508
Epoch: 26       Training Loss: 1.253091         Validation Loss: 0.314421
Epoch: 27       Training Loss: 1.254100         Validation Loss: 0.312774
Epoch: 28       Training Loss: 1.244802         Validation Loss: 0.311225
Epoch: 29       Training Loss: 1.242637         Validation Loss: 0.310512
Validation loss decreased (0.311076 --&gt; 0.310512).  Saving model ...
Epoch: 30       Training Loss: 1.245316         Validation Loss: 0.311031
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
figure.suptitle("Filter Size 3 Training/Validation Loss", weight="bold")
x = numpy.arange(len(training_loss_1))
axe.plot(x, training_loss_1, label="Training")
axe.plot(x, validation_loss_1, label="Validation")
axe.set_xlabel("Epoch")
axe.set_ylabel("Cross-Entropy Loss")
labeled = False
for improvement in improvements_1:
    label = "_" if labeled else "Model Improved"
    axe.axvline(improvement, color='r', linestyle='--', label=label)
    labeled = True
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="model_1_training.png" src="posts/nano/cnn/cifar-10/model_1_training.png"></p>
</div>
<p>So it looks like there isn't much difference between the models, but the filter size of 3 did slightly better.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org92525ad">
<h2 id="org92525ad">Load the Model with the Lowest Validation Loss</h2>
<div class="outline-text-2" id="text-org92525ad">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_2.load_state_dict(torch.load(outcome["hyperparameters_file"]))
best_model = model_2
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org82d7ed1">
<h2 id="org82d7ed1">Test the Trained Network</h2>
<div class="outline-text-2" id="text-org82d7ed1">
<p>Test your trained model on previously unseen data! A "good" result will be a CNN that gets around 70% (or more, try your best!) accuracy on these test images.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def test(best_model):
    criterion = nn.CrossEntropyLoss()
    # track test loss
    test_loss = 0.0
    class_correct = list(0. for i in range(10))
    class_total = list(0. for i in range(10))

    best_model.to(device)
    best_model.eval()
    # iterate over test data
    for data, target in test_loader:
        # move tensors to GPU if CUDA is available
        data, target = data.to(device), target.to(device)
        # forward pass: compute predicted outputs by passing inputs to the model
        output = best_model(data)
        # calculate the batch loss
        loss = criterion(output, target)
        # update test loss 
        test_loss += loss.item() * data.size(0)
        # convert output probabilities to predicted class
        _, pred = torch.max(output, 1)    
        # compare predictions to true label
        correct_tensor = pred.eq(target.data.view_as(pred))
        correct = (
            numpy.squeeze(correct_tensor.numpy())
            if not train_on_gpu
            else numpy.squeeze(correct_tensor.cpu().numpy()))
        # calculate test accuracy for each object class
        for i in range(BATCH_SIZE):
            label = target.data[i]
            class_correct[label] += correct[i].item()
            class_total[label] += 1

    # average test loss
    test_loss = test_loss/len(test_loader.dataset)
    print('Test Loss: {:.6f}\n'.format(test_loss))

    for i in range(10):
        if class_total[i] &gt; 0:
            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
                classes[i], 100 * class_correct[i] / class_total[i],
                numpy.sum(class_correct[i]), numpy.sum(class_total[i])))
        else:
            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

    print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
        100. * numpy.sum(class_correct) / numpy.sum(class_total),
        numpy.sum(class_correct), numpy.sum(class_total)))
</pre></div>
<p>dataiter = iter(test_loader) images, labels = dataiter.next() images.numpy()</p>
<p>if train_on_gpu: images = images.cuda()</p>
<p>output = model(images)</p>
<p>_, preds_tensor = torch.max(output, 1) preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())</p>
<p>fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[]) imshow(images[idx]) ax.set_title("{} ({})".format(classes[preds[idx]], classes[labels[idx]]), color=("green" if preds[idx]==labels[idx].item() else "red"))</p>
</div>
</div>
<div class="outline-2" id="outline-container-orga0ee2aa">
<h2 id="orga0ee2aa">Make it Easier</h2>
<div class="outline-text-2" id="text-orga0ee2aa">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>means = deviations = (0.5, 0.5, 0.5)
train_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(means, deviations)
    ])
test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(means,
                         deviations)])
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>training_data = datasets.CIFAR10(path.folder, train=True,
                              download=True, transform=train_transform)
test_data = datasets.CIFAR10(path.folder, train=False,
                             download=True, transform=test_transforms)
</pre></div>
<pre class="example">
Files already downloaded and verified
Files already downloaded and verified

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>indices = list(range(len(training_data)))
training_indices, validation_indices = train_test_split(
    indices,
    test_size=VALIDATION_FRACTION)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_sampler = SubsetRandomSampler(training_indices)
valid_sampler = SubsetRandomSampler(validation_indices)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE,
    sampler=train_sampler, num_workers=NUM_WORKERS)
valid_loader = torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE, 
    sampler=valid_sampler, num_workers=NUM_WORKERS)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, 
    num_workers=NUM_WORKERS)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def load_and_train(model_number:int=3, epochs:int=100) -&gt; dict:
    """Load the model using hyperparameters in the dict

    Args:
     model_number: identifier for the model (and its pickles)
     epochs: how many times to repeat training

    Returns:
     outcome: trained model and outcome dict
    """
    model = CNN()
    model = model.to(device)
    start = datetime.now()
    outcome = train_and_pickle(
        model,
        epochs=epochs,
        model_number=model_number)
    model.load_state_dict(torch.load(outcome["hyperparameters_file"],
                                     map_location=device))
    test(model)
    ended = datetime.now()
    print("Ended: {}".format(ended))
    print("Elapsed: {}".format(ended - start))
    return outcome
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_3 = CNN()
model_3.to(device)
start = datetime.now()
outcome = train_and_pickle(
    model_3,
    epochs=100,
    model_number=3)
print("Elapsed: {}".format(datetime.now() - start))
model_3.load_state_dict(torch.load(outcome["hyperparameters_file"],
                                   map_location=device))
test(model_3)
</pre></div>
<pre class="example">
Epoch: 404      Training Loss: 1.766535         Validation Loss: 0.404297
Validation loss decreased (inf --&gt; 0.404297).  Saving model ...
Epoch: 405      Training Loss: 1.539740         Validation Loss: 0.351437
Validation loss decreased (0.404297 --&gt; 0.351437).  Saving model ...
Epoch: 406      Training Loss: 1.408876         Validation Loss: 0.327341
Validation loss decreased (0.351437 --&gt; 0.327341).  Saving model ...
Epoch: 407      Training Loss: 1.325226         Validation Loss: 0.303032
Validation loss decreased (0.327341 --&gt; 0.303032).  Saving model ...
Epoch: 408      Training Loss: 1.260864         Validation Loss: 0.291623
Validation loss decreased (0.303032 --&gt; 0.291623).  Saving model ...
Epoch: 409      Training Loss: 1.214102         Validation Loss: 0.283056
Validation loss decreased (0.291623 --&gt; 0.283056).  Saving model ...
Epoch: 410      Training Loss: 1.178166         Validation Loss: 0.275751
Validation loss decreased (0.283056 --&gt; 0.275751).  Saving model ...
Epoch: 411      Training Loss: 1.146879         Validation Loss: 0.264309
Validation loss decreased (0.275751 --&gt; 0.264309).  Saving model ...
Epoch: 412      Training Loss: 1.121644         Validation Loss: 0.258764
Validation loss decreased (0.264309 --&gt; 0.258764).  Saving model ...
Epoch: 413      Training Loss: 1.097969         Validation Loss: 0.252846
Validation loss decreased (0.258764 --&gt; 0.252846).  Saving model ...
Epoch: 414      Training Loss: 1.078815         Validation Loss: 0.250729
Validation loss decreased (0.252846 --&gt; 0.250729).  Saving model ...
Epoch: 415      Training Loss: 1.055899         Validation Loss: 0.241823
Validation loss decreased (0.250729 --&gt; 0.241823).  Saving model ...
Epoch: 416      Training Loss: 1.041387         Validation Loss: 0.238933
Validation loss decreased (0.241823 --&gt; 0.238933).  Saving model ...
Epoch: 417      Training Loss: 1.029270         Validation Loss: 0.234940
Validation loss decreased (0.238933 --&gt; 0.234940).  Saving model ...
Epoch: 418      Training Loss: 1.016113         Validation Loss: 0.232727
Validation loss decreased (0.234940 --&gt; 0.232727).  Saving model ...
Epoch: 419      Training Loss: 1.005521         Validation Loss: 0.226466
Validation loss decreased (0.232727 --&gt; 0.226466).  Saving model ...
Epoch: 420      Training Loss: 0.992684         Validation Loss: 0.226542
Epoch: 421      Training Loss: 0.978596         Validation Loss: 0.225691
Validation loss decreased (0.226466 --&gt; 0.225691).  Saving model ...
Epoch: 422      Training Loss: 0.976063         Validation Loss: 0.228258
Epoch: 423      Training Loss: 0.961974         Validation Loss: 0.221933
Validation loss decreased (0.225691 --&gt; 0.221933).  Saving model ...
Epoch: 424      Training Loss: 0.954803         Validation Loss: 0.220159
Validation loss decreased (0.221933 --&gt; 0.220159).  Saving model ...
Epoch: 425      Training Loss: 0.948879         Validation Loss: 0.219641
Validation loss decreased (0.220159 --&gt; 0.219641).  Saving model ...
Epoch: 426      Training Loss: 0.945494         Validation Loss: 0.220472
Epoch: 427      Training Loss: 0.935160         Validation Loss: 0.215726
Validation loss decreased (0.219641 --&gt; 0.215726).  Saving model ...
Epoch: 428      Training Loss: 0.928077         Validation Loss: 0.215445
Validation loss decreased (0.215726 --&gt; 0.215445).  Saving model ...
Epoch: 429      Training Loss: 0.925603         Validation Loss: 0.212353
Validation loss decreased (0.215445 --&gt; 0.212353).  Saving model ...
Epoch: 430      Training Loss: 0.921984         Validation Loss: 0.208420
Validation loss decreased (0.212353 --&gt; 0.208420).  Saving model ...
Epoch: 431      Training Loss: 0.912180         Validation Loss: 0.218620
Epoch: 432      Training Loss: 0.909916         Validation Loss: 0.208612
Epoch: 433      Training Loss: 0.902665         Validation Loss: 0.208177
Validation loss decreased (0.208420 --&gt; 0.208177).  Saving model ...
Epoch: 434      Training Loss: 0.899616         Validation Loss: 0.210920
Epoch: 435      Training Loss: 0.895718         Validation Loss: 0.212328
Epoch: 436      Training Loss: 0.883933         Validation Loss: 0.204341
Validation loss decreased (0.208177 --&gt; 0.204341).  Saving model ...
Epoch: 437      Training Loss: 0.888972         Validation Loss: 0.206792
Epoch: 438      Training Loss: 0.878481         Validation Loss: 0.204317
Validation loss decreased (0.204341 --&gt; 0.204317).  Saving model ...
Epoch: 439      Training Loss: 0.879559         Validation Loss: 0.204447
Epoch: 440      Training Loss: 0.871985         Validation Loss: 0.203039
Validation loss decreased (0.204317 --&gt; 0.203039).  Saving model ...
Epoch: 441      Training Loss: 0.870123         Validation Loss: 0.202717
Validation loss decreased (0.203039 --&gt; 0.202717).  Saving model ...
Epoch: 442      Training Loss: 0.870877         Validation Loss: 0.201654
Validation loss decreased (0.202717 --&gt; 0.201654).  Saving model ...
Epoch: 443      Training Loss: 0.863020         Validation Loss: 0.204858
Epoch: 444      Training Loss: 0.861419         Validation Loss: 0.202981
Epoch: 445      Training Loss: 0.864864         Validation Loss: 0.200853
Validation loss decreased (0.201654 --&gt; 0.200853).  Saving model ...
Epoch: 446      Training Loss: 0.859879         Validation Loss: 0.202888
Epoch: 447      Training Loss: 0.859062         Validation Loss: 0.199505
Validation loss decreased (0.200853 --&gt; 0.199505).  Saving model ...
Epoch: 448      Training Loss: 0.853924         Validation Loss: 0.196931
Validation loss decreased (0.199505 --&gt; 0.196931).  Saving model ...
Epoch: 449      Training Loss: 0.849512         Validation Loss: 0.201266
Epoch: 450      Training Loss: 0.845482         Validation Loss: 0.196021
Validation loss decreased (0.196931 --&gt; 0.196021).  Saving model ...
Epoch: 451      Training Loss: 0.844360         Validation Loss: 0.195308
Validation loss decreased (0.196021 --&gt; 0.195308).  Saving model ...
Epoch: 452      Training Loss: 0.844023         Validation Loss: 0.197164
Epoch: 453      Training Loss: 0.839186         Validation Loss: 0.194882
Validation loss decreased (0.195308 --&gt; 0.194882).  Saving model ...
Epoch: 454      Training Loss: 0.838193         Validation Loss: 0.198097
Epoch: 455      Training Loss: 0.837155         Validation Loss: 0.197095
Epoch: 456      Training Loss: 0.831614         Validation Loss: 0.195633
Epoch: 457      Training Loss: 0.827912         Validation Loss: 0.195327
Epoch: 458      Training Loss: 0.830631         Validation Loss: 0.192197
Validation loss decreased (0.194882 --&gt; 0.192197).  Saving model ...
Epoch: 459      Training Loss: 0.825767         Validation Loss: 0.195351
Epoch: 460      Training Loss: 0.824248         Validation Loss: 0.192982
Epoch: 461      Training Loss: 0.822047         Validation Loss: 0.191864
Validation loss decreased (0.192197 --&gt; 0.191864).  Saving model ...
Epoch: 462      Training Loss: 0.824057         Validation Loss: 0.191095
Validation loss decreased (0.191864 --&gt; 0.191095).  Saving model ...
Epoch: 463      Training Loss: 0.821909         Validation Loss: 0.190179
Validation loss decreased (0.191095 --&gt; 0.190179).  Saving model ...
Epoch: 464      Training Loss: 0.820941         Validation Loss: 0.193425
Epoch: 465      Training Loss: 0.820359         Validation Loss: 0.193750
Epoch: 466      Training Loss: 0.815640         Validation Loss: 0.194663
Epoch: 467      Training Loss: 0.818372         Validation Loss: 0.192682
Epoch: 468      Training Loss: 0.817113         Validation Loss: 0.192452
Epoch: 469      Training Loss: 0.817581         Validation Loss: 0.196727
Epoch: 470      Training Loss: 0.809651         Validation Loss: 0.190927
Epoch: 471      Training Loss: 0.811329         Validation Loss: 0.194151
Epoch: 472      Training Loss: 0.806093         Validation Loss: 0.192417
Epoch: 473      Training Loss: 0.806517         Validation Loss: 0.189602
Validation loss decreased (0.190179 --&gt; 0.189602).  Saving model ...
Epoch: 474      Training Loss: 0.807954         Validation Loss: 0.191487
Epoch: 475      Training Loss: 0.807010         Validation Loss: 0.191636
Epoch: 476      Training Loss: 0.801799         Validation Loss: 0.190896
Epoch: 477      Training Loss: 0.798797         Validation Loss: 0.187708
Validation loss decreased (0.189602 --&gt; 0.187708).  Saving model ...
Epoch: 478      Training Loss: 0.799128         Validation Loss: 0.189194
Epoch: 479      Training Loss: 0.799459         Validation Loss: 0.194036
Epoch: 480      Training Loss: 0.795995         Validation Loss: 0.190724
Epoch: 481      Training Loss: 0.798655         Validation Loss: 0.190721
Epoch: 482      Training Loss: 0.792206         Validation Loss: 0.188309
Epoch: 483      Training Loss: 0.799025         Validation Loss: 0.187985
Epoch: 484      Training Loss: 0.791694         Validation Loss: 0.186556
Validation loss decreased (0.187708 --&gt; 0.186556).  Saving model ...
Epoch: 485      Training Loss: 0.784249         Validation Loss: 0.184879
Validation loss decreased (0.186556 --&gt; 0.184879).  Saving model ...
Epoch: 486      Training Loss: 0.793165         Validation Loss: 0.185806
Epoch: 487      Training Loss: 0.791051         Validation Loss: 0.189010
Epoch: 488      Training Loss: 0.787608         Validation Loss: 0.186931
Epoch: 489      Training Loss: 0.789344         Validation Loss: 0.195780
Epoch: 490      Training Loss: 0.792061         Validation Loss: 0.191099
Epoch: 491      Training Loss: 0.786356         Validation Loss: 0.189476
Epoch: 492      Training Loss: 0.784223         Validation Loss: 0.192026
Epoch: 493      Training Loss: 0.785188         Validation Loss: 0.189652
Epoch: 494      Training Loss: 0.782519         Validation Loss: 0.188833
Epoch: 495      Training Loss: 0.786059         Validation Loss: 0.192020
Epoch: 496      Training Loss: 0.782317         Validation Loss: 0.187162
Epoch: 497      Training Loss: 0.785475         Validation Loss: 0.191352
Epoch: 498      Training Loss: 0.778186         Validation Loss: 0.193208
Epoch: 499      Training Loss: 0.780198         Validation Loss: 0.190525
Epoch: 500      Training Loss: 0.778074         Validation Loss: 0.194126
Epoch: 501      Training Loss: 0.778832         Validation Loss: 0.186440
Epoch: 502      Training Loss: 0.776556         Validation Loss: 0.188577
Epoch: 503      Training Loss: 0.774062         Validation Loss: 0.190385
Epoch: 504      Training Loss: 0.776408         Validation Loss: 0.188763
Elapsed: 0:28:16.205032
Test Loss: 0.925722

Test Accuracy of airplane: 70% (703/1000)
Test Accuracy of automobile: 75% (753/1000)
Test Accuracy of  bird: 47% (470/1000)
Test Accuracy of   cat: 56% (562/1000)
Test Accuracy of  deer: 69% (697/1000)
Test Accuracy of   dog: 53% (536/1000)
Test Accuracy of  frog: 80% (803/1000)
Test Accuracy of horse: 67% (670/1000)
Test Accuracy of  ship: 82% (825/1000)
Test Accuracy of truck: 75% (756/1000)

Test Accuracy (Overall): 67% (6775/10000)
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>test(model_3)
</pre></div>
<pre class="example">
Test Loss: 0.954966

Test Accuracy of airplane: 62% (629/1000)
Test Accuracy of automobile: 76% (766/1000)
Test Accuracy of  bird: 50% (506/1000)
Test Accuracy of   cat: 44% (449/1000)
Test Accuracy of  deer: 66% (666/1000)
Test Accuracy of   dog: 52% (521/1000)
Test Accuracy of  frog: 82% (827/1000)
Test Accuracy of horse: 72% (721/1000)
Test Accuracy of  ship: 82% (829/1000)
Test Accuracy of truck: 67% (672/1000)

Test Accuracy (Overall): 65% (6586/10000)
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>outcome = load_and_train(outcome)
</pre></div>
<pre class="example">
Validation loss decreased (inf --&gt; 0.396734).  Saving model ...
Validation loss decreased (0.396734 --&gt; 0.353406).  Saving model ...
Validation loss decreased (0.353406 --&gt; 0.312288).  Saving model ...
Validation loss decreased (0.312288 --&gt; 0.292421).  Saving model ...
Validation loss decreased (0.292421 --&gt; 0.281344).  Saving model ...
Validation loss decreased (0.281344 --&gt; 0.267129).  Saving model ...
Validation loss decreased (0.267129 --&gt; 0.259950).  Saving model ...
Validation loss decreased (0.259950 --&gt; 0.255096).  Saving model ...
Validation loss decreased (0.255096 --&gt; 0.249626).  Saving model ...
Epoch: 110      Training Loss: 1.074378         Validation Loss: 0.241995
Validation loss decreased (0.249626 --&gt; 0.241995).  Saving model ...
Validation loss decreased (0.241995 --&gt; 0.234253).  Saving model ...
Validation loss decreased (0.234253 --&gt; 0.233744).  Saving model ...
Validation loss decreased (0.233744 --&gt; 0.226195).  Saving model ...
Validation loss decreased (0.226195 --&gt; 0.225804).  Saving model ...
Validation loss decreased (0.225804 --&gt; 0.223489).  Saving model ...
Validation loss decreased (0.223489 --&gt; 0.221263).  Saving model ...
Validation loss decreased (0.221263 --&gt; 0.217546).  Saving model ...
Validation loss decreased (0.217546 --&gt; 0.215720).  Saving model ...
Validation loss decreased (0.215720 --&gt; 0.213332).  Saving model ...
Epoch: 120      Training Loss: 0.952941         Validation Loss: 0.209708
Validation loss decreased (0.213332 --&gt; 0.209708).  Saving model ...
Validation loss decreased (0.209708 --&gt; 0.207232).  Saving model ...
Validation loss decreased (0.207232 --&gt; 0.205873).  Saving model ...
Validation loss decreased (0.205873 --&gt; 0.199750).  Saving model ...
Epoch: 130      Training Loss: 0.898597         Validation Loss: 0.197858
Validation loss decreased (0.199750 --&gt; 0.197858).  Saving model ...
Validation loss decreased (0.197858 --&gt; 0.195818).  Saving model ...
Validation loss decreased (0.195818 --&gt; 0.194920).  Saving model ...
Validation loss decreased (0.194920 --&gt; 0.194267).  Saving model ...
Validation loss decreased (0.194267 --&gt; 0.193904).  Saving model ...
Epoch: 140      Training Loss: 0.856769         Validation Loss: 0.203387
Validation loss decreased (0.193904 --&gt; 0.187780).  Saving model ...
Epoch: 150      Training Loss: 0.842055         Validation Loss: 0.190620
Validation loss decreased (0.187780 --&gt; 0.186874).  Saving model ...
Validation loss decreased (0.186874 --&gt; 0.183554).  Saving model ...
Epoch: 160      Training Loss: 0.821771         Validation Loss: 0.186012
Validation loss decreased (0.183554 --&gt; 0.183435).  Saving model ...
Validation loss decreased (0.183435 --&gt; 0.183237).  Saving model ...
Epoch: 170      Training Loss: 0.807321         Validation Loss: 0.185445
Epoch: 180      Training Loss: 0.796137         Validation Loss: 0.182606
Validation loss decreased (0.183237 --&gt; 0.182606).  Saving model ...
Validation loss decreased (0.182606 --&gt; 0.180978).  Saving model ...
Validation loss decreased (0.180978 --&gt; 0.179344).  Saving model ...
Epoch: 190      Training Loss: 0.792454         Validation Loss: 0.181462
Epoch: 200      Training Loss: 0.777160         Validation Loss: 0.187384
Ended: 2018-12-14 15:45:54.887063
Elapsed: 1:09:07.537337
Test Loss: 0.913029

Test Accuracy of airplane: 71% (715/1000)
Test Accuracy of automobile: 80% (803/1000)
Test Accuracy of  bird: 44% (445/1000)
Test Accuracy of   cat: 49% (496/1000)
Test Accuracy of  deer: 73% (733/1000)
Test Accuracy of   dog: 54% (540/1000)
Test Accuracy of  frog: 78% (788/1000)
Test Accuracy of horse: 74% (747/1000)
Test Accuracy of  ship: 84% (840/1000)
Test Accuracy of truck: 75% (750/1000)

Test Accuracy (Overall): 68% (6857/10000)
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>outcome = load_and_train(outcome)
</pre></div>
<pre class="example">
Validation loss decreased (inf --&gt; 0.405255).  Saving model ...
Validation loss decreased (0.405255 --&gt; 0.347472).  Saving model ...
Validation loss decreased (0.347472 --&gt; 0.318586).  Saving model ...
Validation loss decreased (0.318586 --&gt; 0.301050).  Saving model ...
Validation loss decreased (0.301050 --&gt; 0.287208).  Saving model ...
Validation loss decreased (0.287208 --&gt; 0.279541).  Saving model ...
Epoch: 410      Training Loss: 1.188729         Validation Loss: 0.269707
Validation loss decreased (0.279541 --&gt; 0.269707).  Saving model ...
Validation loss decreased (0.269707 --&gt; 0.262379).  Saving model ...
Validation loss decreased (0.262379 --&gt; 0.254531).  Saving model ...
Validation loss decreased (0.254531 --&gt; 0.252625).  Saving model ...
Validation loss decreased (0.252625 --&gt; 0.240125).  Saving model ...
Validation loss decreased (0.240125 --&gt; 0.235959).  Saving model ...
Validation loss decreased (0.235959 --&gt; 0.234190).  Saving model ...
Validation loss decreased (0.234190 --&gt; 0.231890).  Saving model ...
Validation loss decreased (0.231890 --&gt; 0.226316).  Saving model ...
Epoch: 420      Training Loss: 1.003592         Validation Loss: 0.228944
Validation loss decreased (0.226316 --&gt; 0.224643).  Saving model ...
Validation loss decreased (0.224643 --&gt; 0.222303).  Saving model ...
Validation loss decreased (0.222303 --&gt; 0.221804).  Saving model ...
Validation loss decreased (0.221804 --&gt; 0.219019).  Saving model ...
Validation loss decreased (0.219019 --&gt; 0.211782).  Saving model ...
Epoch: 430      Training Loss: 0.933949         Validation Loss: 0.211028
Validation loss decreased (0.211782 --&gt; 0.211028).  Saving model ...
Validation loss decreased (0.211028 --&gt; 0.210736).  Saving model ...
Validation loss decreased (0.210736 --&gt; 0.207784).  Saving model ...
Validation loss decreased (0.207784 --&gt; 0.204068).  Saving model ...
Validation loss decreased (0.204068 --&gt; 0.202933).  Saving model ...
Validation loss decreased (0.202933 --&gt; 0.201327).  Saving model ...
Epoch: 440      Training Loss: 0.893833         Validation Loss: 0.201305
Validation loss decreased (0.201327 --&gt; 0.201305).  Saving model ...
Validation loss decreased (0.201305 --&gt; 0.200246).  Saving model ...
Validation loss decreased (0.200246 --&gt; 0.199212).  Saving model ...
Validation loss decreased (0.199212 --&gt; 0.198127).  Saving model ...
Validation loss decreased (0.198127 --&gt; 0.197780).  Saving model ...
Epoch: 450      Training Loss: 0.869887         Validation Loss: 0.193194
Validation loss decreased (0.197780 --&gt; 0.193194).  Saving model ...
Validation loss decreased (0.193194 --&gt; 0.192689).  Saving model ...
Validation loss decreased (0.192689 --&gt; 0.191178).  Saving model ...
Epoch: 460      Training Loss: 0.845976         Validation Loss: 0.193641
Validation loss decreased (0.191178 --&gt; 0.190434).  Saving model ...
Epoch: 470      Training Loss: 0.831866         Validation Loss: 0.190801
Validation loss decreased (0.190434 --&gt; 0.189088).  Saving model ...
Epoch: 480      Training Loss: 0.814776         Validation Loss: 0.190064
Validation loss decreased (0.189088 --&gt; 0.189077).  Saving model ...
Validation loss decreased (0.189077 --&gt; 0.188256).  Saving model ...
Validation loss decreased (0.188256 --&gt; 0.185333).  Saving model ...
Epoch: 490      Training Loss: 0.804873         Validation Loss: 0.190370
Epoch: 500      Training Loss: 0.792694         Validation Loss: 0.188568
Ended: 2018-12-14 22:04:39.786682
Elapsed: 0:28:03.912152
Test Loss: 0.933276

Test Accuracy of airplane: 75% (756/1000)
Test Accuracy of automobile: 74% (744/1000)
Test Accuracy of  bird: 48% (482/1000)
Test Accuracy of   cat: 44% (443/1000)
Test Accuracy of  deer: 68% (686/1000)
Test Accuracy of   dog: 50% (502/1000)
Test Accuracy of  frog: 80% (801/1000)
Test Accuracy of horse: 68% (681/1000)
Test Accuracy of  ship: 82% (823/1000)
Test Accuracy of truck: 77% (770/1000)

Test Accuracy (Overall): 66% (6688/10000)
</pre>
<p>The overall test-accuracy is going down - is it overfitting?</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>with open("model_3_outcomes.pkl", "rb") as reader:
    outcome = pickle.load(reader)

outcome = load_and_train(outcome)
</pre></div>
<pre class="example">
Validation loss decreased (inf --&gt; 0.400317).  Saving model ...
Validation loss decreased (0.400317 --&gt; 0.339392).  Saving model ...
Epoch: 810      Training Loss: 1.361320         Validation Loss: 0.310385
Validation loss decreased (0.339392 --&gt; 0.310385).  Saving model ...
Validation loss decreased (0.310385 --&gt; 0.295311).  Saving model ...
Validation loss decreased (0.295311 --&gt; 0.283410).  Saving model ...
Validation loss decreased (0.283410 --&gt; 0.274456).  Saving model ...
Validation loss decreased (0.274456 --&gt; 0.266069).  Saving model ...
Validation loss decreased (0.266069 --&gt; 0.262745).  Saving model ...
Validation loss decreased (0.262745 --&gt; 0.247262).  Saving model ...
Validation loss decreased (0.247262 --&gt; 0.237769).  Saving model ...
Epoch: 820      Training Loss: 1.028606         Validation Loss: 0.236005
Validation loss decreased (0.237769 --&gt; 0.236005).  Saving model ...
Validation loss decreased (0.236005 --&gt; 0.230968).  Saving model ...
Validation loss decreased (0.230968 --&gt; 0.228058).  Saving model ...
Validation loss decreased (0.228058 --&gt; 0.224573).  Saving model ...
Validation loss decreased (0.224573 --&gt; 0.223884).  Saving model ...
Validation loss decreased (0.223884 --&gt; 0.219913).  Saving model ...
Validation loss decreased (0.219913 --&gt; 0.217769).  Saving model ...
Epoch: 830      Training Loss: 0.942998         Validation Loss: 0.215061
Validation loss decreased (0.217769 --&gt; 0.215061).  Saving model ...
Validation loss decreased (0.215061 --&gt; 0.212656).  Saving model ...
Validation loss decreased (0.212656 --&gt; 0.212616).  Saving model ...
Validation loss decreased (0.212616 --&gt; 0.210596).  Saving model ...
Validation loss decreased (0.210596 --&gt; 0.207554).  Saving model ...
Epoch: 840      Training Loss: 0.900498         Validation Loss: 0.208390
Validation loss decreased (0.207554 --&gt; 0.206364).  Saving model ...
Validation loss decreased (0.206364 --&gt; 0.205531).  Saving model ...
Validation loss decreased (0.205531 --&gt; 0.203900).  Saving model ...
Epoch: 850      Training Loss: 0.872049         Validation Loss: 0.205466
Validation loss decreased (0.203900 --&gt; 0.198664).  Saving model ...
Validation loss decreased (0.198664 --&gt; 0.196482).  Saving model ...
Validation loss decreased (0.196482 --&gt; 0.195664).  Saving model ...
Epoch: 860      Training Loss: 0.845757         Validation Loss: 0.198456
Validation loss decreased (0.195664 --&gt; 0.193952).  Saving model ...
Epoch: 870      Training Loss: 0.826413         Validation Loss: 0.195060
Validation loss decreased (0.193952 --&gt; 0.193670).  Saving model ...
Validation loss decreased (0.193670 --&gt; 0.192782).  Saving model ...
Validation loss decreased (0.192782 --&gt; 0.188631).  Saving model ...
Epoch: 880      Training Loss: 0.818928         Validation Loss: 0.199424
Epoch: 890      Training Loss: 0.808009         Validation Loss: 0.191352
Epoch: 900      Training Loss: 0.801281         Validation Loss: 0.196643
Ended: 2018-12-14 22:37:13.843477
Elapsed: 0:29:26.736300
Test Loss: 0.945705

Test Accuracy of airplane: 72% (725/1000)
Test Accuracy of automobile: 78% (782/1000)
Test Accuracy of  bird: 47% (473/1000)
Test Accuracy of   cat: 48% (488/1000)
Test Accuracy of  deer: 70% (705/1000)
Test Accuracy of   dog: 52% (527/1000)
Test Accuracy of  frog: 77% (776/1000)
Test Accuracy of horse: 69% (696/1000)
Test Accuracy of  ship: 84% (844/1000)
Test Accuracy of truck: 70% (703/1000)

Test Accuracy (Overall): 67% (6719/10000)
</pre>
<p>It looks like the overall accuracy dropped slightly beacause the best categories (truck, ship, frog) did worse but the worst categories did slightly better - although not bird for some reason.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orga02a9cd">
<h2 id="orga02a9cd">Take two</h2>
<div class="outline-text-2" id="text-orga02a9cd">
<p>It looks like I wasn't loading the model between each round of epochs…</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>outcome = load_and_train(model_number=4, epochs=200)
</pre></div>
<pre class="example">
Epoch: 0        Training Loss: 1.784692         Validation Loss: 0.410727
Validation loss decreased (inf --&gt; 0.410727).  Saving model ...
Validation loss decreased (0.410727 --&gt; 0.360800).  Saving model ...
Validation loss decreased (0.360800 --&gt; 0.314237).  Saving model ...
Validation loss decreased (0.314237 --&gt; 0.293987).  Saving model ...
Validation loss decreased (0.293987 --&gt; 0.283064).  Saving model ...
Validation loss decreased (0.283064 --&gt; 0.275761).  Saving model ...
Validation loss decreased (0.275761 --&gt; 0.270119).  Saving model ...
Validation loss decreased (0.270119 --&gt; 0.261688).  Saving model ...
Validation loss decreased (0.261688 --&gt; 0.254598).  Saving model ...
Epoch: 10       Training Loss: 1.092668         Validation Loss: 0.254406
Validation loss decreased (0.254598 --&gt; 0.254406).  Saving model ...
Validation loss decreased (0.254406 --&gt; 0.248653).  Saving model ...
Validation loss decreased (0.248653 --&gt; 0.245797).  Saving model ...
Validation loss decreased (0.245797 --&gt; 0.240849).  Saving model ...
Validation loss decreased (0.240849 --&gt; 0.238558).  Saving model ...
Validation loss decreased (0.238558 --&gt; 0.237812).  Saving model ...
Validation loss decreased (0.237812 --&gt; 0.230956).  Saving model ...
Epoch: 20       Training Loss: 0.991010         Validation Loss: 0.225704
Validation loss decreased (0.230956 --&gt; 0.225704).  Saving model ...
Validation loss decreased (0.225704 --&gt; 0.221112).  Saving model ...
Validation loss decreased (0.221112 --&gt; 0.218632).  Saving model ...
Epoch: 30       Training Loss: 0.938513         Validation Loss: 0.220019
Validation loss decreased (0.218632 --&gt; 0.216886).  Saving model ...
Validation loss decreased (0.216886 --&gt; 0.215869).  Saving model ...
Validation loss decreased (0.215869 --&gt; 0.214766).  Saving model ...
Validation loss decreased (0.214766 --&gt; 0.212452).  Saving model ...
Epoch: 40       Training Loss: 0.896510         Validation Loss: 0.212819
Validation loss decreased (0.212452 --&gt; 0.209142).  Saving model ...
Validation loss decreased (0.209142 --&gt; 0.208595).  Saving model ...
Validation loss decreased (0.208595 --&gt; 0.205967).  Saving model ...
Validation loss decreased (0.205967 --&gt; 0.205484).  Saving model ...
Epoch: 50       Training Loss: 0.875811         Validation Loss: 0.207912
Validation loss decreased (0.205484 --&gt; 0.205164).  Saving model ...
Epoch: 60       Training Loss: 0.856581         Validation Loss: 0.208312
Validation loss decreased (0.205164 --&gt; 0.204649).  Saving model ...
Validation loss decreased (0.204649 --&gt; 0.203608).  Saving model ...
Epoch: 70       Training Loss: 0.846062         Validation Loss: 0.214614
Validation loss decreased (0.203608 --&gt; 0.203064).  Saving model ...
Epoch: 80       Training Loss: 0.826153         Validation Loss: 0.212527
Validation loss decreased (0.203064 --&gt; 0.201932).  Saving model ...
Validation loss decreased (0.201932 --&gt; 0.200173).  Saving model ...
Epoch: 90       Training Loss: 0.823697         Validation Loss: 0.204494
Validation loss decreased (0.200173 --&gt; 0.199886).  Saving model ...
Validation loss decreased (0.199886 --&gt; 0.198804).  Saving model ...
Epoch: 100      Training Loss: 0.808043         Validation Loss: 0.205323
Epoch: 110      Training Loss: 0.805417         Validation Loss: 0.201136
Epoch: 120      Training Loss: 0.805155         Validation Loss: 0.204370
Epoch: 130      Training Loss: 0.793174         Validation Loss: 0.214048
Validation loss decreased (0.198804 --&gt; 0.194650).  Saving model ...
Epoch: 140      Training Loss: 0.783871         Validation Loss: 0.200537
Epoch: 150      Training Loss: 0.781592         Validation Loss: 0.203295
Epoch: 160      Training Loss: 0.774657         Validation Loss: 0.199732
Epoch: 170      Training Loss: 0.770487         Validation Loss: 0.205331
Epoch: 180      Training Loss: 0.767693         Validation Loss: 0.202990
Epoch: 190      Training Loss: 0.767225         Validation Loss: 0.203797
Epoch: 200      Training Loss: 0.769268         Validation Loss: 0.196108
Test Loss: 0.974566

Test Accuracy of airplane: 70% (707/1000)
Test Accuracy of automobile: 73% (732/1000)
Test Accuracy of  bird: 45% (453/1000)
Test Accuracy of   cat: 53% (533/1000)
Test Accuracy of  deer: 71% (719/1000)
Test Accuracy of   dog: 42% (429/1000)
Test Accuracy of  frog: 81% (814/1000)
Test Accuracy of horse: 66% (666/1000)
Test Accuracy of  ship: 82% (823/1000)
Test Accuracy of truck: 72% (720/1000)

Test Accuracy (Overall): 65% (6596/10000)
Ended: 2018-12-15 08:33:22.925579
Elapsed: 0:55:24.733457
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>outcome = load_and_train(model_number=4, epochs=200)
</pre></div>
<pre class="example">
Validation loss decreased (inf --&gt; 0.203577).  Saving model ...
Validation loss decreased (0.203577 --&gt; 0.201161).  Saving model ...
Validation loss decreased (0.201161 --&gt; 0.198027).  Saving model ...
Epoch: 210      Training Loss: 0.785905         Validation Loss: 0.199885
Epoch: 220      Training Loss: 0.780148         Validation Loss: 0.199842
Validation loss decreased (0.198027 --&gt; 0.197471).  Saving model ...
Epoch: 230      Training Loss: 0.773492         Validation Loss: 0.206471
Validation loss decreased (0.197471 --&gt; 0.195811).  Saving model ...
Epoch: 240      Training Loss: 0.777896         Validation Loss: 0.201046
Epoch: 250      Training Loss: 0.767602         Validation Loss: 0.203973
Epoch: 260      Training Loss: 0.765374         Validation Loss: 0.205219
Epoch: 270      Training Loss: 0.764604         Validation Loss: 0.202613
Epoch: 280      Training Loss: 0.755534         Validation Loss: 0.201307
Epoch: 290      Training Loss: 0.754538         Validation Loss: 0.199495
Epoch: 300      Training Loss: 0.759395         Validation Loss: 0.206451
Epoch: 310      Training Loss: 0.750621         Validation Loss: 0.203110
Epoch: 320      Training Loss: 0.751456         Validation Loss: 0.206920
Epoch: 330      Training Loss: 0.747122         Validation Loss: 0.199856
Epoch: 340      Training Loss: 0.742640         Validation Loss: 0.211159
Epoch: 350      Training Loss: 0.743110         Validation Loss: 0.214833
Epoch: 360      Training Loss: 0.741861         Validation Loss: 0.207520
Epoch: 370      Training Loss: 0.740826         Validation Loss: 0.210348
Epoch: 380      Training Loss: 0.740333         Validation Loss: 0.207724
Epoch: 390      Training Loss: 0.739157         Validation Loss: 0.204985
Epoch: 400      Training Loss: 0.742582         Validation Loss: 0.204150
Test Loss: 0.979350

Test Accuracy of airplane: 64% (648/1000)
Test Accuracy of automobile: 75% (751/1000)
Test Accuracy of  bird: 43% (430/1000)
Test Accuracy of   cat: 50% (507/1000)
Test Accuracy of  deer: 76% (766/1000)
Test Accuracy of   dog: 44% (443/1000)
Test Accuracy of  frog: 81% (818/1000)
Test Accuracy of horse: 63% (630/1000)
Test Accuracy of  ship: 86% (868/1000)
Test Accuracy of truck: 68% (680/1000)

Test Accuracy (Overall): 65% (6541/10000)
Ended: 2018-12-15 11:19:36.845565
Elapsed: 0:55:22.008796
</pre></div>
</div>
<div class="outline-2" id="outline-container-org307af43">
<h2 id="org307af43">Change the Training and Validation Sets</h2>
<div class="outline-text-2" id="text-org307af43">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>INDICES = list(range(len(training_data)))
DataIterators = (torch.utils.data.dataloader.DataLoader,
                 torch.utils.data.dataloader.DataLoader)

def split_data() -&gt; DataIterators:
    training_indices, validation_indices = train_test_split(
        INDICES,
        test_size=VALIDATION_FRACTION)
    train_sampler = SubsetRandomSampler(training_indices)
    valid_sampler = SubsetRandomSampler(validation_indices)
    train_loader = torch.utils.data.DataLoader(
        training_data, batch_size=BATCH_SIZE,
        sampler=train_sampler, num_workers=NUM_WORKERS)
    valid_loader = torch.utils.data.DataLoader(
        training_data, batch_size=BATCH_SIZE, 
        sampler=valid_sampler, num_workers=NUM_WORKERS)
    return train_loader, valid_loader
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_loader, valid_loader = split_data()
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>for epoch in range(8):
    outcome = load_and_train(model_number=4, epochs=50)
</pre></div>
<pre class="example">
Validation loss decreased (inf --&gt; 0.178021).  Saving model ...
Validation loss decreased (0.178021 --&gt; 0.164977).  Saving model ...
Epoch: 410      Training Loss: 0.790843         Validation Loss: 0.180614
Epoch: 420      Training Loss: 0.779451         Validation Loss: 0.184705
Epoch: 430      Training Loss: 0.776067         Validation Loss: 0.188225
Epoch: 440      Training Loss: 0.767443         Validation Loss: 0.189623
Epoch: 450      Training Loss: 0.763348         Validation Loss: 0.190223
Test Loss: 0.994385

Test Accuracy of airplane: 63% (632/1000)
Test Accuracy of automobile: 73% (738/1000)
Test Accuracy of  bird: 43% (432/1000)
Test Accuracy of   cat: 55% (551/1000)
Test Accuracy of  deer: 73% (731/1000)
Test Accuracy of   dog: 38% (384/1000)
Test Accuracy of  frog: 82% (828/1000)
Test Accuracy of horse: 63% (632/1000)
Test Accuracy of  ship: 88% (880/1000)
Test Accuracy of truck: 65% (658/1000)

Test Accuracy (Overall): 64% (6466/10000)
Ended: 2018-12-15 11:57:44.922535
Elapsed: 0:14:05.152783
Validation loss decreased (inf --&gt; 0.170476).  Saving model ...
Epoch: 810      Training Loss: 0.791785         Validation Loss: 0.185611
Epoch: 820      Training Loss: 0.775938         Validation Loss: 0.185072
Epoch: 830      Training Loss: 0.776210         Validation Loss: 0.187146
Epoch: 840      Training Loss: 0.768063         Validation Loss: 0.182017
Epoch: 850      Training Loss: 0.769061         Validation Loss: 0.196850
Test Loss: 1.012101

Test Accuracy of airplane: 62% (624/1000)
Test Accuracy of automobile: 73% (738/1000)
Test Accuracy of  bird: 42% (429/1000)
Test Accuracy of   cat: 55% (551/1000)
Test Accuracy of  deer: 73% (730/1000)
Test Accuracy of   dog: 42% (420/1000)
Test Accuracy of  frog: 85% (854/1000)
Test Accuracy of horse: 60% (604/1000)
Test Accuracy of  ship: 84% (843/1000)
Test Accuracy of truck: 67% (679/1000)

Test Accuracy (Overall): 64% (6472/10000)
Ended: 2018-12-15 12:12:04.058599
Elapsed: 0:14:19.132241
Validation loss decreased (inf --&gt; 0.174863).  Saving model ...
Epoch: 1610     Training Loss: 0.797948         Validation Loss: 0.176395
Validation loss decreased (0.174863 --&gt; 0.172779).  Saving model ...
Validation loss decreased (0.172779 --&gt; 0.170694).  Saving model ...
Epoch: 1620     Training Loss: 0.789980         Validation Loss: 0.178468
Epoch: 1630     Training Loss: 0.772959         Validation Loss: 0.183980
Epoch: 1640     Training Loss: 0.776142         Validation Loss: 0.198711
Epoch: 1650     Training Loss: 0.767914         Validation Loss: 0.208851
Test Loss: 0.987713

Test Accuracy of airplane: 62% (624/1000)
Test Accuracy of automobile: 74% (743/1000)
Test Accuracy of  bird: 43% (436/1000)
Test Accuracy of   cat: 52% (525/1000)
Test Accuracy of  deer: 73% (734/1000)
Test Accuracy of   dog: 47% (473/1000)
Test Accuracy of  frog: 83% (831/1000)
Test Accuracy of horse: 63% (631/1000)
Test Accuracy of  ship: 84% (845/1000)
Test Accuracy of truck: 68% (682/1000)

Test Accuracy (Overall): 65% (6524/10000)
Ended: 2018-12-15 12:26:50.701191
Elapsed: 0:14:46.638712
Validation loss decreased (inf --&gt; 0.181906).  Saving model ...
Validation loss decreased (0.181906 --&gt; 0.175381).  Saving model ...
Validation loss decreased (0.175381 --&gt; 0.169833).  Saving model ...
Epoch: 3220     Training Loss: 0.776567         Validation Loss: 0.178259
Epoch: 3230     Training Loss: 0.777072         Validation Loss: 0.180300
Epoch: 3240     Training Loss: 0.770289         Validation Loss: 0.192919
Epoch: 3250     Training Loss: 0.762633         Validation Loss: 0.192530
Epoch: 3260     Training Loss: 0.760599         Validation Loss: 0.195964
Test Loss: 0.982302

Test Accuracy of airplane: 66% (665/1000)
Test Accuracy of automobile: 75% (756/1000)
Test Accuracy of  bird: 44% (444/1000)
Test Accuracy of   cat: 56% (565/1000)
Test Accuracy of  deer: 68% (686/1000)
Test Accuracy of   dog: 40% (407/1000)
Test Accuracy of  frog: 85% (855/1000)
Test Accuracy of horse: 63% (639/1000)
Test Accuracy of  ship: 84% (844/1000)
Test Accuracy of truck: 68% (683/1000)

Test Accuracy (Overall): 65% (6544/10000)
Ended: 2018-12-15 12:41:47.333383
Elapsed: 0:14:56.629183
Validation loss decreased (inf --&gt; 0.187802).  Saving model ...
Validation loss decreased (0.187802 --&gt; 0.184430).  Saving model ...
Validation loss decreased (0.184430 --&gt; 0.183925).  Saving model ...
Validation loss decreased (0.183925 --&gt; 0.180367).  Saving model ...
Validation loss decreased (0.180367 --&gt; 0.173719).  Saving model ...
Epoch: 6440     Training Loss: 0.778801         Validation Loss: 0.190905
Epoch: 6450     Training Loss: 0.771958         Validation Loss: 0.182070
Epoch: 6460     Training Loss: 0.764318         Validation Loss: 0.190349
Epoch: 6470     Training Loss: 0.766295         Validation Loss: 0.192508
Epoch: 6480     Training Loss: 0.761968         Validation Loss: 0.189583
Test Loss: 0.987995

Test Accuracy of airplane: 66% (661/1000)
Test Accuracy of automobile: 76% (763/1000)
Test Accuracy of  bird: 44% (443/1000)
Test Accuracy of   cat: 55% (557/1000)
Test Accuracy of  deer: 72% (728/1000)
Test Accuracy of   dog: 41% (415/1000)
Test Accuracy of  frog: 85% (853/1000)
Test Accuracy of horse: 60% (600/1000)
Test Accuracy of  ship: 84% (849/1000)
Test Accuracy of truck: 66% (669/1000)

Test Accuracy (Overall): 65% (6538/10000)
Ended: 2018-12-15 12:56:04.438153
Elapsed: 0:14:17.094202
Validation loss decreased (inf --&gt; 0.191682).  Saving model ...
Validation loss decreased (0.191682 --&gt; 0.182732).  Saving model ...
Validation loss decreased (0.182732 --&gt; 0.181846).  Saving model ...
Epoch: 12870    Training Loss: 0.770414         Validation Loss: 0.185177
Validation loss decreased (0.181846 --&gt; 0.179913).  Saving model ...
Epoch: 12880    Training Loss: 0.772306         Validation Loss: 0.191702
Epoch: 12890    Training Loss: 0.768497         Validation Loss: 0.181795
Epoch: 12900    Training Loss: 0.760247         Validation Loss: 0.183884
Epoch: 12910    Training Loss: 0.757400         Validation Loss: 0.197759
Test Loss: 0.995634

Test Accuracy of airplane: 64% (648/1000)
Test Accuracy of automobile: 75% (755/1000)
Test Accuracy of  bird: 37% (377/1000)
Test Accuracy of   cat: 55% (557/1000)
Test Accuracy of  deer: 72% (726/1000)
Test Accuracy of   dog: 45% (459/1000)
Test Accuracy of  frog: 85% (857/1000)
Test Accuracy of horse: 59% (590/1000)
Test Accuracy of  ship: 84% (842/1000)
Test Accuracy of truck: 69% (696/1000)

Test Accuracy (Overall): 65% (6507/10000)
Ended: 2018-12-15 13:10:05.720077
Elapsed: 0:14:01.278026
Validation loss decreased (inf --&gt; 0.190403).  Saving model ...
Validation loss decreased (0.190403 --&gt; 0.187068).  Saving model ...
Epoch: 25730    Training Loss: 0.768132         Validation Loss: 0.185507
Validation loss decreased (0.187068 --&gt; 0.185507).  Saving model ...
Validation loss decreased (0.185507 --&gt; 0.177258).  Saving model ...
Epoch: 25740    Training Loss: 0.772002         Validation Loss: 0.190112
Epoch: 25750    Training Loss: 0.760312         Validation Loss: 0.195855
Epoch: 25760    Training Loss: 0.759808         Validation Loss: 0.204542
Epoch: 25770    Training Loss: 0.756103         Validation Loss: 0.193606
Test Loss: 0.979529

Test Accuracy of airplane: 66% (663/1000)
Test Accuracy of automobile: 76% (769/1000)
Test Accuracy of  bird: 39% (396/1000)
Test Accuracy of   cat: 57% (578/1000)
Test Accuracy of  deer: 74% (749/1000)
Test Accuracy of   dog: 41% (414/1000)
Test Accuracy of  frog: 83% (833/1000)
Test Accuracy of horse: 61% (618/1000)
Test Accuracy of  ship: 84% (844/1000)
Test Accuracy of truck: 68% (687/1000)

Test Accuracy (Overall): 65% (6551/10000)
Ended: 2018-12-15 13:24:12.319440
Elapsed: 0:14:06.595121
Validation loss decreased (inf --&gt; 0.186117).  Saving model ...
Validation loss decreased (0.186117 --&gt; 0.182822).  Saving model ...
Epoch: 51460    Training Loss: 0.767829         Validation Loss: 0.189161
Epoch: 51470    Training Loss: 0.763347         Validation Loss: 0.194681
Validation loss decreased (0.182822 --&gt; 0.179458).  Saving model ...
Epoch: 51480    Training Loss: 0.756280         Validation Loss: 0.187176
Epoch: 51490    Training Loss: 0.757250         Validation Loss: 0.198088
Epoch: 51500    Training Loss: 0.754145         Validation Loss: 0.204468
Test Loss: 0.973007

Test Accuracy of airplane: 67% (676/1000)
Test Accuracy of automobile: 74% (749/1000)
Test Accuracy of  bird: 41% (415/1000)
Test Accuracy of   cat: 57% (579/1000)
Test Accuracy of  deer: 75% (752/1000)
Test Accuracy of   dog: 41% (412/1000)
Test Accuracy of  frog: 81% (815/1000)
Test Accuracy of horse: 65% (653/1000)
Test Accuracy of  ship: 85% (850/1000)
Test Accuracy of truck: 69% (696/1000)

Test Accuracy (Overall): 65% (6597/10000)
Ended: 2018-12-15 13:38:06.475872
Elapsed: 0:13:54.151685
</pre>
<p>So, this model seems pretty much stuck. I cheated and peaked at the lecturer's solution, but this post is getting too long so I'll save that for another one.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
figure.suptitle("Loss")
x = list(range(len(outcome["training_loss"])))
training = numpy.array(outcome["training_loss"])
limit = 500
axe.plot(x[:limit], training[:limit], ".", label="Training")
axe.plot(x[:limit], outcome["validation_loss"][:limit], ".", label="Validation")
legend = axe.legend()
</pre></div>
<div class="figure">
<p><img alt="final_model.png" src="posts/nano/cnn/cifar-10/final_model.png"></p>
</div>
<p>So it looks like there's something wrong with my code. I'll have to figure this out (or just stick with straight epochs).</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/cnn/visualizing-max-pooling/">Visualizing Max Pooling</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/cnn/visualizing-max-pooling/" rel="bookmark"><time class="published dt-published" datetime="2018-12-03T07:25:07-08:00" itemprop="datePublished" title="2018-12-03 07:25">2018-12-03 07:25</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/cnn/visualizing-max-pooling/#org6b2f0f2">Introduction</a></li>
<li><a href="posts/nano/cnn/visualizing-max-pooling/#orga4e9838">Set Up</a></li>
<li><a href="posts/nano/cnn/visualizing-max-pooling/#orgbfa49fd">Define and visualize the filters</a></li>
<li><a href="posts/nano/cnn/visualizing-max-pooling/#org9a804c5">Define convolutional and pooling layers</a></li>
<li><a href="posts/nano/cnn/visualizing-max-pooling/#org5f31af5">Visualize the output of each filter</a></li>
<li><a href="posts/nano/cnn/visualizing-max-pooling/#orgc4bae0d">ReLu activation</a></li>
<li><a href="posts/nano/cnn/visualizing-max-pooling/#org2a904f0">Visualize the output of the pooling layer</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org6b2f0f2">
<h2 id="org6b2f0f2">Introduction</h2>
<div class="outline-text-2" id="text-org6b2f0f2">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>In this notebook, we will visualize the output of a maxpooling layer in a CNN.</p>
<p>A convolutional layer + activation function, followed by a pooling layer, and a linear layer (to create a desired output size) make up the basic layers of a CNN.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orga4e9838">
<h2 id="orga4e9838">Set Up</h2>
<div class="outline-text-2" id="text-orga4e9838"></div>
<div class="outline-3" id="outline-container-org5663b47">
<h3 id="org5663b47">Imports</h3>
<div class="outline-text-3" id="text-org5663b47"></div>
<div class="outline-4" id="outline-container-org6283859">
<h4 id="org6283859">PyPi</h4>
<div class="outline-text-4" id="text-org6283859">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from dotenv import load_dotenv
import cv2
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgbfd0958">
<h4 id="orgbfd0958">This Project</h4>
<div class="outline-text-4" id="text-orgbfd0958">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfd6f27d">
<h3 id="orgfd6f27d">Plotting</h3>
<div class="outline-text-3" id="text-orgfd6f27d">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Latin Modern Sans", "Lato"],
                "figure.figsize": (14, 12)},
            font_scale=3)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgbbaa723">
<h3 id="orgbbaa723">Load the Data</h3>
<div class="outline-text-3" id="text-orgbbaa723">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>load_dotenv()
path = DataPathTwo("rodin.jpg", "CNN")
print(path.from_folder)
assert path.from_folder.is_file()
</pre></div>
<pre class="example">
/home/brunhilde/datasets/cnn/rodin.jpg

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>bgr_img = cv2.imread(str(path.from_folder))
</pre></div>
</div>
<div class="outline-4" id="outline-container-org7707081">
<h4 id="org7707081">Convert To Grayscale</h4>
<div class="outline-text-4" id="text-org7707081">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgbb83230">
<h4 id="orgbb83230">Normalize: Rescale Entries To Lie In [0,1]</h4>
<div class="outline-text-4" id="text-orgbb83230">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>gray_img = gray_img.astype("float32")/255
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>image = pyplot.imshow(gray_img, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="gray_image.png" src="posts/nano/cnn/visualizing-max-pooling/gray_image.png"></p>
</div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgbfa49fd">
<h2 id="orgbfa49fd">Define and visualize the filters</h2>
<div class="outline-text-2" id="text-orgbfa49fd">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>filter_vals = numpy.array([[-1, -1, -1],
                           [-1, 8, -1],
                           [-1, -1, -1]])
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print('Filter shape: ', filter_vals.shape)
</pre></div>
<pre class="example">
Filter shape:  (3, 3)

</pre></div>
<div class="outline-3" id="outline-container-org48090ff">
<h3 id="org48090ff">Defining four different filters,</h3>
<div class="outline-text-3" id="text-org48090ff">
<p>All of these are linear combinations of the <code>filter_vals</code> defined above</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>filter_1 = filter_vals
filter_2 = -filter_1
filter_3 = filter_1.T
filter_4 = -filter_3
filters = numpy.array([filter_1, filter_2, filter_3, filter_4])
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print('Filter 1: \n', filter_4)
</pre></div>
<pre class="example">
Filter 1: 
 [[ 1  1  1]
 [ 1 -8  1]
 [ 1  1  1]]

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org9a804c5">
<h2 id="org9a804c5">Define convolutional and pooling layers</h2>
<div class="outline-text-2" id="text-org9a804c5">
<p>You've seen how to define a convolutional layer, next is a <b>Pooling Layer</b>.</p>
<p>In the next cell, we initialize a convolutional layer so that it contains all the created filters. Then add a maxpooling layer, <a href="http://pytorch.org/docs/stable/_modules/torch/nn/modules/pooling.html">documented here</a>, with a kernel size of (2x2) so you can see that the image resolution has been reduced after this step.</p>
<p>A maxpooling layer reduces the x-y size of an input and only keeps the most <b>active</b> pixel values. Below is an example of a 2x2 pooling kernel, with a stride of 2, appied to a small patch of grayscale pixel values; reducing the x-y size of the patch by a factor of 2. Only the maximum pixel values in 2x2 remain in the new, pooled output.</p>
<p>Define a neural network with a convolutional layer with four filters <i>and</i> a pooling layer of size (2, 2).</p>
</div>
<div class="outline-3" id="outline-container-org2decc22">
<h3 id="org2decc22">The Model</h3>
<div class="outline-text-3" id="text-org2decc22">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Net(nn.Module):
    """A convolutional neural network to process 4 filters

    Args:
     weight: matrix of filters
    """
    def __init__(self, weight: numpy.ndarray) -&gt; None:
        super(Net, self).__init__()
        # initializes the weights of the convolutional layer to be the weights of the 4 defined filters
        k_height, k_width = weight.shape[2:]
        # assumes there are 4 grayscale filters
        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)
        self.conv.weight = torch.nn.Parameter(weight)
        # define a pooling layer
        self.pool = nn.MaxPool2d(2, 2)
        return

    def forward(self, x: torch.Tensor):
        """calculates the output of a convolutional layer

        Args:
         x: image to process

        Returns:
         layers: convolutional, activated, and pooled layers
        """
        conv_x = self.conv(x)
        activated_x = F.relu(conv_x)

        # applies pooling layer
        pooled_x = self.pool(activated_x)

        # returns all layers
        return conv_x, activated_x, pooled_x
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgdfeac42">
<h3 id="orgdfeac42">instantiate the model and set the weights</h3>
<div class="outline-text-3" id="text-orgdfeac42">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)
model = Net(weight)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print(model)
</pre></div>
<pre class="example">
Net(
  (conv): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), bias=False)
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org5f31af5">
<h2 id="org5f31af5">Visualize the output of each filter</h2>
<div class="outline-text-2" id="text-org5f31af5">
<p>First, we'll define a helper function, <code>viz_layer</code> that takes in a specific layer and number of filters (optional argument), and displays the output of that layer once an image has been passed through.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def viz_layer(layer, n_filters= 4):
    fig = pyplot.figure(figsize=(20, 20))

    for i in range(n_filters):
        ax = fig.add_subplot(1, n_filters, i+1)
        # grab layer outputs
        ax.imshow(numpy.squeeze(layer[0,i].data.numpy()), cmap='gray')
        ax.set_title('Output %s' % str(i+1))
    return
</pre></div>
<p>Let's look at the output of a convolutional layer after a ReLu activation function is applied.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgc4bae0d">
<h2 id="orgc4bae0d">ReLu activation</h2>
<div class="outline-text-2" id="text-orgc4bae0d">
<p>A ReLu function turns all negative pixel values in 0's (black). See the equation pictured below for input pixel values, <code>x</code>.</p>
<div class="figure">
<p><img alt="gray_image.png" src="posts/nano/cnn/visualizing-max-pooling/gray_image.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-org21af29c">
<h3 id="org21af29c">Visualize All the Filters</h3>
<div class="outline-text-3" id="text-org21af29c">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>fig = pyplot.figure(figsize=(12, 6))
fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)
for i in range(4):
    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])
    ax.imshow(filters[i], cmap='gray')
    ax.set_title('Filter %s' % str(i+1))
</pre></div>
<div class="figure">
<p><img alt="filters.png" src="posts/nano/cnn/visualizing-max-pooling/filters.png"></p>
</div>
</div>
<div class="outline-4" id="outline-container-orgd4beb89">
<h4 id="orgd4beb89">convert the image into an input Tensor</h4>
<div class="outline-text-4" id="text-orgd4beb89">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf2773ac">
<h4 id="orgf2773ac">get all the layers</h4>
<div class="outline-text-4" id="text-orgf2773ac">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>conv_layer, activated_layer, pooled_layer = model(gray_img_tensor)
</pre></div>
<p>visualize the output of the activated conv layer</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>viz_layer(activated_layer)
</pre></div>
<div class="figure">
<p><img alt="activated_layer.png" src="posts/nano/cnn/visualizing-max-pooling/activated_layer.png"></p>
</div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org2a904f0">
<h2 id="org2a904f0">Visualize the output of the pooling layer</h2>
<div class="outline-text-2" id="text-org2a904f0">
<p>Then, take a look at the output of a pooling layer. The pooling layer takes as input the feature maps pictured above and reduces the dimensionality of those maps, by some pooling factor, by constructing a new, smaller image of only the maximum (brightest) values in a given kernel area.</p>
<p>Take a look at the values on the x, y axes to see how the image has changed size.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>viz_layer(pooled_layer)
</pre></div>
<div class="figure">
<p><img alt="pooled_layer.png" src="posts/nano/cnn/visualizing-max-pooling/pooled_layer.png"></p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/cnn/visualizing-convolving/">Visualizing Convolving</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/cnn/visualizing-convolving/" rel="bookmark"><time class="published dt-published" datetime="2018-12-02T19:23:25-08:00" itemprop="datePublished" title="2018-12-02 19:23">2018-12-02 19:23</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/cnn/visualizing-convolving/#orgf9fa4a8">Introduction</a></li>
<li><a href="posts/nano/cnn/visualizing-convolving/#org7967c48">Imports</a></li>
<li><a href="posts/nano/cnn/visualizing-convolving/#org301758c">The Image</a></li>
<li><a href="posts/nano/cnn/visualizing-convolving/#org41b57bc">Plot the Image</a></li>
<li><a href="posts/nano/cnn/visualizing-convolving/#org2cd302c">Define and Visualize the Filters</a></li>
<li><a href="posts/nano/cnn/visualizing-convolving/#orgd776a05">Defining four different filters,</a></li>
<li><a href="posts/nano/cnn/visualizing-convolving/#orgd0a09e7">Define a convolutional layer</a></li>
<li><a href="posts/nano/cnn/visualizing-convolving/#org68b522f">ReLu activation</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf9fa4a8">
<h2 id="orgf9fa4a8">Introduction</h2>
<div class="outline-text-2" id="text-orgf9fa4a8">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>In this notebook, we visualize four filtered outputs (a.k.a. activation maps) of a convolutional layer.</p>
<p>In this example, <b>we</b> are defining four filters that are applied to an input image by initializing the <b>weights</b> of a convolutional layer, but a trained CNN will learn the values of these weights.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org7967c48">
<h2 id="org7967c48">Imports</h2>
<div class="outline-text-2" id="text-org7967c48"></div>
<div class="outline-3" id="outline-container-orgbb92f9a">
<h3 id="orgbb92f9a">PyPi</h3>
<div class="outline-text-3" id="text-orgbb92f9a">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from dotenv import load_dotenv
import cv2
import matplotlib.pyplot as pyplot
import numpy
import seaborn
import torch
import torch.nn as nn
import torch.nn.functional as F
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf0e6bdd">
<h3 id="orgf0e6bdd">This Project</h3>
<div class="outline-text-3" id="text-orgf0e6bdd">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2af9fb0">
<h3 id="org2af9fb0">Set Up Plotting</h3>
<div class="outline-text-3" id="text-org2af9fb0">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Latin Modern Sans", "Lato"],
                "figure.figsize": (14, 12)},
            font_scale=3)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org301758c">
<h2 id="org301758c">The Image</h2>
<div class="outline-text-2" id="text-org301758c">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>load_dotenv()
path = DataPathTwo("udacity_sdc.png", folder_key="CNN")
</pre></div>
</div>
<div class="outline-3" id="outline-container-org0a1d94f">
<h3 id="org0a1d94f">Load the Image</h3>
<div class="outline-text-3" id="text-org0a1d94f">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>bgr_img = cv2.imread(str(path.from_folder))
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5bf2647">
<h3 id="org5bf2647">Convert It To Grayscale</h3>
<div class="outline-text-3" id="text-org5bf2647">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org26b767b">
<h3 id="org26b767b">Normalize By Rescaling the Entries To Lie In [0,1]</h3>
<div class="outline-text-3" id="text-org26b767b">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>gray_img = gray_img.astype("float32")/255
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org41b57bc">
<h2 id="org41b57bc">Plot the Image</h2>
<div class="outline-text-2" id="text-org41b57bc">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>image = pyplot.imshow(gray_img, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="grayscale.png" src="posts/nano/cnn/visualizing-convolving/grayscale.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org2cd302c">
<h2 id="org2cd302c">Define and Visualize the Filters</h2>
<div class="outline-text-2" id="text-org2cd302c">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>filter_vals = numpy.array([[-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1]])

print('Filter shape: ', filter_vals.shape)
</pre></div>
<pre class="example">
Filter shape:  (4, 4)

</pre></div>
</div>
<div class="outline-2" id="outline-container-orgd776a05">
<h2 id="orgd776a05">Defining four different filters,</h2>
<div class="outline-text-2" id="text-orgd776a05">
<p>All of these are linear combinations of the <code>filter_vals</code> defined above.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>filter_1 = filter_vals
filter_2 = -filter_1
filter_3 = filter_1.T
filter_4 = -filter_3
filters = numpy.array([filter_1, filter_2, filter_3, filter_4])
</pre></div>
<p>Here's what <code>filter_1</code> has.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print('Filter 1: \n', filter_1)
</pre></div>
<pre class="example">
Filter 1: 
 [[-1 -1  1  1]
 [-1 -1  1  1]
 [-1 -1  1  1]
 [-1 -1  1  1]]

</pre></div>
<div class="outline-3" id="outline-container-org46e6bfc">
<h3 id="org46e6bfc">Visualize All Four Filters</h3>
<div class="outline-text-3" id="text-org46e6bfc">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>fig = pyplot.figure(figsize=(10, 5))
for i in range(4):
    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])
    ax.imshow(filters[i], cmap='gray')
    ax.set_title('Filter %s' % str(i+1))
    width, height = filters[i].shape
    for x in range(width):
        for y in range(height):
            ax.annotate(str(filters[i][x][y]), xy=(y,x),
                        horizontalalignment='center',
                        verticalalignment='center',
                        color='white' if filters[i][x][y]&lt;0 else 'black')
</pre></div>
<div class="figure">
<p><img alt="four_filters.png" src="posts/nano/cnn/visualizing-convolving/four_filters.png"></p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd0a09e7">
<h2 id="orgd0a09e7">Define a convolutional layer</h2>
<div class="outline-text-2" id="text-orgd0a09e7">
<p>The various layers that make up any neural network are documented, <a href="http://pytorch.org/docs/stable/nn.html">here</a>. For a convolutional neural network, we'll start by defining a:</p>
<ul class="org-ul">
<li>Convolutional Layer</li>
</ul>
<p>Initialize a single convolutional layer so that it contains all your created filters. Note that you are not training this network; you are initializing the weights in a convolutional layer so that you can visualize what happens after a forward pass through this network!</p>
</div>
<div class="outline-3" id="outline-container-org991965d">
<h3 id="org991965d"><code>__init__</code> and <code>forward</code></h3>
<div class="outline-text-3" id="text-org991965d">
<p>To define a neural network in PyTorch, you define the layers of a model in the <code>__init__</code> method and define the forward behavior of a network that applyies those initialized layers to an input (<code>x</code>) in the <code>forward</code> method. In PyTorch we convert all inputs into the Tensor datatype, which is similar to a list data type in Python.</p>
<p>Below is a class called <code>Net</code> that has a convolutional layer that can contain four 3x3 grayscale filters.</p>
<p>This will be a neural network with a single convolutional layer with four filters.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Net(nn.Module):
    """CNN To apply 4 filters

    initializes the weights of the convolutional layer to be the 
    weights of the 4 defined filters

    Args:
     weights: array with the four filters
    """
    def __init__(self, weight):
        super(Net, self).__init__()
        k_height, k_width = weight.shape[2:]
        # assumes there are 4 grayscale filters
        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)
        self.conv.weight = torch.nn.Parameter(weight)
        return

    def forward(self, x):
        """calculates the output of a convolutional layer
        pre- and post-activation

        Args:
         x: the image to apply the convolution to

        Returns:
         tuple: convolution output, relu output
        """
        conv_x = self.conv(x)
        activated_x = F.relu(conv_x)

        # returns both layers
        return conv_x, activated_x
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org90aa8b7">
<h3 id="org90aa8b7">Instantiate the Model and Set the Weights</h3>
<div class="outline-text-3" id="text-org90aa8b7">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)
model = Net(weight)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print(model)
</pre></div>
<pre class="example">
Net(
  (conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False)
)

</pre></div>
</div>
<div class="outline-3" id="outline-container-org80fd83d">
<h3 id="org80fd83d">Visualize the output of each filter</h3>
<div class="outline-text-3" id="text-org80fd83d">
<p>First, we'll define a helper function, <code>viz_layer</code> that takes in a specific layer and number of filters (optional argument), and displays the output of that layer once an image has been passed through.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def viz_layer(layer, n_filters= 4):
    fig = pyplot.figure(figsize=(20, 20))

    for i in range(n_filters):
        ax = fig.add_subplot(1, n_filters, i+1, xticks=[], yticks=[])
        # grab layer outputs
        ax.imshow(numpy.squeeze(layer[0,i].data.numpy()), cmap='gray')
        ax.set_title('Output %s' % str(i+1))
    return
</pre></div>
<p>Let's look at the output of a convolutional layer, before and after a ReLu activation function is applied. First, here's our original image again.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>image = pyplot.imshow(gray_img, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="gray_2.png" src="posts/nano/cnn/visualizing-convolving/gray_2.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7cab41d">
<h3 id="org7cab41d">visualize all filters</h3>
<div class="outline-text-3" id="text-org7cab41d">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>fig = pyplot.figure(figsize=(12, 6))
fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)
for i in range(4):
    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])
    ax.imshow(filters[i], cmap='gray')
    ax.set_title('Filter %s' % str(i+1))
</pre></div>
<div class="figure">
<p><img alt="filtered.png" src="posts/nano/cnn/visualizing-convolving/filtered.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgaf0a0d8">
<h3 id="orgaf0a0d8">Convert The Image Into An Input Tensor</h3>
<div class="outline-text-3" id="text-orgaf0a0d8">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgdd5c313">
<h3 id="orgdd5c313">Get The Convolutional Layer (Pre and Post Activation)</h3>
<div class="outline-text-3" id="text-orgdd5c313">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>conv_layer, activated_layer = model(gray_img_tensor)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org75e2925">
<h3 id="org75e2925">Visualize the Output of a Convolutional Layer</h3>
<div class="outline-text-3" id="text-org75e2925">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>viz_layer(conv_layer)
</pre></div>
<div class="figure">
<p><img alt="layer_1.png" src="posts/nano/cnn/visualizing-convolving/layer_1.png"></p>
</div>
<p>Sort of gives it a bas-relief look.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org68b522f">
<h2 id="org68b522f">ReLu activation</h2>
<div class="outline-text-2" id="text-org68b522f">
<p>In this model, we've used an activation function that scales the output of the convolutional layer. We've chose a ReLu function to do this, and this function simply turns all negative pixel values to 0's (black). See the equation pictured below for input pixel values, <code>x</code>.</p>
<p>Visualize the output of an activated conv layer after a ReLu is applied.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>viz_layer(activated_layer)
</pre></div>
<div class="figure">
<p><img alt="activated_layer.png" src="posts/nano/cnn/visualizing-convolving/activated_layer.png"></p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/cnn/custom-filters/">Custom Filters</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/cnn/custom-filters/" rel="bookmark"><time class="published dt-published" datetime="2018-12-02T16:06:32-08:00" itemprop="datePublished" title="2018-12-02 16:06">2018-12-02 16:06</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/cnn/custom-filters/#orgf2169a0">Introduction</a></li>
<li><a href="posts/nano/cnn/custom-filters/#orgbea05a3">Set Up</a></li>
<li><a href="posts/nano/cnn/custom-filters/#orgb49f9bb">Read in the image</a></li>
<li><a href="posts/nano/cnn/custom-filters/#org87415cd">Convert the image to grayscale</a></li>
<li><a href="posts/nano/cnn/custom-filters/#orgb0652e0">Create a custom kernel</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf2169a0">
<h2 id="orgf2169a0">Introduction</h2>
<div class="outline-text-2" id="text-orgf2169a0">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgbea05a3">
<h2 id="orgbea05a3">Set Up</h2>
<div class="outline-text-2" id="text-orgbea05a3"></div>
<div class="outline-3" id="outline-container-org33d3224">
<h3 id="org33d3224">Imports</h3>
<div class="outline-text-3" id="text-org33d3224"></div>
<div class="outline-4" id="outline-container-org442e1ec">
<h4 id="org442e1ec">From PyPi</h4>
<div class="outline-text-4" id="text-org442e1ec">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from dotenv import load_dotenv
import matplotlib.pyplot as pyplot
import matplotlib.image as mpimg
import cv2
import numpy
import seaborn
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4b34961">
<h4 id="org4b34961">This Project</h4>
<div class="outline-text-4" id="text-org4b34961">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPathTwo
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org8f4f89d">
<h3 id="org8f4f89d">Set Up</h3>
<div class="outline-text-3" id="text-org8f4f89d">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Latin Modern Sans", "Lato"],
                "figure.figsize": (14, 12)},
            font_scale=3)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb49f9bb">
<h2 id="orgb49f9bb">Read in the image</h2>
<div class="outline-text-2" id="text-orgb49f9bb">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>load_dotenv()
path = DataPathTwo("curved_lane.jpg", folder_key="CNN")
print(path.from_folder)
assert path.from_folder.is_file()
</pre></div>
<pre class="example">
/home/hades/datasets/cnn/curved_lane.jpg

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>image = mpimg.imread(path.from_folder)

axe_image = pyplot.imshow(image)
</pre></div>
<div class="figure">
<p><img alt="curved_lane.png" src="posts/nano/cnn/custom-filters/curved_lane.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org87415cd">
<h2 id="org87415cd">Convert the image to grayscale</h2>
<div class="outline-text-2" id="text-org87415cd">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
axe_image = pyplot.imshow(gray, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="gray_curved.png" src="posts/nano/cnn/custom-filters/gray_curved.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb0652e0">
<h2 id="orgb0652e0">Create a custom kernel</h2>
<div class="outline-text-2" id="text-orgb0652e0">
<p>Below, you've been given one common type of edge detection filter: a Sobel operator.</p>
<p>The Sobel filter is very commonly used in edge detection and in finding patterns in intensity in an image. Applying a Sobel filter to an image is a way of <b>taking (an approximation) of the derivative of the image</b> in the x or y direction, separately. The operators look as follows.</p>
<div class="figure">
<p><img alt="sobel_ops.png" src="posts/nano/cnn/custom-filters/sobel_ops.png"></p>
</div>
<p>For a challenge, see if you can put the image through a series of filters: first one that blurs the image (takes an average of pixels), and then one that detects the edges.</p>
<p>3x3 array for edge detection</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>sobel_y = numpy.array([[ -1, -2, -1], 
                       [  0, 0, 0], 
                       [ 1, 2, 1]])
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>filtered_image = cv2.filter2D(gray, -1, sobel_y)

axe_image = pyplot.imshow(filtered_image, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="sobel_1.png" src="posts/nano/cnn/custom-filters/sobel_1.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-org730936e">
<h3 id="org730936e">Prewitt</h3>
<div class="outline-text-3" id="text-org730936e">
<p>This matrix is from <a href="https://hipersayanx.blogspot.com/2015/08/convolutional-edge-detection-filters.html">this</a> blog post.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>prewitt = numpy.array([[-1, -1, -1],
                       [0, 0, 0],
                       [1, 1, 1]])
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>filtered_prewitt = cv2.filter2D(gray, -1, prewitt)

axe_image = pyplot.imshow(filtered_prewitt, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="prewitt.png" src="posts/nano/cnn/custom-filters/prewitt.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgec887fb">
<h3 id="orgec887fb">Sharpen</h3>
<div class="outline-text-3" id="text-orgec887fb">
<p>This is from the <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">Wikipedia article</a> about kernels for image processing.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>mask = numpy.array([[0, -1, 0],
                    [-1, 5, -1],
                    [0, -1, 0]])
sharpened = cv2.filter2D(gray, -1, mask)

axe_image = pyplot.imshow(sharpened, cmap='gray')
</pre></div>
<div class="figure">
<p><img alt="sharpen.png" src="posts/nano/cnn/custom-filters/sharpen.png"></p>
</div>
<p>This one isn't so obvious, but if you compare it to the original grayscale image you'll see that it is a little less blurry.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/">MNIST Multi-Layer Perceptron with Validation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/" rel="bookmark"><time class="published dt-published" datetime="2018-11-27T12:02:56-08:00" itemprop="datePublished" title="2018-11-27 12:02">2018-11-27 12:02</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org9067dbd">Introduction</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#orgd5804de">Setup</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org44ce4f2">The Data</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org9fba43a">Visualize a Batch of Training Data</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org7d4f6af">Define the Network Architecture</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org3a2415b">Specify the Loss Function and Optimizer</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#orgab8ea99">Train the Network</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org099589a">Testing the Best Model</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org95b5e2b">Visualize Test Results</a></li>
<li><a href="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/#org5b3e388">Object-Oriented Trainer</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org9067dbd">
<h2 id="org9067dbd">Introduction</h2>
<div class="outline-text-2" id="text-org9067dbd">
<p>This is from <a href="https://github.com/udacity/deep-learning-v2-pytorch.git">Udacity's Deep Learning Repository</a> which supports their Deep Learning Nanodegree.</p>
<p>We are going to train a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multi-Layer Perceptron</a> to classify images from the <a href="http://yann.lecun.com/exdb/mnist/">MNIST database</a> of hand-written digits.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgd5804de">
<h2 id="orgd5804de">Setup</h2>
<div class="outline-text-2" id="text-orgd5804de"></div>
<div class="outline-3" id="outline-container-orgd4679c0">
<h3 id="orgd4679c0">Imports</h3>
<div class="outline-text-3" id="text-orgd4679c0"></div>
<div class="outline-4" id="outline-container-org666165a">
<h4 id="org666165a">From Python</h4>
<div class="outline-text-4" id="text-org666165a">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> <span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
 <span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
 <span class="kn">import</span> <span class="nn">gc</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb0e3e49">
<h4 id="orgb0e3e49">From PyPi</h4>
<div class="outline-text-4" id="text-orgb0e3e49">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> <span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
 <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
 <span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
 <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">pyplot</span>
 <span class="kn">import</span> <span class="nn">seaborn</span>
 <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
 <span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
 <span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>
 <span class="kn">import</span> <span class="nn">torch</span>
 <span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb5b1e91">
<h4 id="orgb5b1e91">This Project</h4>
<div class="outline-text-4" id="text-orgb5b1e91">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> <span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPathTwo</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd8c1292">
<h3 id="orgd8c1292">Setup the Plotting</h3>
<div class="outline-text-3" id="text-orgd8c1292">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> <span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
 <span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
             <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span>
                 <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                 <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                 <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">12</span><span class="p">)},</span>
             <span class="n">font_scale</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgae3b2c9">
<h3 id="orgae3b2c9">Types</h3>
<div class="outline-text-3" id="text-orgae3b2c9">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> <span class="n">Outcome</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org44ce4f2">
<h2 id="org44ce4f2">The Data</h2>
<div class="outline-text-2" id="text-org44ce4f2"></div>
<div class="outline-3" id="outline-container-orgad28344">
<h3 id="orgad28344">The Path To the Data</h3>
<div class="outline-text-3" id="text-orgad28344">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">()</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">DataPathTwo</span><span class="p">(</span><span class="n">folder_key</span><span class="o">=</span><span class="s2">"MNIST"</span><span class="p">)</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="o">.</span><span class="n">exists</span><span class="p">()</span>
</pre></div>
<pre class="example">
/home/hades/datasets/MNIST

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgc6bec9d">
<h3 id="orgc6bec9d">Some Settings</h3>
<div class="outline-text-3" id="text-orgc6bec9d">
<p>Since I downloaded the data earlier for some other exercise forking sub-processes is probably unnecessary, and for the training and testing we'll use a relatively small batch-size of 20.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">WORKERS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">VALIDATION_PROPORTION</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.01</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org02a00ca">
<h3 id="org02a00ca">A Transform</h3>
<div class="outline-text-3" id="text-org02a00ca">
<p>We're just going to convert the images to tensors.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1afa35f">
<h3 id="org1afa35f">Split Up the Training and Testing Data</h3>
<div class="outline-text-3" id="text-org1afa35f">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">training_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                            <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                           <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc7e01b9">
<h3 id="orgc7e01b9">Make a Validation Set</h3>
<div class="outline-text-3" id="text-orgc7e01b9">
<p>Now we're going to re-split the training-data into training and validation data. First we're going to generate indices for each set using sklearn's <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code>train_test_split</code></a>.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_data</span><span class="p">)))</span>
<span class="n">training_indices</span><span class="p">,</span> <span class="n">validation_indices</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">VALIDATION_PROPORTION</span><span class="p">)</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_indices</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_indices</span><span class="p">))</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_indices</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span> <span class="o">==</span> <span class="n">VALIDATION_PROPORTION</span>
</pre></div>
<pre class="example">
48000
12000

</pre>
<p>Now that we have our indices we need to create some samplers that can be passed to the Data Loaders. We need them to create the batches from our data.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">training_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="n">training_indices</span><span class="p">)</span>
<span class="n">validation_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="n">validation_indices</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org01f2177">
<h3 id="org01f2177">Create The Data Loaders</h3>
<div class="outline-text-3" id="text-org01f2177">
<p>Now we will create the batch-iterators.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">training_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">training_sampler</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">WORKERS</span><span class="p">)</span>
</pre></div>
<p>For the validation batch we pass in the training data and use the validation-sampler to create a separate set of batches.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">validation_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">validation_sampler</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">WORKERS</span><span class="p">)</span>
</pre></div>
<p>Since we're not splitting the testing data it doesn't get a sampler.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">test_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">test_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">WORKERS</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org9fba43a">
<h2 id="org9fba43a">Visualize a Batch of Training Data</h2>
<div class="outline-text-2" id="text-org9fba43a">
<p>Our first step is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data.</p>
</div>
<div class="outline-3" id="outline-container-org6ec9b92">
<h3 id="org6ec9b92">Grab a batch</h3>
<div class="outline-text-3" id="text-org6ec9b92">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">training_batches</span><span class="p">)</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
<p>Now that we have a batch we're going to plot the images in the batch, along with the corresponding labels.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">figure</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"First Batch"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">BATCH_SIZE</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">index</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
    <span class="c1"># print out the correct label for each image</span>
    <span class="c1"># .item() gets the value contained in a Tensor</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()))</span>
</pre></div>
<div class="figure">
<p><img alt="batch.png" src="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/batch.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org174eaf2">
<h3 id="org174eaf2">View a Single Image</h3>
<div class="outline-text-3" id="text-org174eaf2">
<p>Now we're going to take a closer look at the second image in the batch.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">figure</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span> 
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span> <span class="n">fontsize</span><span class="o">=</span><span class="s2">"xx-large"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">/</span><span class="mf">2.5</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">width</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">height</span><span class="p">):</span>
        <span class="n">val</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">image</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">!=</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">val</span><span class="p">),</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">x</span><span class="p">),</span>
                    <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
                    <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="s1">'white'</span> <span class="k">if</span> <span class="n">image</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span><span class="o">&lt;</span><span class="n">threshold</span> <span class="k">else</span> <span class="s1">'black'</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="image.png" src="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/image.png"></p>
</div>
<p>We're looking at a single image with the normalized values for each pixel superimposed on it. It looks like black is 0 and white is 1, although for this image most of the 'white' pixels are just a little less than one.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7d4f6af">
<h2 id="org7d4f6af">Define the Network <a href="http://pytorch.org/docs/stable/nn.html">Architecture</a></h2>
<div class="outline-text-2" id="text-org7d4f6af">
<p>The architecture will be responsible for seeing as input a 784-dim Tensor of pixel values for each image, and producing a Tensor of length 10 (our number of classes) that indicates the class scores for an input image. This particular example uses two hidden layers and dropout to avoid overfitting.</p>
<p>These values are based on the <a href="https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py">keras</a> example implementation.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">INPUT_NODES</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span>
<span class="n">HIDDEN_NODES_1</span> <span class="o">=</span> <span class="n">HIDDEN_NODES_2</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">DROPOUT</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">CLASSES</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">MultiLayerPerceptron</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""A Multi-Layer Perceptron</span>

<span class="sd">    This is a network with 2 hidden layers</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_layer_1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">INPUT_NODES</span><span class="p">,</span> <span class="n">HIDDEN_NODES_1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_layer_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_NODES_1</span><span class="p">,</span> <span class="n">HIDDEN_NODES_2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">HIDDEN_NODES_2</span><span class="p">,</span> <span class="n">CLASSES</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">DROPOUT</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""One feed-forward through the network</span>

<span class="sd">       Args:</span>
<span class="sd">        x: a 28 x 28 tensor</span>

<span class="sd">       Returns:</span>
<span class="sd">        tensor: output of the network without activation</span>
<span class="sd">       """</span>
        <span class="c1"># flatten image input</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">INPUT_NODES</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_layer_1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fully_connected_layer_2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-org033fba5">
<h3 id="org033fba5">Initialize the Neural Network</h3>
<div class="outline-text-3" id="text-org033fba5">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MultiLayerPerceptron</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example">
MultiLayerPerceptron(
  (fully_connected_layer_1): Linear(in_features=784, out_features=512, bias=True)
  (fully_connected_layer_2): Linear(in_features=512, out_features=512, bias=True)
  (output): Linear(in_features=512, out_features=10, bias=True)
  (dropout): Dropout(p=0.2)
)

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgfc287eb">
<h3 id="orgfc287eb">A Little CUDA</h3>
<div class="outline-text-3" id="text-orgfc287eb">
<p>This sets it up to use CUDA (if available).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Using {} GPUs"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">"Only 1 GPU available"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Only 1 GPU available

</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org3a2415b">
<h2 id="org3a2415b">Specify the <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">Loss Function</a> and <a href="http://pytorch.org/docs/stable/optim.html">Optimizer</a></h2>
<div class="outline-text-2" id="text-org3a2415b">
<p>We're going to use <a href="http://pytorch.org/docs/stable/nn.html#loss-functions">cross-entropy loss</a> for classification. PyTorch's cross entropy function applies a softmax function to the output layer <b>and</b> then calculates the log loss (so you don't want to do softmax as part of the model output).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgab8ea99">
<h2 id="orgab8ea99">Train the Network</h2>
<div class="outline-text-2" id="text-orgab8ea99">
<p>We're going to do a quasi-search by optimizing over 50 epochs and keeping the model that has the best validation score.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="c1"># number of epochs to train the model</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">SAVED_MODEL</span><span class="o">=</span> <span class="s1">'multilayer_perceptron.pt'</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> <span class="k">def</span> <span class="nf">process_batch</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Outcome</span><span class="p">:</span>
     <span class="sd">"""process one batch of the data</span>

<span class="sd">     Args:</span>
<span class="sd">      model: model to predict target</span>
<span class="sd">      data: data to use to predict target</span>
<span class="sd">      target: what we're trying to predict</span>
<span class="sd">      device: cpu or gpu</span>

<span class="sd">     Returns:</span>
<span class="sd">      outcome: loss and correct count</span>
<span class="sd">     """</span>
     <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
     <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
     <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
     <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
     <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
          <span class="n">batches</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
          <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Outcome</span><span class="p">:</span>
    <span class="sd">"""Perform one forward pass through the batches</span>

<span class="sd">    Args:</span>
<span class="sd">     model: thing to train</span>
<span class="sd">     batches: batch-iterator of training data</span>
<span class="sd">     device: cpu or cuda device</span>

<span class="sd">    Returns:</span>
<span class="sd">     outcome: cumulative loss, accuracy for the batches</span>
<span class="sd">    """</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="n">process_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">total_correct</span> <span class="o">+=</span> <span class="n">correct</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">total_correct</span><span class="o">/</span><span class="n">count</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">batches</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
             <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Outcome</span><span class="p">:</span>
    <span class="sd">"""Calculate the loss for the model</span>

<span class="sd">    Args:</span>
<span class="sd">     model: the model to validate</span>
<span class="sd">     batches: the batch-iterator of validation data</span>
<span class="sd">     device: cuda or cpu</span>

<span class="sd">    Returns:</span>
<span class="sd">     Outcome: Cumulative loss, Accuracy over batches</span>
<span class="sd">    """</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="n">process_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">count</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">total_correct</span> <span class="o">+=</span> <span class="n">correct</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">total_correct</span><span class="o">/</span><span class="n">count</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="c1"># initialize tracker for minimum validation loss</span>
<span class="n">lowest_validation_loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">Inf</span>
<span class="n">training_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">validation_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">training_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">validation_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPOCHS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">training_batches</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">training_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">mean_training_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">training_batches</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">training_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">validation_batches</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">validation_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">mean_validation_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_batches</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">validation_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mean_validation_loss</span> <span class="o">&lt;=</span> <span class="n">lowest_validation_loss</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Epoch {}: Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">epoch</span><span class="p">,</span>
            <span class="n">lowest_validation_loss</span><span class="p">,</span>
            <span class="n">mean_validation_loss</span><span class="p">))</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">SAVED_MODEL</span><span class="p">)</span>
        <span class="n">lowest_validation_loss</span> <span class="o">=</span> <span class="n">mean_validation_loss</span>
</pre></div>
<pre class="example">
Epoch 1: Validation loss decreased (inf --&gt; 0.076556).  Saving model ...
Epoch 2: Validation loss decreased (0.076556 --&gt; 0.058478).  Saving model ...
Epoch 3: Validation loss decreased (0.058478 --&gt; 0.049405).  Saving model ...
Epoch 4: Validation loss decreased (0.049405 --&gt; 0.043155).  Saving model ...
Epoch 5: Validation loss decreased (0.043155 --&gt; 0.037079).  Saving model ...
Epoch 6: Validation loss decreased (0.037079 --&gt; 0.032932).  Saving model ...
Epoch 7: Validation loss decreased (0.032932 --&gt; 0.029682).  Saving model ...
Epoch 8: Validation loss decreased (0.029682 --&gt; 0.028046).  Saving model ...
Epoch 9: Validation loss decreased (0.028046 --&gt; 0.025318).  Saving model ...
Epoch 10: Validation loss decreased (0.025318 --&gt; 0.023867).  Saving model ...
Epoch 11: Validation loss decreased (0.023867 --&gt; 0.022447).  Saving model ...
Epoch 12: Validation loss decreased (0.022447 --&gt; 0.021411).  Saving model ...
Epoch 13: Validation loss decreased (0.021411 --&gt; 0.020793).  Saving model ...
Epoch 14: Validation loss decreased (0.020793 --&gt; 0.019830).  Saving model ...
Epoch 15: Validation loss decreased (0.019830 --&gt; 0.018676).  Saving model ...
Epoch 16: Validation loss decreased (0.018676 --&gt; 0.018644).  Saving model ...
Epoch 17: Validation loss decreased (0.018644 --&gt; 0.017666).  Saving model ...
Epoch 18: Validation loss decreased (0.017666 --&gt; 0.017635).  Saving model ...
Epoch 20: Validation loss decreased (0.017635 --&gt; 0.016688).  Saving model ...
Epoch 21: Validation loss decreased (0.016688 --&gt; 0.016489).  Saving model ...
Epoch 22: Validation loss decreased (0.016489 --&gt; 0.016364).  Saving model ...
Epoch 23: Validation loss decreased (0.016364 --&gt; 0.015944).  Saving model ...
Epoch 24: Validation loss decreased (0.015944 --&gt; 0.015633).  Saving model ...
Epoch 26: Validation loss decreased (0.015633 --&gt; 0.015446).  Saving model ...
Epoch 27: Validation loss decreased (0.015446 --&gt; 0.015257).  Saving model ...
Epoch 30: Validation loss decreased (0.015257 --&gt; 0.015216).  Saving model ...
Epoch 31: Validation loss decreased (0.015216 --&gt; 0.015175).  Saving model ...
Epoch 34: Validation loss decreased (0.015175 --&gt; 0.014866).  Saving model ...
Epoch 36: Validation loss decreased (0.014866 --&gt; 0.014530).  Saving model ...
</pre>
<p>The training and validation loss seems surprisingly good.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">training_losses</span><span class="p">)))</span>
<span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Loss Per Batch"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">validation_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Validation"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="losses.png" src="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/losses.png"></p>
</div>
<p>So it looks like it improves fairly quickly then after 36 epochs the model stops improving.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org099589a">
<h2 id="org099589a">Testing the Best Model</h2>
<div class="outline-text-2" id="text-org099589a">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">SAVED_MODEL</span><span class="p">))</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">class_correct</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mf">0.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="n">class_total</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="mf">0.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_batches</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># calculate the loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="c1"># update test loss </span>
    <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">*</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># convert output probabilities to predicted class</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># compare predictions to true label</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)))</span>
    <span class="c1"># calculate test accuracy for each object class</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">class_correct</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="n">correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">class_total</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># calculate and print avg test loss</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="n">test_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_batches</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">'Test Loss: {:.6f}</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Test Accuracy of </span><span class="si">%5s</span><span class="s1">: </span><span class="si">%2d%%</span><span class="s1"> (</span><span class="si">%2d</span><span class="s1">/</span><span class="si">%2d</span><span class="s1">)'</span> <span class="o">%</span> <span class="p">(</span>
            <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">class_correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_correct</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_total</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Test Accuracy of </span><span class="si">%5s</span><span class="s1">: N/A (no training examples)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>
<pre class="example">
Test Loss: 0.059497

Test Accuracy of     0: 99% (974/980)
Test Accuracy of     1: 99% (1127/1135)
Test Accuracy of     2: 97% (1009/1032)
Test Accuracy of     3: 98% (994/1010)
Test Accuracy of     4: 97% (960/982)
Test Accuracy of     5: 97% (867/892)
Test Accuracy of     6: 98% (941/958)
Test Accuracy of     7: 98% (1008/1028)
Test Accuracy of     8: 97% (947/974)
Test Accuracy of     9: 97% (986/1009)
</pre></div>
</div>
<div class="outline-2" id="outline-container-org95b5e2b">
<h2 id="org95b5e2b">Visualize Test Results</h2>
<div class="outline-text-2" id="text-org95b5e2b">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_batches</span><span class="p">)</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="c1"># matplotlib doesn't like the CUDA and the model doesn't like the CPU... too bad for the model.</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">"cpu"</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># prep images for display</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># plot the images in the batch, along with predicted and true labels</span>
<span class="n">figure</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">title</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Test Predictions"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">,</span> <span class="n">position</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">))</span>

<span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">numpy</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">figure</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">index</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="n">index</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"{} ({})"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span> <span class="nb">str</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())),</span>
                 <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="s2">"green"</span> <span class="k">if</span> <span class="n">preds</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">==</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">else</span> <span class="s2">"red"</span><span class="p">))</span>
<span class="n">figure</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="test_results.png" src="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/test_results.png"></p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5b3e388">
<h2 id="org5b3e388">Object-Oriented Trainer</h2>
<div class="outline-text-2" id="text-org5b3e388">
<p>This just bundles up the earlier stuff.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> <span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
     <span class="sd">"""Train-test-validate the model</span>

<span class="sd">     Args:</span>
<span class="sd">      train: training batches</span>
<span class="sd">      validate: validation batches</span>
<span class="sd">      test: testing batches</span>
<span class="sd">      epochs: number of times to repeat training over the batches</span>
<span class="sd">      model_filename: name to save the hyperparameters of best model</span>
<span class="sd">      learning_rate: how much to update the weights</span>
<span class="sd">     """</span>
     <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
                  <span class="n">validate</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
                  <span class="n">test</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">,</span>
                  <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                  <span class="n">model_filename</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"multilayer_perceptron.pth"</span><span class="p">,</span>
                  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">training_batches</span> <span class="o">=</span> <span class="n">train</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">validation_batches</span> <span class="o">=</span> <span class="n">validate</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">test_batches</span> <span class="o">=</span> <span class="n">test</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">save_as</span> <span class="o">=</span> <span class="n">model_filename</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="bp">None</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_criterion</span> <span class="o">=</span> <span class="bp">None</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="bp">None</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="bp">None</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">validation_losses</span> <span class="o">=</span> <span class="p">[]</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">training_losses</span> <span class="o">=</span> <span class="p">[]</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">validation_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">training_accuracies</span> <span class="o">=</span> <span class="p">[]</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">best_parameters</span> <span class="o">=</span> <span class="bp">None</span>
         <span class="k">return</span>

     <span class="nd">@property</span>
     <span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
         <span class="sd">"""The Multi-Layer Perceptron"""</span>
         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span> <span class="o">=</span> <span class="n">MultiLayerPerceptron</span><span class="p">()</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span>

     <span class="nd">@property</span>
     <span class="k">def</span> <span class="nf">criterion</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
         <span class="sd">"""The Loss Measurer"""</span>
         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_criterion</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_criterion</span>

     <span class="nd">@property</span>
     <span class="k">def</span> <span class="nf">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
         <span class="sd">"""The gradient descent"""</span>
         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                                               <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>

     <span class="nd">@property</span>
     <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
         <span class="sd">"""The CPU or GPU"""</span>
         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">)</span>
         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>


     <span class="k">def</span> <span class="nf">process_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Outcome</span><span class="p">:</span>
         <span class="sd">"""process one batch of the data</span>

<span class="sd">        Args:</span>
<span class="sd">         data: data to use to predict target</span>
<span class="sd">         target: what we're trying to predict</span>
<span class="sd">         device: cpu or gpu</span>

<span class="sd">        Returns:</span>
<span class="sd">         outcome: loss and correct count</span>
<span class="sd">        """</span>
         <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
         <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
         <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
         <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
         <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

     <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Outcome</span><span class="p">:</span>
         <span class="sd">"""Perform one forward pass through the batches</span>

<span class="sd">        Returns:</span>
<span class="sd">         outcome: cumulative loss, accuracy for the batches</span>
<span class="sd">        """</span>
         <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
         <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
         <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
         <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_batches</span><span class="p">:</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
             <span class="n">loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_batch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
             <span class="n">count</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
             <span class="n">total_correct</span> <span class="o">+=</span> <span class="n">correct</span>
             <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span>
             <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
             <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
             <span class="k">del</span> <span class="n">loss</span>
         <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_loss</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_correct</span><span class="o">/</span><span class="n">count</span><span class="p">)</span>

     <span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Outcome</span><span class="p">:</span>
         <span class="sd">"""Calculate the loss for the model</span>

<span class="sd">        Returns:</span>
<span class="sd">         Outcome: Cumulative loss, Accuracy over batches</span>
<span class="sd">        """</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
         <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
         <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
         <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
         <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_batches</span><span class="p">:</span>
             <span class="n">loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process_batch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
             <span class="n">count</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
             <span class="n">total_correct</span> <span class="o">+=</span> <span class="n">correct</span>
             <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
             <span class="k">del</span> <span class="n">loss</span>
         <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_loss</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="n">total_correct</span><span class="o">/</span><span class="n">count</span><span class="p">)</span>

     <span class="k">def</span> <span class="nf">run_training</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
         <span class="sd">"""Runs the training and validation"""</span>
         <span class="n">lowest_validation_loss</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">Inf</span>
         <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
             <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
             <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">training_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
             <span class="n">mean_training_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_batches</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">training_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
             <span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">validation_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
             <span class="n">mean_validation_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_batches</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">validation_accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>

             <span class="k">if</span> <span class="n">mean_validation_loss</span> <span class="o">&lt;=</span> <span class="n">lowest_validation_loss</span><span class="p">:</span>
                 <span class="k">print</span><span class="p">(</span><span class="s1">'Epoch {}: Validation loss decreased ({:.6f} --&gt; {:.6f}).  Saving model ...'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                     <span class="n">epoch</span><span class="p">,</span>
                     <span class="n">lowest_validation_loss</span><span class="p">,</span>
                     <span class="n">mean_validation_loss</span><span class="p">))</span>
                 <span class="bp">self</span><span class="o">.</span><span class="n">best_parameters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
                 <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_as</span><span class="p">)</span>
                 <span class="n">lowest_validation_loss</span> <span class="o">=</span> <span class="n">mean_validation_loss</span>
         <span class="k">return</span>

     <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
         <span class="sd">"""Test Our Model"""</span>
         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_parameters</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
             <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">"call ``run_training`` or set ``best_parameters"</span><span class="p">)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_parameters</span><span class="p">)</span>
         <span class="n">test_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
         <span class="n">digits</span> <span class="o">=</span> <span class="mi">10</span>
         <span class="n">class_correct</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="n">digits</span>
         <span class="n">class_total</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="n">digits</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

         <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_batches</span><span class="p">:</span>
             <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
             <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
             <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
             <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

             <span class="n">_</span><span class="p">,</span> <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
             <span class="n">correct</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span>
                 <span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">predictions</span><span class="p">)))</span>
             <span class="c1"># calculate test accuracy for each object class</span>
             <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
                 <span class="n">label</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                 <span class="n">class_correct</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="n">correct</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                 <span class="n">class_total</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

         <span class="c1"># calculate and print avg test loss</span>
         <span class="n">test_loss</span> <span class="o">=</span> <span class="n">test_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">test_batches</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
         <span class="k">print</span><span class="p">(</span><span class="s1">'Test Loss: {:.6f}</span><span class="se">\n</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>

         <span class="k">for</span> <span class="n">digit</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
             <span class="k">if</span> <span class="n">class_total</span><span class="p">[</span><span class="n">digit</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                 <span class="k">print</span><span class="p">(</span><span class="s1">'Test Accuracy of </span><span class="si">%5s</span><span class="s1">: </span><span class="si">%2d%%</span><span class="s1"> (</span><span class="si">%2d</span><span class="s1">/</span><span class="si">%2d</span><span class="s1">)'</span> <span class="o">%</span> <span class="p">(</span>
                     <span class="nb">str</span><span class="p">(</span><span class="n">digit</span><span class="p">),</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">class_correct</span><span class="p">[</span><span class="n">digit</span><span class="p">]</span> <span class="o">/</span> <span class="n">class_total</span><span class="p">[</span><span class="n">digit</span><span class="p">],</span>
                     <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_correct</span><span class="p">[</span><span class="n">digit</span><span class="p">]),</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">class_total</span><span class="p">[</span><span class="n">digit</span><span class="p">])))</span>
             <span class="k">else</span><span class="p">:</span>
                 <span class="k">print</span><span class="p">(</span><span class="s1">'Test Accuracy of </span><span class="si">%5s</span><span class="s1">: N/A (no training examples)'</span> <span class="o">%</span> <span class="p">(</span><span class="n">classes</span><span class="p">[</span><span class="n">digit</span><span class="p">]))</span>
         <span class="k">return</span>
</pre></div>
<p>For some reason, this raises an error when the backward propagation step is run.</p>
<pre class="example">
RuntimeError: CUDA error: out of memory
</pre>
<p>So I can't run it until I figure out what's going on. <b>Update</b> - it looks like casting the outputs of the functions to floats solved the problem. Apparently even they look like floats, whatever the <code>item()</code> method returns prevents the freeing up of the memory, so casting them to floats fixes the memory problem.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">training_batches</span><span class="p">,</span> <span class="n">validation_batches</span><span class="p">,</span> <span class="n">test_batches</span><span class="p">)</span>
 <span class="n">trainer</span><span class="o">.</span><span class="n">run_training</span><span class="p">()</span>
</pre></div>
<pre class="example">
Epoch 1: Validation loss decreased (inf --&gt; 0.077417).  Saving model ...
Epoch 2: Validation loss decreased (0.077417 --&gt; 0.058746).  Saving model ...
Epoch 3: Validation loss decreased (0.058746 --&gt; 0.048325).  Saving model ...
Epoch 4: Validation loss decreased (0.048325 --&gt; 0.040851).  Saving model ...
Epoch 5: Validation loss decreased (0.040851 --&gt; 0.036083).  Saving model ...
Epoch 6: Validation loss decreased (0.036083 --&gt; 0.032722).  Saving model ...
Epoch 7: Validation loss decreased (0.032722 --&gt; 0.028545).  Saving model ...
Epoch 8: Validation loss decreased (0.028545 --&gt; 0.026376).  Saving model ...
Epoch 9: Validation loss decreased (0.026376 --&gt; 0.024063).  Saving model ...
Epoch 10: Validation loss decreased (0.024063 --&gt; 0.023637).  Saving model ...
Epoch 11: Validation loss decreased (0.023637 --&gt; 0.021980).  Saving model ...
Epoch 12: Validation loss decreased (0.021980 --&gt; 0.020723).  Saving model ...
Epoch 13: Validation loss decreased (0.020723 --&gt; 0.019802).  Saving model ...
Epoch 14: Validation loss decreased (0.019802 --&gt; 0.019013).  Saving model ...
Epoch 15: Validation loss decreased (0.019013 --&gt; 0.018458).  Saving model ...
Epoch 16: Validation loss decreased (0.018458 --&gt; 0.017919).  Saving model ...
Epoch 17: Validation loss decreased (0.017919 --&gt; 0.017918).  Saving model ...
Epoch 18: Validation loss decreased (0.017918 --&gt; 0.017127).  Saving model ...
Epoch 19: Validation loss decreased (0.017127 --&gt; 0.016704).  Saving model ...
Epoch 20: Validation loss decreased (0.016704 --&gt; 0.016167).  Saving model ...
Epoch 22: Validation loss decreased (0.016167 --&gt; 0.016154).  Saving model ...
Epoch 23: Validation loss decreased (0.016154 --&gt; 0.015817).  Saving model ...
Epoch 24: Validation loss decreased (0.015817 --&gt; 0.015352).  Saving model ...
Epoch 25: Validation loss decreased (0.015352 --&gt; 0.015075).  Saving model ...
Epoch 27: Validation loss decreased (0.015075 --&gt; 0.015059).  Saving model ...
Epoch 28: Validation loss decreased (0.015059 --&gt; 0.014940).  Saving model ...
Epoch 32: Validation loss decreased (0.014940 --&gt; 0.014644).  Saving model ...
Epoch 34: Validation loss decreased (0.014644 --&gt; 0.014383).  Saving model ...
Epoch 46: Validation loss decreased (0.014383 --&gt; 0.014357).  Saving model ...
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">training_accuracies</span><span class="p">)))</span>
<span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Model Accuracy"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">training_accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">validation_accuracies</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Validation"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="accuracy.png" src="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/accuracy.png"></p>
</div>
<p>Although the validation loss decreases for a while, it nearly reaches its peak accuracy around 10 epochs. The training worked out a little differently this time, so here's the losses again.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">training_losses</span><span class="p">)))</span>
<span class="n">figure</span><span class="p">,</span> <span class="n">axe</span> <span class="o">=</span> <span class="n">pyplot</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">figure</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">"Loss Per Batch"</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s2">"bold"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">training_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Training"</span><span class="p">)</span>
<span class="n">axe</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">validation_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Validation"</span><span class="p">)</span>
<span class="n">legend</span> <span class="o">=</span> <span class="n">axe</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="losses_2.png" src="posts/nano/cnn/mnist-multi-layer-perceptron-with-validation/losses_2.png"></p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nano/dog-breed-classifier/dog-breed-classification/">Dog Breed Classification</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nano/dog-breed-classifier/dog-breed-classification/" rel="bookmark"><time class="published dt-published" datetime="2018-11-26T13:11:29-08:00" itemprop="datePublished" title="2018-11-26 13:11">2018-11-26 13:11</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nano/dog-breed-classifier/dog-breed-classification/#orga6215ab">Introduction</a></li>
<li><a href="posts/nano/dog-breed-classifier/dog-breed-classification/#org32fa4b8">Set Up</a></li>
<li><a href="posts/nano/dog-breed-classifier/dog-breed-classification/#orgc5626e4">A Human Face Detector</a></li>
<li><a href="posts/nano/dog-breed-classifier/dog-breed-classification/#orgde5c67e">A Dog Detector</a></li>
<li><a href="posts/nano/dog-breed-classifier/dog-breed-classification/#org05711dc">Combine The Detectors</a></li>
<li><a href="posts/nano/dog-breed-classifier/dog-breed-classification/#org46b857e">A Dog Breed Classifier</a></li>
<li><a href="posts/nano/dog-breed-classifier/dog-breed-classification/#org7d44bc0">The Dog Breed Classifier</a></li>
<li><a href="posts/nano/dog-breed-classifier/dog-breed-classification/#org954ac7e">Some Sample applications</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orga6215ab">
<h2 id="orga6215ab">Introduction</h2>
<div class="outline-text-2" id="text-orga6215ab">
<p>This application is a dog-breed classifier. It takes as input an image and detects if it's an image of either a human or a dog and if it's either one of those then it finds the dog-breed classification that the subject of the image most resembles. If it's neither a human or a dog then it emits an error message. To do this I'm going to try two libraries for each of the human face-detectors and dog detectors and I'm also going to try three Neural Networks to try and classify the dog breeds.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org32fa4b8">
<h2 id="org32fa4b8">Set Up</h2>
<div class="outline-text-2" id="text-org32fa4b8">
<p>This section does some preliminary set-up for the code that comes later.</p>
</div>
<div class="outline-3" id="outline-container-org29d288c">
<h3 id="org29d288c">Imports</h3>
<div class="outline-text-3" id="text-org29d288c"></div>
<div class="outline-4" id="outline-container-orgd8346ea">
<h4 id="orgd8346ea">Python</h4>
<div class="outline-text-4" id="text-orgd8346ea">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from functools import partial
from pathlib import Path
import os
import warnings
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5cfa1f4">
<h4 id="org5cfa1f4">From Pypi</h4>
<div class="outline-text-4" id="text-org5cfa1f4">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from dotenv import load_dotenv
from PIL import Image, ImageFile
from torchvision import datasets
import cv2
import face_recognition
import matplotlib.cbook
warnings.filterwarnings("ignore", category=matplotlib.cbook.mplDeprecation)
import matplotlib.pyplot as pyplot
import matplotlib.image as mpimage
import matplotlib.patches as patches
import numpy
try:
    import pyttsx3
    SPEAKABLE = True
except ImportError:
    print("pyttsx3 not available")
    SPEAKABLE = False
import seaborn
import torch
import torchvision.models as models
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optimizer
import torchvision.transforms as transforms
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga92cdbb">
<h4 id="orga92cdbb">This Project</h4>
<div class="outline-text-4" id="text-orga92cdbb">
<p>This is code that I wrote to maybe make it easier to work with.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>from neurotic.tangles.data_paths import DataPathTwo
from neurotic.tangles.timer import Timer
from neurotic.constants.imagenet_map import imagenet
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb8782cd">
<h3 id="orgb8782cd">Plotting</h3>
<div class="outline-text-3" id="text-orgb8782cd">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=1)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5017b31">
<h3 id="org5017b31">Set the Random Seed</h3>
<div class="outline-text-3" id="text-org5017b31">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>numpy.random.seed(seed=2019)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgaa34923">
<h3 id="orgaa34923">Check If CUDA Is Available</h3>
<div class="outline-text-3" id="text-orgaa34923">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
</pre></div>
<pre class="example">
cuda

</pre></div>
</div>
<div class="outline-3" id="outline-container-org876137f">
<h3 id="org876137f">Handle Truncated Images</h3>
<div class="outline-text-3" id="text-org876137f">
<p>There seems to be at least one image that is truncated which will cause an exception when it's loaded so this next setting lets us ignore the error and keep working.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>ImageFile.LOAD_TRUNCATED_IMAGES = True
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgeea9430">
<h3 id="orgeea9430">Build the Timer</h3>
<div class="outline-text-3" id="text-orgeea9430">
<p>The timer times how long a code-block takes to run so that if I run it more than once I'll know if it will take a while.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>timer = Timer(beep=SPEAKABLE)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf83395e">
<h3 id="orgf83395e">The Data Paths</h3>
<div class="outline-text-3" id="text-orgf83395e">
<p>The data-sets are hosted online and need to be downloaded.</p>
<ul class="org-ul">
<li>This is a download link for the <a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip">dog dataset</a>.</li>
<li>This is a download link for the the <a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip">human dataset</a>.</li>
</ul>
<p>I've already downloaded them and put the path to the folders in a <code>.env</code> file so this next block gets the paths so we can load the data later.</p>
</div>
<div class="outline-4" id="outline-container-orga8ac9c7">
<h4 id="orga8ac9c7">The Model Path</h4>
<div class="outline-text-4" id="text-orga8ac9c7">
<p>The models turn out to take up a lot of space so I'm saving them outside of the repository.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>MODEL_PATH = DataPathTwo(folder_key="MODELS")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4937da0">
<h4 id="org4937da0">Dog Paths</h4>
<div class="outline-text-4" id="text-org4937da0">
<p>This is a class to hold the paths for the dog Images</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class DogPaths:
    """holds the paths to the dog images"""
    def __init__(self) -&gt; None:
        self._main = None
        self._training = None
        self._testing = None
        self._validation = None
        self._breed_count = None
        load_dotenv()
        return

    @property
    def main(self) -&gt; DataPathTwo:
        """The path to the main folder"""
        if self._main is None:
            self._main = DataPathTwo(folder_key="DOG_PATH")
        return self._main

    @property
    def training(self) -&gt; DataPathTwo:
        """Path to the training images"""
        if self._training is None:
            self._training = DataPathTwo(folder_key="DOG_TRAIN")
        return self._training

    @property
    def validation(self) -&gt; DataPathTwo:
        """Path to the validation images"""
        if self._validation is None:
            self._validation = DataPathTwo(folder_key="DOG_VALIDATE")
        return self._validation

    @property
    def testing(self) -&gt; DataPathTwo:
        """Path to the testing images"""
        if self._testing is None:
            self._testing = DataPathTwo(folder_key="DOG_TEST")
        return self._testing

    @property
    def breed_count(self) -&gt; int:
        """Counts the number of dog breeds

        This assumes that the training folder has all the breeds
        """
        if self._breed_count is None:
            self._breed_count = len(set(self.training.folder.iterdir()))
        return self._breed_count

    def check(self) -&gt; None:
        """Checks that the folders are valid

        Raises: 
         AssertionError: folder doesn't exist
        """
        self.main.check_folder()
        self.training.check_folder()
        self.validation.check_folder()
        self.testing.check_folder()
        return
</pre></div>
<p>Now I'll build the dog-paths.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dog_paths = DogPaths()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc197487">
<h4 id="orgc197487">Human Path</h4>
<div class="outline-text-4" id="text-orgc197487">
<p>This is the path to the downloaded <a href="http://vis-www.cs.umass.edu/lfw/">Labeled Faces in the Wild</a> data set.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>human_path = DataPathTwo(folder_key="HUMAN_PATH")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org063873e">
<h4 id="org063873e">Check the Paths</h4>
<div class="outline-text-4" id="text-org063873e">
<p>This makes sure that the folders exist and shows where they are.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print(dog_paths.main.folder)
print(dog_paths.training.folder)
print(dog_paths.testing.folder)
print(dog_paths.validation.folder)
dog_paths.check()
print(human_path.folder)
human_path.check_folder()
</pre></div>
<pre class="example">
/home/hades/data/datasets/dog-breed-classification/dogImages
/home/hades/data/datasets/dog-breed-classification/dogImages/train
/home/hades/data/datasets/dog-breed-classification/dogImages/test
/home/hades/data/datasets/dog-breed-classification/dogImages/valid
/home/hades/data/datasets/dog-breed-classification/lfw

</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga10e9a9">
<h3 id="orga10e9a9">Count The Breeds</h3>
<div class="outline-text-3" id="text-orga10e9a9">
<p>To build the neural network I'll need to know how many dog breeds there are. I made it an attribute of the <code>DogPath</code> class and I'll just inspect it here.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print("Number of Dog Breeds: {}".format(dog_paths.breed_count))
</pre></div>
<pre class="example">
Number of Dog Breeds: 133

</pre></div>
</div>
<div class="outline-3" id="outline-container-org21cf244">
<h3 id="org21cf244">Load the Files</h3>
<div class="outline-text-3" id="text-org21cf244">
<p>For this first part we're going to load in all the files and ignore the train-validation-test split for the dog-images.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>timer.start()
human_files = numpy.array(list(human_path.folder.glob("*/*")))
dog_files = numpy.array(list(dog_paths.main.folder.glob("*/*/*")))
timer.end()
</pre></div>
<pre class="example">
Started: 2019-01-13 14:05:09.566221
Ended: 2019-01-13 14:05:42.932863
Elapsed: 0:00:33.366642

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print('There are {:,} total human images.'.format(len(human_files)))
print('There are {:,} total dog images.'.format(len(dog_files)))
</pre></div>
<pre class="example">
There are 13,233 total human images.
There are 8,351 total dog images.

</pre>
<p>So we have a bit more human images than dog images.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org098ab0b">
<h3 id="org098ab0b">Some Helper Code</h3>
<div class="outline-text-3" id="text-org098ab0b">
<p>This is code meant to help with the other code.</p>
</div>
<div class="outline-4" id="outline-container-org6b5099d">
<h4 id="org6b5099d">Tee</h4>
<div class="outline-text-4" id="text-org6b5099d">
<p>I wrote this for the jupyter notebook because it loses the output if the server disconnects. I think it will also make it easier to use multiproccessing so I can train things in parallel. But I don't think I'm using it right now.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Tee:
    """Save the input to a file and print it

    Args:
     log_name: name to give the log    
     directory_path: path to the directory for the file
    """
    def __init__(self, log_name: str, 
                 directory_name: str="../../../logs/dog-breed-classifier") -&gt; None:
        self.directory_name = directory_name
        self.log_name = log_name
        self._path = None
        self._log = None
        return

    @property
    def path(self) -&gt; Path:
        """path to the log-file"""
        if self._path is None:
            self._path = Path(self.directory_name).expanduser()
            assert self._path.is_dir()
            self._path = self._path.joinpath(self.log_name)
        return self._path

    @property
    def log(self):
        """File object to write log to"""
        if self._log is None:
            self._log = self.path.open("w", buffering=1)
        return self._log

    def __call__(self, line: str) -&gt; None:
        """Writes to the file and stdout

        Args:
         line: text to emit
        """
        self.log.write("{}\n".format(line))
        print(line)
        return
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgdd9328f">
<h4 id="orgdd9328f">F1 Scorer</h4>
<div class="outline-text-4" id="text-orgdd9328f">
<p>I'm going to be comparing two models for both the humans and dogs, this scorer will focus on the F1 score, but will emit some other information as well.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> class F1Scorer:
     """Calculates the F1 and other scores

     Args:
      predictor: callable that gets passed and image and outputs boolean
      true_images: images that should be predicted as True
      false_images: images that shouldn't be matched by the predictor
      done_message: what to announce when done
     """
     def __init__(self, predictor: callable, true_images:list,
                  false_images: list,
                  done_message: str="Scoring Done") -&gt; None:
         self.predictor = predictor
         self.true_images = true_images
         self.false_images = false_images
         self.done_message = done_message
         self._timer = None
         self._false_image_predictions = None
         self._true_image_predictions = None
         self._false_positives = None
         self._false_negatives = None
         self._true_positives = None
         self._true_negatives = None
         self._false_positive_rate = None
         self._precision = None
         self._recall = None
         self._f1 = None
         self._accuracy = None
         self._specificity = None
         return

     @property
     def timer(self) -&gt; Timer:
         if self._timer is None:
             self._timer = Timer(message=self.done_message, emit=False)
         return self._timer

     @property
     def false_image_predictions(self) -&gt; list:
         """Predictions made on the false-images"""
         if self._false_image_predictions is None:
             self._false_image_predictions = [self.predictor(str(image))
                                              for image in self.false_images]
         return self._false_image_predictions

     @property
     def true_image_predictions(self) -&gt; list:
         """Predictions on the true-images"""
         if self._true_image_predictions is None:
             self._true_image_predictions = [self.predictor(str(image))
                                             for image in self.true_images]
         return self._true_image_predictions

     @property
     def true_positives(self) -&gt; int:
         """count of correct positive predictions"""
         if self._true_positives is None:
             self._true_positives = sum(self.true_image_predictions)
         return self._true_positives

     @property
     def false_positives(self) -&gt; int:
         """Count of incorrect positive predictions"""
         if self._false_positives is None:
             self._false_positives = sum(self.false_image_predictions)
         return self._false_positives

     @property
     def false_negatives(self) -&gt; int:
         """Count of images that were incorrectly classified as negative"""
         if self._false_negatives is None:
             self._false_negatives = len(self.true_images) - self.true_positives
         return self._false_negatives

     @property
     def true_negatives(self) -&gt; int:
         """Count of images that were correctly ignored"""
         if self._true_negatives is None:
             self._true_negatives = len(self.false_images) - self.false_positives
         return self._true_negatives

     @property
     def accuracy(self) -&gt; float:
         """fraction of correct predictions"""
         if self._accuracy is None:
             self._accuracy = (
                 (self.true_positives + self.true_negatives)
                 /(len(self.true_images) + len(self.false_images)))
         return self._accuracy

     @property
     def precision(self) -&gt; float:
         """True-Positive with penalty for false positives"""
         if self._precision is None:
             self._precision = self.true_positives/(
                 self.true_positives + self.false_positives)
         return self._precision

     @property
     def recall(self) -&gt; float:
         """fraction of correct images correctly predicted"""
         if self._recall is None:
             self._recall = (
                 self.true_positives/len(self.true_images))
         return self._recall

     @property
     def false_positive_rate(self) -&gt; float:
         """fraction of incorrect images predicted as positive"""
         if self._false_positive_rate is None:
             self._false_positive_rate = (
                 self.false_positives/len(self.false_images))
         return self._false_positive_rate

     @property
     def specificity(self) -&gt; float:
         """metric for how much to believe a negative prediction

         Specificity is 1 - false positive rate so you only need one or the other
         """
         if self._specificity is None:
             self._specificity = self.true_negatives/(self.true_negatives
                                                      + self.false_positives)
         return self._specificity

     @property
     def f1(self) -&gt; float:
         """Harmonic Mean of the precision and recall"""
         if self._f1 is None:
             TP = 2 * self.true_positives
             self._f1 = (TP)/(TP + self.false_negatives + self.false_positives)
         return self._f1

     def __call__(self) -&gt; None:
         """Emits the F1 and other scores as an org-table
         """
         self.timer.start()
         print("|Metric|Value|")
         print("|-+-|")
         print("|Accuracy|{:.2f}|".format(self.accuracy))
         print("|Precision|{:.2f}|".format(self.precision))
         print("|Recall|{:.2f}|".format(self.recall))
         print("|Specificity|{:.2f}".format(self.specificity))
         # print("|False Positive Rate|{:.2f}|".format(self.false_positive_rate))
         print("|F1|{:.2f}|".format(self.f1))
         self.timer.end()
         print("|Elapsed|{}|".format(self.timer.ended - self.timer.started))
         return
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8488a83">
<h4 id="org8488a83">Get Human</h4>
<div class="outline-text-4" id="text-org8488a83">
<p>This will grab the name of the person in an image file (based on the file name).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def get_name(path: Path) -&gt; str:
    """Extracts the name of the person from the file name

    Args:
     path: path to the file

    Returns:
     the name extracted from the file name
    """
    return " ".join(path.name.split("_")[:-1]).title()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org74bdf1a">
<h4 id="org74bdf1a">Display Image</h4>
<div class="outline-text-4" id="text-org74bdf1a">
<p>A little matplotlib helper.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def display_image(image: Path, title: str, is_file: bool=True) -&gt; tuple:
    """Plot the image

    Args:
     image: path to the image file or image
     title: title for the image
     is_file: first argument is a file name, not an array

    Returns:
     figure, axe
    """
    figure, axe = pyplot.subplots()
    figure.suptitle(title, weight="bold")
    axe.tick_params(dict(axis="both",
                         which="both",
                         bottom=False,
                         top=False))
    axe.get_xaxis().set_ticks([])
    axe.get_yaxis().set_ticks([])
    if is_file:
        image = Image.open(image)
    image = axe.imshow(image)
    return figure, axe
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4fc7535">
<h4 id="org4fc7535">First Prediction</h4>
<div class="outline-text-4" id="text-org4fc7535">
<p>This function is used to grab images that register as false-positives.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def first_prediction(source: list, start:int=0) -&gt; int:
    """Gets the index of the first True prediction

    Args:
     source: list of True/False predictions
     start: index to start the search from

    Returns:
     index of first True prediction found
    """
    for index, prediction in enumerate(source[start:]):
        if prediction:
            print("{}: {}".format(start + index, prediction))
            break
    return start + index
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org3de0fb8">
<h3 id="org3de0fb8">Some Constants</h3>
<div class="outline-text-3" id="text-org3de0fb8">
<p>The pre-trained models need to be normalized using the following means and standard deviations.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>MEANS = [0.485, 0.456, 0.406]
DEVIATIONS = [0.229, 0.224, 0.225]
</pre></div>
<p>I'm going to offload the models that I move to the GPU while exploring before doing the final implementation so this list is to keep track of all of them.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>MODELS = []
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc5626e4">
<h2 id="orgc5626e4">A Human Face Detector</h2>
<div class="outline-text-2" id="text-orgc5626e4">
<p>I'm going to need a way to tell if an image has a human in it (or not), so I'll build two versions of a detector, one using <a href="https://opencv.org/">OpenCV</a>, and one using <a href="http://dlib.net/">dlib</a>.</p>
<p>For each detector I'm going to look at an example image before running an assessment of how well it did so I'll select one at random here.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>sample_face = numpy.random.choice(human_files, 1)[0]
sample_name = get_name(sample_face)
print(sample_name)
</pre></div>
<pre class="example">
David Anderson

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = display_image(sample_face, sample_name)
</pre></div>
<div class="figure">
<p><img alt="sample_human.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/sample_human.png"></p>
</div>
</div>
<div class="outline-3" id="outline-container-org4af541f">
<h3 id="org4af541f">The Data Sets</h3>
<div class="outline-text-3" id="text-org4af541f">
<p>To save some time I'm going to assess the detectors using random images from the data sets.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>count = int(.1 * len(human_files))
human_files_short = numpy.random.choice(human_files, count)
dog_files_short = numpy.random.choice(dog_files, count)
print("{:,}".format(count))
</pre></div>
<pre class="example">
1323

</pre></div>
</div>
<div class="outline-3" id="outline-container-orgd9ff267">
<h3 id="orgd9ff267">The Scorer</h3>
<div class="outline-text-3" id="text-orgd9ff267">
<p>I'm going to re-use the same scorer for the dlib face-detector so to make it simpler I'll attach the correct images to the <code>F1Scorer</code> class.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>human_scorer = partial(F1Scorer,
                       true_images=human_files_short,
                       false_images=dog_files_short)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org3f30ac5">
<h3 id="org3f30ac5">OpenCV</h3>
<div class="outline-text-3" id="text-org3f30ac5">
<p>Here I'll use OpenCV's implementation of <a href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html">Haar feature-based cascade classifiers</a> (which you can grab from <a href="https://github.com/opencv/opencv/tree/master/data/haarcascades">github</a>) to detect human faces in images.</p>
</div>
<div class="outline-4" id="outline-container-org1c03963">
<h4 id="org1c03963">Extract the Pre-Trained Face Detector</h4>
<div class="outline-text-4" id="text-org1c03963">
<p>First I'll grab the path to the XML file that defines the classifier.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>haar_path = DataPathTwo("haarcascade_frontalface_alt.xml", folder_key="HAAR_CASCADES")
print(haar_path.from_folder)
assert haar_path.from_folder.is_file()
</pre></div>
<pre class="example">
/home/hades/data/datasets/dog-breed-classification/haarcascades/haarcascade_frontalface_alt.xml

</pre>
<p>Now we can load it.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>face_cascade = cv2.CascadeClassifier(str(haar_path.from_folder))
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org261f2fd">
<h4 id="org261f2fd">Inspect An Image</h4>
<div class="outline-text-4" id="text-org261f2fd">
<p>First let's see what the face detector detects by looking at a single image.</p>
</div>
<ul class="org-ul">
<li><a id="orgbb8157b"></a>Load a Color (BGR) Image<br>
<div class="outline-text-5" id="text-orgbb8157b">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>image = cv2.imread(str(sample_face))
print(image.shape)
</pre></div>
<pre class="example">
(250, 250, 3)

</pre>
<p>So the image is a 250x250 pixel image with three channels. Since we're loading it with <code>cv2</code> the three channels are Blue, Green, and Red.</p>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org5e2789e">
<h4 id="org5e2789e">Convert the BGR Image To Grayscale</h4>
<div class="outline-text-4" id="text-org5e2789e">
<p>To do the face-detection we need to convert the image to a grayscale image.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgff0f8fd">
<h4 id="orgff0f8fd">Find Some Faces In the Image</h4>
<div class="outline-text-4" id="text-orgff0f8fd">
<p>Now we can find the coordinates for bounding boxes for any faces that OpenCV finds in the image.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>faces = face_cascade.detectMultiScale(gray)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print('Number of faces detected:', len(faces))
</pre></div>
<pre class="example">
Number of faces detected: 1

</pre></div>
</div>
<div class="outline-4" id="outline-container-org776f7c3">
<h4 id="org776f7c3">Show Us the Box</h4>
<div class="outline-text-4" id="text-org776f7c3">
<p>The boxes are defined using a four-tuple with the <i>x</i> and <i>y</i> coordinates of the top-left corner of the box first followed by the width and height of the box. This next block adds the box to the image.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>for (x,y,w,h) in faces:
    # add bounding box to color image
    cv2.rectangle(image, (x,y), (x+w,y+h), (255,0,0), 2)
</pre></div>
<p>To display the image we need to convert it to RGB.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>cv_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
</pre></div>
<p>Now we can display the image with the bounding box.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = display_image(cv_rgb, "OpenCV Face-Detection Bounding Box", False)
</pre></div>
<div class="figure">
<p><img alt="face_bounded.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/face_bounded.png"></p>
</div>
</div>
</div>
<div class="outline-4" id="outline-container-orgbf51b5b">
<h4 id="orgbf51b5b">Write a Human Face Detector</h4>
<div class="outline-text-4" id="text-orgbf51b5b">
<p>Now that we know how it works, we can use the OpenCV face-recognizer to tell us if the image has a human in it (because there will be at least one bounding-box).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span># returns "True" if face is detected in image stored at img_path
def face_detector(image_path: str) -&gt; bool:
    """Detects human faces in an image

    Args:
     image_path: path to the image to check

    Returns:
     True if there was at least one face in the image
    """
    image = cv2.imread(image_path)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray)
    return len(faces) &gt; 0
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge5297cd">
<h4 id="orge5297cd">Assess the Human Face Detector</h4>
<div class="outline-text-4" id="text-orge5297cd">
<p>Here I'll check how well the face detector does using an F1 score. I'll also show some other metrics, but F1 is the single-value that I'll be focused on.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>open_cv_scorer = human_scorer(face_detector)
open_cv_scorer()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Metric</th>
<th class="org-right" scope="col">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Accuracy</td>
<td class="org-right">0.94</td>
</tr>
<tr>
<td class="org-left">Precision</td>
<td class="org-right">0.90</td>
</tr>
<tr>
<td class="org-left">Recall</td>
<td class="org-right">0.99</td>
</tr>
<tr>
<td class="org-left">Specificity</td>
<td class="org-right">0.89</td>
</tr>
<tr>
<td class="org-left">F1</td>
<td class="org-right">0.94</td>
</tr>
<tr>
<td class="org-left">Elapsed</td>
<td class="org-right">0:02:42.880287</td>
</tr>
</tbody>
</table>
<p>Overall the model seems to have done quite well. It was better at <i>recall</i> than <i>specificity</i> so it tended to classify some dogs as humans (around 11 %).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dogman_index = first_prediction(open_cv_scorer.false_image_predictions)
</pre></div>
<pre class="example">
2: True

</pre>
<p>It looks like the third dog image was classified as a human by OpenCV.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>source = dog_files_short[dogman_index]
name = get_name(source)
figure, axe = display_image(source,
                            "Dog-Human OpenCV Prediction ({})".format(name))
</pre></div>
<div class="figure">
<p><img alt="dog_man.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/dog_man.png"></p>
</div>
<p>I guess I can see where this might look like a human face. Maybe.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org60dc2f2">
<h3 id="org60dc2f2">DLIB</h3>
<div class="outline-text-3" id="text-org60dc2f2">
<p>I'm also going to test <a href="https://github.com/ageitgey/face_recognition"><code>face_recognition</code></a>, a python interface to <a href="http://dlib.net/">dlib's</a> facial recognition code. Unlike <code>OpenCV</code>, <code>face_recognition</code> doesn't require you to do the image-conversions before looking for faces.</p>
</div>
<div class="outline-4" id="outline-container-orgbacd58d">
<h4 id="orgbacd58d">Inspect an Image</h4>
<div class="outline-text-4" id="text-orgbacd58d">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>image = face_recognition.load_image_file(sample_face)
locations = face_recognition.face_locations(image)
image = mpimage.imread(sample_face)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = display_image(image, "dlib Face Recognition Bounding-Box", False)
top, right, bottom, left = locations[0]
width = right - left
height = top - bottom
rectangle = patches.Rectangle((top, right), width, height, fill=False)
patch = axe.add_patch(rectangle)
</pre></div>
<div class="figure">
<p><img alt="dlib_box.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/dlib_box.png"></p>
</div>
<p>This box seems to be more tightly cropped than the Open CV version.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org0c07e78">
<h4 id="org0c07e78">The Face Detecor</h4>
<div class="outline-text-4" id="text-org0c07e78">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def face_recognition_check(image_path: str) -&gt; bool:
    """This decides if an image has a face in it

    Args:
     image_path: path to an image
    Returns:
     True if there's at least one face in the image
    """
    image = face_recognition.load_image_file(str(image_path))
    locations = face_recognition.face_locations(image)
    return len(locations) &gt; 0
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1f2ab85">
<h4 id="org1f2ab85">Assess the Face Detector</h4>
<div class="outline-text-4" id="text-org1f2ab85">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dlib_dog_humans = human_scorer(face_recognition_check)
dlib_dog_humans()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Metric</th>
<th class="org-right" scope="col">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Accuracy</td>
<td class="org-right">0.95</td>
</tr>
<tr>
<td class="org-left">Precision</td>
<td class="org-right">0.92</td>
</tr>
<tr>
<td class="org-left">Recall</td>
<td class="org-right">1.00</td>
</tr>
<tr>
<td class="org-left">Specificity</td>
<td class="org-right">0.91</td>
</tr>
<tr>
<td class="org-left">F1</td>
<td class="org-right">0.96</td>
</tr>
<tr>
<td class="org-left">Elapsed</td>
<td class="org-right">0:09:28.752909</td>
</tr>
</tbody>
</table>
<p>Dlib took around four times as long to run as OpenCV did, but did better overall.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dlib_dog_human_index = first_prediction(dlib_dog_humans.false_image_predictions)
</pre></div>
<pre class="example">
5: True

</pre>
<p>The dlib model didn't have a false positive for the third image like the OpenCV model did, but it did get the sixth image wrong.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>source = dog_files_short[dlib_dog_human_index]
name = get_name(source)
figure, axe = display_image(source,
                            "Dog-Human DLib Prediction ({})".format(name))
</pre></div>
<div class="figure">
<p><img alt="dlib_dog_man.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/dlib_dog_man.png"></p>
</div>
<p>These photos with humans and dogs in them seem problematic.</p>
<p><code>face_recognition</code> provides another model based on a CNN that I wanted to try but it gives me out-of-memory errors so I'll have to save that for later.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgde5c67e">
<h2 id="orgde5c67e">A Dog Detector</h2>
<div class="outline-text-2" id="text-orgde5c67e">
<p>Now I'll take two pre-trained CNNs and use <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a> to have them detect dogs in images.</p>
</div>
<div class="outline-3" id="outline-container-org8d58037">
<h3 id="org8d58037">A Dog Detector Function</h3>
<div class="outline-text-3" id="text-org8d58037">
<p>If you look at the imagenet <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a">dictionary</a>, you'll see that the categories for dogs have indices from 151 to 268, so without altering our models we can check if an image is a dog by seeing if they classify the image within this range of values.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>DOG_LOWER, DOG_UPPER = 150, 260
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def dog_detector(img_path: Path, predictor: object):
    """Predicts if the image is a dog

    Args:
     img_path: path to image file
     predictor: callable that maps the image to an ID

    Returns:
     is-dog: True if the image contains a dog
    """
    return DOG_LOWER &lt; predictor(img_path) &lt; DOG_UPPER
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org18d8708">
<h3 id="org18d8708">The VGG-16 Model</h3>
<div class="outline-text-3" id="text-org18d8708">
<p>I'm going to use a VGG-16 model, along with weights that have been trained on <a href="http://www.image-net.org/">ImageNet</a>, a data set containing objects from one of <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a">1000 categories</a>.</p>
<p>Pytorch comes with a VGG 16 model built-in so we just have to declare it with the <code>pretrained=True</code> argument to download and load it.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>timer.start()
VGG16 = models.vgg16(pretrained=True)
VGG16.eval()
VGG16.to(device)
MODELS.append(VGG16)
timer.end()
</pre></div>
<pre class="example">
Started: 2019-01-13 14:43:39.512124
Ended: 2019-01-13 14:44:07.819057
Elapsed: 0:00:28.306933

</pre>
<p><b>Note:</b> The first time you run this it has to download the state dictionary so it will take much longer than it would once you've run it at least once.</p>
</div>
<div class="outline-4" id="outline-container-orge94b824">
<h4 id="orge94b824">Making Predictions With the VGG 16 Model</h4>
<div class="outline-text-4" id="text-orge94b824">
<p>In order to use the images with our model we have to run them through a transform. Even then, the forward-pass expects you to pass it a batch, not a single image, so you have to add an extra (fourth) dimension to the images to represent the batch. I found out how to fix the dimensions (using <a href="https://pytorch.org/docs/stable/tensors.html?highlight=unsqueeze#torch.Tensor.unsqueeze">unsqueeze</a> to add an empty dimension) from <a href="http://blog.outcome.io/pytorch-quick-start-classifying-an-image/">this blog post</a>.</p>
<p>This next block sets up the transforms. Each pre-trained model expects a specific image-size for the inputs. In this case the <code>VGG16</code> model expects a 224 x 224 image (which is why I set the <code>IMAGE_SIZE</code> to 224).</p>
<p>The images also have to be normalized using a specific set of means and standard deviations, but since pytorch uses the same ones for all the models I defined them at the top of this document because I'll be using them later for the inception model as well.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>IMAGE_SIZE = 224
IMAGE_HALF_SIZE = IMAGE_SIZE//2

vgg_transform = transforms.Compose([transforms.Resize(255),
                                    transforms.CenterCrop(IMAGE_SIZE),
                                    transforms.ToTensor(),
                                    transforms.Normalize(MEANS,
                                                         DEVIATIONS)])
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org82134de">
<h4 id="org82134de">VGG16 Predict</h4>
<div class="outline-text-4" id="text-org82134de">
<p>This is a function to predict what class an image is.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def VGG16_predict(img_path: str) -&gt; int:
    '''
    Uses a pre-trained VGG-16 model to obtain the index corresponding to 
    predicted ImageNet class for image at specified path

    Args:
        img_path: path to an image

    Returns:
        Index corresponding to VGG-16 model's prediction
    '''
    image = Image.open(str(img_path))
    image = vgg_transform(image).unsqueeze(0).to(device)
    output = VGG16(image)
    probabilities = torch.exp(output)
    top_probability, top_class = probabilities.topk(1, dim=1)
    return top_class.item()
</pre></div>
<p>Let's see what the model predicts for an image.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>path = numpy.random.choice(dog_files_short)
print(path)
classification = VGG16_predict(path)
print(imagenet[classification])
</pre></div>
<pre class="example">
/home/hades/data/datasets/dog-breed-classification/dogImages/valid/044.Cane_corso/Cane_corso_03122.jpg
American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier

</pre>
<p>Our classifier recognizes that the image is a dog, but thinks that it's a Terrire, not a Cane Corso. Here's what it saw.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>name = get_name(path)
figure, axe = display_image(path, name)
</pre></div>
<div class="figure">
<p><img alt="vgg_misclassified.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/vgg_misclassified.png"></p>
</div>
<p>And this is what it thought it was (a bull-mastiff).</p>
<div class="figure">
<p><img alt="american_staffordshire_terrier.jpg" src="posts/nano/dog-breed-classifier/dog-breed-classification/american_staffordshire_terrier.jpg"></p>
</div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf3bd91e">
<h3 id="orgf3bd91e">Assess the Dog Detector</h3>
<div class="outline-text-3" id="text-orgf3bd91e">
<p>Now, as with the human face-detectors, I'll calculate some metrics to see how the VGG16 dog-detector does.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>dog_scorer = partial(F1Scorer, true_images=dog_files_short,
                     false_images=human_files_short)
vgg_predictor = partial(dog_detector, predictor=VGG16_predict)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> vgg_scorer = dog_scorer(vgg_predictor)
 vgg_scorer()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Metric</th>
<th class="org-right" scope="col">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Accuracy</td>
<td class="org-right">0.95</td>
</tr>
<tr>
<td class="org-left">Precision</td>
<td class="org-right">0.99</td>
</tr>
<tr>
<td class="org-left">Recall</td>
<td class="org-right">0.92</td>
</tr>
<tr>
<td class="org-left">Specificity</td>
<td class="org-right">0.99</td>
</tr>
<tr>
<td class="org-left">F1</td>
<td class="org-right">0.95</td>
</tr>
<tr>
<td class="org-left">Elapsed</td>
<td class="org-right">0:02:37.257690</td>
</tr>
</tbody>
</table>
<p>Unlike the face-detectors, the VGG16 dog detector did better at avoiding false-positives than it did at detecting dogs.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org96f3d77">
<h3 id="org96f3d77">Inception</h3>
<div class="outline-text-3" id="text-org96f3d77">
<p>The previous detector used the VGG 16 model, but now I'll try the <a href="http://pytorch.org/docs/master/torchvision/models.html#inception-v3">Inception-v3</a> model, which was designed to use less resources than the VGG model, to do some dog-detection.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> timer.start()
 inception = models.inception_v3(pretrained=True)
 inception.to(device)
 inception.eval()
 MODELS.append(inception)
 timer.end()
</pre></div>
<pre class="example">
Started: 2019-01-13 18:45:27.709998
Ended: 2019-01-13 18:45:31.775443
Elapsed: 0:00:04.065445

</pre></div>
<div class="outline-4" id="outline-container-orgdc1f1a3">
<h4 id="orgdc1f1a3">Making a Prediction</h4>
<div class="outline-text-4" id="text-orgdc1f1a3">
<p>This was my original dog detector using the Inception model, but when I tried it out it raised an error. See the next section for more information and the fix.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> def inception_predicts(image_path: str) -&gt; int:
     """Predicts the category of the image

     Args:
      image_path: path to the image file

     Returns:
      classification: the resnet ID for the image
     """
     image = Image.open(str(image_path))
     image = vgg_transform(image).unsqueeze(0).to(device)
     output = inception(image)
     probabilities = torch.exp(output)
     top_probability, top_class = probabilities.topk(1, dim=1)
     return top_class.item()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc3a9f69">
<h4 id="orgc3a9f69">Troubleshooting the Error</h4>
<div class="outline-text-4" id="text-orgc3a9f69">
<p>The <code>inception_predicts</code> is throwing a Runtime Error saying that the sizes must be non-negative. I'll grab a file here to check it out.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> for path in dog_files_short:
     try:
         prediction = inception_predicts(path)
     except RuntimeError as error:
         print(error)
         print(path)
         break
</pre></div>
<pre class="example">
Given input size: (2048x5x5). Calculated output size: (2048x0x0). Output size is too small at /pytorch/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63
/home/hades/data/datasets/dog-breed-classification/dogImages/valid/044.Cane_corso/Cane_corso_03122.jpg

</pre>
<p>So this dog raised an error, let's see what it looks like.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> name = get_name(path)
 figure, axe = display_image(path, "Error-Producing Image ({})".format(name))
</pre></div>
<div class="figure">
<p><img alt="inception_error.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/inception_error.png"></p>
</div>
</div>
<ul class="org-ul">
<li><a id="org38738d6"></a>Why did this raise an error?<br>
<div class="outline-text-5" id="text-org38738d6">
<p>I couldn't find anyplace where pytorch documents it, but if you look at <a href="https://pytorch.org/docs/stable/_modules/torchvision/models/inception.html#inception_v3">the source code</a> you can see that they are expecting an image size of 299 pixels, so we need a diferent transform from that used by the VGG model.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span> INCEPTION_IMAGE_SIZE = 299
 inception_transforms = transforms.Compose([
     transforms.Resize(INCEPTION_IMAGE_SIZE),
     transforms.CenterCrop(INCEPTION_IMAGE_SIZE),
     transforms.ToTensor(),
     transforms.Normalize(MEANS,
                          DEVIATIONS)])
</pre></div>
<p>Now try it again with the new transforms.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def inception_predicts_two(image_path: str) -&gt; int:
    """Predicts the category of the image

    Args:
     image_path: path to the image file

    Returns:
     classification: the resnet ID for the image
    """
    image = Image.open(str(image_path))
    image = inception_transforms(image).unsqueeze(0).to(device)
    output = inception(image)
    probabilities = torch.exp(output)
    top_probability, top_class = probabilities.topk(1, dim=1)
    return top_class.item()
</pre></div>
<p>Does this fix it?</p>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org2b8ab53">
<h4 id="org2b8ab53">The Score</h4>
<div class="outline-text-4" id="text-org2b8ab53">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>inception_predictor = partial(dog_detector, predictor=inception_predicts_two)
inception_scorer = dog_scorer(inception_predictor)
inception_scorer()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Metric</th>
<th class="org-right" scope="col">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Accuracy</td>
<td class="org-right">0.95</td>
</tr>
<tr>
<td class="org-left">Precision</td>
<td class="org-right">0.99</td>
</tr>
<tr>
<td class="org-left">Recall</td>
<td class="org-right">0.91</td>
</tr>
<tr>
<td class="org-left">Specificity</td>
<td class="org-right">0.99</td>
</tr>
<tr>
<td class="org-left">F1</td>
<td class="org-right">0.95</td>
</tr>
<tr>
<td class="org-left">Elapsed</td>
<td class="org-right">0:03:00.836240</td>
</tr>
</tbody>
</table>
<p>The inception had a little more false positives but also more true positives so in the end it came up about the same on the F1 score as the VGG 16 model. They both took about the same amount of time.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>inception_human_dog = first_prediction(inception_scorer.false_image_predictions)
</pre></div>
<pre class="example">
34: True

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
source = human_files_short[inception_human_dog]
name = " ".join(
    os.path.splitext(
        os.path.basename(source))[0].split("_")[:-1]).title()
figure.suptitle("Human-Dog Inception Prediction ({})".format(
    name), weight="bold")
image = Image.open(source)
image = axe.imshow(image)
</pre></div>
<div class="figure">
<p><img alt="inception_man_dog.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/inception_man_dog.png"></p>
</div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org05711dc">
<h2 id="org05711dc">Combine The Detectors</h2>
<div class="outline-text-2" id="text-org05711dc">
<p>Since jupyter (or org-babel) lets you run cells out of sequence I've spent too much time chasing bugs that weren't really bugs, I just hadn't run the right cell. To try and ameliorate that I'm going to use class-based code for the actual implementations.</p>
</div>
<div class="outline-3" id="outline-container-org477dd74">
<h3 id="org477dd74">The Dog Detector</h3>
<div class="outline-text-3" id="text-org477dd74">
<p>The Dog Detector builds the parts of the deep learning model that are needed to check if there are dogs in the image.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class DogDetector:
    """Detects dogs

    Args:
     model_definition: definition for the model
     device: where to run the model (CPU or CUDA)
     image_size: what to resize the file to (depends on the model-definition)
     means: mean for each channel
     deviations: standard deviation for each channel
     dog_lower_bound: index below where dogs start
     dog_upper_bound: index above where dogs end
    """
    def __init__(self,
                 model_definition: nn.Module=models.inception_v3,
                 image_size: int=INCEPTION_IMAGE_SIZE,
                 means: list=MEANS,
                 deviations = DEVIATIONS,
                 dog_lower_bound: int=DOG_LOWER,
                 dog_upper_bound: int=DOG_UPPER,
                 device: torch.device=None) -&gt; None:
        self.model_definition = model_definition
        self.image_size = image_size
        self.means = means
        self.deviations = deviations
        self.dog_lower_bound = dog_lower_bound
        self.dog_upper_bound = dog_upper_bound
        self._device = device
        self._model = None
        self._transform = None
        return

    @property
    def device(self) -&gt; torch.device:
        """The device to add the model to"""
        if self._device is None:
            self._device = torch.device("cuda"
                                        if torch.cuda.is_available()
                                        else "cpu")
        return self._device

    @property
    def model(self) -&gt; nn.Module:
        """Build the model"""
        if self._model is None:
            self._model = self.model_definition(pretrained=True)
            self._model.to(self.device)
            self._model.eval()
        return self._model

    @property
    def transform(self) -&gt; transforms.Compose:
        """The transformer for the image data"""
        if self._transform is None:
            self._transform = transforms.Compose([
                transforms.Resize(self.image_size),
                transforms.CenterCrop(self.image_size),
                transforms.ToTensor(),
                transforms.Normalize(self.means,
                                     self.deviations)])
        return self._transform

    def __call__(self, image_path: str) -&gt; bool:
        """Checks if there is a dog in the image"""
        image = Image.open(str(image_path))
        image = self.transform(image).unsqueeze(0).to(self.device)
        output = self.model(image)
        probabilities = torch.exp(output)
        _, top_class = probabilities.topk(1, dim=1)
        return self.dog_lower_bound &lt; top_class.item() &lt; self.dog_upper_bound
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga7bc2af">
<h3 id="orga7bc2af">The Species Detector</h3>
<div class="outline-text-3" id="text-orga7bc2af">
<p>The Species Detector holds the human and dog detectors.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class SpeciesDetector:
    """Detect dogs and humans

    Args:
     device: where to put the dog-detecting model
    """
    def __init__(self, device: torch.device=None) -&gt; None:
        self.device = device
        self._dog_detector = None
        return

    @property
    def dog_detector(self) -&gt; DogDetector:
        """Neural Network dog-detector"""
        if self._dog_detector is None:
            self._dog_detector = DogDetector(device=self.device)
        return self._dog_detector

    def is_human(self, image_path: str) -&gt; bool:
        """Checks if the image is a human

        Args:
         image_path: path to the image

        Returns:
         True if there is a human face in the image
        """
        image = face_recognition.load_image_file(str(image_path))
        faces = face_recognition.face_locations(image)
        return len(faces) &gt; 0

    def is_dog(self, image_path: str) -&gt; bool:        
        """Checks if there is a dog in the image"""
        return self.dog_detector(image_path)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org46b857e">
<h2 id="org46b857e">A Dog Breed Classifier</h2>
<div class="outline-text-2" id="text-org46b857e">
<p>Although the Inception model does do some classification of dogs, we want an even more fine-tuned model. First I'm going to try to build a naive CNN from scratch, then I'm going to use the Inception model and transfer learning to build a better classifier.</p>
</div>
<div class="outline-3" id="outline-container-org81243ee">
<h3 id="org81243ee">A Naive Model</h3>
<div class="outline-text-3" id="text-org81243ee"></div>
<div class="outline-4" id="outline-container-org7e245c5">
<h4 id="org7e245c5">The Data Transformers</h4>
<div class="outline-text-4" id="text-org7e245c5">
<p>For the naive model I'm going to use the image-size the VGG model uses (<a href="https://arxiv.org/abs/1409.1556">the original VGG paper</a> describes the input as being 224 x 224). No particular reason except I've worked with that size before so I think it might make troubleshooting a little easier. The <code>Resize</code> transform scales the image so that the smaller edge matches the size we give it. I found out the hard way that not all the input images are square so we need to then crop them back to the right size after scaling.</p>
<p>Here's the training tranforms:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomRotation">RandomRotation</a></li>
<li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomResizedCrop">RandomResizedCrop</a></li>
<li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip">RandomHorizontalFlip</a></li>
</ul>
<p>For testing and using:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize">Resize</a></li>
<li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.CenterCrop">CenterCrop</a></li>
</ul>
<p>For both:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor">ToTensor</a></li>
<li><a href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize">Normalize</a></li>
</ul>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>IMAGE_SIZE = 224
IMAGE_HALF_SIZE = IMAGE_SIZE//2

train_transform = transforms.Compose([
    transforms.RandomRotation(30),
    transforms.RandomResizedCrop(IMAGE_SIZE),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(MEANS,
                         DEVIATIONS)])

test_transform = transforms.Compose([transforms.Resize(255),
                                      transforms.CenterCrop(IMAGE_SIZE),
                                      transforms.ToTensor(),
                                      transforms.Normalize(MEANS,
                                                           DEVIATIONS)])
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3866155">
<h4 id="org3866155">Load the Data</h4>
<div class="outline-text-4" id="text-org3866155">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>training = datasets.ImageFolder(root=str(dog_paths.training.folder),
                                transform=train_transform)
validation = datasets.ImageFolder(root=str(dog_paths.validation.folder),
                                  transform=test_transform)
testing = datasets.ImageFolder(root=str(dog_paths.testing.folder),
                               transform=test_transform)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3e3f6ff">
<h4 id="org3e3f6ff">Build the Batch Loaders</h4>
<div class="outline-text-4" id="text-org3e3f6ff">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>BATCH_SIZE = 35
WORKERS = 0

train_batches = torch.utils.data.DataLoader(training, batch_size=BATCH_SIZE,
                                            shuffle=True, num_workers=WORKERS)
validation_batches = torch.utils.data.DataLoader(
    validation, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)
test_batches = torch.utils.data.DataLoader(
    testing, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)

loaders_scratch = dict(train=train_batches,
                       validate=validation_batches,
                       test=test_batches)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5143674">
<h4 id="org5143674">The Network</h4>
<div class="outline-text-4" id="text-org5143674">
<p>This is only going to be a three-layer model. I started out trying to make a really big one but between the computation time and running out of memory I decided to limit the scope since the transfer model is the real one I want anyway, this is just for practice. The first block defines the parameters for the network.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>LAYER_ONE_OUT = 16
LAYER_TWO_OUT = LAYER_ONE_OUT * 2
LAYER_THREE_OUT = LAYER_TWO_OUT * 2

KERNEL = 3
PADDING = 1
FULLY_CONNECTED_OUT = 500
</pre></div>
<p>This next block does one pass through what the network is going to be doing so I can make sure the inputs and outputs are the correct size.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>conv_1 = nn.Conv2d(3, LAYER_ONE_OUT, KERNEL, padding=PADDING)
conv_2 = nn.Conv2d(LAYER_ONE_OUT, LAYER_TWO_OUT, KERNEL, padding=PADDING)
conv_3 = nn.Conv2d(LAYER_TWO_OUT, LAYER_THREE_OUT, KERNEL, padding=PADDING)

pool = nn.MaxPool2d(2, 2)
dropout = nn.Dropout(0.25)

fully_connected_1 = nn.Linear((IMAGE_HALF_SIZE//4)**2 * LAYER_THREE_OUT, FULLY_CONNECTED_OUT)
fully_connected_2 = nn.Linear(FULLY_CONNECTED_OUT, dog_paths.breed_count)

dataiter = iter(loaders_scratch['train'])
images, labels = dataiter.next()

x = pool(F.relu(conv_1(images)))
print(x.shape)
assert x.shape == torch.Size([BATCH_SIZE, 16, IMAGE_HALF_SIZE, IMAGE_HALF_SIZE])

x = pool(F.relu(conv_2(x)))
print(x.shape)
assert x.shape == torch.Size([BATCH_SIZE, LAYER_TWO_OUT, IMAGE_HALF_SIZE//2, IMAGE_HALF_SIZE//2])

x = pool(F.relu(conv_3(x)))
print(x.shape)
assert x.shape == torch.Size([BATCH_SIZE, LAYER_THREE_OUT, IMAGE_HALF_SIZE//4, IMAGE_HALF_SIZE//4])

x = x.view(-1, ((IMAGE_HALF_SIZE//4)**2) * LAYER_THREE_OUT)
print(x.shape)
x = fully_connected_1(x)
print(x.shape)
x = fully_connected_2(x)
print(x.shape)
</pre></div>
<pre class="example">
torch.Size([10, 16, 112, 112])
torch.Size([10, 32, 56, 56])
torch.Size([10, 64, 28, 28])
torch.Size([10, 50176])
torch.Size([10, 500])
torch.Size([10, 133])

</pre></div>
</div>
<div class="outline-4" id="outline-container-orgf4c647e">
<h4 id="orgf4c647e">The Class</h4>
<div class="outline-text-4" id="text-orgf4c647e">
<p>This is the actual implementation based on the previous code.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class NaiveNet(nn.Module):
    """Naive Neural Network to classify dog breeds"""
    def __init__(self) -&gt; None:
        super().__init__()
        self.conv1 = nn.Conv2d(3, LAYER_ONE_OUT,
                               KERNEL, padding=PADDING)
        self.conv2 = nn.Conv2d(LAYER_ONE_OUT, LAYER_TWO_OUT,
                               KERNEL, padding=PADDING)
        self.conv3 = nn.Conv2d(LAYER_TWO_OUT, LAYER_THREE_OUT,
                               KERNEL, padding=PADDING)
        # max pooling layer
        self.pool = nn.MaxPool2d(2, 2)
        # linear layer
        self.fc1 = nn.Linear((IMAGE_HALF_SIZE//4)**2 * LAYER_THREE_OUT, FULLY_CONNECTED_OUT)
        self.fc2 = nn.Linear(FULLY_CONNECTED_OUT, BREEDS)
        # dropout layer (p=0.25)
        self.dropout = nn.Dropout(0.25)
        return


    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        """The forward pass method

        Args:
         x: a n x 224 x 224 x 3 tensor

        Returns:
         tensor of probabilities
        """
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))

        x = x.view(-1, (IMAGE_HALF_SIZE//4)**2 * LAYER_THREE_OUT)
        x = self.dropout(x)

        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)        
        return x
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>naive_model = NaiveNet()
naive_model.to(device)
MODELS.append(naive_model)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org7010317">
<h4 id="org7010317">The Loss Function and Optimizer</h4>
<div class="outline-text-4" id="text-org7010317">
<p>For loss measurement I'm going to use <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss">Cross Entropy Loss</a> and <a href="https://pytorch.org/docs/stable/optim.html#torch.optim.SGD">Stochastic Gradient Descent</a> for backward propagation.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>criterion_scratch = nn.CrossEntropyLoss()
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>optimizer_scratch = optimizer.SGD(naive_model.parameters(),
                                  lr=0.001,
                                  momentum=0.9)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org41fbcca">
<h4 id="org41fbcca">Train and Validate the Model</h4>
<div class="outline-text-4" id="text-org41fbcca"></div>
<ul class="org-ul">
<li><a id="org4bae3a6"></a>The Trainer<br>
<div class="outline-text-5" id="text-org4bae3a6">
<p>Another class to try and get everything bundled into one place.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Trainer:
    """Trains, validates, and tests the model

    Args:
     training_batches: batch-loaders for training
     validation_batches: batch-loaders for validation
     testing_batches: batch-loaders for testing
     model: the network to train
     model_path: where to save the best model
     optimizer: the gradient descent object
     criterion: object to do backwards propagation
     device: where to put the data (cuda or cpu)
     epochs: number of times to train on the data set
     epoch_start: number to start the epoch count with
     load_model: whether to load the model from a file
     beep: whether timer should emit sounds
     is_inception: expecte two outputs in training
    """
    def __init__(self,
                 training_batches: torch.utils.data.DataLoader,
                 validation_batches: torch.utils.data.DataLoader,
                 testing_batches: torch.utils.data.DataLoader,
                 model: nn.Module,
                 model_path: Path,
                 optimizer: optimizer.SGD,
                 criterion: nn.CrossEntropyLoss,
                 device: torch.device=None,
                 epochs: int=10,
                 epoch_start: int=1,
                 is_inception: bool=False,
                 load_model: bool=False,
                 beep: bool=False) -&gt; None:
        self.training_batches = training_batches
        self.validation_batches = validation_batches
        self.testing_batches = testing_batches
        self.model = model
        self.model_path = model_path
        self.optimizer = optimizer
        self.criterion = criterion
        self.epochs = epochs
        self.is_inception = is_inception
        self.beep = beep
        self._epoch_start = None
        self.epoch_start = epoch_start
        self.load_model = load_model
        self._timer = None
        self._epoch_end = None
        self._device = device
        return

    @property
    def epoch_start(self) -&gt; int:
        """The number to start the epoch count"""
        return self._epoch_start

    @epoch_start.setter
    def epoch_start(self, new_start: int) -&gt; None:
        """Sets the epoch start, removes the epoch end"""
        self._epoch_start = new_start
        self._epoch_end = None
        return

    @property
    def device(self) -&gt; torch.device:
        """The device to put the data on"""
        if self._device is None:
            self._device = torch.device("cuda" if torch.cuda.is_available()
                                        else "cpu")
        return self._device

    @property
    def epoch_end(self) -&gt; int:
        """the end of the epochs (not inclusive)"""
        if self._epoch_end is None:
            self._epoch_end = self.epoch_start + self.epochs
        return self._epoch_end

    @property
    def timer(self) -&gt; Timer:
        """something to emit times"""
        if self._timer is None:
            self._timer = Timer(beep=self.beep)
        return self._timer

    def forward(self, batches: torch.utils.data.DataLoader,
                training: bool) -&gt; tuple:
        """runs the forward pass

        Args:
         batches: data-loader
         training: if true, runs the training, otherwise validates
        Returns:
         tuple: loss, correct, total
        """
        forward_loss = 0
        correct = 0

        if training:
            self.model.train()
        else:
            self.model.eval()
        for data, target in batches:
            data, target = data.to(self.device), target.to(self.device)
            if training:
                self.optimizer.zero_grad()
            if training and self.is_inception:
                # throw away the auxiliary output
                output, _ = self.model(data)
            output = self.model(data)
            loss = self.criterion(output, target)
            if training:
                loss.backward()
                self.optimizer.step()
            forward_loss += loss.item() * data.size(0)

            predictions = output.data.max(1, keepdim=True)[1]
            correct += numpy.sum(
                numpy.squeeze(
                    predictions.eq(
                        target.data.view_as(predictions))).cpu().numpy())
        forward_loss /= len(batches.dataset)
        return forward_loss, correct, len(batches.dataset)

    def train(self) -&gt; tuple:
        """Runs the training

        Returns:
         training loss, correct, count
        """
        return self.forward(batches=self.training_batches, training=True)

    def validate(self) -&gt; tuple:
        """Runs the validation

        Returns:
         validation loss, correct, count
        """
        return self.forward(batches=self.validation_batches, training=False)

    def test(self) -&gt; None:
        """Runs the testing

        """
        self.timer.start()
        self.model.load_state_dict(torch.load(self.model_path))
        loss, correct, total = self.forward(batches=self.testing_batches,
                                            training=False)
        print("Test Loss: {:.3f}".format(loss))
        print("Test Accuracy: {:.2f} ({}/{})".format(100 * correct/total,
                                                     correct, total))
        self.timer.end()
        return

    def train_and_validate(self):
        """Trains and Validates the model
        """
        validation_loss_min = numpy.Inf
        for epoch in range(self.epoch_start, self.epoch_end):
            self.timer.start()
            training_loss, training_correct, training_count = self.train()
            (validation_loss, validation_correct,
             validation_count) = self.validate()
            self.timer.end()
            print(("Epoch: {}\t"
                   "Training - Loss: {:.2f}\t"
                   "Accuracy: {:.2f}\t"
                   "Validation - Loss: {:.2f}\t"
                   "Accuracy: {:.2f}").format(
                       epoch,
                       training_loss,
                       training_correct/training_count,
                       validation_loss,
                       validation_correct/validation_count,
                ))

            if validation_loss &lt; validation_loss_min:
                print(
                    ("Validation loss decreased ({:.6f} --&gt; {:.6f}). "
                     "Saving model ...").format(
                         validation_loss_min,
                         validation_loss))
                torch.save(self.model.state_dict(), self.model_path)
                validation_loss_min = validation_loss
        return

    def __call__(self) -&gt; None:
        """Trains, Validates, and Tests the model"""
        if self.load_model and self.model_path.is_file():
            self.model.load_state_dict(torch.load(self.model_path))
        print("Starting Training")
        self.timer.start()
        self.train_and_validate()
        self.timer.end()
        print("\nStarting Testing")
        self.test()
        return
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgb601857">
<h4 id="orgb601857">Broken Images</h4>
<div class="outline-text-4" id="text-orgb601857">
<p>I noted at the beginning of the notebook that at least one of the images is raising an OSError:</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="ne">OSError</span><span class="p">:</span> <span class="n">image</span> <span class="nb">file</span> <span class="ow">is</span> <span class="n">truncated</span> <span class="p">(</span><span class="mi">150</span> <span class="nb">bytes</span> <span class="ow">not</span> <span class="n">processed</span><span class="p">)</span>
</pre></div>
<p>This is the part of the notebook where I originally found out what was going on (because it kept crashing during training).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>timer.start()
broken = None
for image in dog_files:
    try:
        opened = Image.open(image)
        opened.convert("RGB")
    except OSError as error:
        print("{}: {}".format(error, image))
        broken = image
timer.end()
</pre></div>
<pre class="example">
image file is truncated (150 bytes not processed): /home/hades/datasets/dog-breed-classification/dogImages/train/098.Leonberger/Leonberger_06571.jpg
Ended: 2018-12-30 15:10:19.141003
Elapsed: 0:02:29.804925

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>figure, axe = pyplot.subplots()
name = " ".join(broken.name.split("_")[:-1]).title()
figure.suptitle("Truncated Image ({})".format(name), weight="bold")
image = Image.open(broken)
axe_image = axe.imshow(image)
</pre></div>
<div class="figure">
<p><img alt="truncated_dog.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/truncated_dog.png"></p>
</div>
<p>I got the solution from <a href="https://stackoverflow.com/questions/12984426/python-pil-ioerror-image-file-truncated-with-big-images">this Stack Overflow post</a>, I don't know why but the image seems to be missing some pixels or something. Oh, well. The key to making it work:</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">ImageFile</span><span class="o">.</span><span class="n">LOAD_TRUNCATED_IMAGES</span> <span class="o">=</span> <span class="bp">True</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1e4ece9">
<h4 id="org1e4ece9">Train the Model</h4>
<div class="outline-text-4" id="text-org1e4ece9">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>NAIVE_PATH = MODEL_PATH.folder.joinpath("model_scratch.pt")
scratch_log = Tee(log_name="scratch_train.log")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org89cb987">
<h4 id="org89cb987">Test the Model</h4>
<div class="outline-text-4" id="text-org89cb987">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def test(test_batches: torch.utils.data.DataLoader,
         model: nn.Module,
         criterion: nn.CrossEntropyLoss) -&gt; None:
    """Test the model

    Args:
     test_batches: batch loader of test images
     model: the network to test
     criterion: calculator for the loss
    """
    test_loss = 0.
    correct = 0.
    total = 0.

    model.eval()
    for data, target in test_batches:
        data, target = data.to(device), target.to(device)
        output = model(data)
        loss = criterion(output, target)
        test_loss += loss.item() * data.size(0)
        # convert output probabilities to predicted class
        predictions = output.data.max(1, keepdim=True)[1]
        # compare predictions to true label
        correct += numpy.sum(
            numpy.squeeze(
                predictions.eq(
                    target.data.view_as(predictions))).cpu().numpy())
        total += data.size(0)
    test_loss /= len(test_batches.dataset)
    print('Test Loss: {:.6f}\n'.format(test_loss))
    print('\nTest Accuracy: %2d%% (%2d/%2d)' % (
        100. * correct / total, correct, total))
    return
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org5e325f9">
<h3 id="org5e325f9">Train and Test</h3>
<div class="outline-text-3" id="text-org5e325f9">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>def train_and_test(train_batches: torch.utils.data.DataLoader,
                   validate_batches: torch.utils.data.DataLoader,
                   test_batches: torch.utils.data.DataLoader,
                   model: nn.Module,
                   model_path: Path,
                   optimizer: optimizer.SGD,
                   criterion: nn.CrossEntropyLoss,
                   epochs: int=10,
                   epoch_start: int=1,
                   load_model: bool=False) -&gt; None:
    """Trains and Tests the Model

    Args:
     train_batches: batch-loaders for training
     validate_batches: batch-loaders for validation
     test_batches: batch-loaders for testing
     model: the network to train
     model_path: where to save the best model
     optimizer: the gradient descent object
     criterion: object to do backwards propagation
     epochs: number of times to train on the data set
     epoch_start: number to start the epoch count with
     load_model: whether to load the model from a file
    """
    if load_model and model_path.is_file():
        model.load_state_dict(torch.load(model_path))
    print("Starting Training")
    timer.start()
    model_scratch = train(epochs=epochs,
                          epoch_start=epoch_start,
                          train_batches=train_batches,
                          validation_batches=validate_batches,
                          model=model,
                          optimizer=optimizer, 
                          criterion=criterion,
                          save_path=model_path)
    timer.end()
    # load the best model
    model.load_state_dict(torch.load(model_path))
    print("Starting Testing")
    timer.start()
    test(test_batches, model, criterion)
    timer.end()
    return
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org407d01a">
<h3 id="org407d01a">Train the Model</h3>
<div class="outline-text-3" id="text-org407d01a">
<p>When I originally wrote this I was using this functional-style of training and testing, which was hard to use, but since it's so expensive to train the model (in terms of time, and to some degree server cost) I'm not going to re-do it so the code here looks a little different from the one I used for the transfer model.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_path = DataPathTwo(
    folder_key="MODELS",
    filename="model_scratch.pt")
assert model_path.folder.is_dir()
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_and_test(epochs=10,
               train_batches=loaders_scratch["train"],
               validate_batches=loaders_scratch["validate"],
               test_batches=loaders_scratch["test"],
               model=model_scratch,
               optimizer=optimizer_scratch, 
               criterion=criterion_scratch,
               epoch_start=0,
               model_path=model_path.from_folder,
               load_model=False)
next_start = 11
</pre></div>
<pre class="example">
Starting Training
Ended: 2019-01-01 16:35:14.192989
Elapsed: 0:03:23.778459
Epoch: 0        Training Loss: 3.946975         Validation Loss: 3.758706
Validation loss decreased (inf --&gt; 3.758706). Saving model ...
Ended: 2019-01-01 16:38:39.497147
Elapsed: 0:03:24.517456
Epoch: 1        Training Loss: 3.880984         Validation Loss: 3.695643
Validation loss decreased (3.758706 --&gt; 3.695643). Saving model ...
Ended: 2019-01-01 16:42:04.190248
Elapsed: 0:03:23.903292
Epoch: 2        Training Loss: 3.870710         Validation Loss: 3.718353
Ended: 2019-01-01 16:45:28.479552
Elapsed: 0:03:23.718292
Epoch: 3        Training Loss: 3.836664         Validation Loss: 3.740289
Ended: 2019-01-01 16:48:53.605419
Elapsed: 0:03:24.555708
Epoch: 4        Training Loss: 3.819701         Validation Loss: 3.659244
Validation loss decreased (3.695643 --&gt; 3.659244). Saving model ...
Ended: 2019-01-01 16:52:33.198097
Elapsed: 0:03:38.805586
Epoch: 5        Training Loss: 3.778872         Validation Loss: 3.756706
Ended: 2019-01-01 16:56:16.822584
Elapsed: 0:03:43.055469
Epoch: 6        Training Loss: 3.752981         Validation Loss: 3.679196
Ended: 2019-01-01 16:59:42.861936
Elapsed: 0:03:25.469331
Epoch: 7        Training Loss: 3.730930         Validation Loss: 3.608311
Validation loss decreased (3.659244 --&gt; 3.608311). Saving model ...
Ended: 2019-01-01 17:03:10.958002
Elapsed: 0:03:27.305644
Epoch: 8        Training Loss: 3.705110         Validation Loss: 3.636201
Ended: 2019-01-01 17:06:38.939991
Elapsed: 0:03:27.412824
Epoch: 9        Training Loss: 3.665519         Validation Loss: 3.595410
Validation loss decreased (3.608311 --&gt; 3.595410). Saving model ...
Ended: 2019-01-01 17:06:39.733176
Elapsed: 0:03:28.206009
Starting Testing
Test Loss: 3.642843


Test Accuracy: 14% (125/836)
Ended: 2019-01-01 17:07:11.142926
Elapsed: 0:00:30.815650
</pre>
<p>Hmm, seems suspiciously good all of a sudden. It looks like my GPU is faster than paper space's, too..</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_and_test(epochs=10,
               train_batches=loaders_scratch["train"],
               validate_batches=loaders_scratch["validate"],
               test_batches=loaders_scratch["test"],
               model=model_scratch,
               optimizer=optimizer_scratch, 
               criterion=criterion_scratch,
               epoch_start=next_start,
               model_path=model_path.from_folder,
               load_model=True)
next_start = 21
</pre></div>
<pre class="example">
Starting Training
Ended: 2019-01-01 17:29:46.425198
Elapsed: 0:03:40.954699
Epoch: 0        Training Loss: 3.662736         Validation Loss: 3.631118
Validation loss decreased (inf --&gt; 3.631118). Saving model ...
Ended: 2019-01-01 17:33:12.797754
Elapsed: 0:03:25.528229
Epoch: 1        Training Loss: 3.612436         Validation Loss: 3.610919
Validation loss decreased (3.631118 --&gt; 3.610919). Saving model ...
Ended: 2019-01-01 17:36:49.466848
Elapsed: 0:03:35.831733
Epoch: 2        Training Loss: 3.612902         Validation Loss: 3.590953
Validation loss decreased (3.610919 --&gt; 3.590953). Saving model ...
Ended: 2019-01-01 17:40:17.511898
Elapsed: 0:03:27.192943
Epoch: 3        Training Loss: 3.564542         Validation Loss: 3.566365
Validation loss decreased (3.590953 --&gt; 3.566365). Saving model ...
Ended: 2019-01-01 17:43:45.639219
Elapsed: 0:03:27.309572
Epoch: 4        Training Loss: 3.551703         Validation Loss: 3.608934
Ended: 2019-01-01 17:47:32.854824
Elapsed: 0:03:46.646159
Epoch: 5        Training Loss: 3.542706         Validation Loss: 3.533696
Validation loss decreased (3.566365 --&gt; 3.533696). Saving model ...
Ended: 2019-01-01 17:51:02.330525
Elapsed: 0:03:28.506819
Epoch: 6        Training Loss: 3.532894         Validation Loss: 3.531388
Validation loss decreased (3.533696 --&gt; 3.531388). Saving model ...
Ended: 2019-01-01 17:54:25.844725
Elapsed: 0:03:22.697779
Epoch: 7        Training Loss: 3.482241         Validation Loss: 3.564429
Ended: 2019-01-01 17:57:48.563069
Elapsed: 0:03:22.148237
Epoch: 8        Training Loss: 3.485189         Validation Loss: 3.624133
Ended: 2019-01-01 18:01:11.755236
Elapsed: 0:03:22.621310
Epoch: 9        Training Loss: 3.461059         Validation Loss: 3.594314
Ended: 2019-01-01 18:01:12.326268
Elapsed: 0:03:23.192342
Starting Testing
Test Loss: 3.537503


Test Accuracy: 16% (138/836)
Ended: 2019-01-01 18:01:42.764907
Elapsed: 0:00:29.747148
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_and_test(epochs=10,
               train_batches=loaders_scratch["train"],
               validate_batches=loaders_scratch["validate"],
               test_batches=loaders_scratch["test"],
               model=model_scratch,
               optimizer=optimizer_scratch, 
               criterion=criterion_scratch,
               epoch_start=next_start,
               model_path=model_path.from_folder,
               load_model=True)
next_start = 31
</pre></div>
<pre class="example">
Starting Training
Ended: 2019-01-01 18:45:17.404562
Elapsed: 0:03:23.081286
Epoch: 21       Training Loss: 3.510303         Validation Loss: 3.555182
Validation loss decreased (inf --&gt; 3.555182). Saving model ...
Ended: 2019-01-01 18:48:41.215171
Elapsed: 0:03:22.949288
Epoch: 22       Training Loss: 3.485824         Validation Loss: 3.570289
Ended: 2019-01-01 18:52:04.635395
Elapsed: 0:03:22.849569
Epoch: 23       Training Loss: 3.438656         Validation Loss: 3.543221
Validation loss decreased (3.555182 --&gt; 3.543221). Saving model ...
Ended: 2019-01-01 18:55:28.409018
Elapsed: 0:03:22.980693
Epoch: 24       Training Loss: 3.387092         Validation Loss: 3.649569
Ended: 2019-01-01 18:58:51.555922
Elapsed: 0:03:22.576946
Epoch: 25       Training Loss: 3.381217         Validation Loss: 3.529994
Validation loss decreased (3.543221 --&gt; 3.529994). Saving model ...
Ended: 2019-01-01 19:02:15.743200
Elapsed: 0:03:23.359857
Epoch: 26       Training Loss: 3.379801         Validation Loss: 3.514583
Validation loss decreased (3.529994 --&gt; 3.514583). Saving model ...
Ended: 2019-01-01 19:05:40.243125
Elapsed: 0:03:23.700481
Epoch: 27       Training Loss: 3.334058         Validation Loss: 3.469988
Validation loss decreased (3.514583 --&gt; 3.469988). Saving model ...
Ended: 2019-01-01 19:09:04.218270
Elapsed: 0:03:23.150903
Epoch: 28       Training Loss: 3.347201         Validation Loss: 3.456167
Validation loss decreased (3.469988 --&gt; 3.456167). Saving model ...
Ended: 2019-01-01 19:12:27.711756
Elapsed: 0:03:22.677622
Epoch: 29       Training Loss: 3.320286         Validation Loss: 3.444669
Validation loss decreased (3.456167 --&gt; 3.444669). Saving model ...
Ended: 2019-01-01 19:15:51.375887
Elapsed: 0:03:22.875358
Epoch: 30       Training Loss: 3.314001         Validation Loss: 3.460704
Ended: 2019-01-01 19:15:51.946497
Elapsed: 0:03:23.445968
Starting Testing
Test Loss: 3.492875


Test Accuracy: 17% (146/836)
Ended: 2019-01-01 19:16:10.729405
Elapsed: 0:00:18.109680
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_and_test(epochs=10,
               train_batches=loaders_scratch["train"],
               validate_batches=loaders_scratch["validate"],
               test_batches=loaders_scratch["test"],
               model=model_scratch,
               optimizer=optimizer_scratch, 
               criterion=criterion_scratch,
               epoch_start=next_start,
               model_path=model_path.from_folder,
               load_model=True)
next_start = 41
</pre></div>
<pre class="example">
Starting Training
Ended: 2019-01-01 20:15:25.906348
Elapsed: 0:05:12.167322
Epoch: 31       Training Loss: 3.311046         Validation Loss: 3.446478
Validation loss decreased (inf --&gt; 3.446478). Saving model ...
Ended: 2019-01-01 20:19:13.168084
Elapsed: 0:03:46.461085
Epoch: 32       Training Loss: 3.270769         Validation Loss: 3.550049
Ended: 2019-01-01 20:22:38.973465
Elapsed: 0:03:25.195274
Epoch: 33       Training Loss: 3.221883         Validation Loss: 3.489280
Ended: 2019-01-01 20:26:02.049299
Elapsed: 0:03:22.483931
Epoch: 34       Training Loss: 3.271723         Validation Loss: 3.507546
Ended: 2019-01-01 20:29:24.932614
Elapsed: 0:03:22.292605
Epoch: 35       Training Loss: 3.197156         Validation Loss: 3.475409
Ended: 2019-01-01 20:32:47.569786
Elapsed: 0:03:22.046763
Epoch: 36       Training Loss: 3.210177         Validation Loss: 3.477707
Ended: 2019-01-01 20:36:09.752175
Elapsed: 0:03:21.592504
Epoch: 37       Training Loss: 3.199346         Validation Loss: 3.577469
Ended: 2019-01-01 20:39:32.831340
Elapsed: 0:03:22.489048
Epoch: 38       Training Loss: 3.158563         Validation Loss: 3.442629
Validation loss decreased (3.446478 --&gt; 3.442629). Saving model ...
Ended: 2019-01-01 20:42:56.293868
Elapsed: 0:03:22.664005
Epoch: 39       Training Loss: 3.152231         Validation Loss: 3.470943
Ended: 2019-01-01 20:46:18.983529
Elapsed: 0:03:22.098438
Epoch: 40       Training Loss: 3.124298         Validation Loss: 3.429367
Validation loss decreased (3.442629 --&gt; 3.429367). Saving model ...
Ended: 2019-01-01 20:46:19.801009
Elapsed: 0:03:22.915918
Starting Testing
Test Loss: 3.348011


Test Accuracy: 21% (179/836)
Ended: 2019-01-01 20:46:42.494502
Elapsed: 0:00:22.094465
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_and_test(epochs=10,
               train_batches=loaders_scratch["train"],
               validate_batches=loaders_scratch["validate"],
               test_batches=loaders_scratch["test"],
               model=model_scratch,
               optimizer=optimizer_scratch, 
               criterion=criterion_scratch,
               epoch_start=next_start,
               model_path=model_path.from_folder,
               load_model=True)
next_start = 51
</pre></div>
<pre class="example">
Starting Training
Ended: 2019-01-01 22:01:17.285699
Elapsed: 0:03:24.381614
Epoch: 41       Training Loss: 3.095166         Validation Loss: 3.418227
Validation loss decreased (inf --&gt; 3.418227). Saving model ...
Ended: 2019-01-01 22:04:43.173252
Elapsed: 0:03:25.033381
Epoch: 42       Training Loss: 3.089258         Validation Loss: 3.419117
Ended: 2019-01-01 22:08:07.709900
Elapsed: 0:03:23.945667
Epoch: 43       Training Loss: 3.071535         Validation Loss: 3.433646
Ended: 2019-01-01 22:11:33.153513
Elapsed: 0:03:24.853880
Epoch: 44       Training Loss: 3.058665         Validation Loss: 3.454817
Ended: 2019-01-01 22:14:59.899762
Elapsed: 0:03:26.156530
Epoch: 45       Training Loss: 3.072674         Validation Loss: 3.494963
Ended: 2019-01-01 22:18:26.207188
Elapsed: 0:03:25.746042
Epoch: 46       Training Loss: 3.043788         Validation Loss: 3.430311
Ended: 2019-01-01 22:21:51.975083
Elapsed: 0:03:25.177310
Epoch: 47       Training Loss: 3.015571         Validation Loss: 3.382248
Validation loss decreased (3.418227 --&gt; 3.382248). Saving model ...
Ended: 2019-01-01 22:25:18.237087
Elapsed: 0:03:25.403639
Epoch: 48       Training Loss: 2.972451         Validation Loss: 3.449296
Ended: 2019-01-01 22:28:44.315967
Elapsed: 0:03:25.498810
Epoch: 49       Training Loss: 2.989183         Validation Loss: 3.428347
Ended: 2019-01-01 22:32:10.738134
Elapsed: 0:03:25.832058
Epoch: 50       Training Loss: 2.966034         Validation Loss: 3.501775
Ended: 2019-01-01 22:32:11.326703
Elapsed: 0:03:26.420627
Starting Testing
Test Loss: 3.485910


Test Accuracy: 18% (156/836)
Ended: 2019-01-01 22:32:41.884173
Elapsed: 0:00:29.644028
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_and_test(epochs=10,
               train_batches=loaders_scratch["train"],
               validate_batches=loaders_scratch["validate"],
               test_batches=loaders_scratch["test"],
               model=model_scratch,
               optimizer=optimizer_scratch, 
               criterion=criterion_scratch,
               epoch_start=next_start,
               model_path=model_path.from_folder,
               load_model=True)
next_start = 61
</pre></div>
<pre class="example">
Starting Training
Ended: 2019-01-01 22:39:53.821378
Elapsed: 0:04:15.535643
Epoch: 51       Training Loss: 3.024161         Validation Loss: 3.409968
Validation loss decreased (inf --&gt; 3.409968). Saving model ...
Ended: 2019-01-01 22:43:47.462698
Elapsed: 0:03:52.776151
Epoch: 52       Training Loss: 2.979377         Validation Loss: 3.512004
Ended: 2019-01-01 22:47:35.580770
Elapsed: 0:03:47.528679
Epoch: 53       Training Loss: 2.983352         Validation Loss: 3.499196
Ended: 2019-01-01 22:50:58.662565
Elapsed: 0:03:22.501398
Epoch: 54       Training Loss: 2.944738         Validation Loss: 3.458440
Ended: 2019-01-01 22:54:21.531858
Elapsed: 0:03:22.279749
Epoch: 55       Training Loss: 2.921185         Validation Loss: 3.581930
Ended: 2019-01-01 22:57:44.017339
Elapsed: 0:03:21.925483
Epoch: 56       Training Loss: 2.928508         Validation Loss: 3.449956
Ended: 2019-01-01 23:01:06.668710
Elapsed: 0:03:22.061753
Epoch: 57       Training Loss: 2.887215         Validation Loss: 3.559204
Ended: 2019-01-01 23:04:29.439919
Elapsed: 0:03:22.181396
Epoch: 58       Training Loss: 2.909253         Validation Loss: 3.458249
Ended: 2019-01-01 23:07:51.804139
Elapsed: 0:03:21.803807
Epoch: 59       Training Loss: 2.864969         Validation Loss: 3.599446
Ended: 2019-01-01 23:11:14.184534
Elapsed: 0:03:21.789954
Epoch: 60       Training Loss: 2.820693         Validation Loss: 3.432991
Ended: 2019-01-01 23:11:14.775507
Elapsed: 0:03:22.380927
Starting Testing
Test Loss: 3.370016


Test Accuracy: 21% (176/836)
Ended: 2019-01-01 23:11:44.949942
Elapsed: 0:00:29.259563
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>next_start = 61
train_and_test(epochs=10,
               train_batches=loaders_scratch["train"],
               validate_batches=loaders_scratch["validate"],
               test_batches=loaders_scratch["test"],
               model=model_scratch,
               optimizer=optimizer_scratch, 
               criterion=criterion_scratch,
               epoch_start=next_start,
               model_path=model_path.from_folder,
               load_model=True)
next_start = 71
</pre></div>
<pre class="example">
Starting Training
Ended: 2019-01-01 23:31:00.034455
Elapsed: 0:03:21.658811
Epoch: 61       Training Loss: 2.968425         Validation Loss: 3.469985
Validation loss decreased (inf --&gt; 3.469985). Saving model ...
Ended: 2019-01-01 23:34:24.012685
Elapsed: 0:03:22.630721
Epoch: 62       Training Loss: 2.980103         Validation Loss: 3.449017
Validation loss decreased (3.469985 --&gt; 3.449017). Saving model ...
Ended: 2019-01-01 23:37:47.137370
Elapsed: 0:03:22.315870
Epoch: 63       Training Loss: 2.945722         Validation Loss: 3.497296
Ended: 2019-01-01 23:41:09.932696
Elapsed: 0:03:22.226620
Epoch: 64       Training Loss: 2.940117         Validation Loss: 3.398626
Validation loss decreased (3.449017 --&gt; 3.398626). Saving model ...
Ended: 2019-01-01 23:44:33.204607
Elapsed: 0:03:22.484337
Epoch: 65       Training Loss: 2.913762         Validation Loss: 3.465828
Ended: 2019-01-01 23:47:55.682608
Elapsed: 0:03:21.909285
Epoch: 66       Training Loss: 2.877373         Validation Loss: 3.525525
Ended: 2019-01-01 23:51:18.110150
Elapsed: 0:03:21.859021
Epoch: 67       Training Loss: 2.889807         Validation Loss: 3.499459
Ended: 2019-01-01 23:54:40.142934
Elapsed: 0:03:21.464199
Epoch: 68       Training Loss: 2.882748         Validation Loss: 3.364801
Validation loss decreased (3.398626 --&gt; 3.364801). Saving model ...
Ended: 2019-01-01 23:58:02.359285
Elapsed: 0:03:21.435096
Epoch: 69       Training Loss: 2.886337         Validation Loss: 3.488435
Ended: 2019-01-02 00:01:26.616419
Elapsed: 0:03:23.688341
Epoch: 70       Training Loss: 2.867836         Validation Loss: 3.417904
Ended: 2019-01-02 00:01:27.309412
Elapsed: 0:03:24.381334
Starting Testing
Test Loss: 3.359312


Test Accuracy: 22% (191/836)
Ended: 2019-01-02 00:02:29.963462
Elapsed: 0:01:01.964477
</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>train_and_test(epochs=10,
               train_batches=loaders_scratch["train"],
               validate_batches=loaders_scratch["validate"],
               test_batches=loaders_scratch["test"],
               model=model_scratch,
               optimizer=optimizer_scratch, 
               criterion=criterion_scratch,
               epoch_start=next_start,
               model_path=model_path.from_folder,
               load_model=True)
next_start = 81
</pre></div>
<pre class="example">
Starting Training
Ended: 2019-01-02 00:13:59.560043
Elapsed: 0:09:26.402859
Epoch: 71       Training Loss: 2.847764         Validation Loss: 3.462033
Validation loss decreased (inf --&gt; 3.462033). Saving model ...
Ended: 2019-01-02 00:21:40.896206
Elapsed: 0:07:40.511212
Epoch: 72       Training Loss: 2.852644         Validation Loss: 3.469687
Ended: 2019-01-02 00:29:05.309753
Elapsed: 0:07:23.845532
Epoch: 73       Training Loss: 2.840424         Validation Loss: 3.545896
Ended: 2019-01-02 00:33:46.928392
Elapsed: 0:04:41.026761
Epoch: 74       Training Loss: 2.813888         Validation Loss: 3.552435
Ended: 2019-01-02 00:37:18.057707
Elapsed: 0:03:30.560704
Epoch: 75       Training Loss: 2.807452         Validation Loss: 3.491534
Ended: 2019-01-02 00:40:41.064242
Elapsed: 0:03:22.438088
Epoch: 76       Training Loss: 2.802119         Validation Loss: 3.429099
Validation loss decreased (3.462033 --&gt; 3.429099). Saving model ...
Ended: 2019-01-02 00:44:04.191818
Elapsed: 0:03:22.138587
Epoch: 77       Training Loss: 2.809226         Validation Loss: 3.482573
Ended: 2019-01-02 00:47:26.187167
Elapsed: 0:03:21.427162
Epoch: 78       Training Loss: 2.767340         Validation Loss: 3.473212
Ended: 2019-01-02 00:50:48.717819
Elapsed: 0:03:21.962244
Epoch: 79       Training Loss: 2.750881         Validation Loss: 3.435359
Ended: 2019-01-02 00:54:11.744891
Elapsed: 0:03:22.458406
Epoch: 80       Training Loss: 2.739076         Validation Loss: 3.466524
Ended: 2019-01-02 00:54:12.313860
Elapsed: 0:03:23.027375
Starting Testing
Test Loss: 3.505263


Test Accuracy: 21% (183/836)
Ended: 2019-01-02 00:54:42.938753
Elapsed: 0:00:29.924658
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgb330281">
<h3 id="orgb330281">Debug the CUDA Error</h3>
<div class="outline-text-3" id="text-orgb330281">
<p>The previous blocks of code raised an exception when I first ran it.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">cuda</span> <span class="n">runtime</span> <span class="n">error</span> <span class="p">(</span><span class="mi">59</span><span class="p">)</span> <span class="p">:</span> <span class="n">device</span><span class="o">-</span><span class="n">side</span> <span class="k">assert</span> <span class="n">triggered</span> <span class="n">at</span> <span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">aten</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">THC</span><span class="o">/</span><span class="n">generic</span><span class="o">/</span><span class="n">THCTensorMath</span><span class="o">.</span><span class="n">cu</span><span class="p">:</span><span class="mi">26</span>
</pre></div>
<p>And points to this line as the point where it crashes.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
<p>Re-running it gives a similar but different error.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="ne">RuntimeError</span><span class="p">:</span> <span class="n">CUDA</span> <span class="n">error</span><span class="p">:</span> <span class="n">device</span><span class="o">-</span><span class="n">side</span> <span class="k">assert</span> <span class="n">triggered</span>
</pre></div>
<p>Happening here:</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
<p>According to <a href="https://github.com/pytorch/pytorch/issues/1010">this bug report</a> on GitHub, there's two things happening. One is that once the exception happens the CUDA session is dead so trying to move the data to CUDA raises an error just because we are trying to use it (and you can't until you restart the python session). In that same thread they note that the original exception indicates something wrong with the classes being output by the network. One error they list is if there's a negative label, another if the label is out of range for the number of categories, but In my case it might be that I was only outputting 10 classes (I copied the CIFAR model), not the 133 you need for the dog-breeds.</p>
</div>
<div class="outline-4" id="outline-container-org6401991">
<h4 id="org6401991">Load The Best Model</h4>
<div class="outline-text-4" id="text-org6401991">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_scratch.load_state_dict(torch.load('model_scratch.pt'))
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org00f2b4d">
<h4 id="org00f2b4d">Test It</h4>
<div class="outline-text-4" id="text-org00f2b4d">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>test(loaders_scratch["test"], model_scratch, criterion_scratch)
</pre></div>
<pre class="example">
Test Loss: 3.492875


Test Accuracy: 17% (146/836)

</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf75a022">
<h3 id="orgf75a022">Transfer Learning Model</h3>
<div class="outline-text-3" id="text-orgf75a022">
<p>Now I'm going to use transfer learning to make a model to classify dog images by breed.</p>
</div>
<div class="outline-4" id="outline-container-orgc1e279f">
<h4 id="orgc1e279f">The Data Transformer</h4>
<div class="outline-text-4" id="text-orgc1e279f">
<p>As I noted earlier, the <code>Inception V3</code> model expects a different image size so we can't re-use the previous data-transforms.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Transformer:
    """builds the data-sets

    Args:
     means: list of means for each channel
     deviations: list of standard deviations for each channel
     image_size: size to crop the image to
    """
    def __init__(self,
                 means: list=[0.485, 0.456, 0.406],
                 deviations: list=[0.229, 0.224, 0.225],
                 image_size: int=299) -&gt; None:
        self.means = means
        self.deviations = deviations
        self.image_size = image_size
        self._training = None
        self._testing = None
        return

    @property
    def training(self) -&gt; transforms.Compose:
        """The image transformers for the training"""
        if self._training is None:
            self._training = transforms.Compose([
                transforms.RandomRotation(30),
                transforms.RandomResizedCrop(self.image_size),
                transforms.RandomHorizontalFlip(),
                transforms.ToTensor(),
                transforms.Normalize(self.means,
                                     self.deviations)])
        return self._training

    @property
    def testing(self) -&gt; transforms.Compose:
        """Image transforms for the testing"""
        if self._testing is None:
            self._testing = transforms.Compose(
                [transforms.Resize(350),
                 transforms.CenterCrop(self.image_size),
                 transforms.ToTensor(),
                 transforms.Normalize(self.means,
                                      self.deviations)])
        return self._testing
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb1e92f5">
<h4 id="orgb1e92f5">The Data Set Loader</h4>
<div class="outline-text-4" id="text-orgb1e92f5">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class DataSets:
    """Builds the data-sets

    Args:
     paths: object with the paths to the data-sets
    """
    def __init__(self, paths: DogPaths=None, transformer: Transformer=None) -&gt; None:
        self._paths = paths
        self._transformer = transformer
        self._training = None
        self._validation = None
        self._testing = None
        return

    @property
    def paths(self) -&gt; DogPaths:
        """Object with the paths to the image files"""
        if self._paths is None:
            self._paths = DogPaths()
        return self._paths

    @property
    def transformer(self) -&gt; Transformer:
        """Object with the image transforms"""
        if self._transformer is None:
            self._transformer = Transformer()
        return self._transformer

    @property
    def training(self) -&gt; datasets.ImageFolder:
        """The training data set"""
        if self._training is None:
            self._training = datasets.ImageFolder(
                root=self.paths.training.folder,
                transform=self.transformer.training)
        return self._training

    @property
    def validation(self) -&gt; datasets.ImageFolder:
        """The validation dataset"""
        if self._validation is None:
            self._validation = datasets.ImageFolder(
                root=self.paths.validation.folder,
                transform=self.transformer.testing)
        return self._validation

    @property
    def testing(self) -&gt; datasets.ImageFolder:
        """The test set"""
        if self._testing is None:
            self._testing = datasets.ImageFolder(
                root=self.paths.testing.folder,
                transform=self.transformer.testing)
        return self._testing
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org68a3971">
<h4 id="org68a3971">The Batch Loader</h4>
<div class="outline-text-4" id="text-org68a3971">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Batches:
    """The data batch loaders

    Args:
     datasets: a data-set builder
     batch_size: the size of each batch loaded
     workers: the number of processes to use
    """
    def __init__(self, datasets: DataSets,
                 batch_size: int=20,
                 workers: int=0) -&gt; None:
        self.datasets = datasets
        self.batch_size = batch_size
        self.workers = workers
        self._training = None
        self._validation = None
        self._testing = None
        return

    @property
    def training(self) -&gt; torch.utils.data.DataLoader:
        """The training batches"""
        if self._training is None:
            self._training = torch.utils.data.DataLoader(
                self.datasets.training,
                batch_size=self.batch_size,
                shuffle=True, num_workers=self.workers)
        return self._training

    @property
    def validation(self) -&gt; torch.utils.data.DataLoader:
        """The validation batches"""
        if self._validation is None:
            self._validation = torch.utils.data.DataLoader(
                self.datasets.validation,
                batch_size=self.batch_size,
                shuffle=True, num_workers=self.workers)
        return self._validation

    @property
    def testing(self) -&gt; torch.utils.data.DataLoader:
        """The testing batches"""
        if self._testing is None:
            self._testing = torch.utils.data.DataLoader(
                self.datasets.testing,
                batch_size=self.batch_size,
                shuffle=True, num_workers=self.workers)
        return self._testing
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc1bf10a">
<h4 id="orgc1bf10a">The Inception Dog Classifier</h4>
<div class="outline-text-4" id="text-orgc1bf10a">
<p>Although the constructor for the pytorch Inception model takes an <code>aux_logits</code> parameter, if you set it to false then it will raise an error saying there are unexpected keys in the state dict. But if you don't set it False it will return a tuple from the <code>forward</code> method so either set it to False after the constructor or catch a tuple as the output <code>(x, aux)</code> and throw away the second part (or figure out how to combine them). I decided to leave it set because it is supposed to help with training and changed the training function to handle it. But I don't really show that in this notebook. I'll have to re-write things later.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class Inception:
    """Sets up the model, criterion, and optimizer for the transfer learning

    Args:
     classes: number of outputs for the final layer
     device: processor to use
     model_path: path to a saved model
     learning_rate: learning rate for the optimizer
     momentum: momentum for the optimizer
    """
    def __init__(self, classes: int,
                 device: torch.device=None,
                 model_path: str=None,
                 learning_rate: float=0.001, momentum: float=0.9) -&gt; None:
        self.classes = classes
        self.model_path = model_path
        self.learning_rate = learning_rate
        self.momentum = momentum
        self._device = device
        self._model = None
        self._classifier_inputs = None
        self._criterion = None
        self._optimizer = None
        return

    @property
    def device(self) -&gt; torch.device:
        """Processor to use (cpu or cuda)"""
        if self._device is None:
            self._device = torch.device(
                "cuda" if torch.cuda.is_available() else "cpu")
        return self._device

    @property
    def model(self) -&gt; models.inception_v3:
        """The inception model"""
        if self._model is None:
            self._model = models.inception_v3(pretrained=True)
            for parameter in self._model.parameters():
                parameter.requires_grad = False
            classifier_inputs = self._model.fc.in_features
            self._model.fc = nn.Linear(in_features=classifier_inputs,
                                       out_features=self.classes,
                                       bias=True)
            self._model.to(self.device)
            if self.model_path:
                self._model.load_state_dict(torch.load(self.model_path))
        return self._model

    @property
    def criterion(self) -&gt; nn.CrossEntropyLoss:
        """The loss callable"""
        if self._criterion is None:
            self._criterion = nn.CrossEntropyLoss()
        return self._criterion

    @property
    def optimizer(self) -&gt; optimizer.SGD:
        """The Gradient Descent object"""
        if self._optimizer is None:
            self._optimizer = optimizer.SGD(
                self.model.parameters(),
                lr=self.learning_rate,
                momentum=self.momentum)
        return self._optimizer
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf2d6da7">
<h4 id="orgf2d6da7">Disecting the Inception Class</h4>
<div class="outline-text-4" id="text-orgf2d6da7">
<p>The <code>Inception</code> class bundles together a bunch of stuff that was originally being done in separate cells. Rather than putting comments all over it I'm going to show what it's doing by describing how I was doing it before I created the class.</p>
</div>
<ul class="org-ul">
<li><a id="orgb7a9b5a"></a>The Model Property<br>
<div class="outline-text-5" id="text-orgb7a9b5a">
<p>The last layer of the classifier in the <code>Inception.model</code> property is the only layer of the pre-trained model that I change. In the case of the <code>Inception V3</code> model there is a single layer called <i>fc</i>, as opposed to multiple layers called <i>classifier</i> as with the <code>VGG16</code> model, so I just re-assign it to a fully-connected layer with the number of outputs that matches the number of dog breeds.</p>
<p>Here's a little inspection to show what it's doing.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_transfer = models.inception_v3(pretrained=True)
print(model_transfer.fc)
</pre></div>
<pre class="example">
Linear(in_features=2048, out_features=1000, bias=True)

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>CLASSIFIER_INPUTS = model_transfer.fc.in_features
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>print(CLASSIFIER_INPUTS) 
print(model_transfer.fc.out_features)
</pre></div>
<pre class="example">
2048
1000

</pre>
<p>The layer we're going to replace has 2,048 inputs and 1,000 outputs. We'll have to match the number of inputs and change it to our 133.</p>
</div>
</li>
<li><a id="org4e3b32c"></a>Freeze the Features Layers<br>
<div class="outline-text-5" id="text-org4e3b32c">
<p>In the <code>model</code> property I'm also freezing the parameters so that the pre-trained parameters don't change when training the last layer.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>for parameter in model_transfer.parameters():
    parameter.requires_grad = False
</pre></div>
</div>
</li>
<li><a id="org3a656a6"></a>The New Classifier<br>
<div class="outline-text-5" id="text-org3a656a6">
<p>This next block of code is also in the <code>Inception.model</code> definition and is where I'm replacing the last layer with out dog-breed-classification layer.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_transfer.fc = nn.Linear(in_features=CLASSIFIER_INPUTS,
                              out_features=BREEDS,
                              bias=True)
</pre></div>
</div>
</li>
<li><a id="org0bfa593"></a>The Loss Function and Optimizer<br>
<div class="outline-text-5" id="text-org0bfa593">
<p>The <code>Inception</code> class uses the same loss and gradient descent definitions as the naive model did (in the <code>criterion</code> and <code>optimizer</code> properties).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>criterion_transfer = nn.CrossEntropyLoss()
optimizer_transfer = optimizer.SGD(model_transfer.parameters(),
                                  lr=0.001,
                                  momentum=0.9)
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org2319294">
<h4 id="org2319294">Transfer CLI</h4>
<div class="outline-text-4" id="text-org2319294">
<p>I made this in order to run the model on paperspace without needing to keep the connection to the server alive (it hadn't occured to me to just save a log file).</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">argparse</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">ImageFile</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optimizer</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="kn">as</span> <span class="nn">models</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="kn">as</span> <span class="nn">transforms</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.tangles.data_paths</span> <span class="kn">import</span> <span class="n">DataPathTwo</span>
<span class="kn">from</span> <span class="nn">neurotic.tangles.timer</span> <span class="kn">import</span> <span class="n">Timer</span>

<span class="c1"># the output won't show up if you don't flush it when redirecting it to a file</span>
<span class="k">print</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="k">print</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">"__main__"</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">"Test or Train the Inception V3 Dog Classifier"</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">"--test-only"</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s2">"store_true"</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">"Only run the test"</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">"--epochs"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">"Training epochs (default: </span><span class="si">%(default)s</span><span class="s2">)"</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">"--epoch-offset"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">"Offset for the output of epochs (default: </span><span class="si">%(default)s</span><span class="s2">)"</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">"--restart"</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s2">"store_true"</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s2">"Wipe out old model."</span><span class="p">)</span>

    <span class="n">arguments</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">data_sets</span> <span class="o">=</span> <span class="n">DataSets</span><span class="p">(</span><span class="n">training_path</span><span class="o">=</span><span class="n">dog_training_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span>
                         <span class="n">validation_path</span><span class="o">=</span><span class="n">dog_validation_path</span><span class="o">.</span><span class="n">folder</span><span class="p">,</span>
                         <span class="n">testing_path</span><span class="o">=</span><span class="n">dog_testing_path</span><span class="o">.</span><span class="n">folder</span><span class="p">)</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">datasets</span><span class="o">=</span><span class="n">data_sets</span><span class="p">)</span>
    <span class="n">inception</span> <span class="o">=</span> <span class="n">Inception</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">data_sets</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="n">arguments</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span>
                      <span class="n">epoch_start</span><span class="o">=</span><span class="n">arguments</span><span class="o">.</span><span class="n">epoch_offset</span><span class="p">,</span>
                      <span class="n">training_batches</span><span class="o">=</span><span class="n">batches</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
                      <span class="n">validation_batches</span><span class="o">=</span><span class="n">batches</span><span class="o">.</span><span class="n">validation</span><span class="p">,</span>
                      <span class="n">testing_batches</span><span class="o">=</span><span class="n">batches</span><span class="o">.</span><span class="n">testing</span><span class="p">,</span>
                      <span class="n">model</span><span class="o">=</span><span class="n">inception</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                      <span class="n">device</span><span class="o">=</span><span class="n">inception</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                      <span class="n">optimizer</span><span class="o">=</span><span class="n">inception</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span>
                      <span class="n">criterion</span><span class="o">=</span><span class="n">inception</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span>
                      <span class="n">model_path</span><span class="o">=</span><span class="n">transfer_path</span><span class="o">.</span><span class="n">from_folder</span><span class="p">,</span>
                      <span class="n">load_model</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">beep</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">arguments</span><span class="o">.</span><span class="n">test_only</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trainer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1127197">
<h4 id="org1127197">The Training</h4>
<div class="outline-text-4" id="text-org1127197">
<p>I re-trained the naive model and trained the inception model on paperspace for 100 epochs each. This took around five hours each so I'm not going to re-run it here, but I'll show how I would train the model and some of the output from the real training. The <code>Tee</code> class isn't integrated with my <code>trainer</code> so I can't really show how to train it that way, so I'll show it the orignal function-based way.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>transfer_path = MODEL_PATH.folder.joinpath("model_transfer.pt")
transfer_log = Tee(log_name="transfer_train.log")
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>EPOCHS = 100
inception = Inception()
train(EPOCHS,
      loaders=loaders_transfer,
      model=inception.model,
      optimizer=inception.optimizer,
      criterion=inception.criterion,
      use_cuda=use_cuda,
      save_path=transfer_model_path,
      print_function=transfer_log,
      is_inception=True)
</pre></div>
<p>And the last lines of the output.</p>
<pre class="example">
Epoch: 98       Training Loss: 0.973978         Validation Loss: 0.416819       Elapsed: 0:03:12.167687
Validation loss decreased (0.417785 --&gt; 0.416819). Saving model ...
Epoch: 99       Training Loss: 0.994163         Validation Loss: 0.418498       Elapsed: 0:03:17.225706
Epoch: 100      Training Loss: 0.998819         Validation Loss: 0.423518       Elapsed: 0:03:18.415953
Training Ended: 2019-01-07 10:55:04.465024
Total Training Time: 5:29:54.161034
</pre></div>
</div>
<div class="outline-4" id="outline-container-org55e331a">
<h4 id="org55e331a">Test It</h4>
<div class="outline-text-4" id="text-org55e331a">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>model_transfer.load_state_dict(torch.load(transfer_model_path))
transfer_test_log = Tee("transfer_test.log")
test(loaders_transfer, model_transfer, criterion_transfer, use_cuda, print_function=transfer_test_log)
</pre></div>
<pre class="example">
Test Loss: 0.425383


Test Accuracy: 87% (734/836)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7d44bc0">
<h2 id="org7d44bc0">The Dog Breed Classifier</h2>
<div class="outline-text-2" id="text-org7d44bc0"></div>
<div class="outline-3" id="outline-container-org03d31dd">
<h3 id="org03d31dd">Dog Predictor</h3>
<div class="outline-text-3" id="text-org03d31dd">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class DogPredictor:
    """Makes dog-breed predictions

    Args:
     model_path: path to the model's state-dict
     device: processor to run the model on
     data_sets: a DataSets object
     inception: an Inception object
    """
    def __init__(self, model_path: str=None,
                 device: torch.device=None,
                 data_sets: DataSets=None,
                 inception: Inception=None) -&gt; None:
        self.model_path = model_path
        self.device = device
        self._data_sets = data_sets
        self._inception = inception
        self._breeds = None
        return

    @property
    def data_sets(self) -&gt; DataSets:
        if self._data_sets is None:
            self._data_sets = DataSets()
        return self._data_sets

    @property
    def inception(self) -&gt; Inception:
        """An Inception object"""
        if self._inception is None:
            self._inception = Inception(
                classes=len(self.data_sets.training.classes),
                model_path=self.model_path,
                device=self.device)
            self._inception.model.eval()
        return self._inception

    @property
    def breeds(self) -&gt; list:
        """A list of dog-breeds"""
        if self._breeds is None:
            self._breeds = [name[4:].replace("_", " ")
                            for name in self.data_sets.training.classes]
        return self._breeds

    def predict_index(self, image_path:str) -&gt; int:
        """Predicts the index of the breed of the dog in the image

        Args:
         image_path: path to the image
        Returns:
         index in the breeds list for the image
        """
        model = self.inception.model        
        image = Image.open(image_path)
        tensor = self.data_sets.transformer.testing(image)
        # add a batch number
        tensor = tensor.unsqueeze_(0)
        tensor = tensor.to(self.inception.device)
        x = torch.autograd.Variable(tensor)
        output = model(x)
        return output.data.cpu().numpy().argmax()

    def __call__(self, image_path) -&gt; str:
        """Predicts the breed of the dog in the image

        Args:
         image_path: path to the image
        Returns:
         name of the breed
        """
        return self.breeds[self.predict_index(image_path)]
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>predictor = DogPredictor(model_path=transfer_path)
files = list(predictor.data_sets.paths.testing.folder.glob("*/*.jpg"))
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>case = numpy.random.choice(files, 1)[0]
print("Sample: {}".format(case))
predicted = predictor(case)
print("Predicted: {}".format(predicted))
</pre></div>
<pre class="example">
Sample: /home/hades/data/datasets/dog-breed-classification/dogImages/test/109.Norwegian_elkhound/Norwegian_elkhound_07137.jpg
Predicted: Norwegian elkhound

</pre>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>for model in MODELS:
    model.cpu()
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org63191c9">
<h3 id="org63191c9">The Dog Breed Classifier</h3>
<div class="outline-text-3" id="text-org63191c9">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>class DogBreedClassifier:
    """Tries To predict the dog-breed for an image

    Args:
     model_path: path to the inception-model
    """
    def __init__(self, model_path: str) -&gt; None:
        self.model_path = model_path
        self._breed_predictor = None
        self._species_detector = None
        return

    @property
    def breed_predictor(self) -&gt; DogPredictor:
        """Predictor of dog-breeds"""
        if self._breed_predictor is None:
            self._breed_predictor = DogPredictor(model_path=self.model_path)
        return self._breed_predictor

    @property
    def species_detector(self) -&gt; SpeciesDetector:
        """Detector of humans and dogs"""
        if self._species_detector is None:
            self._species_detector = SpeciesDetector(
                device=self.breed_predictor.inception.device)
        return self._species_detector

    def render(self, image_path: str, species: str, breed: str) -&gt; None:
        """Renders the image

        Args:
         image_path: path to the image to render
         species: identified species
         breed: identified breed
        """
        name = " ".join(image_path.name.split(".")[0].split("_")).title()
        figure, axe = pyplot.subplots()
        figure.suptitle("{} ({})".format(species, name), weight="bold")
        axe.set_xlabel("Looks like a {}.".format(breed))
        image = Image.open(image_path)
        axe.tick_params(dict(axis="both",
                             which="both",
                             bottom=False,
                             top=False))
        axe.get_xaxis().set_ticks([])
        axe.get_yaxis().set_ticks([])
        axe_image = axe.imshow(image)
        return

    def __call__(self, image_path:str) -&gt; None:
        """detects the dog-breed and displays the image

        Args:
         image_path: path to the image
        """
        image_path = Path(image_path)
        is_dog = self.species_detector.is_dog(image_path)
        is_human = self.species_detector.is_human(image_path)

        if not is_dog and not is_human:
            species = "Error: Neither Human nor Dog"
            breed = "?"
        else:
            breed = self.breed_predictor(image_path)

        if is_dog and is_human:
            species = "Human-Dog Hybrid"
        elif is_dog:
            species = "Dog"
        elif is_human:
            species = "Human"
        self.render(image_path, species, breed)
        return
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org954ac7e">
<h2 id="org954ac7e">Some Sample applications</h2>
<div class="outline-text-2" id="text-org954ac7e">/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>classifier = DogBreedClassifier(model_path=transfer_path)
</pre></div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>case = numpy.random.choice(human_files, 1)[0]
classifier(case)
</pre></div>
<div class="figure">
<p><img alt="test_one.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/test_one.png"></p>
</div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>case = numpy.random.choice(dog_files, 1)[0]
classifier(case)
</pre></div>
<div class="figure">
<p><img alt="test_two.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/test_two.png"></p>
</div>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>case = "rabbit.jpg"
classifier(case)
</pre></div>
<div class="figure">
<p><img alt="test_three.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/test_three.png"></p>
</div>
<p>Rabbit image from <a href="https://commons.wikimedia.org/wiki/File:Oryctolagus_cuniculus_Tasmania_2.jpg">Wikimedia</a>.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>case = "hot_dog.jpg"
classifier(case)
</pre></div>
<div class="figure">
<p><img alt="test_four.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/test_four.png"></p>
</div>
<p>The Hot Dog is also from <a href="https://commons.wikimedia.org/wiki/File:NCI_Visuals_Food_Hot_Dog.jpg">Wikimedia</a>.</p>
/home/brunhilde/.virtualenvs/In-Too-Deep/bin/python3: No module named virtualfish
<div class="highlight">
<pre><span></span>case = human_files_short[34]
classifier(case)
</pre></div>
<div class="figure">
<p><img alt="test_five.png" src="posts/nano/dog-breed-classifier/dog-breed-classification/test_five.png"></p>
</div>
<p>So, somehow my class-based detector got smarter than my function based one and can now tell that this isn't a dog…</p>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="index-7.html" rel="prev">Newer posts</a></li>
<li class="next"><a href="index-5.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer">Contents © 2019 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js">
</script><!-- fancy dates -->
<script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
</script><!-- end fancy dates -->
<script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
