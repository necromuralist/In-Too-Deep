<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>In Too Deep</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/" rel="canonical">
<link href="index-9.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
<link href="/posts/keras/nlp-classification-exercise/" rel="prefetch" type="text/html">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item active"><a class="nav-link" href="/">Cloistered Monkey <span class="sr-only">(active)</span></a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/nlp-classification-exercise/">NLP Classification Exercise</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/nlp-classification-exercise/" rel="bookmark"><time class="published dt-published" datetime="2019-09-29T11:28:06-07:00" itemprop="datePublished" title="2019-09-29 11:28">2019-09-29 11:28</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/nlp-classification-exercise/#org920f178">Beginning</a>
<ul>
<li><a href="/posts/keras/nlp-classification-exercise/#org707d0a0">Imports</a>
<ul>
<li><a href="/posts/keras/nlp-classification-exercise/#orgb686cbb">Python</a></li>
<li><a href="/posts/keras/nlp-classification-exercise/#orgb6a35a6">PyPi</a></li>
<li><a href="/posts/keras/nlp-classification-exercise/#org8421d86">Others</a></li>
</ul>
</li>
<li><a href="/posts/keras/nlp-classification-exercise/#org79cef76">Set Up</a>
<ul>
<li><a href="/posts/keras/nlp-classification-exercise/#org60b750a">The Timer</a></li>
<li><a href="/posts/keras/nlp-classification-exercise/#org128c0ec">The Plotting</a></li>
<li><a href="/posts/keras/nlp-classification-exercise/#org309b81c">The Dataset</a></li>
<li><a href="/posts/keras/nlp-classification-exercise/#org7b69f51">Some Constants</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/nlp-classification-exercise/#org806d5f2">Middle</a>
<ul>
<li><a href="/posts/keras/nlp-classification-exercise/#org2540b03">The Data</a></li>
<li><a href="/posts/keras/nlp-classification-exercise/#orgc10e107">The Tokenizer</a></li>
<li><a href="/posts/keras/nlp-classification-exercise/#org817e0e5">GloVe</a></li>
<li><a href="/posts/keras/nlp-classification-exercise/#org81c4bfa">The Models</a>
<ul>
<li><a href="/posts/keras/nlp-classification-exercise/#org93682b3">A CNN</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/nlp-classification-exercise/#org16b70f5">End</a>
<ul>
<li><a href="/posts/keras/nlp-classification-exercise/#orgece0d38">Citations</a></li>
</ul>
</li>
<li><a href="/posts/keras/nlp-classification-exercise/#orgc874f0e">Raw</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org920f178">
<h2 id="org920f178">Beginning</h2>
<div class="outline-text-2" id="text-org920f178"></div>
<div class="outline-3" id="outline-container-org707d0a0">
<h3 id="org707d0a0">Imports</h3>
<div class="outline-text-3" id="text-org707d0a0"></div>
<div class="outline-4" id="outline-container-orgb686cbb">
<h4 id="orgb686cbb">Python</h4>
<div class="outline-text-4" id="text-orgb686cbb">
<div class="highlight">
<pre><span></span>from argparse import Namespace
from functools import partial
from pathlib import Path
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb6a35a6">
<h4 id="orgb6a35a6">PyPi</h4>
<div class="outline-text-4" id="text-orgb6a35a6">
<div class="highlight">
<pre><span></span>from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import hvplot.pandas
import numpy
import pandas
import tensorflow
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8421d86">
<h4 id="org8421d86">Others</h4>
<div class="outline-text-4" id="text-org8421d86">
<div class="highlight">
<pre><span></span>from graeae import (CountPercentage,
                    EmbedHoloviews,
                    SubPathLoader,
                    Timer,
                    ZipDownloader)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org79cef76">
<h3 id="org79cef76">Set Up</h3>
<div class="outline-text-3" id="text-org79cef76"></div>
<div class="outline-4" id="outline-container-org60b750a">
<h4 id="org60b750a">The Timer</h4>
<div class="outline-text-4" id="text-org60b750a">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org128c0ec">
<h4 id="org128c0ec">The Plotting</h4>
<div class="outline-text-4" id="text-org128c0ec">
<div class="highlight">
<pre><span></span>slug = "nlp-classification-exercise"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{slug}")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org309b81c">
<h4 id="org309b81c">The Dataset</h4>
<div class="outline-text-4" id="text-org309b81c">
<p>It isn't mentioned in the notebook where the data originally came from, but it looks like it's the <a href="http://help.sentiment140.com/home">Sentiment140</a> dataset, which consists of tweets whose sentiment was inferred by emoticons in each tweet.</p>
<div class="highlight">
<pre><span></span>url = "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip"
path = Path("~/data/datasets/texts/sentiment140/").expanduser()
download = ZipDownloader(url, path)
download()
</pre></div>
<pre class="example">
Files exist, not downloading
</pre>
<div class="highlight">
<pre><span></span>columns = ["polarity", "tweet_id", "datetime", "query", "user", "text"]
training = pandas.read_csv(path/"training.1600000.processed.noemoticon.csv", 
                           encoding="latin-1", names=columns, header=None)
testing = pandas.read_csv(path/"testdata.manual.2009.06.14.csv", 
                           encoding="latin-1", names=columns, header=None)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org7b69f51">
<h4 id="org7b69f51">Some Constants</h4>
<div class="outline-text-4" id="text-org7b69f51">
<div class="highlight">
<pre><span></span>Text = Namespace(
    embedding_dim = 100,
    max_length = 16,
    trunc_type='post',
    padding_type='post',
    oov_tok = "&lt;OOV&gt;",
    training_size=16000,
)
</pre></div>
<div class="highlight">
<pre><span></span>Data = Namespace(
    batch_size = 64,
    shuffle_buffer_size=100,
)
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org806d5f2">
<h2 id="org806d5f2">Middle</h2>
<div class="outline-text-2" id="text-org806d5f2"></div>
<div class="outline-3" id="outline-container-org2540b03">
<h3 id="org2540b03">The Data</h3>
<div class="outline-text-3" id="text-org2540b03">
<div class="highlight">
<pre><span></span>print(training.sample().iloc[0])
</pre></div>
<pre class="example">
polarity                                                    4
tweet_id                                           1468852290
datetime                         Tue Apr 07 04:04:10 PDT 2009
query                                                NO_QUERY
user                                              leawoodward
text        Def off now...unexpected day out tomorrow so s...
Name: 806643, dtype: object
</pre>
<div class="highlight">
<pre><span></span>CountPercentage(training.polarity)()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Value</th>
<th class="org-left" scope="col">Count</th>
<th class="org-right" scope="col">Percent (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">4</td>
<td class="org-left">800,000</td>
<td class="org-right">50.00</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-left">800,000</td>
<td class="org-right">50.00</td>
</tr>
</tbody>
</table>
<p>The <code>polarity</code> is what might also be called the "sentiment" of the tweet - <i>0</i> means a negative tweet and <i>4</i> means a positive tweet.</p>
<p>But, for our purposes, we would be better off if the positive polarity was <code>1</code>, not <code>4</code>, so let's convert it.</p>
<div class="highlight">
<pre><span></span>training.loc[training.polarity==4, "polarity"] = 1
counts = CountPercentage(training.polarity)()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Value</th>
<th class="org-left" scope="col">Count</th>
<th class="org-right" scope="col">Percent (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">800,000</td>
<td class="org-right">50.00</td>
</tr>
<tr>
<td class="org-right">0</td>
<td class="org-left">800,000</td>
<td class="org-right">50.00</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="outline-3" id="outline-container-orgc10e107">
<h3 id="orgc10e107">The Tokenizer</h3>
<div class="outline-text-3" id="text-orgc10e107">
<p>As you can see from the sample, the data is still in text form so we need to convert it to a numeric form with a Tokenizer.</p>
<p>First I'll Lower-case it.</p>
<div class="highlight">
<pre><span></span>training.loc[:, "text"] = training.text.str.lower()
</pre></div>
<p>Next we'll fit it to our text.</p>
<div class="highlight">
<pre><span></span>tokenizer = Tokenizer()
with TIMER:
    tokenizer.fit_on_texts(training.text.values)
</pre></div>
<pre class="example">
2019-10-10 07:25:09,065 graeae.timers.timer start: Started: 2019-10-10 07:25:09.065039
WARNING: Logging before flag parsing goes to stderr.
I1010 07:25:09.065394 140436771002176 timer.py:70] Started: 2019-10-10 07:25:09.065039
2019-10-10 07:25:45,389 graeae.timers.timer end: Ended: 2019-10-10 07:25:45.389540
I1010 07:25:45.389598 140436771002176 timer.py:77] Ended: 2019-10-10 07:25:45.389540
2019-10-10 07:25:45,391 graeae.timers.timer end: Elapsed: 0:00:36.324501
I1010 07:25:45.391984 140436771002176 timer.py:78] Elapsed: 0:00:36.324501
</pre>
<p>Now, we can store some of it's values in variables for convenience.</p>
<div class="highlight">
<pre><span></span>word_index = tokenizer.word_index
vocabulary_size = len(tokenizer.word_index)
</pre></div>
<p>Now, we'll convert the texts to sequences and pad them so they are all the same length.</p>
<div class="highlight">
<pre><span></span>with TIMER:
    sequences = tokenizer.texts_to_sequences(training.text.values)
    padded = pad_sequences(sequences, maxlen=Text.max_length,
                           truncating=Text.trunc_type)

    splits = train_test_split(
        padded, training.polarity, test_size=.2)

    training_sequences, test_sequences, training_labels, test_labels = splits
</pre></div>
<pre class="example">
2019-10-10 07:25:51,057 graeae.timers.timer start: Started: 2019-10-10 07:25:51.057684
I1010 07:25:51.057712 140436771002176 timer.py:70] Started: 2019-10-10 07:25:51.057684
2019-10-10 07:26:33,530 graeae.timers.timer end: Ended: 2019-10-10 07:26:33.530338
I1010 07:26:33.530381 140436771002176 timer.py:77] Ended: 2019-10-10 07:26:33.530338
2019-10-10 07:26:33,531 graeae.timers.timer end: Elapsed: 0:00:42.472654
I1010 07:26:33.531477 140436771002176 timer.py:78] Elapsed: 0:00:42.472654
</pre>
<p>Now convert them to <a href="https://www.tensorflow.org/tutorials/load_data/numpy">datasets</a>.</p>
<div class="highlight">
<pre><span></span>training_dataset = tensorflow.data.Dataset.from_tensor_slices(
    (training_sequences, training_labels)
)

testing_dataset = tensorflow.data.Dataset.from_tensor_slices(
    (test_sequences, test_labels)
)

training_dataset = training_dataset.shuffle(Data.shuffle_buffer_size).batch(Data.batch_size)
testing_dataset = testing_dataset.shuffle(Data.shuffle_buffer_size).batch(Data.batch_size)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org817e0e5">
<h3 id="org817e0e5">GloVe</h3>
<div class="outline-text-3" id="text-org817e0e5">
<p>GloVe is short for <i>Global Vectors for Word Representation</i>. It is an <i>unsupervised</i> algorithm that creates vector representations for words. They have a <a href="https://nlp.stanford.edu/projects/glove/">site</a> where you can download pre-trained models or get the code and train one yourself. We're going to use one of their pre-trained models.</p>
<div class="highlight">
<pre><span></span>path = Path("~/models/glove/").expanduser()
url = "http://nlp.stanford.edu/data/glove.6B.zip"
ZipDownloader(url, path)()
</pre></div>
<pre class="example">
Files exist, not downloading
</pre>
<p>The GloVe data is stored as a series of space separated lines with the first column being the word that's encoded and the rest of the columns being the values for the vector. To make this work we're going to split the word off from the vector and put each into a dictionary.</p>
<div class="highlight">
<pre><span></span>embeddings = {}
with TIMER:
    with open(path/"glove.6B.100d.txt") as lines:
        for line in lines:
            tokens = line.split()
            embeddings[tokens[0]] = numpy.array(tokens[1:])
</pre></div>
<pre class="example">
2019-10-06 18:55:11,592 graeae.timers.timer start: Started: 2019-10-06 18:55:11.592880
I1006 18:55:11.592908 140055379531584 timer.py:70] Started: 2019-10-06 18:55:11.592880
2019-10-06 18:55:21,542 graeae.timers.timer end: Ended: 2019-10-06 18:55:21.542689
I1006 18:55:21.542738 140055379531584 timer.py:77] Ended: 2019-10-06 18:55:21.542689
2019-10-06 18:55:21,544 graeae.timers.timer end: Elapsed: 0:00:09.949809
I1006 18:55:21.544939 140055379531584 timer.py:78] Elapsed: 0:00:09.949809
</pre>
<div class="highlight">
<pre><span></span>print(f"{len(embeddings):,}")
</pre></div>
<pre class="example">
400,000
</pre>
<p>So, our vocabulary consists of 400,000 "words" (tokens is more accurate, since they also include punctuation). The problem we have to deal with next is that our data set wasn't part of the dataset used to train the embeddings, so there will probably be some tokens in our data set that aren't in the embeddings. To handle this we need to add zeroed embeddings for the extra tokens.</p>
<p>Rather than adding to the dict, we'll create a matrix of zeros with rows for each word in our datasets vocabulary, then we'll iterate over the words in our dataset and if there's a match in the GloVE embeddings we'll insert it into the matrix.</p>
<div class="highlight">
<pre><span></span>with TIMER:
    embeddings_matrix = numpy.zeros((vocabulary_size+1, Text.embedding_dim));
    for word, index in word_index.items():
        embedding_vector = embeddings.get(word);
        if embedding_vector is not None:
            embeddings_matrix[index] = embedding_vector;
</pre></div>
<pre class="example">
2019-10-06 18:55:46,577 graeae.timers.timer start: Started: 2019-10-06 18:55:46.577855
I1006 18:55:46.577886 140055379531584 timer.py:70] Started: 2019-10-06 18:55:46.577855
2019-10-06 18:55:51,374 graeae.timers.timer end: Ended: 2019-10-06 18:55:51.374706
I1006 18:55:51.374763 140055379531584 timer.py:77] Ended: 2019-10-06 18:55:51.374706
2019-10-06 18:55:51,377 graeae.timers.timer end: Elapsed: 0:00:04.796851
I1006 18:55:51.377207 140055379531584 timer.py:78] Elapsed: 0:00:04.796851
</pre>
<div class="highlight">
<pre><span></span>print(f"{len(embeddings_matrix):,}")
</pre></div>
<pre class="example">
690,961
</pre></div>
</div>
<div class="outline-3" id="outline-container-org81c4bfa">
<h3 id="org81c4bfa">The Models</h3>
<div class="outline-text-3" id="text-org81c4bfa"></div>
<div class="outline-4" id="outline-container-org93682b3">
<h4 id="org93682b3">A CNN</h4>
<div class="outline-text-4" id="text-org93682b3"></div>
<ul class="org-ul">
<li><a id="orgb13acd3"></a>Build<br>
<div class="outline-text-5" id="text-orgb13acd3">
<div class="highlight">
<pre><span></span>convoluted_model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(
        vocabulary_size + 1,
        Text.embedding_dim,
        input_length=Text.max_length,
        weights=[embeddings_matrix],
        trainable=False),
    tensorflow.keras.layers.Conv1D(filters=128,
                                   kernel_size=5,
    activation='relu'),
    tensorflow.keras.layers.GlobalMaxPooling1D(),
    tensorflow.keras.layers.Dense(24, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
convoluted_model.compile(loss="binary_crossentropy", optimizer="rmsprop",
                         metrics=["accuracy"])
</pre></div>
<div class="highlight">
<pre><span></span>print(convoluted_model.summary())
</pre></div>
<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 16, 100)           69096100  
_________________________________________________________________
conv1d (Conv1D)              (None, 12, 128)           64128     
_________________________________________________________________
global_max_pooling1d (Global (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 24)                3096      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 25        
=================================================================
Total params: 69,163,349
Trainable params: 67,249
Non-trainable params: 69,096,100
_________________________________________________________________
None
</pre></div>
</li>
<li><a id="org5eea0f0"></a>Train<br>
<div class="outline-text-5" id="text-org5eea0f0">
<div class="highlight">
<pre><span></span>Training = Namespace(
    size = 0.75,
    epochs = 2,
    verbosity = 2,
    batch_size=128,
    )
</pre></div>
<div class="highlight">
<pre><span></span>with TIMER:
    cnn_history = convoluted_model.fit(training_dataset,
                                       epochs=Training.epochs,
                                       validation_data=testing_dataset,
                                       verbose=Training.verbosity)
</pre></div>
<pre class="example">
2019-10-10 07:27:04,921 graeae.timers.timer start: Started: 2019-10-10 07:27:04.921617
I1010 07:27:04.921657 140436771002176 timer.py:70] Started: 2019-10-10 07:27:04.921617
Epoch 1/2
W1010 07:27:05.154920 140436771002176 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
20000/20000 - 4964s - loss: 0.5091 - accuracy: 0.7454 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/2
20000/20000 - 4935s - loss: 0.4790 - accuracy: 0.7671 - val_loss: 0.4782 - val_accuracy: 0.7677
2019-10-10 10:12:04,382 graeae.timers.timer end: Ended: 2019-10-10 10:12:04.382359
I1010 10:12:04.382491 140436771002176 timer.py:77] Ended: 2019-10-10 10:12:04.382359
2019-10-10 10:12:04,384 graeae.timers.timer end: Elapsed: 2:44:59.460742
I1010 10:12:04.384716 140436771002176 timer.py:78] Elapsed: 2:44:59.460742
</pre></div>
</li>
<li><a id="org8ff3785"></a>Some Plotting<br>
<div class="outline-text-5" id="text-org8ff3785">
<div class="highlight">
<pre><span></span>performance = pandas.DataFrame(cnn_history.history)
plot = performance.hvplot().opts(title="CNN Twitter Sentiment Training Performance",
                                 width=1000,
                                 height=800)
Embed(plot=plot, file_name="cnn_training")()
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org16b70f5">
<h2 id="org16b70f5">End</h2>
<div class="outline-text-2" id="text-org16b70f5"></div>
<div class="outline-3" id="outline-container-orgece0d38">
<h3 id="orgece0d38">Citations</h3>
<div class="outline-text-3" id="text-orgece0d38">
<ul class="org-ul">
<li>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc874f0e">
<h2 id="orgc874f0e">Raw</h2>
<div class="outline-text-2" id="text-orgc874f0e"></div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/embeddings-from-scratch/">Embeddings from Scratch</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/embeddings-from-scratch/" rel="bookmark"><time class="published dt-published" datetime="2019-09-25T13:30:12-07:00" itemprop="datePublished" title="2019-09-25 13:30">2019-09-25 13:30</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/embeddings-from-scratch/#org4e58c95">Beginning</a>
<ul>
<li><a href="/posts/keras/embeddings-from-scratch/#orgd94e184">Imports</a>
<ul>
<li><a href="/posts/keras/embeddings-from-scratch/#orgc57dfe7">Python</a></li>
<li><a href="/posts/keras/embeddings-from-scratch/#org78ec86e">PyPi</a></li>
<li><a href="/posts/keras/embeddings-from-scratch/#org01930d0">Others</a></li>
</ul>
</li>
<li><a href="/posts/keras/embeddings-from-scratch/#orga6ca04a">Set Up</a>
<ul>
<li><a href="/posts/keras/embeddings-from-scratch/#orgde705db">Plotting</a></li>
<li><a href="/posts/keras/embeddings-from-scratch/#org8fe7de7">The Timer</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/embeddings-from-scratch/#org4b39049">Middle</a>
<ul>
<li><a href="/posts/keras/embeddings-from-scratch/#org71fdd44">Some Constants</a></li>
<li><a href="/posts/keras/embeddings-from-scratch/#org5d2d530">The Embeddings Layer</a></li>
<li><a href="/posts/keras/embeddings-from-scratch/#org4282a2c">The Dataset</a>
<ul>
<li><a href="/posts/keras/embeddings-from-scratch/#org69c670f">Add Padding</a></li>
<li><a href="/posts/keras/embeddings-from-scratch/#org15597b8">Checkout a Sample</a></li>
</ul>
</li>
<li><a href="/posts/keras/embeddings-from-scratch/#org240a1e1">Build a Model</a></li>
<li><a href="/posts/keras/embeddings-from-scratch/#orgf7c8a4c">Compile and Train</a></li>
</ul>
</li>
<li><a href="/posts/keras/embeddings-from-scratch/#org1bb9204">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org4e58c95">
<h2 id="org4e58c95">Beginning</h2>
<div class="outline-text-2" id="text-org4e58c95">
<p>This is a walk-through of the tensorflow <a href="https://www.tensorflow.org/beta/tutorials/text/word_embeddings">Word Embeddings</a> tutorial, just to make sure I can do it.</p>
</div>
<div class="outline-3" id="outline-container-orgd94e184">
<h3 id="orgd94e184">Imports</h3>
<div class="outline-text-3" id="text-orgd94e184"></div>
<div class="outline-4" id="outline-container-orgc57dfe7">
<h4 id="orgc57dfe7">Python</h4>
<div class="outline-text-4" id="text-orgc57dfe7">
<div class="highlight">
<pre><span></span>from argparse import Namespace
from functools import partial
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org78ec86e">
<h4 id="org78ec86e">PyPi</h4>
<div class="outline-text-4" id="text-org78ec86e">
<div class="highlight">
<pre><span></span>from tensorflow import keras
from tensorflow.keras import layers
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org01930d0">
<h4 id="org01930d0">Others</h4>
<div class="outline-text-4" id="text-org01930d0">
<div class="highlight">
<pre><span></span>from graeae import EmbedHoloviews, Timer
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orga6ca04a">
<h3 id="orga6ca04a">Set Up</h3>
<div class="outline-text-3" id="text-orga6ca04a"></div>
<div class="outline-4" id="outline-container-orgde705db">
<h4 id="orgde705db">Plotting</h4>
<div class="outline-text-4" id="text-orgde705db">
<div class="highlight">
<pre><span></span>prefix = "../../files/posts/keras/"
slug = "embeddings-from-scratch"

Embed = partial(EmbedHoloviews, folder_path=f"{prefix}{slug}")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8fe7de7">
<h4 id="org8fe7de7">The Timer</h4>
<div class="outline-text-4" id="text-org8fe7de7">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4b39049">
<h2 id="org4b39049">Middle</h2>
<div class="outline-text-2" id="text-org4b39049"></div>
<div class="outline-3" id="outline-container-org71fdd44">
<h3 id="org71fdd44">Some Constants</h3>
<div class="outline-text-3" id="text-org71fdd44">
<div class="highlight">
<pre><span></span>Text = Namespace(
    vocabulary_size=1000,
    embeddings_size=16,
    max_length=500,
    padding="post",
)

Tokens = Namespace(
    padding = "&lt;PAD&gt;",
    start = "&lt;START&gt;",
    unknown = "&lt;UNKNOWN&gt;",
    unused = "&lt;UNUSED&gt;",
)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5d2d530">
<h3 id="org5d2d530">The Embeddings Layer</h3>
<div class="outline-text-3" id="text-org5d2d530">
<div class="highlight">
<pre><span></span>print(layers.Embedding.__doc__)
</pre></div>
<pre class="example">
Turns positive integers (indexes) into dense vectors of fixed size.

  e.g. `[[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]]`

  This layer can only be used as the first layer in a model.

  Example:

  ```python
  model = Sequential()
  model.add(Embedding(1000, 64, input_length=10))
  # the model will take as input an integer matrix of size (batch,
  # input_length).
  # the largest integer (i.e. word index) in the input should be no larger
  # than 999 (vocabulary size).
  # now model.output_shape == (None, 10, 64), where None is the batch
  # dimension.

  input_array = np.random.randint(1000, size=(32, 10))

  model.compile('rmsprop', 'mse')
  output_array = model.predict(input_array)
  assert output_array.shape == (32, 10, 64)
  ```

  Arguments:
    input_dim: int &gt; 0. Size of the vocabulary,
      i.e. maximum integer index + 1.
    output_dim: int &gt;= 0. Dimension of the dense embedding.
    embeddings_initializer: Initializer for the `embeddings` matrix.
    embeddings_regularizer: Regularizer function applied to
      the `embeddings` matrix.
    embeddings_constraint: Constraint function applied to
      the `embeddings` matrix.
    mask_zero: Whether or not the input value 0 is a special "padding"
      value that should be masked out.
      This is useful when using recurrent layers
      which may take variable length input.
      If this is `True` then all subsequent layers
      in the model need to support masking or an exception will be raised.
      If mask_zero is set to True, as a consequence, index 0 cannot be
      used in the vocabulary (input_dim should equal size of
      vocabulary + 1).
    input_length: Length of input sequences, when it is constant.
      This argument is required if you are going to connect
      `Flatten` then `Dense` layers upstream
      (without it, the shape of the dense outputs cannot be computed).

  Input shape:
    2D tensor with shape: `(batch_size, input_length)`.

  Output shape:
    3D tensor with shape: `(batch_size, input_length, output_dim)`.
  
</pre>
<div class="highlight">
<pre><span></span>embedding_layer = layers.Embedding(Text.vocabulary_size, Text.embeddings_size)
</pre></div>
<p>The first argument is the number of possible words in the vocabulary and the second is the number of dimensions. The Emebdding is a sort of lookup table that maps an integer that represents a word to a vector. In this case we're going to build a vocabulary of 1,000 words represented by vectors with a length of 32. The weights in the vectors are learned when we train the model and will encode the distance between words.</p>
<p>The input to the embeddings layer is a 2D tensor of integers with the shape (<code>number of samples</code>, <code>sequence_length</code>). The sequences are integer-encoded sentences of the same length - so you have to pad the shorter sentences to match the longest one (the <code>sequence_length</code>).</p>
<p>The ouput of the embeddings layer is a 3D tensor with the shape (<code>number of samples</code>, <code>sequence_length</code>, <code>embedding_dimensionality</code>).</p>
</div>
</div>
<div class="outline-3" id="outline-container-org4282a2c">
<h3 id="org4282a2c">The Dataset</h3>
<div class="outline-text-3" id="text-org4282a2c">
<div class="highlight">
<pre><span></span>(train_data, test_data), info = tensorflow_datasets.load(
    "imdb_reviews/subwords8k",
    split=(tensorflow_datasets.Split.TRAIN,
           tensorflow_datasets.Split.TEST),
    with_info=True, as_supervised=True)
</pre></div>
<div class="highlight">
<pre><span></span>encoder = info.features["text"].encoder
print(encoder.subwords[:10])
</pre></div>
<pre class="example">
['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br']
</pre></div>
<div class="outline-4" id="outline-container-org69c670f">
<h4 id="org69c670f">Add Padding</h4>
<div class="outline-text-4" id="text-org69c670f">
<div class="highlight">
<pre><span></span>padded_shapes = ([None], ())
train_batches = train_data.shuffle(Text.vocabulary_size).padded_batch(
    10, padded_shapes=padded_shapes)
test_batches = test_data.shuffle(Text.vocabulary_size).padded_batch(
    10, padded_shapes=padded_shapes
)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org15597b8">
<h4 id="org15597b8">Checkout a Sample</h4>
<div class="outline-text-4" id="text-org15597b8">
<div class="highlight">
<pre><span></span>batch, labels = next(iter(train_batches))
print(batch.numpy())
</pre></div>
<pre class="example">
[[  62    9    4 ...    0    0    0]
 [  19 2428    6 ...    0    0    0]
 [ 691    2  594 ... 7961 1457 7975]
 ...
 [6072 5644 8043 ...    0    0    0]
 [ 977   15   57 ...    0    0    0]
 [5646    2    1 ...    0    0    0]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org240a1e1">
<h3 id="org240a1e1">Build a Model</h3>
<div class="outline-text-3" id="text-org240a1e1">
<div class="highlight">
<pre><span></span>model = keras.Sequential([
    layers.Embedding(encoder.vocab_size, Text.embeddings_size),
    layers.GlobalAveragePooling1D(),
    layers.Dense(1, activation="sigmoid")
])
</pre></div>
<div class="highlight">
<pre><span></span>print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 16)          130960    
_________________________________________________________________
global_average_pooling1d (Gl (None, 16)                0         
_________________________________________________________________
dense (Dense)                (None, 1)                 17        
=================================================================
Total params: 130,977
Trainable params: 130,977
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgf7c8a4c">
<h3 id="orgf7c8a4c">Compile and Train</h3>
<div class="outline-text-3" id="text-orgf7c8a4c">
<div class="highlight">
<pre><span></span>model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
ONCE_PER_EPOCH = 2
with TIMER:
    history = model.fit(train_batches, epochs=10,
                        validation_data=test_batches,
                        verbose=ONCE_PER_EPOCH,
                        validation_steps=20)
</pre></div>
<pre class="example">
2019-09-28 17:14:52,764 graeae.timers.timer start: Started: 2019-09-28 17:14:52.764725
I0928 17:14:52.764965 140515023214400 timer.py:70] Started: 2019-09-28 17:14:52.764725
W0928 17:14:52.806057 140515023214400 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Epoch 1/10
 val_loss: 0.3015 - val_accuracy: 0.8900
2019-09-28 17:17:36,036 graeae.timers.timer end: Ended: 2019-09-28 17:17:36.036090
I0928 17:17:36.036139 140515023214400 timer.py:77] Ended: 2019-09-28 17:17:36.036090
2019-09-28 17:17:36,037 graeae.timers.timer end: Elapsed: 0:02:43.271365
I0928 17:17:36.037808 140515023214400 timer.py:78] Elapsed: 0:02:43.271365
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org1bb9204">
<h2 id="org1bb9204">End</h2>
<div class="outline-text-2" id="text-org1bb9204">
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="Training/Validation Performance",
                          width=1000,
                          height=800)
Embed(plot=plot, file_name="training")()
</pre></div>
<object data="/posts/keras/embeddings-from-scratch/training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Amazingly, even with such a simple model, it managed a 92 % validation accuracy.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/imdb-lstm-with-tokenization/">IMDB GRU With Tokenization</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/imdb-lstm-with-tokenization/" rel="bookmark"><time class="published dt-published" datetime="2019-09-23T14:14:04-07:00" itemprop="datePublished" title="2019-09-23 14:14">2019-09-23 14:14</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org8d39b7c">Beginning</a>
<ul>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#orga263ded">Imports</a>
<ul>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org20237f9">Python</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#orgc8e15ee">PyPi</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org1c02a15">Other</a></li>
</ul>
</li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#orgca63888">Set Up</a>
<ul>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#orgeba9e82">The Timer</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org87d9b75">Plotting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org1e0b891">Middle</a>
<ul>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org0d3e147">Set Up the Data</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org7221d6d">Building Up the Tokenizer</a>
<ul>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org952e944">Split Up the Sentences and Their Labels</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org8d7d2de">Some Constants</a></li>
</ul>
</li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#orga808ef0">Build the Tokenizer</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#orgc95e825">Decoder Ring</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org723fbfb">Build the Model</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#orgdb7b5d9">Train it</a></li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#org982950d">Plot It</a></li>
</ul>
</li>
<li><a href="/posts/keras/imdb-lstm-with-tokenization/#orgb4dd8bc">Raw</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org8d39b7c">
<h2 id="org8d39b7c">Beginning</h2>
<div class="outline-text-2" id="text-org8d39b7c">
<p>This is another version of the RNN model to classify the IMDB reviews, but this time we're going to tokenize it ourselves and use a GRU, instead of using the tensorflow-datasets version.</p>
</div>
<div class="outline-3" id="outline-container-orga263ded">
<h3 id="orga263ded">Imports</h3>
<div class="outline-text-3" id="text-orga263ded"></div>
<div class="outline-4" id="outline-container-org20237f9">
<h4 id="org20237f9">Python</h4>
<div class="outline-text-4" id="text-org20237f9">
<div class="highlight">
<pre><span></span>from argparse import Namespace
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc8e15ee">
<h4 id="orgc8e15ee">PyPi</h4>
<div class="outline-text-4" id="text-orgc8e15ee">
<div class="highlight">
<pre><span></span>from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import hvplot.pandas
import numpy
import pandas
import tensorflow
import tensorflow_datasets
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1c02a15">
<h4 id="org1c02a15">Other</h4>
<div class="outline-text-4" id="text-org1c02a15">
<div class="highlight">
<pre><span></span>from graeae import Timer, EmbedHoloviews
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgca63888">
<h3 id="orgca63888">Set Up</h3>
<div class="outline-text-3" id="text-orgca63888"></div>
<div class="outline-4" id="outline-container-orgeba9e82">
<h4 id="orgeba9e82">The Timer</h4>
<div class="outline-text-4" id="text-orgeba9e82">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org87d9b75">
<h4 id="org87d9b75">Plotting</h4>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1e0b891">
<h2 id="org1e0b891">Middle</h2>
<div class="outline-text-2" id="text-org1e0b891"></div>
<div class="outline-3" id="outline-container-org0d3e147">
<h3 id="org0d3e147">Set Up the Data</h3>
<div class="outline-text-3" id="text-org0d3e147">
<div class="highlight">
<pre><span></span>imdb, info = tensorflow_datasets.load("imdb_reviews",
                                      with_info=True,
                                      as_supervised=True)
</pre></div>
<pre class="example">
WARNING: Logging before flag parsing goes to stderr.
W0924 21:52:10.158111 139862640383808 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.
</pre>
<div class="highlight">
<pre><span></span>training, testing = imdb["train"], imdb["test"]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7221d6d">
<h3 id="org7221d6d">Building Up the Tokenizer</h3>
<div class="outline-text-3" id="text-org7221d6d">
<p>Since we didn't pass in a specifier for the configuration we wanted (e.g. <code>imdb/subwords8k</code>) it defaulted to giving us the plain text reviews (and their labels) so we have to build the tokenizer ourselves.</p>
</div>
<div class="outline-4" id="outline-container-org952e944">
<h4 id="org952e944">Split Up the Sentences and Their Labels</h4>
<div class="outline-text-4" id="text-org952e944">
<p>As you might recall, the data set consists of 50,000 IMDB movie reviews categorized as positive or negative. To build the tokenize we first have to split the sentences from their labels</p>
<div class="highlight">
<pre><span></span>training_sentences = []
training_labels = []
testing_sentences = []
testing_labels = []
</pre></div>
<div class="highlight">
<pre><span></span>with TIMER:
    for sentence, label in training:
        training_sentences.append(str(sentence.numpy()))
        training_labels.append(str(label.numpy()))


    for sentence, label in testing:
        testing_sentences.append(str(sentence.numpy))
        testing_labels.append(str(label.numpy()))
</pre></div>
<pre class="example">
2019-09-24 21:52:11,396 graeae.timers.timer start: Started: 2019-09-24 21:52:11.395126
I0924 21:52:11.396310 139862640383808 timer.py:70] Started: 2019-09-24 21:52:11.395126
2019-09-24 21:52:18,667 graeae.timers.timer end: Ended: 2019-09-24 21:52:18.667789
I0924 21:52:18.667830 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:18.667789
2019-09-24 21:52:18,670 graeae.timers.timer end: Elapsed: 0:00:07.272663
I0924 21:52:18.670069 139862640383808 timer.py:78] Elapsed: 0:00:07.272663
</pre>
<div class="highlight">
<pre><span></span>training_labels_final = numpy.array(training_labels)
testing_labels_final = numpy.array(testing_labels)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8d7d2de">
<h4 id="org8d7d2de">Some Constants</h4>
<div class="outline-text-4" id="text-org8d7d2de">
<div class="highlight">
<pre><span></span>Text = Namespace(
    vocab_size = 10000,
    embedding_dim = 16,
    max_length = 120,
    trunc_type='post',
    oov_token = "&lt;OOV&gt;",
)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orga808ef0">
<h3 id="orga808ef0">Build the Tokenizer</h3>
<div class="outline-text-3" id="text-orga808ef0">
<div class="highlight">
<pre><span></span>tokenizer = Tokenizer(num_words=Text.vocab_size, oov_token=Text.oov_token)
with TIMER:
    tokenizer.fit_on_texts(training_sentences)

    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(training_sentences)
    padded = pad_sequences(sequences, maxlen=Text.max_length, truncating=Text.trunc_type)

    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
    testing_padded = pad_sequences(testing_sequences, maxlen=Text.max_length)
</pre></div>
<pre class="example">
2019-09-24 21:52:21,705 graeae.timers.timer start: Started: 2019-09-24 21:52:21.705287
I0924 21:52:21.705317 139862640383808 timer.py:70] Started: 2019-09-24 21:52:21.705287
2019-09-24 21:52:32,152 graeae.timers.timer end: Ended: 2019-09-24 21:52:32.152267
I0924 21:52:32.152314 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:32.152267
2019-09-24 21:52:32,154 graeae.timers.timer end: Elapsed: 0:00:10.446980
I0924 21:52:32.154620 139862640383808 timer.py:78] Elapsed: 0:00:10.446980
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgc95e825">
<h3 id="orgc95e825">Decoder Ring</h3>
<div class="outline-text-3" id="text-orgc95e825">
<div class="highlight">
<pre><span></span>index_to_word = {value: key for key, value in word_index.items()}

def decode_review(text: numpy.array) -&gt; str:
    return " ".join([index_to_word.get(item, "&lt;?&gt;") for item in text])
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org723fbfb">
<h3 id="org723fbfb">Build the Model</h3>
<div class="outline-text-3" id="text-org723fbfb">
<p>This time we're going to build a four-layer model with one Bidirectional layer that uses a <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU">GRU</a> (<a href="https://www.wikiwand.com/en/Gated_recurrent_unit">Gated Recurrent Unit</a>) instead of a LSTM.</p>
<div class="highlight">
<pre><span></span>model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(Text.vocab_size, Text.embedding_dim, input_length=Text.max_length),
    tensorflow.keras.layers.Bidirectional(tensorflow.compat.v2.keras.layers.GRU(32)),
    tensorflow.keras.layers.Dense(6, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
</pre></div>
<div class="highlight">
<pre><span></span>print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
bidirectional (Bidirectional (None, 64)                9600      
_________________________________________________________________
dense (Dense)                (None, 6)                 390       
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 169,997
Trainable params: 169,997
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgdb7b5d9">
<h3 id="orgdb7b5d9">Train it</h3>
<div class="outline-text-3" id="text-orgdb7b5d9">
<div class="highlight">
<pre><span></span>EPOCHS = 50
ONCE_PER_EPOCH = 2
batch_size = 8
history = model.fit(padded, training_labels_final,
                    epochs=EPOCHS,
                    batch_size=batch_size,
                    validation_data=(testing_padded, testing_labels_final),
                    verbose=ONCE_PER_EPOCH)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org982950d">
<h3 id="org982950d">Plot It</h3>
<div class="outline-text-3" id="text-org982950d">
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="GRU Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="gru_training")()
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb4dd8bc">
<h2 id="orgb4dd8bc">Raw</h2>
<div class="outline-text-2" id="text-orgb4dd8bc"></div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/he-used-sarcasm/">He Used Sarcasm</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/he-used-sarcasm/" rel="bookmark"><time class="published dt-published" datetime="2019-09-21T19:01:16-07:00" itemprop="datePublished" title="2019-09-21 19:01">2019-09-21 19:01</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/he-used-sarcasm/#org73fad9b">Beginning</a>
<ul>
<li><a href="/posts/keras/he-used-sarcasm/#orgca22a4c">Imports</a>
<ul>
<li><a href="/posts/keras/he-used-sarcasm/#org848c22b">Python</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org5a2a34f">PyPi</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org2429b30">Other</a></li>
</ul>
</li>
<li><a href="/posts/keras/he-used-sarcasm/#org5ff33ef">Set Up</a>
<ul>
<li><a href="/posts/keras/he-used-sarcasm/#org8a08c46">The Timer</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org5a7adaf">The Plotting</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org5dbd8b7">The Data</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/he-used-sarcasm/#org601f432">Middle</a>
<ul>
<li><a href="/posts/keras/he-used-sarcasm/#org16b0580">Looking At the Data</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org2746241">Set Up the Tokenizing and Training Data</a>
<ul>
<li><a href="/posts/keras/he-used-sarcasm/#orgbb917d1">The Training and Testing Data</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#orgea7d3ee">The Tokenizer</a></li>
</ul>
</li>
<li><a href="/posts/keras/he-used-sarcasm/#org1c098cf">Build The Model</a>
<ul>
<li><a href="/posts/keras/he-used-sarcasm/#org4386310">It's a Sequence of Layers</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org877f64a">Start With An Embedding Layer</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org57e508d">The Convolutional Layer</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org2a7ec91">A Pooling Layer</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#orga9962a0">The Fully-Connected Layers</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org8bc687d">Build It</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#org6a0215d">Compile It</a></li>
</ul>
</li>
<li><a href="/posts/keras/he-used-sarcasm/#org9f6d11b">Train It</a></li>
<li><a href="/posts/keras/he-used-sarcasm/#orga19541a">Plot the Performance</a></li>
</ul>
</li>
<li><a href="/posts/keras/he-used-sarcasm/#org4f45701">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org73fad9b">
<h2 id="org73fad9b">Beginning</h2>
<div class="outline-text-2" id="text-org73fad9b">
<p>This is a look at fitting a model to detect sarcasm using a json blob from Laurence Moroney (<a href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json">https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json</a>).</p>
</div>
<div class="outline-3" id="outline-container-orgca22a4c">
<h3 id="orgca22a4c">Imports</h3>
<div class="outline-text-3" id="text-orgca22a4c"></div>
<div class="outline-4" id="outline-container-org848c22b">
<h4 id="org848c22b">Python</h4>
<div class="outline-text-4" id="text-org848c22b">
<div class="highlight">
<pre><span></span>from argparse import Namespace
from functools import partial
from pathlib import Path
from pprint import pprint
from urllib.parse import urlparse
import json
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5a2a34f">
<h4 id="org5a2a34f">PyPi</h4>
<div class="outline-text-4" id="text-org5a2a34f">
<div class="highlight">
<pre><span></span>from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import hvplot.pandas
import pandas
import tensorflow
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org2429b30">
<h4 id="org2429b30">Other</h4>
<div class="outline-text-4" id="text-org2429b30">
<div class="highlight">
<pre><span></span>from graeae import (
    CountPercentage,
    EmbedHoloviews,
    TextDownloader,
    Timer
)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org5ff33ef">
<h3 id="org5ff33ef">Set Up</h3>
<div class="outline-text-3" id="text-org5ff33ef"></div>
<div class="outline-4" id="outline-container-org8a08c46">
<h4 id="org8a08c46">The Timer</h4>
<div class="outline-text-4" id="text-org8a08c46">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5a7adaf">
<h4 id="org5a7adaf">The Plotting</h4>
<div class="outline-text-4" id="text-org5a7adaf">
<div class="highlight">
<pre><span></span>SLUG = "he-used-sarcasm"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{SLUG}")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5dbd8b7">
<h4 id="org5dbd8b7">The Data</h4>
<div class="outline-text-4" id="text-org5dbd8b7">
<div class="highlight">
<pre><span></span>URL = "https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json"
path = Path("~/data/datasets/text/sarcasm/sarcasm.json").expanduser()
downloader = TextDownloader(URL, path)
with TIMER:
    data = json.loads(downloader.download)
</pre></div>
<pre class="example">
2019-09-22 15:05:27,302 graeae.timers.timer start: Started: 2019-09-22 15:05:27.301225
WARNING: Logging before flag parsing goes to stderr.
I0922 15:05:27.302001 139873020925760 timer.py:70] Started: 2019-09-22 15:05:27.301225
2019-09-22 15:05:27,306 [1mTextDownloader[0m download: /home/hades/data/datasets/text/sarcasm/sarcasm.json exists, opening it
I0922 15:05:27.306186 139873020925760 downloader.py:51] /home/hades/data/datasets/text/sarcasm/sarcasm.json exists, opening it
2019-09-22 15:05:27,367 graeae.timers.timer end: Ended: 2019-09-22 15:05:27.367036
I0922 15:05:27.367099 139873020925760 timer.py:77] Ended: 2019-09-22 15:05:27.367036
2019-09-22 15:05:27,369 graeae.timers.timer end: Elapsed: 0:00:00.065811
I0922 15:05:27.369417 139873020925760 timer.py:78] Elapsed: 0:00:00.065811
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org601f432">
<h2 id="org601f432">Middle</h2>
<div class="outline-text-2" id="text-org601f432"></div>
<div class="outline-3" id="outline-container-org16b0580">
<h3 id="org16b0580">Looking At the Data</h3>
<div class="outline-text-3" id="text-org16b0580">
<div class="highlight">
<pre><span></span>pprint(data[0])
</pre></div>
<pre class="example">
{'article_link': 'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5',
 'headline': "former versace store clerk sues over secret 'black code' for "
             'minority shoppers',
 'is_sarcastic': 0}
</pre>
<p>So our data is a dictionary with three keys - the source of the article, the headline of the article, and whether it's a sarcastic headline or not. There's no citation in the original notebook, but it looks like it might be this one <a href="https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection">on GitHub</a>.</p>
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(data)
data.loc[:, "site"] = data.article_link.apply(lambda link: urlparse(link).netloc)
</pre></div>
<div class="highlight">
<pre><span></span>CountPercentage(data.site, value_label="Site")()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Site</th>
<th class="org-right" scope="col">Count</th>
<th class="org-right" scope="col">Percent (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">www.huffingtonpost.com</td>
<td class="org-right">14403</td>
<td class="org-right">53.93</td>
</tr>
<tr>
<td class="org-left">www.theonion.com</td>
<td class="org-right">5811</td>
<td class="org-right">21.76</td>
</tr>
<tr>
<td class="org-left">local.theonion.com</td>
<td class="org-right">2852</td>
<td class="org-right">10.68</td>
</tr>
<tr>
<td class="org-left">politics.theonion.com</td>
<td class="org-right">1767</td>
<td class="org-right">6.62</td>
</tr>
<tr>
<td class="org-left">entertainment.theonion.com</td>
<td class="org-right">1194</td>
<td class="org-right">4.47</td>
</tr>
<tr>
<td class="org-left">www.huffingtonpost.comhttp:</td>
<td class="org-right">503</td>
<td class="org-right">1.88</td>
</tr>
<tr>
<td class="org-left">sports.theonion.com</td>
<td class="org-right">100</td>
<td class="org-right">0.37</td>
</tr>
<tr>
<td class="org-left">www.huffingtonpost.comhttps:</td>
<td class="org-right">79</td>
<td class="org-right">0.30</td>
</tr>
</tbody>
</table>
<p>So, it looks like there's some problems with the URLs. I don't think that's important, but maybe I can clean up a little anyway.</p>
<div class="highlight">
<pre><span></span>print(
    data[
        data.site.str.contains("www.huffingtonpost.comhttp"
                               )].article_link.value_counts())
</pre></div>
<pre class="example">
https://www.huffingtonpost.comhttp://nymag.com/daily/intelligencer/2016/05/hillary-clinton-candidacy.html                                                                                              2
https://www.huffingtonpost.comhttps://www.facebook.com/HuffPostQueerVoices/videos/1153919084666530/                                                                                                    1
https://www.huffingtonpost.comhttp://live.huffingtonpost.com/r/segment/chris-meloni-star-underground/56d8584f99ec6dca3d00000a                                                                          1
https://www.huffingtonpost.comhttp://www.thestreet.com/story/13223501/1/post-retirement-work-may-not-save-your-golden-years.html                                                                       1
https://www.huffingtonpost.comhttps://www.facebook.com/HuffPostEntertainment/                                                                                                                          1
                                                                                                                                                                                                      ..
https://www.huffingtonpost.comhttp://nymag.com/thecut/2015/10/first-legal-abortionists-tell-their-stories.html?mid=twitter_nymag                                                                       1
https://www.huffingtonpost.comhttp://www.tampabay.com/blogs/the-buzz-florida-politics/marco-rubio-warming-up-to-donald-trump/2275308                                                                   1
https://www.huffingtonpost.comhttp://live.huffingtonpost.com/r/segment/porn-to-pay-for-college/55aeadf62b8c2a2f6f000193                                                                                1
https://www.huffingtonpost.comhttps://www.thedodo.com/dog-mouth-taped-shut-facebook-1481874724.html                                                                                                    1
https://www.huffingtonpost.comhttps://www.washingtonpost.com/politics/ben-carson-to-tell-supporters-he-sees-no-path-forward-for-campaign/2016/03/02/d6bef352-d9b3-11e5-891a-4ed04f4213e8_story.html    1
Name: article_link, Length: 581, dtype: int64
</pre>
<p>That's kind of odd, I don't know what that means, maybe the Huffington Post was citing other sites? I went to go check the GitHub dataset I mentioned but it's actually much larger than this one so I don't know if it's really the source or not.</p>
<div class="highlight">
<pre><span></span>prefixes = ("www.huffingtonpost.comhttp:", "www.huffingtonpost.comhttps:")
for prefix in prefixes:
    data.loc[:, "site"] = data.site.str.replace(
        prefix,
        "www.huffingtonpost.com")

prefixes = ("local.theonion.com",
            "politics.theonion.com",
            "entertainment.theonion.com",
            "sports.theonion.com")

for prefix in prefixes:
    data.loc[:, "site"] = data.site.str.replace(prefix,
                                                "www.theonion.com")
</pre></div>
<div class="highlight">
<pre><span></span>counter = CountPercentage(data.site, value_label="Site")
counter()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">Site</th>
<th class="org-right" scope="col">Count</th>
<th class="org-right" scope="col">Percent (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">www.huffingtonpost.com</td>
<td class="org-right">14985</td>
<td class="org-right">56.10</td>
</tr>
<tr>
<td class="org-left">www.theonion.com</td>
<td class="org-right">11724</td>
<td class="org-right">43.90</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span>plot = counter.table.hvplot.bar(x="Site", y="Count").opts(
    title="Distribution by Site",
    width=1000,
    height=800)
Embed(plot=plot, file_name="site_distribution")()
</pre></div>
<object data="/posts/keras/he-used-sarcasm/site_distribution.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<div class="highlight">
<pre><span></span>counter = CountPercentage(data.is_sarcastic, value_label="Is Sarcastic")
counter()
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-right">
<col class="org-left">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-right" scope="col">Is Sarcastic</th>
<th class="org-left" scope="col">Count</th>
<th class="org-right" scope="col">Percent (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-left">14,985</td>
<td class="org-right">56.10</td>
</tr>
<tr>
<td class="org-right">1</td>
<td class="org-left">11,724</td>
<td class="org-right">43.90</td>
</tr>
</tbody>
</table>
<p>Given that the counts match I'm assuming anything from the Huffington Post is labeled as not sarcastic and anything from the onion is sarcastic.</p>
<div class="highlight">
<pre><span></span>assert all(data[data.site=="www.onion.com"].is_sarcastic)
assert not any(data[data.site=="www.huffingtonpost.com"].is_sarcastic)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2746241">
<h3 id="org2746241">Set Up the Tokenizing and Training Data</h3>
<div class="outline-text-3" id="text-org2746241">
<div class="highlight">
<pre><span></span>print(f"{len(data):,}")
</pre></div>
<pre class="example">
26,709
</pre>
<div class="highlight">
<pre><span></span>Text = Namespace(
    vocabulary_size = 1000,
    embedding_dim = 16,
    max_length = 120,
    truncating_type='post',
    padding_type='post',
    out_of_vocabulary_tok = "&lt;OOV&gt;",
)

# this is actually the default for train_test_split
Training = Namespace(
    size = 0.75,
    epochs = 50,
    verbosity = 2,
    )
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgbb917d1">
<h4 id="orgbb917d1">The Training and Testing Data</h4>
<div class="outline-text-4" id="text-orgbb917d1">
<div class="highlight">
<pre><span></span>x_train, x_test, y_train, y_test = train_test_split(
    data.headline, data.is_sarcastic, train_size=Training.size,
)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgea7d3ee">
<h4 id="orgea7d3ee">The Tokenizer</h4>
<div class="outline-text-4" id="text-orgea7d3ee">
<div class="highlight">
<pre><span></span>tokenizer = Tokenizer(num_words=Text.vocabulary_size,
                      oov_token=Text.out_of_vocabulary_tok)
</pre></div>
<div class="highlight">
<pre><span></span>print(tokenizer.__doc__)
</pre></div>
<pre class="example">
Text tokenization utility class.

    This class allows to vectorize a text corpus, by turning each
    text into either a sequence of integers (each integer being the index
    of a token in a dictionary) or into a vector where the coefficient
    for each token could be binary, based on word count, based on tf-idf...

    # Arguments
        num_words: the maximum number of words to keep, based
            on word frequency. Only the most common `num_words-1` words will
            be kept.
        filters: a string where each element is a character that will be
            filtered from the texts. The default is all punctuation, plus
            tabs and line breaks, minus the `'` character.
        lower: boolean. Whether to convert the texts to lowercase.
        split: str. Separator for word splitting.
        char_level: if True, every character will be treated as a token.
        oov_token: if given, it will be added to word_index and used to
            replace out-of-vocabulary words during text_to_sequence calls

    By default, all punctuation is removed, turning the texts into
    space-separated sequences of words
    (words maybe include the `'` character). These sequences are then
    split into lists of tokens. They will then be indexed or vectorized.

    `0` is a reserved index that won't be assigned to any word.
</pre>
<p>Now that we have a tokenizer we can tokenize our training headlines.</p>
<div class="highlight">
<pre><span></span>help(tokenizer.fit_on_texts)
</pre></div>
<pre class="example">
Help on method fit_on_texts in module keras_preprocessing.text:

fit_on_texts(texts) method of keras_preprocessing.text.Tokenizer instance
    Updates internal vocabulary based on a list of texts.
    
    In the case where texts contains lists,
    we assume each entry of the lists to be a token.
    
    Required before using `texts_to_sequences` or `texts_to_matrix`.
    
    # Arguments
        texts: can be a list of strings,
            a generator of strings (for memory-efficiency),
            or a list of list of strings.

</pre>
<div class="highlight">
<pre><span></span>tokenizer.fit_on_texts(x_train)
</pre></div>
<p>Now that we've fit the headlines we can get the word index, a dict mapping words to their index.</p>
<div class="highlight">
<pre><span></span>word_index = tokenizer.word_index
</pre></div>
<p>Note that the tokenizer doesn't remove stop-words.</p>
<div class="highlight">
<pre><span></span>print("the" in word_index)
</pre></div>
<pre class="example">
True
</pre>
<p>Now we'll convert the training headlines to sequences of numbers.</p>
<div class="highlight">
<pre><span></span>help(tokenizer.texts_to_sequences)
</pre></div>
<pre class="example">
Help on method texts_to_sequences in module keras_preprocessing.text:

texts_to_sequences(texts) method of keras_preprocessing.text.Tokenizer instance
    Transforms each text in texts to a sequence of integers.
    
    Only top `num_words-1` most frequent words will be taken into account.
    Only words known by the tokenizer will be taken into account.
    
    # Arguments
        texts: A list of texts (strings).
    
    # Returns
        A list of sequences.

</pre>
<p>We're also going to have to pad them to make them the same length.</p>
<div class="highlight">
<pre><span></span>help(pad_sequences)
</pre></div>
<pre class="example">
Help on function pad_sequences in module keras_preprocessing.sequence:

pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)
    Pads sequences to the same length.
    
    This function transforms a list of
    `num_samples` sequences (lists of integers)
    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.
    `num_timesteps` is either the `maxlen` argument if provided,
    or the length of the longest sequence otherwise.
    
    Sequences that are shorter than `num_timesteps`
    are padded with `value` at the end.
    
    Sequences longer than `num_timesteps` are truncated
    so that they fit the desired length.
    The position where padding or truncation happens is determined by
    the arguments `padding` and `truncating`, respectively.
    
    Pre-padding is the default.
    
    # Arguments
        sequences: List of lists, where each element is a sequence.
        maxlen: Int, maximum length of all sequences.
        dtype: Type of the output sequences.
            To pad sequences with variable length strings, you can use `object`.
        padding: String, 'pre' or 'post':
            pad either before or after each sequence.
        truncating: String, 'pre' or 'post':
            remove values from sequences larger than
            `maxlen`, either at the beginning or at the end of the sequences.
        value: Float or String, padding value.
    
    # Returns
        x: Numpy array with shape `(len(sequences), maxlen)`
    
    # Raises
        ValueError: In case of invalid values for `truncating` or `padding`,
            or in case of invalid shape for a `sequences` entry.

</pre>
<div class="highlight">
<pre><span></span>training_sequences = tokenizer.texts_to_sequences(x_train)
training_padded = pad_sequences(training_sequences, maxlen=Text.max_length, padding=Text.padding_type, truncating=Text.truncating_type)

testing_sequences = tokenizer.texts_to_sequences(x_test)
testing_padded = pad_sequences(testing_sequences, maxlen=Text.max_length, padding=Text.padding_type, truncating=Text.truncating_type)
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org1c098cf">
<h3 id="org1c098cf">Build The Model</h3>
<div class="outline-text-3" id="text-org1c098cf">
<p>We're going to use a convolutional neural network to try and classify our headlines as sarcastic or not-sarcastic.</p>
</div>
<div class="outline-4" id="outline-container-org4386310">
<h4 id="org4386310">It's a Sequence of Layers</h4>
<div class="outline-text-4" id="text-org4386310">
<div class="highlight">
<pre><span></span>print(tensorflow.keras.Sequential.__doc__)
</pre></div>
<pre class="example">
Linear stack of layers.

  Arguments:
      layers: list of layers to add to the model.

  Example:

  ```python
  # Optionally, the first layer can receive an `input_shape` argument:
  model = Sequential()
  model.add(Dense(32, input_shape=(500,)))
  # Afterwards, we do automatic shape inference:
  model.add(Dense(32))

  # This is identical to the following:
  model = Sequential()
  model.add(Dense(32, input_dim=500))

  # And to the following:
  model = Sequential()
  model.add(Dense(32, batch_input_shape=(None, 500)))

  # Note that you can also omit the `input_shape` argument:
  # In that case the model gets built the first time you call `fit` (or other
  # training and evaluation methods).
  model = Sequential()
  model.add(Dense(32))
  model.add(Dense(32))
  model.compile(optimizer=optimizer, loss=loss)
  # This builds the model for the first time:
  model.fit(x, y, batch_size=32, epochs=10)

  # Note that when using this delayed-build pattern (no input shape specified),
  # the model doesn't have any weights until the first call
  # to a training/evaluation method (since it isn't yet built):
  model = Sequential()
  model.add(Dense(32))
  model.add(Dense(32))
  model.weights  # returns []

  # Whereas if you specify the input shape, the model gets built continuously
  # as you are adding layers:
  model = Sequential()
  model.add(Dense(32, input_shape=(500,)))
  model.add(Dense(32))
  model.weights  # returns list of length 4

  # When using the delayed-build pattern (no input shape specified), you can
  # choose to manually build your model by calling `build(batch_input_shape)`:
  model = Sequential()
  model.add(Dense(32))
  model.add(Dense(32))
  model.build((None, 500))
  model.weights  # returns list of length 4
  ```
  
</pre></div>
</div>
<div class="outline-4" id="outline-container-org877f64a">
<h4 id="org877f64a">Start With An Embedding Layer</h4>
<div class="outline-text-4" id="text-org877f64a">
<div class="highlight">
<pre><span></span>print(tensorflow.keras.layers.Embedding.__doc__)
</pre></div>
<pre class="example">
Turns positive integers (indexes) into dense vectors of fixed size.

  e.g. `[[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]]`

  This layer can only be used as the first layer in a model.

  Example:

  ```python
  model = Sequential()
  model.add(Embedding(1000, 64, input_length=10))
  # the model will take as input an integer matrix of size (batch,
  # input_length).
  # the largest integer (i.e. word index) in the input should be no larger
  # than 999 (vocabulary size).
  # now model.output_shape == (None, 10, 64), where None is the batch
  # dimension.

  input_array = np.random.randint(1000, size=(32, 10))

  model.compile('rmsprop', 'mse')
  output_array = model.predict(input_array)
  assert output_array.shape == (32, 10, 64)
  ```

  Arguments:
    input_dim: int &gt; 0. Size of the vocabulary,
      i.e. maximum integer index + 1.
    output_dim: int &gt;= 0. Dimension of the dense embedding.
    embeddings_initializer: Initializer for the `embeddings` matrix.
    embeddings_regularizer: Regularizer function applied to
      the `embeddings` matrix.
    embeddings_constraint: Constraint function applied to
      the `embeddings` matrix.
    mask_zero: Whether or not the input value 0 is a special "padding"
      value that should be masked out.
      This is useful when using recurrent layers
      which may take variable length input.
      If this is `True` then all subsequent layers
      in the model need to support masking or an exception will be raised.
      If mask_zero is set to True, as a consequence, index 0 cannot be
      used in the vocabulary (input_dim should equal size of
      vocabulary + 1).
    input_length: Length of input sequences, when it is constant.
      This argument is required if you are going to connect
      `Flatten` then `Dense` layers upstream
      (without it, the shape of the dense outputs cannot be computed).

  Input shape:
    2D tensor with shape: `(batch_size, input_length)`.

  Output shape:
    3D tensor with shape: `(batch_size, input_length, output_dim)`.
  
</pre></div>
</div>
<div class="outline-4" id="outline-container-org57e508d">
<h4 id="org57e508d">The Convolutional Layer</h4>
<div class="outline-text-4" id="text-org57e508d">
<div class="highlight">
<pre><span></span>print(tensorflow.keras.layers.Conv1D.__doc__)
</pre></div>
<pre class="example">
1D convolution layer (e.g. temporal convolution).

  This layer creates a convolution kernel that is convolved
  with the layer input over a single spatial (or temporal) dimension
  to produce a tensor of outputs.
  If `use_bias` is True, a bias vector is created and added to the outputs.
  Finally, if `activation` is not `None`,
  it is applied to the outputs as well.

  When using this layer as the first layer in a model,
  provide an `input_shape` argument
  (tuple of integers or `None`, e.g.
  `(10, 128)` for sequences of 10 vectors of 128-dimensional vectors,
  or `(None, 128)` for variable-length sequences of 128-dimensional vectors.

  Arguments:
    filters: Integer, the dimensionality of the output space
      (i.e. the number of output filters in the convolution).
    kernel_size: An integer or tuple/list of a single integer,
      specifying the length of the 1D convolution window.
    strides: An integer or tuple/list of a single integer,
      specifying the stride length of the convolution.
      Specifying any stride value != 1 is incompatible with specifying
      any `dilation_rate` value != 1.
    padding: One of `"valid"`, `"causal"` or `"same"` (case-insensitive).
      `"causal"` results in causal (dilated) convolutions, e.g. output[t]
      does not depend on input[t+1:]. Useful when modeling temporal data
      where the model should not violate the temporal order.
      See [WaveNet: A Generative Model for Raw Audio, section
        2.1](https://arxiv.org/abs/1609.03499).
    data_format: A string,
      one of `channels_last` (default) or `channels_first`.
    dilation_rate: an integer or tuple/list of a single integer, specifying
      the dilation rate to use for dilated convolution.
      Currently, specifying any `dilation_rate` value != 1 is
      incompatible with specifying any `strides` value != 1.
    activation: Activation function to use.
      If you don't specify anything, no activation is applied
      (ie. "linear" activation: `a(x) = x`).
    use_bias: Boolean, whether the layer uses a bias vector.
    kernel_initializer: Initializer for the `kernel` weights matrix.
    bias_initializer: Initializer for the bias vector.
    kernel_regularizer: Regularizer function applied to
      the `kernel` weights matrix.
    bias_regularizer: Regularizer function applied to the bias vector.
    activity_regularizer: Regularizer function applied to
      the output of the layer (its "activation")..
    kernel_constraint: Constraint function applied to the kernel matrix.
    bias_constraint: Constraint function applied to the bias vector.

  Examples:
    ```python
    # Small convolutional model for 128-length vectors with 6 timesteps
    # model.input_shape == (None, 6, 128)
    
    model = Sequential()
    model.add(Conv1D(32, 3, 
              activation='relu', 
              input_shape=(6, 128)))
    
    # now: model.output_shape == (None, 4, 32)
    ```

  Input shape:
    3D tensor with shape: `(batch_size, steps, input_dim)`

  Output shape:
    3D tensor with shape: `(batch_size, new_steps, filters)`
      `steps` value might have changed due to padding or strides.
  
</pre></div>
</div>
<div class="outline-4" id="outline-container-org2a7ec91">
<h4 id="org2a7ec91">A Pooling Layer</h4>
<div class="outline-text-4" id="text-org2a7ec91">
<div class="highlight">
<pre><span></span>print(tensorflow.keras.layers.GlobalMaxPooling1D.__doc__)
</pre></div>
<pre class="example">
Global max pooling operation for temporal data.

  Arguments:
    data_format: A string,
      one of `channels_last` (default) or `channels_first`.
      The ordering of the dimensions in the inputs.
      `channels_last` corresponds to inputs with shape
      `(batch, steps, features)` while `channels_first`
      corresponds to inputs with shape
      `(batch, features, steps)`.

  Input shape:
    - If `data_format='channels_last'`:
      3D tensor with shape:
      `(batch_size, steps, features)`
    - If `data_format='channels_first'`:
      3D tensor with shape:
      `(batch_size, features, steps)`

  Output shape:
    2D tensor with shape `(batch_size, features)`.
  
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga9962a0">
<h4 id="orga9962a0">The Fully-Connected Layers</h4>
<div class="outline-text-4" id="text-orga9962a0">
<p>Finally our output layers.</p>
<div class="highlight">
<pre><span></span>print(tensorflow.keras.layers.Dense.__doc__)
</pre></div>
<pre class="example">
Just your regular densely-connected NN layer.

  `Dense` implements the operation:
  `output = activation(dot(input, kernel) + bias)`
  where `activation` is the element-wise activation function
  passed as the `activation` argument, `kernel` is a weights matrix
  created by the layer, and `bias` is a bias vector created by the layer
  (only applicable if `use_bias` is `True`).

  Note: If the input to the layer has a rank greater than 2, then
  it is flattened prior to the initial dot product with `kernel`.

  Example:

  ```python
  # as first layer in a sequential model:
  model = Sequential()
  model.add(Dense(32, input_shape=(16,)))
  # now the model will take as input arrays of shape (*, 16)
  # and output arrays of shape (*, 32)

  # after the first layer, you don't need to specify
  # the size of the input anymore:
  model.add(Dense(32))
  ```

  Arguments:
    units: Positive integer, dimensionality of the output space.
    activation: Activation function to use.
      If you don't specify anything, no activation is applied
      (ie. "linear" activation: `a(x) = x`).
    use_bias: Boolean, whether the layer uses a bias vector.
    kernel_initializer: Initializer for the `kernel` weights matrix.
    bias_initializer: Initializer for the bias vector.
    kernel_regularizer: Regularizer function applied to
      the `kernel` weights matrix.
    bias_regularizer: Regularizer function applied to the bias vector.
    activity_regularizer: Regularizer function applied to
      the output of the layer (its "activation")..
    kernel_constraint: Constraint function applied to
      the `kernel` weights matrix.
    bias_constraint: Constraint function applied to the bias vector.

  Input shape:
    N-D tensor with shape: `(batch_size, ..., input_dim)`.
    The most common situation would be
    a 2D input with shape `(batch_size, input_dim)`.

  Output shape:
    N-D tensor with shape: `(batch_size, ..., units)`.
    For instance, for a 2D input with shape `(batch_size, input_dim)`,
    the output would have shape `(batch_size, units)`.
  
</pre></div>
</div>
<div class="outline-4" id="outline-container-org8bc687d">
<h4 id="org8bc687d">Build It</h4>
<div class="outline-text-4" id="text-org8bc687d">
<p>I originally added the layers using the <code>model.add</code> method, but then when I tried to train it the output said the layers didn't have gradients and it never improved… I'll have to look into that, but in the meantime, passing them all in seems to work.</p>
<div class="highlight">
<pre><span></span>model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(
        input_dim=Text.vocabulary_size,
        output_dim=Text.embedding_dim,
        input_length=Text.max_length),
    tensorflow.keras.layers.Conv1D(filters=128,
                                   kernel_size=5,
                                   activation='relu'),
    tensorflow.keras.layers.GlobalMaxPooling1D(),
    tensorflow.keras.layers.Dense(24, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6a0215d">
<h4 id="org6a0215d">Compile It</h4>
<div class="outline-text-4" id="text-org6a0215d">
<div class="highlight">
<pre><span></span>print(model.compile.__doc__)
</pre></div>
<pre class="example">
Configures the model for training.

    Arguments:
        optimizer: String (name of optimizer) or optimizer instance.
            See `tf.keras.optimizers`.
        loss: String (name of objective function), objective function or
            `tf.losses.Loss` instance. See `tf.losses`. If the model has
            multiple outputs, you can use a different loss on each output by
            passing a dictionary or a list of losses. The loss value that will
            be minimized by the model will then be the sum of all individual
            losses.
        metrics: List of metrics to be evaluated by the model during training
            and testing. Typically you will use `metrics=['accuracy']`.
            To specify different metrics for different outputs of a
            multi-output model, you could also pass a dictionary, such as
            `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`.
            You can also pass a list (len = len(outputs)) of lists of metrics
            such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or
            `metrics=['accuracy', ['accuracy', 'mse']]`.
        loss_weights: Optional list or dictionary specifying scalar
            coefficients (Python floats) to weight the loss contributions
            of different model outputs.
            The loss value that will be minimized by the model
            will then be the *weighted sum* of all individual losses,
            weighted by the `loss_weights` coefficients.
            If a list, it is expected to have a 1:1 mapping
            to the model's outputs. If a tensor, it is expected to map
            output names (strings) to scalar coefficients.
        sample_weight_mode: If you need to do timestep-wise
            sample weighting (2D weights), set this to `"temporal"`.
            `None` defaults to sample-wise weights (1D).
            If the model has multiple outputs, you can use a different
            `sample_weight_mode` on each output by passing a
            dictionary or a list of modes.
        weighted_metrics: List of metrics to be evaluated and weighted
            by sample_weight or class_weight during training and testing.
        target_tensors: By default, Keras will create placeholders for the
            model's target, which will be fed with the target data during
            training. If instead you would like to use your own
            target tensors (in turn, Keras will not expect external
            Numpy data for these targets at training time), you
            can specify them via the `target_tensors` argument. It can be
            a single tensor (for a single-output model), a list of tensors,
            or a dict mapping output names to target tensors.
        distribute: NOT SUPPORTED IN TF 2.0, please create and compile the
            model under distribution strategy scope instead of passing it to
            compile.
        **kwargs: Any additional arguments.

    Raises:
        ValueError: In case of invalid arguments for
            `optimizer`, `loss`, `metrics` or `sample_weight_mode`.
    
</pre>
<div class="highlight">
<pre><span></span>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, 120, 16)           16000     
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 116, 128)          10368     
_________________________________________________________________
global_max_pooling1d_2 (Glob (None, 128)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 24)                3096      
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 25        
=================================================================
Total params: 29,489
Trainable params: 29,489
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9f6d11b">
<h3 id="org9f6d11b">Train It</h3>
<div class="outline-text-3" id="text-org9f6d11b">
<div class="highlight">
<pre><span></span>print(model.fit.__doc__)
</pre></div>
<pre class="example">
Trains the model for a fixed number of epochs (iterations on a dataset).

    Arguments:
        x: Input data. It could be:
          - A Numpy array (or array-like), or a list of arrays
            (in case the model has multiple inputs).
          - A TensorFlow tensor, or a list of tensors
            (in case the model has multiple inputs).
          - A dict mapping input names to the corresponding array/tensors,
            if the model has named inputs.
          - A `tf.data` dataset. Should return a tuple
            of either `(inputs, targets)` or
            `(inputs, targets, sample_weights)`.
          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`
            or `(inputs, targets, sample weights)`.
        y: Target data. Like the input data `x`,
          it could be either Numpy array(s) or TensorFlow tensor(s).
          It should be consistent with `x` (you cannot have Numpy inputs and
          tensor targets, or inversely). If `x` is a dataset, generator,
          or `keras.utils.Sequence` instance, `y` should
          not be specified (since targets will be obtained from `x`).
        batch_size: Integer or `None`.
            Number of samples per gradient update.
            If unspecified, `batch_size` will default to 32.
            Do not specify the `batch_size` if your data is in the
            form of symbolic tensors, datasets,
            generators, or `keras.utils.Sequence` instances (since they generate
            batches).
        epochs: Integer. Number of epochs to train the model.
            An epoch is an iteration over the entire `x` and `y`
            data provided.
            Note that in conjunction with `initial_epoch`,
            `epochs` is to be understood as "final epoch".
            The model is not trained for a number of iterations
            given by `epochs`, but merely until the epoch
            of index `epochs` is reached.
        verbose: 0, 1, or 2. Verbosity mode.
            0 = silent, 1 = progress bar, 2 = one line per epoch.
            Note that the progress bar is not particularly useful when
            logged to a file, so verbose=2 is recommended when not running
            interactively (eg, in a production environment).
        callbacks: List of `keras.callbacks.Callback` instances.
            List of callbacks to apply during training.
            See `tf.keras.callbacks`.
        validation_split: Float between 0 and 1.
            Fraction of the training data to be used as validation data.
            The model will set apart this fraction of the training data,
            will not train on it, and will evaluate
            the loss and any model metrics
            on this data at the end of each epoch.
            The validation data is selected from the last samples
            in the `x` and `y` data provided, before shuffling. This argument is
            not supported when `x` is a dataset, generator or
           `keras.utils.Sequence` instance.
        validation_data: Data on which to evaluate
            the loss and any model metrics at the end of each epoch.
            The model will not be trained on this data.
            `validation_data` will override `validation_split`.
            `validation_data` could be:
              - tuple `(x_val, y_val)` of Numpy arrays or tensors
              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays
              - dataset
            For the first two cases, `batch_size` must be provided.
            For the last case, `validation_steps` must be provided.
        shuffle: Boolean (whether to shuffle the training data
            before each epoch) or str (for 'batch').
            'batch' is a special option for dealing with the
            limitations of HDF5 data; it shuffles in batch-sized chunks.
            Has no effect when `steps_per_epoch` is not `None`.
        class_weight: Optional dictionary mapping class indices (integers)
            to a weight (float) value, used for weighting the loss function
            (during training only).
            This can be useful to tell the model to
            "pay more attention" to samples from
            an under-represented class.
        sample_weight: Optional Numpy array of weights for
            the training samples, used for weighting the loss function
            (during training only). You can either pass a flat (1D)
            Numpy array with the same length as the input samples
            (1:1 mapping between weights and samples),
            or in the case of temporal data,
            you can pass a 2D array with shape
            `(samples, sequence_length)`,
            to apply a different weight to every timestep of every sample.
            In this case you should make sure to specify
            `sample_weight_mode="temporal"` in `compile()`. This argument is not
            supported when `x` is a dataset, generator, or
           `keras.utils.Sequence` instance, instead provide the sample_weights
            as the third element of `x`.
        initial_epoch: Integer.
            Epoch at which to start training
            (useful for resuming a previous training run).
        steps_per_epoch: Integer or `None`.
            Total number of steps (batches of samples)
            before declaring one epoch finished and starting the
            next epoch. When training with input tensors such as
            TensorFlow data tensors, the default `None` is equal to
            the number of samples in your dataset divided by
            the batch size, or 1 if that cannot be determined. If x is a
            `tf.data` dataset, and 'steps_per_epoch'
            is None, the epoch will run until the input dataset is exhausted.
            This argument is not supported with array inputs.
        validation_steps: Only relevant if `validation_data` is provided and
            is a `tf.data` dataset. Total number of steps (batches of
            samples) to draw before stopping when performing validation
            at the end of every epoch. If validation_data is a `tf.data` dataset
            and 'validation_steps' is None, validation
            will run until the `validation_data` dataset is exhausted.
        validation_freq: Only relevant if validation data is provided. Integer
            or `collections_abc.Container` instance (e.g. list, tuple, etc.).
            If an integer, specifies how many training epochs to run before a
            new validation run is performed, e.g. `validation_freq=2` runs
            validation every 2 epochs. If a Container, specifies the epochs on
            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs
            validation at the end of the 1st, 2nd, and 10th epochs.
        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`
            input only. Maximum size for the generator queue.
            If unspecified, `max_queue_size` will default to 10.
        workers: Integer. Used for generator or `keras.utils.Sequence` input
            only. Maximum number of processes to spin up
            when using process-based threading. If unspecified, `workers`
            will default to 1. If 0, will execute the generator on the main
            thread.
        use_multiprocessing: Boolean. Used for generator or
            `keras.utils.Sequence` input only. If `True`, use process-based
            threading. If unspecified, `use_multiprocessing` will default to
            `False`. Note that because this implementation relies on
            multiprocessing, you should not pass non-picklable arguments to
            the generator as they can't be passed easily to children processes.
        **kwargs: Used for backwards compatibility.

    Returns:
        A `History` object. Its `History.history` attribute is
        a record of training loss values and metrics values
        at successive epochs, as well as validation loss values
        and validation metrics values (if applicable).

    Raises:
        RuntimeError: If the model was never compiled.
        ValueError: In case of mismatch between the provided input data
            and what the model expects.
    
</pre>
<div class="highlight">
<pre><span></span>with TIMER:
    history = model.fit(training_padded,
                        y_train.values,
                        epochs=Training.epochs,
                        validation_data=(testing_padded, y_test.values),
                        verbose=Training.verbosity)
</pre></div>
<pre class="example">
2019-09-22 16:30:20,369 graeae.timers.timer start: Started: 2019-09-22 16:30:20.369886
I0922 16:30:20.369913 139873020925760 timer.py:70] Started: 2019-09-22 16:30:20.369886
Train on 20031 samples, validate on 6678 samples
Epoch 1/50
20031/20031 - 5s - loss: 0.4741 - accuracy: 0.7623 - val_loss: 0.4033 - val_accuracy: 0.8146
Epoch 2/50
20031/20031 - 4s - loss: 0.3663 - accuracy: 0.8366 - val_loss: 0.3980 - val_accuracy: 0.8196
Epoch 3/50
20031/20031 - 4s - loss: 0.3306 - accuracy: 0.8554 - val_loss: 0.3909 - val_accuracy: 0.8240
Epoch 4/50
20031/20031 - 4s - loss: 0.2990 - accuracy: 0.8721 - val_loss: 0.4148 - val_accuracy: 0.8179
Epoch 5/50
20031/20031 - 4s - loss: 0.2697 - accuracy: 0.8867 - val_loss: 0.4050 - val_accuracy: 0.8282
Epoch 6/50
20031/20031 - 4s - loss: 0.2406 - accuracy: 0.9003 - val_loss: 0.4291 - val_accuracy: 0.8212
Epoch 7/50
20031/20031 - 4s - loss: 0.2080 - accuracy: 0.9165 - val_loss: 0.4650 - val_accuracy: 0.8181
Epoch 8/50
20031/20031 - 4s - loss: 0.1824 - accuracy: 0.9272 - val_loss: 0.5053 - val_accuracy: 0.8130
Epoch 9/50
20031/20031 - 4s - loss: 0.1559 - accuracy: 0.9393 - val_loss: 0.5389 - val_accuracy: 0.8065
Epoch 10/50
20031/20031 - 4s - loss: 0.1325 - accuracy: 0.9498 - val_loss: 0.6213 - val_accuracy: 0.8044
Epoch 11/50
20031/20031 - 4s - loss: 0.1104 - accuracy: 0.9599 - val_loss: 0.6902 - val_accuracy: 0.8034
Epoch 12/50
20031/20031 - 4s - loss: 0.0966 - accuracy: 0.9646 - val_loss: 0.7437 - val_accuracy: 0.8035
Epoch 13/50
20031/20031 - 4s - loss: 0.0848 - accuracy: 0.9689 - val_loss: 0.8285 - val_accuracy: 0.7954
Epoch 14/50
20031/20031 - 4s - loss: 0.0693 - accuracy: 0.9753 - val_loss: 0.9121 - val_accuracy: 0.7934
Epoch 15/50
20031/20031 - 4s - loss: 0.0608 - accuracy: 0.9777 - val_loss: 1.0783 - val_accuracy: 0.7931
Epoch 16/50
20031/20031 - 4s - loss: 0.0529 - accuracy: 0.9810 - val_loss: 1.0620 - val_accuracy: 0.7889
Epoch 17/50
20031/20031 - 4s - loss: 0.0506 - accuracy: 0.9819 - val_loss: 1.2497 - val_accuracy: 0.7889
Epoch 18/50
20031/20031 - 4s - loss: 0.0471 - accuracy: 0.9821 - val_loss: 1.2518 - val_accuracy: 0.7963
Epoch 19/50
20031/20031 - 4s - loss: 0.0457 - accuracy: 0.9819 - val_loss: 1.3492 - val_accuracy: 0.7917
Epoch 20/50
20031/20031 - 4s - loss: 0.0392 - accuracy: 0.9851 - val_loss: 1.3702 - val_accuracy: 0.7948
Epoch 21/50
20031/20031 - 4s - loss: 0.0357 - accuracy: 0.9860 - val_loss: 1.4300 - val_accuracy: 0.7948
Epoch 22/50
20031/20031 - 4s - loss: 0.0341 - accuracy: 0.9864 - val_loss: 1.5654 - val_accuracy: 0.7889
Epoch 23/50
20031/20031 - 4s - loss: 0.0360 - accuracy: 0.9860 - val_loss: 1.5615 - val_accuracy: 0.7951
Epoch 24/50
20031/20031 - 4s - loss: 0.0307 - accuracy: 0.9872 - val_loss: 1.6964 - val_accuracy: 0.7953
Epoch 25/50
20031/20031 - 4s - loss: 0.0283 - accuracy: 0.9893 - val_loss: 1.6917 - val_accuracy: 0.7920
Epoch 26/50
20031/20031 - 4s - loss: 0.0365 - accuracy: 0.9850 - val_loss: 1.6935 - val_accuracy: 0.7944
Epoch 27/50
20031/20031 - 4s - loss: 0.0342 - accuracy: 0.9851 - val_loss: 1.7912 - val_accuracy: 0.7853
Epoch 28/50
20031/20031 - 4s - loss: 0.0301 - accuracy: 0.9879 - val_loss: 1.8194 - val_accuracy: 0.7887
Epoch 29/50
20031/20031 - 4s - loss: 0.0254 - accuracy: 0.9887 - val_loss: 1.9231 - val_accuracy: 0.7922
Epoch 30/50
20031/20031 - 4s - loss: 0.0216 - accuracy: 0.9910 - val_loss: 1.9480 - val_accuracy: 0.7914
Epoch 31/50
20031/20031 - 4s - loss: 0.0243 - accuracy: 0.9895 - val_loss: 1.9487 - val_accuracy: 0.7847
Epoch 32/50
20031/20031 - 4s - loss: 0.0241 - accuracy: 0.9891 - val_loss: 2.0333 - val_accuracy: 0.7893
Epoch 33/50
20031/20031 - 4s - loss: 0.0334 - accuracy: 0.9863 - val_loss: 1.9498 - val_accuracy: 0.7937
Epoch 34/50
20031/20031 - 4s - loss: 0.0318 - accuracy: 0.9873 - val_loss: 2.0181 - val_accuracy: 0.7942
Epoch 35/50
20031/20031 - 4s - loss: 0.0273 - accuracy: 0.9882 - val_loss: 2.0254 - val_accuracy: 0.7913
Epoch 36/50
20031/20031 - 4s - loss: 0.0236 - accuracy: 0.9897 - val_loss: 2.1159 - val_accuracy: 0.7937
Epoch 37/50
20031/20031 - 4s - loss: 0.0204 - accuracy: 0.9905 - val_loss: 2.1018 - val_accuracy: 0.7950
Epoch 38/50
20031/20031 - 4s - loss: 0.0187 - accuracy: 0.9916 - val_loss: 2.1939 - val_accuracy: 0.7947
Epoch 39/50
20031/20031 - 4s - loss: 0.0253 - accuracy: 0.9888 - val_loss: 2.2090 - val_accuracy: 0.7920
Epoch 40/50
20031/20031 - 4s - loss: 0.0270 - accuracy: 0.9889 - val_loss: 2.2737 - val_accuracy: 0.7862
Epoch 41/50
20031/20031 - 4s - loss: 0.0234 - accuracy: 0.9893 - val_loss: 2.2559 - val_accuracy: 0.7926
Epoch 42/50
20031/20031 - 4s - loss: 0.0223 - accuracy: 0.9902 - val_loss: 2.3223 - val_accuracy: 0.7884
Epoch 43/50
20031/20031 - 4s - loss: 0.0251 - accuracy: 0.9897 - val_loss: 2.2547 - val_accuracy: 0.7863
Epoch 44/50
20031/20031 - 4s - loss: 0.0209 - accuracy: 0.9900 - val_loss: 2.3917 - val_accuracy: 0.7823
Epoch 45/50
20031/20031 - 4s - loss: 0.0245 - accuracy: 0.9889 - val_loss: 2.4222 - val_accuracy: 0.7881
Epoch 46/50
20031/20031 - 4s - loss: 0.0215 - accuracy: 0.9901 - val_loss: 2.4135 - val_accuracy: 0.7869
Epoch 47/50
20031/20031 - 4s - loss: 0.0229 - accuracy: 0.9896 - val_loss: 2.3287 - val_accuracy: 0.7823
Epoch 48/50
20031/20031 - 4s - loss: 0.0191 - accuracy: 0.9918 - val_loss: 2.4639 - val_accuracy: 0.7845
Epoch 49/50
20031/20031 - 4s - loss: 0.0183 - accuracy: 0.9911 - val_loss: 2.6068 - val_accuracy: 0.7811
Epoch 50/50
20031/20031 - 4s - loss: 0.0229 - accuracy: 0.9897 - val_loss: 2.5152 - val_accuracy: 0.7928
2019-09-22 16:33:41,089 graeae.timers.timer end: Ended: 2019-09-22 16:33:41.089405
I0922 16:33:41.089459 139873020925760 timer.py:77] Ended: 2019-09-22 16:33:41.089405
2019-09-22 16:33:41,091 graeae.timers.timer end: Elapsed: 0:03:20.719519
I0922 16:33:41.091247 139873020925760 timer.py:78] Elapsed: 0:03:20.719519
</pre>
<p>Once again it looks like the model is overfitting, I should add a checkpoint or something.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orga19541a">
<h3 id="orga19541a">Plot the Performance</h3>
<div class="outline-text-3" id="text-orga19541a">
<div class="highlight">
<pre><span></span>performance = pandas.DataFrame(history.history)
plot = performance.hvplot().opts(title="CNN Sarcasm Training Performance",
                                 width=1000,
                                 height=800)
Embed(plot=plot, file_name="cnn_training")()
</pre></div>
<object data="/posts/keras/he-used-sarcasm/cnn_training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>There's something very wrong with the validation. I'll have to look into that.</p>
<div class="highlight">
<pre><span></span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4f45701">
<h2 id="org4f45701">End</h2>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/multi-layer-lstm/">Multi-Layer LSTM</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/multi-layer-lstm/" rel="bookmark"><time class="published dt-published" datetime="2019-09-19T16:07:27-07:00" itemprop="datePublished" title="2019-09-19 16:07">2019-09-19 16:07</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#orga9f2bc6">Beginning</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#org3e04124">Imports</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#org09fb48e">Python</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgf951eb4">PyPi</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgb6f0dc4">Others</a></li>
</ul>
</li>
<li><a href="/posts/keras/multi-layer-lstm/#org1af9382">Set Up</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#orgf3137ad">The Timer</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#org6df0775">Plotting</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgb04ad78">The Dataset</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/multi-layer-lstm/#org1d1a900">Middle</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#org30d66e8">Set Up the Datasets</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orga6c91ee">The Model</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#orge4933e7">Embedding</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgb62fdaa">Bidirectional</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#org139e67b">LSTM</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgf3a365f">Compile It</a></li>
</ul>
</li>
<li><a href="/posts/keras/multi-layer-lstm/#org6a09993">Train the Model</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#org74e7ee4">Looking at the Performance</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orga9f2bc6">
<h2 id="orga9f2bc6">Beginning</h2>
<div class="outline-text-2" id="text-orga9f2bc6"></div>
<div class="outline-3" id="outline-container-org3e04124">
<h3 id="org3e04124">Imports</h3>
<div class="outline-text-3" id="text-org3e04124"></div>
<div class="outline-4" id="outline-container-org09fb48e">
<h4 id="org09fb48e">Python</h4>
<div class="outline-text-4" id="text-org09fb48e">
<div class="highlight">
<pre><span></span>from functools import partial
from pathlib import Path
import pickle
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf951eb4">
<h4 id="orgf951eb4">PyPi</h4>
<div class="outline-text-4" id="text-orgf951eb4">
<div class="highlight">
<pre><span></span>import holoviews
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb6f0dc4">
<h4 id="orgb6f0dc4">Others</h4>
<div class="outline-text-4" id="text-orgb6f0dc4">
<div class="highlight">
<pre><span></span>from graeae import Timer, EmbedHoloviews
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org1af9382">
<h3 id="org1af9382">Set Up</h3>
<div class="outline-text-3" id="text-org1af9382"></div>
<div class="outline-4" id="outline-container-orgf3137ad">
<h4 id="orgf3137ad">The Timer</h4>
<div class="outline-text-4" id="text-orgf3137ad">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6df0775">
<h4 id="org6df0775">Plotting</h4>
<div class="outline-text-4" id="text-org6df0775">
<div class="highlight">
<pre><span></span>Embed = partial(EmbedHoloviews,
                folder_path="../../files/posts/keras/multi-layer-lstm/")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb04ad78">
<h4 id="orgb04ad78">The Dataset</h4>
<div class="outline-text-4" id="text-orgb04ad78">
<p>This once again uses the <a href="https://www.tensorflow.org/datasets/catalog/imdb_reviews">IMDB dataset</a> with 50,000 reviews. It has already been converted from strings to integers - each word is encoded as its own integer. Adding <code>with_info=True</code> returns an object that contains the dictionary with the word to integer mapping. Passing in <code>imdb_reviews/subwords8k</code> limits the vocabulary to 8,000 words.</p>
<p><b>Note:</b> The first time you run this it will download a fairly large dataset so it might appear to hang, but after the first time it is fairly quick.</p>
<div class="highlight">
<pre><span></span>dataset, info = tensorflow_datasets.load("imdb_reviews/subwords8k",
                                         with_info=True,
                                         as_supervised=True)
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1d1a900">
<h2 id="org1d1a900">Middle</h2>
<div class="outline-text-2" id="text-org1d1a900"></div>
<div class="outline-3" id="outline-container-org30d66e8">
<h3 id="org30d66e8">Set Up the Datasets</h3>
<div class="outline-text-3" id="text-org30d66e8">
<div class="highlight">
<pre><span></span>train_dataset, test_dataset = dataset["train"], dataset["test"]
tokenizer = info.features['text'].encoder
</pre></div>
<p>Now we're going to shuffle and padd the data. The <code>BUFFER_SIZE</code> argument sets the size of the data to sample from. In this case 10,000 entries in the training set will be selected to be put in the buffer and then the "shuffle" is created by randomly selecting items from the buffer, replacing each item as it's selected until all the data has been through the buffer. The <code>padded_batch</code> method creates batches of consecutive data and pads them so that they are all the same shape.</p>
<p>The BATCH_SIZE needs to be tuned a little. If it's too big the amount of memory needed might keep the GPU from being able to use it (and it might not generalize), and if it's too small, you will take a long time to train, so you have to do a little tuning. If you train it and the GPU process percentage stays at 0, try reducing the Batch Size.</p>
<p>Also note that if you change the batch-size you have to go back to the previous step and re-define <code>train_dataset</code> and <code>test_dataset</code> because we alter them in the next step and re-altering them makes the shape wrong somehow.</p>
<div class="highlight">
<pre><span></span>BUFFER_SIZE = 10000
# if the batch size is too big it will run out of memory on the GPU 
# so you might have to experiment with this
BATCH_SIZE = 32

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga6c91ee">
<h3 id="orga6c91ee">The Model</h3>
<div class="outline-text-3" id="text-orga6c91ee">
<p>The previous model had one Bidirectional layer, this will add a second one.</p>
</div>
<div class="outline-4" id="outline-container-orge4933e7">
<h4 id="orge4933e7">Embedding</h4>
<div class="outline-text-4" id="text-orge4933e7">
<p>The <a href="https://www.tensorflow.org/guide/embedding">Embedding layer</a> converts our inputs of integers and converts them to vectors of real-numbers, which is a better input for a neural network.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgb62fdaa">
<h4 id="orgb62fdaa">Bidirectional</h4>
<div class="outline-text-4" id="text-orgb62fdaa">
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional">Bidirectional layer</a> is a wrapper for Recurrent Neural Networks.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org139e67b">
<h4 id="org139e67b">LSTM</h4>
<div class="outline-text-4" id="text-org139e67b">
<p>The <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM">LSTM layer</a> implements Long-Short-Term Memory. The first argument is the size of the outputs. This is similar to the model that we ran previously on the same data, but it has an extra layer (so it uses more memory).</p>
<div class="highlight">
<pre><span></span>model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tensorflow.keras.layers.Bidirectional(
        tensorflow.keras.layers.LSTM(64, return_sequences=True)),
    tensorflow.keras.layers.Bidirectional(
        tensorflow.keras.layers.LSTM(32)),
    tensorflow.keras.layers.Dense(64, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
</pre></div>
<div class="highlight">
<pre><span></span>print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         66048     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 64)                41216     
_________________________________________________________________
dense (Dense)                (None, 64)                4160      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 635,329
Trainable params: 635,329
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgf3a365f">
<h4 id="orgf3a365f">Compile It</h4>
<div class="outline-text-4" id="text-orgf3a365f">
<div class="highlight">
<pre><span></span>model.compile(loss='binary_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org6a09993">
<h3 id="org6a09993">Train the Model</h3>
<div class="outline-text-3" id="text-org6a09993">
<div class="highlight">
<pre><span></span>ONCE_PER_EPOCH = 2
NUM_EPOCHS = 10
with TIMER:
    history = model.fit(train_dataset,
                        epochs=NUM_EPOCHS,
                        validation_data=test_dataset,
                        verbose=ONCE_PER_EPOCH)
</pre></div>
<pre class="example">
2019-09-21 17:26:50,395 graeae.timers.timer start: Started: 2019-09-21 17:26:50.394797
I0921 17:26:50.395130 140275698915136 timer.py:70] Started: 2019-09-21 17:26:50.394797
Epoch 1/10
W0921 17:26:51.400280 140275698915136 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
782/782 - 224s - loss: 0.6486 - accuracy: 0.6039 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/10
782/782 - 214s - loss: 0.4941 - accuracy: 0.7661 - val_loss: 0.6706 - val_accuracy: 0.6744
Epoch 3/10
782/782 - 216s - loss: 0.4087 - accuracy: 0.8266 - val_loss: 0.4024 - val_accuracy: 0.8222
Epoch 4/10
782/782 - 217s - loss: 0.2855 - accuracy: 0.8865 - val_loss: 0.3343 - val_accuracy: 0.8645
Epoch 5/10
782/782 - 216s - loss: 0.2097 - accuracy: 0.9217 - val_loss: 0.2936 - val_accuracy: 0.8837
Epoch 6/10
782/782 - 217s - loss: 0.1526 - accuracy: 0.9467 - val_loss: 0.3188 - val_accuracy: 0.8771
Epoch 7/10
782/782 - 215s - loss: 0.1048 - accuracy: 0.9657 - val_loss: 0.3750 - val_accuracy: 0.8710
Epoch 8/10
782/782 - 216s - loss: 0.0764 - accuracy: 0.9757 - val_loss: 0.3821 - val_accuracy: 0.8762
Epoch 9/10
782/782 - 216s - loss: 0.0585 - accuracy: 0.9832 - val_loss: 0.4747 - val_accuracy: 0.8683
Epoch 10/10
782/782 - 216s - loss: 0.0438 - accuracy: 0.9883 - val_loss: 0.4441 - val_accuracy: 0.8704
2019-09-21 18:02:56,353 graeae.timers.timer end: Ended: 2019-09-21 18:02:56.353722
I0921 18:02:56.353781 140275698915136 timer.py:77] Ended: 2019-09-21 18:02:56.353722
2019-09-21 18:02:56,356 graeae.timers.timer end: Elapsed: 0:36:05.958925
I0921 18:02:56.356238 140275698915136 timer.py:78] Elapsed: 0:36:05.958925
</pre></div>
</div>
<div class="outline-3" id="outline-container-org74e7ee4">
<h3 id="org74e7ee4">Looking at the Performance</h3>
<div class="outline-text-3" id="text-org74e7ee4">
<p>To get the history I had to pickle it and then copy it over to the machine with this org-notebook, so you can't just run this notebook and make it work unless everything is run on the same machine (which it wasn't).</p>
<div class="highlight">
<pre><span></span>path = Path("~/history.pkl").expanduser()
with path.open("wb") as writer:
    pickle.dump(history.history, writer)
</pre></div>
<div class="highlight">
<pre><span></span>path = Path("~/history.pkl").expanduser()
with path.open("rb") as reader:
    history = pickle.load(reader)
</pre></div>
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(history)
best = data.val_loss.idxmin()
best_line = holoviews.VLine(best)
plot = (data.hvplot() * best_line).opts(
    title="Two-Layer LSTM Model",
    width=1000,
    height=800)
Embed(plot=plot, file_name="lstm_training")()
</pre></div>
<object data="/posts/keras/multi-layer-lstm/lstm_training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>It looks like the best epoch was the fifth one, with a validation loss of 0.29 and a validation accuracy of 0.88, after that it looks like it overfits. It seems that text might be a harder problem than images.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/imdb-reviews-tensorflow-dataset/">IMDB Reviews Tensorflow Dataset</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/imdb-reviews-tensorflow-dataset/" rel="bookmark"><time class="published dt-published" datetime="2019-09-09T16:24:46-07:00" itemprop="datePublished" title="2019-09-09 16:24">2019-09-09 16:24</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org858cad4">Beginning</a>
<ul>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org37964d8">Imports</a>
<ul>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org14d7452">Python</a></li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#orge7f4b53">PyPi</a></li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#orgc8669c8">Graeae</a></li>
</ul>
</li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#orge4e6fa6">Set Up</a>
<ul>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org7d26401">Plotting</a></li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org973ffb3">Timer</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org0967f83">Middle</a>
<ul>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org5e22d38">Get the Dataset</a>
<ul>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org0ebb0c7">Load It</a></li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#orgaca3d2c">Split It</a></li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#orgfd0a13e">The Tokenizer</a></li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org2df599a">Set Up Data</a></li>
</ul>
</li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#orgddcd230">The Model</a>
<ul>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org32384d7">Compile It</a></li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#orgfc3e3a0">Train It</a></li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#orgf1f4cb8">Plot the Performance</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org024af60">End</a>
<ul>
<li><a href="/posts/keras/imdb-reviews-tensorflow-dataset/#org81e5eab">Citation</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org858cad4">
<h2 id="org858cad4">Beginning</h2>
<div class="outline-text-2" id="text-org858cad4">
<p>We're going to use the <a href="https://www.tensorflow.org/datasets/catalog/imdb_reviews">IMDB Reviews Dataset</a> (used in <a href="https://www.tensorflow.org/tutorials/keras/basic_text_classification">this tutorial</a>) - a set of 50,000 movie reviews taken from the <a href="https://www.imdb.com/">Internet Movie Database</a> that have been classified as either positive or negative. It looks like the original source is from a page on Stanford University's web sight title <a href="http://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a>. The dataset seems to be widely available (the Stanford page and <a href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews">Kaggle</a> for instance) but this will serve as practice for using tensorflow datasets as well.</p>
</div>
<div class="outline-3" id="outline-container-org37964d8">
<h3 id="org37964d8">Imports</h3>
<div class="outline-text-3" id="text-org37964d8"></div>
<div class="outline-4" id="outline-container-org14d7452">
<h4 id="org14d7452">Python</h4>
<div class="outline-text-4" id="text-org14d7452">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge7f4b53">
<h4 id="orge7f4b53">PyPi</h4>
<div class="outline-text-4" id="text-orge7f4b53">
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">tensorflow</span>
<span class="kn">import</span> <span class="nn">tensorflow_datasets</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc8669c8">
<h4 id="orgc8669c8">Graeae</h4>
<div class="outline-text-4" id="text-orgc8669c8">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orge4e6fa6">
<h3 id="orge4e6fa6">Set Up</h3>
<div class="outline-text-3" id="text-orge4e6fa6"></div>
<div class="outline-4" id="outline-container-org7d26401">
<h4 id="org7d26401">Plotting</h4>
<div class="outline-text-4" id="text-org7d26401">
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"imdb-reviews-tensorflow-dataset"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"../../files/posts/keras/</span><span class="si">{SLUG}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org973ffb3">
<h4 id="org973ffb3">Timer</h4>
<div class="outline-text-4" id="text-org973ffb3">
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0967f83">
<h2 id="org0967f83">Middle</h2>
<div class="outline-text-2" id="text-org0967f83"></div>
<div class="outline-3" id="outline-container-org5e22d38">
<h3 id="org5e22d38">Get the Dataset</h3>
<div class="outline-text-3" id="text-org5e22d38"></div>
<div class="outline-4" id="outline-container-org0ebb0c7">
<h4 id="org0ebb0c7">Load It</h4>
<div class="outline-text-4" id="text-org0ebb0c7">
<p>The <a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/load">load</a> function takes quite a few parameters, in this case we're just passing in three - the name of the dataset, <code>with_info</code> which tells it to return both a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">Dataset</a> and a <a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetInfo">DatasetInfo</a> object, and <code>as_supervised</code>, which tells the builder to return the <code>Dataset</code> as a series of <code>(input, label)</code> tuples.</p>
<div class="highlight">
<pre><span></span><span class="n">dataset</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">tensorflow_datasets</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'imdb_reviews/subwords8k'</span><span class="p">,</span>
                                         <span class="n">with_info</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                         <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgaca3d2c">
<h4 id="orgaca3d2c">Split It</h4>
<div class="outline-text-4" id="text-orgaca3d2c">
<p>The <code>dataset</code> is a dict with three keys:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
<pre class="example">
dict_keys(['test', 'train', 'unsupervised'])
</pre>
<p>As you might guess, we don't use the <code>unsupervised</code> key.</p>
<div class="highlight">
<pre><span></span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'train'</span><span class="p">],</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">'test'</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgfd0a13e">
<h4 id="orgfd0a13e">The Tokenizer</h4>
<div class="outline-text-4" id="text-orgfd0a13e">
<p>One of the advantages of using the tensorflow dataset version of this is that it comes with a pre-built tokenizer inside the DatasetInfo object.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">features</span><span class="p">)</span>
</pre></div>
<pre class="example">
FeaturesDict({
    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),
    'text': Text(shape=(None,), dtype=tf.int64, encoder=&lt;SubwordTextEncoder vocab_size=8185&gt;),
})
</pre>
<div class="highlight">
<pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s1">'text'</span><span class="p">]</span><span class="o">.</span><span class="n">encoder</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
<pre class="example">
&lt;SubwordTextEncoder vocab_size=8185&gt;
</pre>
<p>The <code>tokenizer</code> is a <a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder">SubwordTextEncoder</a> with a vocabulary size of 8,185.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org2df599a">
<h4 id="org2df599a">Set Up Data</h4>
<div class="outline-text-4" id="text-org2df599a">
<p>We're going to shuffle the training data and then add padding to both sets so theyre all the same size.</p>
<div class="highlight">
<pre><span></span><span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">BUFFER_SIZE</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">output_shapes</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">padded_batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">output_shapes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgddcd230">
<h3 id="orgddcd230">The Model</h3>
<div class="outline-text-3" id="text-orgddcd230">
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">)),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'sigmoid'</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, 128)               66048     
_________________________________________________________________
dense (Dense)                (None, 64)                8256      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 598,209
Trainable params: 598,209
Non-trainable params: 0
_________________________________________________________________
</pre></div>
<div class="outline-4" id="outline-container-org32384d7">
<h4 id="org32384d7">Compile It</h4>
<div class="outline-text-4" id="text-org32384d7">
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">'binary_crossentropy'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s1">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgfc3e3a0">
<h4 id="orgfc3e3a0">Train It</h4>
<div class="outline-text-4" id="text-orgfc3e3a0">
<div class="highlight">
<pre><span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">SILENT</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">ONCE_PER_EPOCH</span> <span class="o">=</span> <span class="mi">2</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span>
                        <span class="n">validation_data</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="n">ONCE_PER_EPOCH</span><span class="p">)</span>
</pre></div>
<pre class="example">
2019-09-21 15:52:50,469 graeae.timers.timer start: Started: 2019-09-21 15:52:50.469787
I0921 15:52:50.469841 140086305412928 timer.py:70] Started: 2019-09-21 15:52:50.469787
Epoch 1/10
391/391 - 80s - loss: 0.3991 - accuracy: 0.8377 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/10
391/391 - 80s - loss: 0.3689 - accuracy: 0.8571 - val_loss: 0.4595 - val_accuracy: 0.8021
Epoch 3/10
391/391 - 80s - loss: 0.3664 - accuracy: 0.8444 - val_loss: 0.5262 - val_accuracy: 0.7228
Epoch 4/10
391/391 - 80s - loss: 0.5611 - accuracy: 0.7133 - val_loss: 0.6832 - val_accuracy: 0.6762
Epoch 5/10
391/391 - 80s - loss: 0.6151 - accuracy: 0.6597 - val_loss: 0.5164 - val_accuracy: 0.7844
Epoch 6/10
391/391 - 80s - loss: 0.3842 - accuracy: 0.8340 - val_loss: 0.4970 - val_accuracy: 0.7996
Epoch 7/10
391/391 - 80s - loss: 0.2449 - accuracy: 0.9058 - val_loss: 0.3639 - val_accuracy: 0.8463
Epoch 8/10
391/391 - 80s - loss: 0.1896 - accuracy: 0.9306 - val_loss: 0.3698 - val_accuracy: 0.8614
Epoch 9/10
391/391 - 80s - loss: 0.1555 - accuracy: 0.9456 - val_loss: 0.3896 - val_accuracy: 0.8535
Epoch 10/10
391/391 - 80s - loss: 0.1195 - accuracy: 0.9606 - val_loss: 0.4878 - val_accuracy: 0.8428
2019-09-21 16:06:09,935 graeae.timers.timer end: Ended: 2019-09-21 16:06:09.935707
I0921 16:06:09.935745 140086305412928 timer.py:77] Ended: 2019-09-21 16:06:09.935707
2019-09-21 16:06:09,938 graeae.timers.timer end: Elapsed: 0:13:19.465920
I0921 16:06:09.938812 140086305412928 timer.py:78] Elapsed: 0:13:19.465920
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgf1f4cb8">
<h4 id="orgf1f4cb8">Plot the Performance</h4>
<div class="outline-text-4" id="text-orgf1f4cb8">
<ul class="org-ul">
<li><b>Note</b>: This only works if your kernel is on the local machine, running it remotely gives an error, as it tries to save it on the remote machine.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">"loss"</span><span class="p">:</span> <span class="s2">"Training Loss"</span><span class="p">,</span>
                            <span class="s2">"accuracy"</span><span class="p">:</span> <span class="s2">"Training Accuracy"</span><span class="p">,</span>
                            <span class="s2">"val_loss"</span><span class="p">:</span> <span class="s2">"Validation Loss"</span><span class="p">,</span>
                            <span class="s2">"val_accuracy"</span><span class="p">:</span> <span class="s2">"Validation Accuracy"</span><span class="p">})</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">hvplot</span><span class="p">()</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"LSTM IMDB Performance"</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
<span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"model_performance"</span><span class="p">)()</span>
</pre></div>
<object data="/posts/keras/imdb-reviews-tensorflow-dataset/model_performance.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>It looks like I over-trained it, as the loss is getting high. (Also note that I used this notebook to troubleshoot so there was actually one extra epoch that isn't shown).</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org024af60">
<h2 id="org024af60">End</h2>
<div class="outline-text-2" id="text-org024af60"></div>
<div class="outline-3" id="outline-container-org81e5eab">
<h3 id="org81e5eab">Citation</h3>
<div class="outline-text-3" id="text-org81e5eab">
<p>This is the paper where the dataset was originally used.</p>
<ul class="org-ul">
<li>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).</li>
</ul>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/bbc-news-classification/">BBC News Classification</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/bbc-news-classification/" rel="bookmark"><time class="published dt-published" datetime="2019-08-26T15:28:56-07:00" itemprop="datePublished" title="2019-08-26 15:28">2019-08-26 15:28</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/bbc-news-classification/#org8e63ff1">Beginning</a>
<ul>
<li><a href="/posts/keras/bbc-news-classification/#org5cfba84">Imports</a>
<ul>
<li><a href="/posts/keras/bbc-news-classification/#orge18145f">Python</a></li>
<li><a href="/posts/keras/bbc-news-classification/#org585ddfc">PyPi</a></li>
<li><a href="/posts/keras/bbc-news-classification/#orge422c50">Graeae</a></li>
</ul>
</li>
<li><a href="/posts/keras/bbc-news-classification/#org951c1b0">Setup</a>
<ul>
<li><a href="/posts/keras/bbc-news-classification/#org4870203">The Timer</a></li>
<li><a href="/posts/keras/bbc-news-classification/#org79f9d40">The Environment</a></li>
<li><a href="/posts/keras/bbc-news-classification/#orgced57e1">Spacy</a></li>
<li><a href="/posts/keras/bbc-news-classification/#org3850524">Plotting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/bbc-news-classification/#org7d85b26">Middle</a>
<ul>
<li><a href="/posts/keras/bbc-news-classification/#orgd9ce2d8">Load the Datasets</a></li>
<li><a href="/posts/keras/bbc-news-classification/#org81b3d7e">The Tokenizers</a></li>
<li><a href="/posts/keras/bbc-news-classification/#org69f923e">Making the Sequences</a></li>
<li><a href="/posts/keras/bbc-news-classification/#orga1bcafc">Make training and testing sets</a></li>
<li><a href="/posts/keras/bbc-news-classification/#org39ef6fb">The Model</a></li>
<li><a href="/posts/keras/bbc-news-classification/#org84473ca">Plotting the Performance</a></li>
</ul>
</li>
<li><a href="/posts/keras/bbc-news-classification/#org3fb6604">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org8e63ff1">
<h2 id="org8e63ff1">Beginning</h2>
<div class="outline-text-2" id="text-org8e63ff1"></div>
<div class="outline-3" id="outline-container-org5cfba84">
<h3 id="org5cfba84">Imports</h3>
<div class="outline-text-3" id="text-org5cfba84"></div>
<div class="outline-4" id="outline-container-orge18145f">
<h4 id="orge18145f">Python</h4>
<div class="outline-text-4" id="text-orge18145f">
<div class="highlight">
<pre><span></span>from functools import partial
from pathlib import Path
import csv
import random
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org585ddfc">
<h4 id="org585ddfc">PyPi</h4>
<div class="outline-text-4" id="text-org585ddfc">
<div class="highlight">
<pre><span></span>from sklearn.model_selection import train_test_split
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import hvplot.pandas
import numpy
import pandas
import spacy
import tensorflow
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge422c50">
<h4 id="orge422c50">Graeae</h4>
<div class="outline-text-4" id="text-orge422c50">
<div class="highlight">
<pre><span></span>from graeae import EmbedHoloviews, SubPathLoader, Timer
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org951c1b0">
<h3 id="org951c1b0">Setup</h3>
<div class="outline-text-3" id="text-org951c1b0"></div>
<div class="outline-4" id="outline-container-org4870203">
<h4 id="org4870203">The Timer</h4>
<div class="outline-text-4" id="text-org4870203">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org79f9d40">
<h4 id="org79f9d40">The Environment</h4>
<div class="outline-text-4" id="text-org79f9d40">
<div class="highlight">
<pre><span></span>ENVIRONMENT = SubPathLoader('DATASETS')
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgced57e1">
<h4 id="orgced57e1">Spacy</h4>
<div class="outline-text-4" id="text-orgced57e1">
<div class="highlight">
<pre><span></span>spacy.prefer_gpu()
nlp = spacy.load("en_core_web_lg")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3850524">
<h4 id="org3850524">Plotting</h4>
<div class="outline-text-4" id="text-org3850524">
<div class="highlight">
<pre><span></span>SLUG = "bbc-news-classification"
Embed = partial(EmbedHoloviews, folder_path=f"../../files/posts/keras/{SLUG}")
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7d85b26">
<h2 id="org7d85b26">Middle</h2>
<div class="outline-text-2" id="text-org7d85b26"></div>
<div class="outline-3" id="outline-container-orgd9ce2d8">
<h3 id="orgd9ce2d8">Load the Datasets</h3>
<div class="outline-text-3" id="text-orgd9ce2d8">
<div class="highlight">
<pre><span></span>path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()

texts = []
labels = []
with TIMER:
    with path.open() as csvfile:
        lines = csv.DictReader(csvfile)
        for line in lines:
            labels.append(line["category"])
            texts.append(nlp(line["text"]))
</pre></div>
<pre class="example">
WARNING: Logging before flag parsing goes to stderr.
I0908 13:32:14.804769 139839933974336 environment.py:35] Environment Path: /home/athena/.env
I0908 13:32:14.806000 139839933974336 environment.py:90] Environment Path: /home/athena/.config/datasets/env
2019-09-08 13:32:14,806 graeae.timers.timer start: Started: 2019-09-08 13:32:14.806861
I0908 13:32:14.806965 139839933974336 timer.py:70] Started: 2019-09-08 13:32:14.806861
2019-09-08 13:33:37,430 graeae.timers.timer end: Ended: 2019-09-08 13:33:37.430228
I0908 13:33:37.430259 139839933974336 timer.py:77] Ended: 2019-09-08 13:33:37.430228
2019-09-08 13:33:37,431 graeae.timers.timer end: Elapsed: 0:01:22.623367
I0908 13:33:37.431128 139839933974336 timer.py:78] Elapsed: 0:01:22.623367
</pre>
<div class="highlight">
<pre><span></span>print(texts[random.randrange(len(texts))])
</pre></div>
<pre class="example">
candidate resigns over bnp link a prospective candidate for the uk independence party (ukip) has resigned after admitting a  brief attachment  to the british national party(bnp).  nicholas betts-green  who had been selected to fight the suffolk coastal seat  quit after reports in a newspaper that he attended a bnp meeting. the former teacher confirmed he had attended the meeting but said that was the only contact he had with the group. mr betts-green resigned after being questioned by the party s leadership. a ukip spokesman said mr betts-green s resignation followed disclosures in the east anglian daily times last month about his attendance at a bnp meeting.  he did once attend a bnp meeting. he did not like what he saw and heard and will take no further part of it   the spokesman added. a meeting of suffolk coastal ukip members is due to be held next week to discuss a replacement. mr betts-green  of woodbridge  suffolk  has also resigned as ukip s branch chairman.
</pre>
<p>So, it looks like the text has been lower-cased but there's still punctuation and extra white-space.</p>
<div class="highlight">
<pre><span></span>print(f"Rows: {len(labels):,}")
print(f"Unique Labels: {len(set(labels)):,}")
</pre></div>
<pre class="example">
Rows: 2,225
Unique Labels: 5
</pre>
<p>Since there's only five maybe we should plot it.</p>
<div class="highlight">
<pre><span></span>labels_frame = pandas.DataFrame({"label": labels})
counts = labels_frame.label.value_counts().reset_index().rename(
    columns={"index": "Category", "label": "Articles"})
plot = counts.hvplot.bar("Category", "Articles").opts(
    title="Count of BBC News Articles by Category",
    height=800, width=1000)
Embed(plot=plot, file_name="bbc_category_counts")()
</pre></div>
<object data="/posts/keras/bbc-news-classification/bbc_category_counts.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>It looks like the categories are somewhat unevenly distributed. Now to normalize the tokens.</p>
<div class="highlight">
<pre><span></span>with TIMER:
    cleaned = [[token.lemma_ for token in text if not any((token.is_stop, token.is_space, token.is_punct))]
               for text in texts]
</pre></div>
<pre class="example">
2019-09-08 13:33:40,257 graeae.timers.timer start: Started: 2019-09-08 13:33:40.257908
I0908 13:33:40.257930 139839933974336 timer.py:70] Started: 2019-09-08 13:33:40.257908
2019-09-08 13:33:40,810 graeae.timers.timer end: Ended: 2019-09-08 13:33:40.810135
I0908 13:33:40.810176 139839933974336 timer.py:77] Ended: 2019-09-08 13:33:40.810135
2019-09-08 13:33:40,811 graeae.timers.timer end: Elapsed: 0:00:00.552227
I0908 13:33:40.811067 139839933974336 timer.py:78] Elapsed: 0:00:00.552227
</pre></div>
</div>
<div class="outline-3" id="outline-container-org81b3d7e">
<h3 id="org81b3d7e">The Tokenizers</h3>
<div class="outline-text-3" id="text-org81b3d7e">
<p>Even though I've already tokenized the texts, we need to eventually one-hot-encode them so I'll use the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">tensorflow keras Tokenizer</a>.</p>
<p><b>Note:</b> The labels tokenizer doesn't get the out-of-vocabulary token, only the text-tokenizer does.</p>
<div class="highlight">
<pre><span></span>tokenizer = Tokenizer(num_words=1000, oov_token="&lt;OOV&gt;")
labels_tokenizer = Tokenizer()
labels_tokenizer.fit_on_texts(labels)
</pre></div>
<p>The <code>num_words</code> is the total amount of words that will be kept in the word index - I don't know why a thousand, I just found that in the "answer" notebook. The <code>oov_token</code> is what's used when a word is encountered outside of the words we're building into our word-index (<i>Out Of Vocabulary</i>). The next step is to create the word-index by fitting the tokenizer to the text.</p>
<div class="highlight">
<pre><span></span>with TIMER:
    tokenizer.fit_on_texts(cleaned)
</pre></div>
<pre class="example">
2019-09-08 14:59:30,671 graeae.timers.timer start: Started: 2019-09-08 14:59:30.671536
I0908 14:59:30.671563 139839933974336 timer.py:70] Started: 2019-09-08 14:59:30.671536
2019-09-08 14:59:30,862 graeae.timers.timer end: Ended: 2019-09-08 14:59:30.862483
I0908 14:59:30.862523 139839933974336 timer.py:77] Ended: 2019-09-08 14:59:30.862483
2019-09-08 14:59:30,863 graeae.timers.timer end: Elapsed: 0:00:00.190947
I0908 14:59:30.863504 139839933974336 timer.py:78] Elapsed: 0:00:00.190947
</pre>
<p>The tokenizer now has a dictionary named <code>word_index</code> that holds the words:index pairs for all the tokens found (it only uses the <code>num_words</code> when you call tokenizer's methods according to <a href="https://stackoverflow.com/questions/46202519/keras-tokenizer-num-words-doesnt-seem-to-work">Stack Overflow</a>).</p>
<div class="highlight">
<pre><span></span>print(f"{len(tokenizer.word_index):,}")
</pre></div>
<pre class="example">
24,339
</pre></div>
</div>
<div class="outline-3" id="outline-container-org69f923e">
<h3 id="org69f923e">Making the Sequences</h3>
<div class="outline-text-3" id="text-org69f923e">
<p>I've trained the Tokenizer so that it has a word-index, but now we have to one hot encode our texts and pad them so they're all the same length.</p>
<div class="highlight">
<pre><span></span>MAX_LENGTH = 120
sequences = tokenizer.texts_to_sequences(cleaned)
padded = pad_sequences(sequences, padding="post", maxlen=MAX_LENGTH)
labels_sequenced = labels_tokenizer.texts_to_sequences(labels)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga1bcafc">
<h3 id="orga1bcafc">Make training and testing sets</h3>
<div class="outline-text-3" id="text-orga1bcafc">
<div class="highlight">
<pre><span></span>TESTING = 0.2
x_train, x_test, y_train, y_test = train_test_split(
    padded, labels_sequenced,
    test_size=TESTING)
x_train, x_validation, y_train, y_validation = train_test_split(
    x_train, y_train, test_size=TESTING)

y_train = numpy.array(y_train)
y_test = numpy.array(y_test)
y_validation = numpy.array(y_validation)

print(f"Training: {x_train.shape}")
print(f"Validation: {x_validation.shape}")
print(f"Testing: {x_test.shape}")
</pre></div>
<pre class="example">
Training: (1424, 120)
Validation: (356, 120)
Testing: (445, 120)
</pre>
<p><b>Note:</b> I originally forgot to pass the <code>TESTING</code> variable with the keyword <code>test_size</code> and got an error that I couldn't use a Singleton array - don't forget the keywords when you pass in anything other than the data to <code>train_test_split</code>.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org39ef6fb">
<h3 id="org39ef6fb">The Model</h3>
<div class="outline-text-3" id="text-org39ef6fb">
<div class="highlight">
<pre><span></span>vocabulary_size = 1000
embedding_dimension = 16
max_length=120

model = tensorflow.keras.Sequential([
    layers.Embedding(vocabulary_size, embedding_dimension,
                     input_length=max_length),
    layers.GlobalAveragePooling1D(),
    layers.Dense(24, activation="relu"),
    layers.Dense(6, activation="softmax"),
])
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 120, 16)           16000     
_________________________________________________________________
global_average_pooling1d_1 ( (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 24)                408       
_________________________________________________________________
dense_3 (Dense)              (None, 6)                 150       
=================================================================
Total params: 16,558
Trainable params: 16,558
Non-trainable params: 0
_________________________________________________________________
None
</pre>
<div class="highlight">
<pre><span></span>model.fit(x_train, y_train, epochs=30,
          validation_data=(x_validation, y_validation), verbose=2)
</pre></div>
<pre class="example">
Train on 1424 samples, validate on 356 samples
Epoch 1/30
1424/1424 - 0s - loss: 1.7623 - accuracy: 0.2879 - val_loss: 1.7257 - val_accuracy: 0.5000
Epoch 2/30
1424/1424 - 0s - loss: 1.6871 - accuracy: 0.5190 - val_loss: 1.6332 - val_accuracy: 0.5281
Epoch 3/30
1424/1424 - 0s - loss: 1.5814 - accuracy: 0.4782 - val_loss: 1.5118 - val_accuracy: 0.4944
Epoch 4/30
1424/1424 - 0s - loss: 1.4417 - accuracy: 0.4677 - val_loss: 1.3543 - val_accuracy: 0.5365
Epoch 5/30
1424/1424 - 0s - loss: 1.2706 - accuracy: 0.5934 - val_loss: 1.1850 - val_accuracy: 0.7022
Epoch 6/30
1424/1424 - 0s - loss: 1.1075 - accuracy: 0.6749 - val_loss: 1.0387 - val_accuracy: 0.8006
Epoch 7/30
1424/1424 - 0s - loss: 0.9606 - accuracy: 0.8483 - val_loss: 0.9081 - val_accuracy: 0.8567
Epoch 8/30
1424/1424 - 0s - loss: 0.8244 - accuracy: 0.8869 - val_loss: 0.7893 - val_accuracy: 0.8848
Epoch 9/30
1424/1424 - 0s - loss: 0.6963 - accuracy: 0.9164 - val_loss: 0.6747 - val_accuracy: 0.8961
Epoch 10/30
1424/1424 - 0s - loss: 0.5815 - accuracy: 0.9228 - val_loss: 0.5767 - val_accuracy: 0.9185
Epoch 11/30
1424/1424 - 0s - loss: 0.4831 - accuracy: 0.9375 - val_loss: 0.4890 - val_accuracy: 0.9270
Epoch 12/30
1424/1424 - 0s - loss: 0.3991 - accuracy: 0.9473 - val_loss: 0.4195 - val_accuracy: 0.9326
Epoch 13/30
1424/1424 - 0s - loss: 0.3321 - accuracy: 0.9508 - val_loss: 0.3669 - val_accuracy: 0.9438
Epoch 14/30
1424/1424 - 0s - loss: 0.2800 - accuracy: 0.9572 - val_loss: 0.3268 - val_accuracy: 0.9494
Epoch 15/30
1424/1424 - 0s - loss: 0.2385 - accuracy: 0.9656 - val_loss: 0.2936 - val_accuracy: 0.9438
Epoch 16/30
1424/1424 - 0s - loss: 0.2053 - accuracy: 0.9740 - val_loss: 0.2693 - val_accuracy: 0.9466
Epoch 17/30
1424/1424 - 0s - loss: 0.1775 - accuracy: 0.9761 - val_loss: 0.2501 - val_accuracy: 0.9466
Epoch 18/30
1424/1424 - 0s - loss: 0.1557 - accuracy: 0.9789 - val_loss: 0.2332 - val_accuracy: 0.9494
Epoch 19/30
1424/1424 - 0s - loss: 0.1362 - accuracy: 0.9831 - val_loss: 0.2189 - val_accuracy: 0.9522
Epoch 20/30
1424/1424 - 0s - loss: 0.1209 - accuracy: 0.9853 - val_loss: 0.2082 - val_accuracy: 0.9551
Epoch 21/30
1424/1424 - 0s - loss: 0.1070 - accuracy: 0.9860 - val_loss: 0.1979 - val_accuracy: 0.9579
Epoch 22/30
1424/1424 - 0s - loss: 0.0952 - accuracy: 0.9888 - val_loss: 0.1897 - val_accuracy: 0.9551
Epoch 23/30
1424/1424 - 0s - loss: 0.0854 - accuracy: 0.9902 - val_loss: 0.1815 - val_accuracy: 0.9579
Epoch 24/30
1424/1424 - 0s - loss: 0.0765 - accuracy: 0.9916 - val_loss: 0.1761 - val_accuracy: 0.9522
Epoch 25/30
1424/1424 - 0s - loss: 0.0689 - accuracy: 0.9930 - val_loss: 0.1729 - val_accuracy: 0.9579
Epoch 26/30
1424/1424 - 0s - loss: 0.0618 - accuracy: 0.9951 - val_loss: 0.1680 - val_accuracy: 0.9551
Epoch 27/30
1424/1424 - 0s - loss: 0.0559 - accuracy: 0.9958 - val_loss: 0.1633 - val_accuracy: 0.9551
Epoch 28/30
1424/1424 - 0s - loss: 0.0505 - accuracy: 0.9958 - val_loss: 0.1594 - val_accuracy: 0.9579
Epoch 29/30
1424/1424 - 0s - loss: 0.0457 - accuracy: 0.9965 - val_loss: 0.1559 - val_accuracy: 0.9522
Epoch 30/30
1424/1424 - 0s - loss: 0.0416 - accuracy: 0.9972 - val_loss: 0.1544 - val_accuracy: 0.9551
</pre>
<p>It seems to get good suprisingly fast - it might be overfitting toward the end.</p>
<div class="highlight">
<pre><span></span>loss, accuracy =model.evaluate(x_test, y_test, verbose=0)
print(f"Loss: {loss: .2f} Accuracy: {accuracy:.2f}")
</pre></div>
<pre class="example">
Loss:  0.16 Accuracy: 0.95
</pre>
<p>It does pretty well, even on the test set.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org84473ca">
<h3 id="org84473ca">Plotting the Performance</h3>
<div class="outline-text-3" id="text-org84473ca">
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(model.history.history)
plot = data.hvplot().opts(title="Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="model_performance")()
</pre></div>
<object data="/posts/keras/bbc-news-classification/model_performance.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Unlike with the image classifications, the validation performance never quite matches the training performance (although it's quite good), probably because we aren't doing any kind of augmentation the way you tend to do with images.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org3fb6604">
<h2 id="org3fb6604">End</h2>
<div class="outline-text-2" id="text-org3fb6604">
<p>Okay, so we seem to have a decent model, but is that really the end-game? No, we want to be able to predict what classification a new input should get.</p>
<div class="highlight">
<pre><span></span>index_to_label = {value:key for (key, value) in labels_tokenizer.word_index.items()}

def category(text: str) -&gt; None:
    """Categorizes the text

    Args:
     text: text to categorize
    """
    text = tokenizer.texts_to_sequences([text])
    predictions = model.predict(pad_sequences(text, maxlen=MAX_LENGTH))
    print(f"Predicted Category: {index_to_label[predictions.argmax()]}")
    return
</pre></div>
<div class="highlight">
<pre><span></span>text = "crickets are nutritious and delicious but make for such a silly game"
category(text)
</pre></div>
<pre class="example">
Predicted Category: sport
</pre>
<div class="highlight">
<pre><span></span>text = "i like butts that are big and round, something something like a xxx throw down, and so does the house of parliament"
category(text)
</pre></div>
<pre class="example">
Predicted Category: sport
</pre>
<p>It kind of looks like it's biased toward sports.</p>
<div class="highlight">
<pre><span></span>text = "tv future hand viewer home theatre"
category(text)
</pre></div>
<pre class="example">
Predicted Category: sport
</pre>
<p>Something isn't right here.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/cleaning-the-bbc-news-archive/">Cleaning the BBC News Archive</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/cleaning-the-bbc-news-archive/" rel="bookmark"><time class="published dt-published" datetime="2019-08-25T17:14:54-07:00" itemprop="datePublished" title="2019-08-25 17:14">2019-08-25 17:14</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org93042a0">Beginning</a>
<ul>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org2b7c21b">Imports</a>
<ul>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org4260c54">Python</a></li>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#orga4317bf">PyPi</a></li>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org869296d">Graeae</a></li>
</ul>
</li>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org93f3d39">Set Up</a>
<ul>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org4ce9007">The Environment</a></li>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#orge2ca725">The Timer</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org50477e6">Middle</a>
<ul>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#orgfb3dcdd">The DataSet</a></li>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org8003a91">The Tokenizer</a>
<ul>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org26eef0c">Convert the Texts To Sequences</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org744e9b3">End</a>
<ul>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org802af8d">Sources</a>
<ul>
<li><a href="/posts/keras/cleaning-the-bbc-news-archive/#org2a0e68f">The Original Dataset</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org93042a0">
<h2 id="org93042a0">Beginning</h2>
<div class="outline-text-2" id="text-org93042a0">
<p>This is an initial look at cleaning up <a href="http://mlg.ucd.ie/datasets/bbc.html">a text dataset</a> from the BBC News archives. Although the exercise sites this as the source the dataset provided doesn't look like the actual raw dataset which is broken up into folders that classify the contents and each news item is in a separate file. Instead we're starting with a <a href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv">partially pre-processed</a> CSV that has been lower-cased and the classification is given as the first column in the dataset.</p>
</div>
<div class="outline-3" id="outline-container-org2b7c21b">
<h3 id="org2b7c21b">Imports</h3>
<div class="outline-text-3" id="text-org2b7c21b"></div>
<div class="outline-4" id="outline-container-org4260c54">
<h4 id="org4260c54">Python</h4>
<div class="outline-text-4" id="text-org4260c54">
<div class="highlight">
<pre><span></span>from pathlib import Path
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga4317bf">
<h4 id="orga4317bf">PyPi</h4>
<div class="outline-text-4" id="text-orga4317bf">
<div class="highlight">
<pre><span></span>from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org869296d">
<h4 id="org869296d">Graeae</h4>
<div class="outline-text-4" id="text-org869296d">
<div class="highlight">
<pre><span></span>from graeae import SubPathLoader, Timer
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org93f3d39">
<h3 id="org93f3d39">Set Up</h3>
<div class="outline-text-3" id="text-org93f3d39"></div>
<div class="outline-4" id="outline-container-org4ce9007">
<h4 id="org4ce9007">The Environment</h4>
<div class="outline-text-4" id="text-org4ce9007">
<div class="highlight">
<pre><span></span>ENVIRONMENT = SubPathLoader("DATASETS")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge2ca725">
<h4 id="orge2ca725">The Timer</h4>
<div class="outline-text-4" id="text-orge2ca725">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org50477e6">
<h2 id="org50477e6">Middle</h2>
<div class="outline-text-2" id="text-org50477e6"></div>
<div class="outline-3" id="outline-container-orgfb3dcdd">
<h3 id="orgfb3dcdd">The DataSet</h3>
<div class="outline-text-3" id="text-orgfb3dcdd">
<div class="highlight">
<pre><span></span>bbc_path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()
with TIMER:
    data = pandas.read_csv(bbc_path/"bbc-text.csv")
</pre></div>
<pre class="example">
2019-08-25 18:51:38,411 graeae.timers.timer start: Started: 2019-08-25 18:51:38.411196
2019-08-25 18:51:38,658 graeae.timers.timer end: Ended: 2019-08-25 18:51:38.658181
2019-08-25 18:51:38,658 graeae.timers.timer end: Elapsed: 0:00:00.246985
</pre>
<div class="highlight">
<pre><span></span>print(data.shape)
</pre></div>
<pre class="example">
(2225, 2)
</pre>
<div class="highlight">
<pre><span></span>print(data.sample().iloc[0])
</pre></div>
<pre class="example">
category                                                sport
text        bell set for england debut bath prop duncan be...
Name: 2134, dtype: object
</pre>
<p>So we have two columns - <code>category</code> and <code>text</code>, text being the one we have to clean up.</p>
<div class="highlight">
<pre><span></span>print(data.text.dtype)
</pre></div>
<pre class="example">
object
</pre>
<p>That's not such an informative answer, but I checked and each row of text is a single string.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org8003a91">
<h3 id="org8003a91">The Tokenizer</h3>
<div class="outline-text-3" id="text-org8003a91">
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">Keras Tokenizer</a> tokenizes the text for us as well as removing the punctuation, lower-casing the text, and some other things. We're also going to use a Out-of-Vocabulary token of "&lt;OOV&gt;" to identify words that are outside of the vocabulary when converting new texts to sequences.</p>
<div class="highlight">
<pre><span></span>tokenizer = Tokenizer(oov_token="&lt;OOV&gt;", num_words=100)
tokenizer.fit_on_texts(data.text)
word_index = tokenizer.word_index
print(len(word_index))
</pre></div>
<pre class="example">
29727
</pre>
<p>The word-index is a dict that maps words found in the documents to counts.</p>
</div>
<div class="outline-4" id="outline-container-org26eef0c">
<h4 id="org26eef0c">Convert the Texts To Sequences</h4>
<div class="outline-text-4" id="text-org26eef0c">
<p>We're going to convert each of our texts to a sequence of numbers representing the words in them (one-hot-encoding). The <code>pad_sequences</code> function adds zeros to the end of sequences that are shorter than the longest one so that they are all the same size.</p>
<div class="highlight">
<pre><span></span>sequences = tokenizer.texts_to_sequences(data.text)
padded = pad_sequences(sequences, padding="post")
print(padded[0])
print(padded.shape)
</pre></div>
<pre class="example">
[1 1 7 ... 0 0 0]
(2225, 4491)
</pre>
<p>Strangely there doesn't appear to be a good way to use stopwords. Maybe sklearn is more appropriate here.</p>
<div class="highlight">
<pre><span></span>vectorizer = CountVectorizer(stop_words=stopwords.words("english"),
                             lowercase=True, min_df=3,
                             max_df=0.9, max_features=5000)
vectors = vectorizer.fit_transform(data.text)
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org744e9b3">
<h2 id="org744e9b3">End</h2>
<div class="outline-text-2" id="text-org744e9b3"></div>
<div class="outline-3" id="outline-container-org802af8d">
<h3 id="org802af8d">Sources</h3>
<div class="outline-text-3" id="text-org802af8d"></div>
<div class="outline-4" id="outline-container-org2a0e68f">
<h4 id="org2a0e68f">The Original Dataset</h4>
<div class="outline-text-4" id="text-org2a0e68f">
<ul class="org-ul">
<li>D. Greene and P. Cunningham. "Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering", Proc. ICML 2006. [PDF] [BibTeX].</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/sign-language-exercise/">Sign Language Exercise</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/sign-language-exercise/" rel="bookmark"><time class="published dt-published" datetime="2019-08-25T13:59:38-07:00" itemprop="datePublished" title="2019-08-25 13:59">2019-08-25 13:59</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/sign-language-exercise/#orgf53749f">Beginning</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org4627241">Imports</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org07419c6">Python</a></li>
<li><a href="/posts/keras/sign-language-exercise/#orge725e15">PyPi</a></li>
<li><a href="/posts/keras/sign-language-exercise/#orgafdafa5">Graeae</a></li>
</ul>
</li>
<li><a href="/posts/keras/sign-language-exercise/#org4b02b55">Set Up</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org38f1064">Plotting</a></li>
<li><a href="/posts/keras/sign-language-exercise/#org9518ea5">Timer</a></li>
<li><a href="/posts/keras/sign-language-exercise/#org8ead7fb">The Environment</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/sign-language-exercise/#org71c980b">Middle</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org8f9a251">The Datasets</a></li>
<li><a href="/posts/keras/sign-language-exercise/#org91c939c">Data Generators</a></li>
<li><a href="/posts/keras/sign-language-exercise/#org676c8f8">The Model</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org0faf67c">Train It</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/sign-language-exercise/#org3f07bc0">End</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org547ea47">Source</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf53749f">
<h2 id="orgf53749f">Beginning</h2>
<div class="outline-text-2" id="text-orgf53749f">
<p>This data I'm using is the <a href="https://www.kaggle.com/datamunge/sign-language-mnist/home">Sign-Language MNIST</a> set (hosted on Kaggle). It's a drop-in replacement for the MNIST dataset that contains images of hands showing letters in American Sign Language that was created by taking 1,704 photos of hands showing letters in the alphabet and then using ImageMagick to alter the photos to create a training set with 27,455 images and a test set with 7,172 images.</p>
</div>
<div class="outline-3" id="outline-container-org4627241">
<h3 id="org4627241">Imports</h3>
<div class="outline-text-3" id="text-org4627241"></div>
<div class="outline-4" id="outline-container-org07419c6">
<h4 id="org07419c6">Python</h4>
<div class="outline-text-4" id="text-org07419c6">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge725e15">
<h4 id="orge725e15">PyPi</h4>
<div class="outline-text-4" id="text-orge725e15">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing</span> <span class="kn">import</span> <span class="n">image</span> <span class="k">as</span> <span class="n">keras_image</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="nn">matplotlib_image</span>

<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">tensorflow</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgafdafa5">
<h4 id="orgafdafa5">Graeae</h4>
<div class="outline-text-4" id="text-orgafdafa5">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">SubPathLoader</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org4b02b55">
<h3 id="org4b02b55">Set Up</h3>
<div class="outline-text-3" id="text-org4b02b55"></div>
<div class="outline-4" id="outline-container-org38f1064">
<h4 id="org38f1064">Plotting</h4>
<div class="outline-text-4" id="text-org38f1064">
<div class="highlight">
<pre><span></span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Open Sans"</span><span class="p">,</span> <span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">FIGURE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">EmbedHoloviews</span><span class="p">,</span>
    <span class="n">folder_path</span><span class="o">=</span><span class="s2">"../../files/posts/keras/sign-language-exercise/"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org9518ea5">
<h4 id="org9518ea5">Timer</h4>
<div class="outline-text-4" id="text-org9518ea5">
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8ead7fb">
<h4 id="org8ead7fb">The Environment</h4>
<div class="outline-text-4" id="text-org8ead7fb">
<div class="highlight">
<pre><span></span><span class="n">ENVIRONMENT</span> <span class="o">=</span> <span class="n">SubPathLoader</span><span class="p">(</span><span class="s2">"DATASETS"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org71c980b">
<h2 id="org71c980b">Middle</h2>
<div class="outline-text-2" id="text-org71c980b"></div>
<div class="outline-3" id="outline-container-org8f9a251">
<h3 id="org8f9a251">The Datasets</h3>
<div class="outline-text-3" id="text-org8f9a251">
<div class="highlight">
<pre><span></span><span class="n">root_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">ENVIRONMENT</span><span class="p">[</span><span class="s2">"SIGN_LANGUAGE_MNIST"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">test_or_train</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Gets the MNIST data</span>

<span class="sd">    The pixels are reshaped so that they are 28x28</span>
<span class="sd">    Also, an extra dimension is added to make the shape:</span>
<span class="sd">     (&lt;rows&gt;, 28, 28, 1)</span>

<span class="sd">    Also converts the labels to a categorical (so there are 25 columns)</span>

<span class="sd">    Args:</span>
<span class="sd">     test_or_train: which data set to load</span>

<span class="sd">    Returns: </span>
<span class="sd">     images, labels: numpy arrays with the data</span>
<span class="sd">    """</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">root_path</span><span class="o">/</span><span class="sa">f</span><span class="s2">"sign_mnist_</span><span class="si">{test_or_train}</span><span class="s2">.csv"</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> 
    <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">label</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">pixels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">column</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">"pixel"</span><span class="p">)]]</span>
    <span class="n">pixels</span> <span class="o">=</span> <span class="n">pixels</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(((</span><span class="nb">len</span><span class="p">(</span><span class="n">pixels</span><span class="p">),</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">pixels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pixels</span><span class="p">,</span> <span class="n">labels</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org9248f4d"></a>Training<br>
<div class="outline-text-5" id="text-org9248f4d">
<p>The data is a CSV with the first column being the labels and the rest of the columns holding the pixel values. To make it work with our networks we need to re-shape the data so that we have a shape of (&lt;rows&gt;, 28, 28). The 28 comes from the fact that there are 784 pixel columns(28 x 28 = 784).</p>
<div class="highlight">
<pre><span></span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="s2">"train"</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">train_images</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">27455</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">27455</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
</pre></div>
<pre class="example">
(27455, 25)
(27455, 28, 28, 1)
</pre>
<p>As you can see, there's a lot of columns in the original set. The first one is the "label" and the rest are the "pixel" columns.</p>
<div class="highlight">
<pre><span></span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="s2">"test"</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">test_images</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">7172</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">test_labels</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">7172</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
</pre></div>
<pre class="example">
(7172, 25)
(7172, 28, 28, 1)
</pre>
<p><b>Note:</b> The original exercise calls for doing this with the python <code>csv</code> module. But why?</p>
</div>
</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org91c939c">
<h3 id="org91c939c">Data Generators</h3>
<div class="outline-text-3" id="text-org91c939c">
<div class="highlight">
<pre><span></span><span class="n">training_data_generator</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
    <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span>
    <span class="n">rotation_range</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
    <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fill_mode</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">)</span>

<span class="n">validation_data_generator</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>

<span class="n">train_generator</span> <span class="o">=</span> <span class="n">training_data_generator</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span>
        <span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">validation_generator</span> <span class="o">=</span> <span class="n">validation_data_generator</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span>
        <span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org676c8f8">
<h3 id="org676c8f8">The Model</h3>
<div class="outline-text-3" id="text-org676c8f8">
<p>Part of the exercise requires that we only use two convolutional layers.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1"># Input Layer/convolution</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="c1"># The second convolution</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
    <span class="c1"># Flatten</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="c1"># Fully-connected and output layers</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">),</span>
<span class="p">])</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
<pre class="example">
Model: "sequential_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_12 (Conv2D)           (None, 26, 26, 64)        640       
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 13, 13, 64)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 11, 11, 128)       73856     
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 5, 5, 128)         0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 3200)              0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 3200)              0         
_________________________________________________________________
dense_12 (Dense)             (None, 512)               1638912   
_________________________________________________________________
dense_13 (Dense)             (None, 25)                12825     
=================================================================
Total params: 1,726,233
Trainable params: 1,726,233
Non-trainable params: 0
_________________________________________________________________
</pre></div>
<div class="outline-4" id="outline-container-org0faf67c">
<h4 id="org0faf67c">Train It</h4>
<div class="outline-text-4" id="text-org0faf67c">
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">"categorical_crossentropy"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">"rmsprop"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">"accuracy"</span><span class="p">])</span>
<span class="n">MODELS</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models/sign-language-mnist/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">MODELS</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">MODELS</span><span class="o">/</span><span class="s2">"two-cnn-layers.hdf5"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
    <span class="nb">str</span><span class="p">(</span><span class="n">best_model</span><span class="p">),</span> <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_accuracy"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">train_generator</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">],</span>
                        <span class="n">validation_data</span> <span class="o">=</span> <span class="n">validation_generator</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<pre class="example">
2019-08-25 16:25:13,710 graeae.timers.timer start: Started: 2019-08-25 16:25:13.710604
I0825 16:25:13.710640 140637170140992 timer.py:70] Started: 2019-08-25 16:25:13.710604
Epoch 1/25

Epoch 00001: val_accuracy improved from -inf to 0.45427, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 8s - loss: 2.6016 - accuracy: 0.2048 - val_loss: 1.5503 - val_accuracy: 0.4543
Epoch 2/25

Epoch 00002: val_accuracy improved from 0.45427 to 0.71403, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 1.8267 - accuracy: 0.4160 - val_loss: 0.8762 - val_accuracy: 0.7140
Epoch 3/25

Epoch 00003: val_accuracy improved from 0.71403 to 0.74888, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 1.4297 - accuracy: 0.5323 - val_loss: 0.7413 - val_accuracy: 0.7489
Epoch 4/25

Epoch 00004: val_accuracy improved from 0.74888 to 0.76157, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 1.1984 - accuracy: 0.6100 - val_loss: 0.6402 - val_accuracy: 0.7616
Epoch 5/25

Epoch 00005: val_accuracy improved from 0.76157 to 0.84816, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 1.0498 - accuracy: 0.6570 - val_loss: 0.4581 - val_accuracy: 0.8482
Epoch 6/25

Epoch 00006: val_accuracy improved from 0.84816 to 0.85778, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.9340 - accuracy: 0.6944 - val_loss: 0.4195 - val_accuracy: 0.8578
Epoch 7/25

Epoch 00007: val_accuracy improved from 0.85778 to 0.90240, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.8522 - accuracy: 0.7189 - val_loss: 0.3270 - val_accuracy: 0.9024
Epoch 8/25

Epoch 00008: val_accuracy did not improve from 0.90240
858/858 - 7s - loss: 0.7963 - accuracy: 0.7410 - val_loss: 0.3144 - val_accuracy: 0.8887
Epoch 9/25

Epoch 00009: val_accuracy did not improve from 0.90240
858/858 - 7s - loss: 0.7388 - accuracy: 0.7560 - val_loss: 0.3184 - val_accuracy: 0.8984
Epoch 10/25

Epoch 00010: val_accuracy improved from 0.90240 to 0.92777, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.7127 - accuracy: 0.7692 - val_loss: 0.2045 - val_accuracy: 0.9278
Epoch 11/25

Epoch 00011: val_accuracy improved from 0.92777 to 0.93572, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 9s - loss: 0.6798 - accuracy: 0.7792 - val_loss: 0.1813 - val_accuracy: 0.9357
Epoch 12/25

Epoch 00012: val_accuracy improved from 0.93572 to 0.94046, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.6506 - accuracy: 0.7875 - val_loss: 0.1857 - val_accuracy: 0.9405
Epoch 13/25

Epoch 00013: val_accuracy improved from 0.94046 to 0.94074, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.6365 - accuracy: 0.7941 - val_loss: 0.1691 - val_accuracy: 0.9407
Epoch 14/25

Epoch 00014: val_accuracy improved from 0.94074 to 0.95706, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.6127 - accuracy: 0.8028 - val_loss: 0.1426 - val_accuracy: 0.9571
Epoch 15/25

Epoch 00015: val_accuracy did not improve from 0.95706
858/858 - 7s - loss: 0.6009 - accuracy: 0.8076 - val_loss: 0.1925 - val_accuracy: 0.9265
Epoch 16/25

Epoch 00016: val_accuracy improved from 0.95706 to 0.96207, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.5883 - accuracy: 0.8121 - val_loss: 0.1393 - val_accuracy: 0.9621
Epoch 17/25

Epoch 00017: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5785 - accuracy: 0.8127 - val_loss: 0.2188 - val_accuracy: 0.9250
Epoch 18/25

Epoch 00018: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5728 - accuracy: 0.8158 - val_loss: 0.2003 - val_accuracy: 0.9350
Epoch 19/25

Epoch 00019: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5633 - accuracy: 0.8225 - val_loss: 0.1452 - val_accuracy: 0.9578
Epoch 20/25

Epoch 00020: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5536 - accuracy: 0.8223 - val_loss: 0.1341 - val_accuracy: 0.9605
Epoch 21/25

Epoch 00021: val_accuracy did not improve from 0.96207
858/858 - 8s - loss: 0.5477 - accuracy: 0.8252 - val_loss: 0.1500 - val_accuracy: 0.9442
Epoch 22/25

Epoch 00022: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5367 - accuracy: 0.8291 - val_loss: 0.1435 - val_accuracy: 0.9568
Epoch 23/25

Epoch 00023: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5425 - accuracy: 0.8336 - val_loss: 0.1598 - val_accuracy: 0.9615
Epoch 24/25

Epoch 00024: val_accuracy did not improve from 0.96207
858/858 - 8s - loss: 0.5243 - accuracy: 0.8330 - val_loss: 0.1749 - val_accuracy: 0.9483
Epoch 25/25

Epoch 00025: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5163 - accuracy: 0.8379 - val_loss: 0.1353 - val_accuracy: 0.9587
2019-08-25 16:28:20,707 graeae.timers.timer end: Ended: 2019-08-25 16:28:20.707567
I0825 16:28:20.707660 140637170140992 timer.py:77] Ended: 2019-08-25 16:28:20.707567
2019-08-25 16:28:20,712 graeae.timers.timer end: Elapsed: 0:03:06.996963
I0825 16:28:20.712478 140637170140992 timer.py:78] Elapsed: 0:03:06.996963
</pre>
<div class="highlight">
<pre><span></span><span class="n">predictor</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">best_model</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">hvplot</span><span class="p">()</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Sign Language MNIST Training and Validation"</span><span class="p">,</span>
                          <span class="n">fontsize</span><span class="o">=</span><span class="p">{</span><span class="s2">"title"</span><span class="p">:</span> <span class="mi">16</span><span class="p">},</span>
                          <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
<span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"training"</span><span class="p">)()</span>
</pre></div>
<object data="/posts/keras/sign-language-exercise/training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>I'm not sure why these small networks do so well, bit this one seems to be doing fairly well.</p>
<div class="highlight">
<pre><span></span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="o">=</span><span class="n">predictor</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Loss: </span><span class="si">{loss:.2f}</span><span class="s2">, Accuracy: </span><span class="si">{accuracy:.2f}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Loss: 4.36, Accuracy: 0.72
</pre>
<p>So, actually, the performance drops quite a bit outside of the training, even though I'm using the same data-set.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org3f07bc0">
<h2 id="org3f07bc0">End</h2>
<div class="outline-text-2" id="text-org3f07bc0"></div>
<div class="outline-3" id="outline-container-org547ea47">
<h3 id="org547ea47">Source</h3>
<div class="outline-text-3" id="text-org547ea47">
<ul class="org-ul">
<li>The exercise comes from <a href="https://github.com/lmoroney/dlaicourse/tree/master/Exercises/Exercise%208%20-%20Multiclass%20with%20Signs">DLAIcourse Exercise 8</a> - Multiclass With Signs</li>
<li>The Data Set comes from <a href="https://www.kaggle.com/datamunge/sign-language-mnist/home">Kaggle</a></li>
</ul>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="/posts/keras/rock-paper-scissors/">Rock-Paper-Scissors</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/rock-paper-scissors/" rel="bookmark"><time class="published dt-published" datetime="2019-08-19T15:16:52-07:00" itemprop="datePublished" title="2019-08-19 15:16">2019-08-19 15:16</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org1c38a2a">Beginning</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org57a9744">Imports</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org027f33d">Python</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#org7b8c8eb">PyPi</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#orge2f49a1">graeae</a></li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#org82b17cf">Set Up</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org9e402a9">Plotting</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#org8c9b846">The Timer</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#orgc51667d">The Environment</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#org6a0debc">Middle</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#orgec23a3a">The Data</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#orga0f1b84">Downloading it</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#orgb09867f">Some Examples</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#org2508a05">Data Generators</a></li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#org548a53c">A Four-CNN Model</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#orga901181">Definition</a></li>
<li><a href="/posts/keras/rock-paper-scissors/#org6f892b7">Compile and Fit</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#org032027c">End</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#org81ae583">Some Test Images</a>
<ul>
<li><a href="/posts/keras/rock-paper-scissors/#orgb5289bc">What If we re-train the model, will it get better?</a></li>
</ul>
</li>
<li><a href="/posts/keras/rock-paper-scissors/#org901842b">Sources</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org1c38a2a">
<h2 id="org1c38a2a">Beginning</h2>
<div class="outline-text-2" id="text-org1c38a2a"></div>
<div class="outline-3" id="outline-container-org57a9744">
<h3 id="org57a9744">Imports</h3>
<div class="outline-text-3" id="text-org57a9744"></div>
<div class="outline-4" id="outline-container-org027f33d">
<h4 id="org027f33d">Python</h4>
<div class="outline-text-4" id="text-org027f33d">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">random</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org7b8c8eb">
<h4 id="org7b8c8eb">PyPi</h4>
<div class="outline-text-4" id="text-org7b8c8eb">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing</span> <span class="kn">import</span> <span class="n">image</span> <span class="k">as</span> <span class="n">keras_image</span>
<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="nn">matplotlib_image</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">tensorflow</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge2f49a1">
<h4 id="orge2f49a1">graeae</h4>
<div class="outline-text-4" id="text-orge2f49a1">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">SubPathLoader</span><span class="p">,</span> <span class="n">Timer</span><span class="p">,</span> <span class="n">ZipDownloader</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org82b17cf">
<h3 id="org82b17cf">Set Up</h3>
<div class="outline-text-3" id="text-org82b17cf"></div>
<div class="outline-4" id="outline-container-org9e402a9">
<h4 id="org9e402a9">Plotting</h4>
<div class="outline-text-4" id="text-org9e402a9">
<div class="highlight">
<pre><span></span><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'matplotlib'</span><span class="p">,</span> <span class="s1">'inline'</span><span class="p">)</span>
<span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">'config'</span><span class="p">,</span> <span class="s2">"InlineBackend.figure_format = 'retina'"</span><span class="p">)</span>
<span class="n">seaborn</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">"whitegrid"</span><span class="p">,</span>
            <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">"axes.grid"</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
                <span class="s2">"font.family"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"sans-serif"</span><span class="p">],</span>
                <span class="s2">"font.sans-serif"</span><span class="p">:</span> <span class="p">[</span><span class="s2">"Open Sans"</span><span class="p">,</span> <span class="s2">"Latin Modern Sans"</span><span class="p">,</span> <span class="s2">"Lato"</span><span class="p">],</span>
                <span class="s2">"figure.figsize"</span><span class="p">:</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)},</span>
            <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">FIGURE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span>
                <span class="n">folder_path</span><span class="o">=</span><span class="s2">"../../files/posts/keras/rock-paper-scissors/"</span><span class="p">)</span>
<span class="n">holoviews</span><span class="o">.</span><span class="n">extension</span><span class="p">(</span><span class="s2">"bokeh"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8c9b846">
<h4 id="org8c9b846">The Timer</h4>
<div class="outline-text-4" id="text-org8c9b846">
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc51667d">
<h4 id="orgc51667d">The Environment</h4>
<div class="outline-text-4" id="text-orgc51667d">
<div class="highlight">
<pre><span></span><span class="n">ENVIRONMENT</span> <span class="o">=</span> <span class="n">SubPathLoader</span><span class="p">(</span><span class="s2">"DATASETS"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org6a0debc">
<h2 id="org6a0debc">Middle</h2>
<div class="outline-text-2" id="text-org6a0debc"></div>
<div class="outline-3" id="outline-container-orgec23a3a">
<h3 id="orgec23a3a">The Data</h3>
<div class="outline-text-3" id="text-orgec23a3a"></div>
<div class="outline-4" id="outline-container-orga0f1b84">
<h4 id="orga0f1b84">Downloading it</h4>
<div class="outline-text-4" id="text-orga0f1b84">
<div class="highlight">
<pre><span></span><span class="n">TRAINING_URL</span> <span class="o">=</span> <span class="s2">"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip"</span>
<span class="n">TEST_URL</span> <span class="o">=</span> <span class="s2">"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip"</span>
<span class="n">OUT_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">ENVIRONMENT</span><span class="p">[</span><span class="s2">"ROCK_PAPER_SCISSORS"</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">download_train</span> <span class="o">=</span> <span class="n">ZipDownloader</span><span class="p">(</span><span class="n">TRAINING_URL</span><span class="p">,</span> <span class="n">OUT_PATH</span><span class="o">/</span><span class="s2">"train"</span><span class="p">)</span>
<span class="n">download_test</span> <span class="o">=</span> <span class="n">ZipDownloader</span><span class="p">(</span><span class="n">TEST_URL</span><span class="p">,</span> <span class="n">OUT_PATH</span><span class="o">/</span><span class="s2">"test"</span><span class="p">)</span>
<span class="n">download_train</span><span class="p">()</span>
<span class="n">download_test</span><span class="p">()</span>
</pre></div>
<pre class="example">
I0825 15:06:11.577721 139626236733248 environment.py:35] Environment Path: /home/athena/.env
I0825 15:06:11.578975 139626236733248 environment.py:90] Environment Path: /home/athena/.config/datasets/env
Files exist, not downloading
Files exist, not downloading
</pre>
<p>The data structure for the folders is fairly deep, so I'll make some shortcuts.</p>
<div class="highlight">
<pre><span></span><span class="n">TRAINING</span> <span class="o">=</span> <span class="n">OUT_PATH</span><span class="o">/</span><span class="s2">"train/rps"</span>
<span class="n">rocks</span> <span class="o">=</span> <span class="n">TRAINING</span><span class="o">/</span><span class="s2">"rock"</span>
<span class="n">papers</span> <span class="o">=</span> <span class="n">TRAINING</span><span class="o">/</span><span class="s2">"paper"</span>
<span class="n">scissors</span> <span class="o">=</span> <span class="n">TRAINING</span><span class="o">/</span><span class="s2">"scissors"</span>
<span class="k">assert</span> <span class="n">papers</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">rocks</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">scissors</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">rock_images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">rocks</span><span class="o">.</span><span class="n">iterdir</span><span class="p">())</span>
<span class="n">paper_images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">papers</span><span class="o">.</span><span class="n">iterdir</span><span class="p">())</span>
<span class="n">scissors_images</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">scissors</span><span class="o">.</span><span class="n">iterdir</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Rocks: {len(rock_images):,}"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Papers: {len(paper_images):,}"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Scissors: {len(scissors_images):,}"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Rocks: 840
Papers: 840
Scissors: 840
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgb09867f">
<h4 id="orgb09867f">Some Examples</h4>
<div class="outline-text-4" id="text-orgb09867f">
<div class="highlight">
<pre><span></span><span class="n">count</span><span class="o">=</span><span class="mi">1</span>
<span class="n">rock_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">rock_images</span><span class="p">[:</span><span class="n">count</span><span class="p">])</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">matplotlib_image</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">rock_sample</span><span class="p">))</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Rock"</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'Off'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="rock.png" src="/posts/keras/rock-paper-scissors/rock.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">paper_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">paper_images</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">matplotlib_image</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">paper_sample</span><span class="p">))</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Paper"</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'Off'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="paper.png" src="/posts/keras/rock-paper-scissors/paper.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">scissors_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">scissors_images</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">matplotlib_image</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">scissors_sample</span><span class="p">))</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Scissors"</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'Off'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="scissors.png" src="/posts/keras/rock-paper-scissors/scissors.png"></p>
</div>
</div>
</div>
<div class="outline-4" id="outline-container-org2508a05">
<h4 id="org2508a05">Data Generators</h4>
<div class="outline-text-4" id="text-org2508a05">
<p><b>Note:</b> I was originally using <code>keras_preprocessing.image.ImageDataGenerator</code> and getting</p>
<div class="highlight">
<pre><span></span><span class="ne">AttributeError</span><span class="p">:</span> <span class="s1">'DirectoryIterator'</span> <span class="nb">object</span> <span class="n">has</span> <span class="n">no</span> <span class="n">attribute</span> <span class="s1">'shape'</span>
</pre></div>
<p>Make sure to use <code>tensorflow.keras.preprocessing.image.ImageDataGenerator</code> instead.</p>
<div class="highlight">
<pre><span></span><span class="n">VALIDATION</span> <span class="o">=</span> <span class="n">OUT_PATH</span><span class="o">/</span><span class="s2">"test/rps-test-set"</span>
<span class="n">training_data_generator</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
      <span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">,</span>
          <span class="n">rotation_range</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
      <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">horizontal_flip</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
      <span class="n">fill_mode</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">)</span>

<span class="n">validation_data_generator</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">rescale</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="mi">255</span><span class="p">)</span>

<span class="n">train_generator</span> <span class="o">=</span> <span class="n">training_data_generator</span><span class="o">.</span><span class="n">flow_from_directory</span><span class="p">(</span>
        <span class="n">TRAINING</span><span class="p">,</span>
        <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span><span class="mi">150</span><span class="p">),</span>
        <span class="n">class_mode</span><span class="o">=</span><span class="s1">'categorical'</span>
<span class="p">)</span>

<span class="n">validation_generator</span> <span class="o">=</span> <span class="n">validation_data_generator</span><span class="o">.</span><span class="n">flow_from_directory</span><span class="p">(</span>
        <span class="n">VALIDATION</span><span class="p">,</span>
        <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span><span class="mi">150</span><span class="p">),</span>
        <span class="n">class_mode</span><span class="o">=</span><span class="s1">'categorical'</span>
<span class="p">)</span>
</pre></div>
<pre class="example">
Found 2520 images belonging to 3 classes.
Found 372 images belonging to 3 classes.
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org548a53c">
<h3 id="org548a53c">A Four-CNN Model</h3>
<div class="outline-text-3" id="text-org548a53c"></div>
<div class="outline-4" id="outline-container-orga901181">
<h4 id="orga901181">Definition</h4>
<div class="outline-text-4" id="text-orga901181">
<p>This is a hand-crafted, relatively shallow Convolutional Neural Network. The input shape matches our <code>target_size</code> arguments for the data-generators. There are four convolutional layers with a filter size of 3 x 3 each follewd by a max-pooling layer. The first two layers have 64 nodes while the two following those have 128 nodes. The convolution layers are followed by a layer to flatten the input and add dropout before reaching our fully connected and output layer which uses softmax to predict the most likely category. Since we have three categories (rock, paper, or scissors) the final layer has three nodes.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1"># Input Layer/convolution</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="c1"># The second convolution</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
    <span class="c1"># The third convolution</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
    <span class="c1"># The fourth convolution</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span>
    <span class="c1"># Flatten</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="c1"># Fully-connected and output layers</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="p">),</span>
    <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">'softmax'</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
<p>Here's a summary of the layers.</p>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
<pre class="example">
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_16 (Conv2D)           (None, 148, 148, 64)      1792      
_________________________________________________________________
max_pooling2d_16 (MaxPooling (None, 74, 74, 64)        0         
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 72, 72, 64)        36928     
_________________________________________________________________
max_pooling2d_17 (MaxPooling (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_18 (MaxPooling (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_19 (Conv2D)           (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_19 (MaxPooling (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_4 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_4 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_8 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_9 (Dense)              (None, 3)                 1539      
=================================================================
Total params: 3,473,475
Trainable params: 3,473,475
Non-trainable params: 0
_________________________________________________________________
</pre>
<p>You can see that the convolutional layers lose two pixels on output, so the filters are stopping when their edges match the image (so the 3 x 3 filter stops with the center one pixel away from the edge of the image). Additionally, our max-pooling layers are cutting the size of the convolutional layers' output in half, so as we progress through the network the inputs are getting smaller and smaller before reaching the fully-connected layers.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org6f892b7">
<h4 id="org6f892b7">Compile and Fit</h4>
<div class="outline-text-4" id="text-org6f892b7">
<p>Now we need to compile and train the model.</p>
<p><b>Note:</b> The metrics can change with your settings - make sure the <code>monitor=</code> parameter is pointing to a key in the history. If you see this in the output:</p>
<pre class="example">
Can save best model only with val_acc available, skipping.
</pre>
<p>You might have the wrong name for your metric (it isn't <code>val_acc</code>).</p>
<div class="highlight">
<pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span> <span class="o">=</span> <span class="s1">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">'rmsprop'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">])</span>
<span class="n">MODELS</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models/rock-paper-scissors/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">MODELS</span><span class="o">/</span><span class="s2">"four-layer-cnn.hdf5"</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="n">tensorflow</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
    <span class="nb">str</span><span class="p">(</span><span class="n">best_model</span><span class="p">),</span> <span class="n">monitor</span><span class="o">=</span><span class="s2">"val_accuracy"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
    <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">train_generator</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">],</span>
                        <span class="n">validation_data</span> <span class="o">=</span> <span class="n">validation_generator</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<pre class="example">
2019-08-25 15:06:17,145 graeae.timers.timer start: Started: 2019-08-25 15:06:17.145536
I0825 15:06:17.145575 139626236733248 timer.py:70] Started: 2019-08-25 15:06:17.145536
Epoch 1/25

Epoch 00001: val_accuracy improved from -inf to 0.61559, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 15s - loss: 1.1174 - accuracy: 0.3996 - val_loss: 0.8997 - val_accuracy: 0.6156
Epoch 2/25

Epoch 00002: val_accuracy improved from 0.61559 to 0.93817, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.8115 - accuracy: 0.6381 - val_loss: 0.2403 - val_accuracy: 0.9382
Epoch 3/25

Epoch 00003: val_accuracy improved from 0.93817 to 0.97043, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.5604 - accuracy: 0.7750 - val_loss: 0.2333 - val_accuracy: 0.9704
Epoch 4/25

Epoch 00004: val_accuracy improved from 0.97043 to 0.98387, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.3926 - accuracy: 0.8496 - val_loss: 0.0681 - val_accuracy: 0.9839
Epoch 5/25

Epoch 00005: val_accuracy improved from 0.98387 to 0.99194, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.2746 - accuracy: 0.8925 - val_loss: 0.0395 - val_accuracy: 0.9919
Epoch 6/25

Epoch 00006: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.2018 - accuracy: 0.9246 - val_loss: 0.1427 - val_accuracy: 0.9328
Epoch 7/25

Epoch 00007: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.2052 - accuracy: 0.9238 - val_loss: 0.4212 - val_accuracy: 0.8253
Epoch 8/25

Epoch 00008: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1649 - accuracy: 0.9460 - val_loss: 0.1079 - val_accuracy: 0.9597
Epoch 9/25

Epoch 00009: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1678 - accuracy: 0.9452 - val_loss: 0.0782 - val_accuracy: 0.9597
Epoch 10/25

Epoch 00010: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1388 - accuracy: 0.9508 - val_loss: 0.0425 - val_accuracy: 0.9731
Epoch 11/25

Epoch 00011: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1207 - accuracy: 0.9611 - val_loss: 0.0758 - val_accuracy: 0.9570
Epoch 12/25

Epoch 00012: val_accuracy did not improve from 0.99194
79/79 - 14s - loss: 0.1195 - accuracy: 0.9639 - val_loss: 0.1392 - val_accuracy: 0.9489
Epoch 13/25

Epoch 00013: val_accuracy improved from 0.99194 to 1.00000, saving model to /home/athena/models/rock-paper-scissors/four-layer-cnn.hdf5
79/79 - 14s - loss: 0.1182 - accuracy: 0.9583 - val_loss: 0.0147 - val_accuracy: 1.0000
Epoch 14/25

Epoch 00014: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0959 - accuracy: 0.9722 - val_loss: 0.1264 - val_accuracy: 0.9543
Epoch 15/25

Epoch 00015: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.1225 - accuracy: 0.9643 - val_loss: 0.1124 - val_accuracy: 0.9677
Epoch 16/25

Epoch 00016: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0959 - accuracy: 0.9706 - val_loss: 0.0773 - val_accuracy: 0.9677
Epoch 17/25

Epoch 00017: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0817 - accuracy: 0.9687 - val_loss: 0.0120 - val_accuracy: 1.0000
Epoch 18/25

Epoch 00018: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.1308 - accuracy: 0.9627 - val_loss: 0.1058 - val_accuracy: 0.9758
Epoch 19/25

Epoch 00019: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0967 - accuracy: 0.9675 - val_loss: 0.0356 - val_accuracy: 0.9866
Epoch 20/25

Epoch 00020: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0785 - accuracy: 0.9726 - val_loss: 0.0474 - val_accuracy: 0.9704
Epoch 21/25

Epoch 00021: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0962 - accuracy: 0.9710 - val_loss: 0.0774 - val_accuracy: 0.9677
Epoch 22/25

Epoch 00022: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0802 - accuracy: 0.9754 - val_loss: 0.1592 - val_accuracy: 0.9516
Epoch 23/25

Epoch 00023: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0909 - accuracy: 0.9714 - val_loss: 0.1123 - val_accuracy: 0.9382
Epoch 24/25

Epoch 00024: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0573 - accuracy: 0.9782 - val_loss: 0.0609 - val_accuracy: 0.9785
Epoch 25/25

Epoch 00025: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0860 - accuracy: 0.9778 - val_loss: 0.1106 - val_accuracy: 0.9677
2019-08-25 15:12:11,360 graeae.timers.timer end: Ended: 2019-08-25 15:12:11.360361
I0825 15:12:11.360388 139626236733248 timer.py:77] Ended: 2019-08-25 15:12:11.360361
2019-08-25 15:12:11,361 graeae.timers.timer end: Elapsed: 0:05:54.214825
I0825 15:12:11.361701 139626236733248 timer.py:78] Elapsed: 0:05:54.214825
</pre>
<p>That did surprisingly well… is it really that easy a problem?</p>
<div class="highlight">
<pre><span></span><span class="n">predictor</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">best_model</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">hvplot</span><span class="p">()</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Rock, Paper, Scissors Training and Validation"</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
<span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"training"</span><span class="p">)()</span>
</pre></div>
<object data="/posts/keras/rock-paper-scissors/training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Looking at the validation accuracy it appears that it starts to overfit at the end. Strangely, the validation loss, up until the overfitting, is lower than the training loss, and the validation accuracy is better almost throughout - perhaps this is because the image augmentation for the training set is too hard.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org032027c">
<h2 id="org032027c">End</h2>
<div class="outline-text-2" id="text-org032027c"></div>
<div class="outline-3" id="outline-container-org81ae583">
<h3 id="org81ae583">Some Test Images</h3>
<div class="outline-text-3" id="text-org81ae583">
<div class="highlight">
<pre><span></span><span class="n">base</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/test_images"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">paper</span> <span class="o">=</span> <span class="n">base</span><span class="o">/</span><span class="s2">"Rock-paper-scissors_(paper).png"</span>

<span class="n">image_</span> <span class="o">=</span> <span class="n">matplotlib_image</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">paper</span><span class="p">))</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Paper Test Case"</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'Off'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="test_paper.png" src="/posts/keras/rock-paper-scissors/test_paper.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">classifications</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s2">"Paper"</span><span class="p">,</span> <span class="s2">"Rock"</span><span class="p">,</span> <span class="s2">"Scissors"</span><span class="p">)))</span>
<span class="n">image_</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">paper</span><span class="p">),</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">image_</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classifications</span><span class="p">[</span><span class="n">classes</span><span class="o">.</span><span class="n">argmax</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Paper
</pre>
<div class="highlight">
<pre><span></span><span class="n">base</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/test_images"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">rock</span> <span class="o">=</span> <span class="n">base</span><span class="o">/</span><span class="s2">"Rock-paper-scissors_(rock).png"</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">matplotlib_image</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">rock</span><span class="p">))</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Rock Test Case"</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'Off'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="test_rock.png" src="/posts/keras/rock-paper-scissors/test_rock.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">base</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/test_images"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">rock</span> <span class="o">=</span> <span class="n">base</span><span class="o">/</span><span class="s2">"Rock-paper-scissors_(rock).png"</span>
<span class="n">image_</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">rock</span><span class="p">),</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">image_</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classifications</span><span class="p">[</span><span class="n">classes</span><span class="o">.</span><span class="n">argmax</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Rock
</pre>
<div class="highlight">
<pre><span></span><span class="n">base</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/test_images"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">scissors</span> <span class="o">=</span> <span class="n">base</span><span class="o">/</span><span class="s2">"Rock-paper-scissors_(scissors).png"</span>

<span class="n">image</span> <span class="o">=</span> <span class="n">matplotlib_image</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">scissors</span><span class="p">))</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"Scissors Test Case"</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'Off'</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
<div class="figure">
<p><img alt="test_scissors.png" src="/posts/keras/rock-paper-scissors/test_scissors.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">image_</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">scissors</span><span class="p">),</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">image_</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classifications</span><span class="p">[</span><span class="n">classes</span><span class="o">.</span><span class="n">argmax</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Paper
</pre></div>
<div class="outline-4" id="outline-container-orgb5289bc">
<h4 id="orgb5289bc">What If we re-train the model, will it get better?</h4>
<div class="outline-text-4" id="text-orgb5289bc">
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">train_generator</span><span class="p">,</span>
                        <span class="n">epochs</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">],</span>
                        <span class="n">validation_data</span> <span class="o">=</span> <span class="n">validation_generator</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<pre class="example">
2019-08-25 15:21:37,706 graeae.timers.timer start: Started: 2019-08-25 15:21:37.706175
I0825 15:21:37.706199 139626236733248 timer.py:70] Started: 2019-08-25 15:21:37.706175
Epoch 1/25

Epoch 00001: val_accuracy did not improve from 1.00000
79/79 - 15s - loss: 0.0792 - accuracy: 0.9798 - val_loss: 0.1101 - val_accuracy: 0.9543
Epoch 2/25

Epoch 00002: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0691 - accuracy: 0.9798 - val_loss: 0.1004 - val_accuracy: 0.9570
Epoch 3/25

Epoch 00003: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0850 - accuracy: 0.9762 - val_loss: 0.0098 - val_accuracy: 1.0000
Epoch 4/25

Epoch 00004: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0799 - accuracy: 0.9730 - val_loss: 0.1022 - val_accuracy: 0.9409
Epoch 5/25

Epoch 00005: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0767 - accuracy: 0.9758 - val_loss: 0.1134 - val_accuracy: 0.9328
Epoch 6/25

Epoch 00006: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0747 - accuracy: 0.9833 - val_loss: 0.0815 - val_accuracy: 0.9731
Epoch 7/25

Epoch 00007: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0680 - accuracy: 0.9817 - val_loss: 0.1476 - val_accuracy: 0.9059
Epoch 8/25

Epoch 00008: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0669 - accuracy: 0.9821 - val_loss: 0.0202 - val_accuracy: 0.9866
Epoch 9/25

Epoch 00009: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0809 - accuracy: 0.9774 - val_loss: 0.3860 - val_accuracy: 0.8844
Epoch 10/25

Epoch 00010: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0583 - accuracy: 0.9817 - val_loss: 0.0504 - val_accuracy: 0.9812
Epoch 11/25

Epoch 00011: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0691 - accuracy: 0.9806 - val_loss: 0.0979 - val_accuracy: 0.9624
Epoch 12/25

Epoch 00012: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0459 - accuracy: 0.9881 - val_loss: 0.1776 - val_accuracy: 0.9167
Epoch 13/25

Epoch 00013: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0648 - accuracy: 0.9821 - val_loss: 0.0770 - val_accuracy: 0.9435
Epoch 14/25

Epoch 00014: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0549 - accuracy: 0.9825 - val_loss: 0.0075 - val_accuracy: 1.0000
Epoch 15/25

Epoch 00015: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0575 - accuracy: 0.9829 - val_loss: 0.1787 - val_accuracy: 0.9167
Epoch 16/25

Epoch 00016: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0665 - accuracy: 0.9778 - val_loss: 0.0230 - val_accuracy: 0.9866
Epoch 17/25

Epoch 00017: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0557 - accuracy: 0.9825 - val_loss: 0.0431 - val_accuracy: 0.9785
Epoch 18/25

Epoch 00018: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0628 - accuracy: 0.9817 - val_loss: 0.2121 - val_accuracy: 0.8952
Epoch 19/25

Epoch 00019: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0580 - accuracy: 0.9841 - val_loss: 0.0705 - val_accuracy: 0.9651
Epoch 20/25

Epoch 00020: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0578 - accuracy: 0.9810 - val_loss: 0.3318 - val_accuracy: 0.8925
Epoch 21/25

Epoch 00021: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0500 - accuracy: 0.9821 - val_loss: 0.2106 - val_accuracy: 0.8925
Epoch 22/25

Epoch 00022: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0520 - accuracy: 0.9829 - val_loss: 0.1040 - val_accuracy: 0.9382
Epoch 23/25

Epoch 00023: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0693 - accuracy: 0.9853 - val_loss: 0.6132 - val_accuracy: 0.8575
Epoch 24/25

Epoch 00024: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0553 - accuracy: 0.9849 - val_loss: 0.3048 - val_accuracy: 0.8817
Epoch 25/25

Epoch 00025: val_accuracy did not improve from 1.00000
79/79 - 14s - loss: 0.0376 - accuracy: 0.9877 - val_loss: 0.0121 - val_accuracy: 1.0000
2019-08-25 15:27:32,278 graeae.timers.timer end: Ended: 2019-08-25 15:27:32.278250
I0825 15:27:32.278276 139626236733248 timer.py:77] Ended: 2019-08-25 15:27:32.278250
2019-08-25 15:27:32,279 graeae.timers.timer end: Elapsed: 0:05:54.572075
I0825 15:27:32.279404 139626236733248 timer.py:78] Elapsed: 0:05:54.572075
</pre>
<p>So, your validation went up to 100%, is it a super-classifier?</p>
<div class="highlight">
<pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">hvplot</span><span class="p">()</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Rock, Paper, Scissors Re-Training and Re-Validation"</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
<span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"re_training"</span><span class="p">)()</span>
</pre></div>
<object data="/posts/keras/rock-paper-scissors/re_training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<div class="highlight">
<pre><span></span><span class="n">predictor</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">best_model</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">classifications</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s2">"Paper"</span><span class="p">,</span> <span class="s2">"Rock"</span><span class="p">,</span> <span class="s2">"Scissors"</span><span class="p">)))</span>
<span class="n">image_</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">paper</span><span class="p">),</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">image_</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classifications</span><span class="p">[</span><span class="n">classes</span><span class="o">.</span><span class="n">argmax</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Rock
</pre>
<div class="highlight">
<pre><span></span><span class="n">classifications</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s2">"Paper"</span><span class="p">,</span> <span class="s2">"Rock"</span><span class="p">,</span> <span class="s2">"Scissors"</span><span class="p">)))</span>
<span class="n">image_</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">rock</span><span class="p">),</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">image_</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classifications</span><span class="p">[</span><span class="n">classes</span><span class="o">.</span><span class="n">argmax</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Rock
</pre>
<div class="highlight">
<pre><span></span><span class="n">classifications</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="s2">"Paper"</span><span class="p">,</span> <span class="s2">"Rock"</span><span class="p">,</span> <span class="s2">"Scissors"</span><span class="p">)))</span>
<span class="n">image_</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">load_img</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">scissors</span><span class="p">),</span> <span class="n">target_size</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras_image</span><span class="o">.</span><span class="n">img_to_array</span><span class="p">(</span><span class="n">image_</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classifications</span><span class="p">[</span><span class="n">classes</span><span class="o">.</span><span class="n">argmax</span><span class="p">()])</span>
</pre></div>
<pre class="example">
Paper
</pre>
<p>I don't have a large test set, but just from these three it seems like the model got worse.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org901842b">
<h3 id="org901842b">Sources</h3>
<div class="outline-text-3" id="text-org901842b">
<ul class="org-ul">
<li>The <a href="http://www.laurencemoroney.com/rock-paper-scissors-dataset/">Rock-Paper-Scissors</a> dataset was created by Laurence Moroney (lmoroney@gmail.com / laurencemoroney.com).</li>
<li>The test images came from the Wikipedia article on the <a href="https://en.wikipedia.org/wiki/Rock%E2%80%93paper%E2%80%93scissors?oldformat=true">Rock-paper-scissors game</a>.</li>
</ul>
</div>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="next"><a href="/index-9.html" rel="next">Older posts</a></li>
</ul>
<!--End of body content-->
<footer id="footer">Contents © 2020 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
