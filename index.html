<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neurotic Networking</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/" rel="canonical">
<link href="index-15.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
<link href="posts/nlp/word-embeddings-visualizing-the-embeddings/" rel="prefetch" type="text/html">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/Neurotic-Networking/"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item active"><a class="nav-link" href=".">Cloistered Monkey <span class="sr-only">(active)</span></a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-visualizing-the-embeddings/">Word Embeddings: Visualizing the Embeddings</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/" rel="bookmark"><time class="published dt-published" datetime="2020-12-16T15:44:25-08:00" itemprop="datePublished" title="2020-12-16 15:44">2020-12-16 15:44</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#org03849a3">Extracting and Visualizing the Embeddings</a>
<ul>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#orgd34c266">Imports</a></li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#orgfee9e98">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#org17fd13c">Middle</a>
<ul>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#orgb015ac8">Set It Up</a></li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#org506af30">Visualizing</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#org353c78b">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org03849a3">
<h2 id="org03849a3">Extracting and Visualizing the Embeddings</h2>
<div class="outline-text-2" id="text-org03849a3">
<p>In the <a href="posts/nlp/word-embeddings-training-the-model/">previous post</a> we built a Continuous Bag of Words model to predict a word based on the fraction of words each word surrounding it made up within a window (e.g. the fraction of the four words surrounding the word that each word made up). Now we're going to use the weights of the model as word embeddings and see if we can visualize them.</p>
</div>
<div class="outline-3" id="outline-container-orgd34c266">
<h3 id="orgd34c266">Imports</h3>
<div class="outline-text-3" id="text-orgd34c266">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Batches</span><span class="p">,</span>
    <span class="n">CBOW</span><span class="p">,</span>
    <span class="n">DataCleaner</span><span class="p">,</span>
    <span class="n">MetaData</span><span class="p">,</span>
    <span class="n">TheTrainer</span><span class="p">,</span>
    <span class="p">)</span>
<span class="c1"># my other stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfee9e98">
<h3 id="orgfee9e98">Set Up</h3>
<div class="outline-text-3" id="text-orgfee9e98">
<div class="highlight">
<pre><span></span><span class="n">cleaner</span> <span class="o">=</span> <span class="n">DataCleaner</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">MetaData</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">)</span>
<span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">speak</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"word-embeddings-visualizing-the-embeddings"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layer</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="p">,</span> <span class="n">emit_point</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">trainer</span><span class="p">()</span>
</pre></div>
<pre class="example">
2020-12-16 16:32:17,189 graeae.timers.timer start: Started: 2020-12-16 16:32:17.189213
50: loss=9.88889093658385
new learning rate: 0.0198
100: loss=9.138356897918037
150: loss=9.149555378031549
new learning rate: 0.013068000000000001
200: loss=9.077599951734605
2020-12-16 16:32:37,403 graeae.timers.timer end: Ended: 2020-12-16 16:32:37.403860
2020-12-16 16:32:37,405 graeae.timers.timer end: Elapsed: 0:00:20.214647
250: loss=8.607763835003631
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">best_loss</span><span class="p">)</span>
</pre></div>
<pre class="example">
8.186490214727549
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org17fd13c">
<h2 id="org17fd13c">Middle</h2>
<div class="outline-text-2" id="text-org17fd13c"></div>
<div class="outline-3" id="outline-container-orgb015ac8">
<h3 id="orgb015ac8">Set It Up</h3>
<div class="outline-text-3" id="text-orgb015ac8">
<p>We're going to use the method of averaging the weights of the two layers to form the embeddings.</p>
<div class="highlight">
<pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">best_weights</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">T</span>
              <span class="o">+</span> <span class="n">trainer</span><span class="o">.</span><span class="n">best_weights</span><span class="o">.</span><span class="n">hidden_weights</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</pre></div>
<p>And now our words.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"queen"</span><span class="p">,</span><span class="s2">"lord"</span><span class="p">,</span><span class="s2">"man"</span><span class="p">,</span> <span class="s2">"woman"</span><span class="p">,</span><span class="s2">"dog"</span><span class="p">,</span><span class="s2">"wolf"</span><span class="p">,</span>
         <span class="s2">"rich"</span><span class="p">,</span><span class="s2">"happy"</span><span class="p">,</span><span class="s2">"sad"</span><span class="p">]</span>
</pre></div>
<p>Now we need to translate the words into their indices so we can grab the rows in the mebedding that match.</p>
<div class="highlight">
<pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span> 
</pre></div>
<pre class="example">
(10, 50) [2745, 3951, 2961, 3023, 5675, 1452, 5674, 4191, 2316, 4278]
</pre>
<p>There are 10 rows to match our ten words and 50 columns to match the number chosen for the hidden layer.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org506af30">
<h3 id="org506af30">Visualizing</h3>
<div class="outline-text-3" id="text-org506af30">
<p>We're going to use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn's PCA</a> for Principal Component Analysis. The <code>n_components</code> argument is the number of components it will keep - we'll keep 2.</p>
<div class="highlight">
<pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">pca_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">reduced</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"X"</span><span class="p">,</span> <span class="s2">"Y"</span><span class="p">])</span>

<span class="n">pca_data</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">points</span> <span class="o">=</span> <span class="n">pca_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span>
                                 <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">pca_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">,</span> <span class="n">text_baseline</span><span class="o">=</span><span class="s2">"top"</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span> <span class="o">*</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"PCA Embeddings"</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"embeddings_pca"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/word-embeddings-visualizing-the-embeddings/embeddings_pca.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Well, that's pretty horrible. Might need work.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org353c78b">
<h2 id="org353c78b">End</h2>
<div class="outline-text-2" id="text-org353c78b">
<p>This is the final post in the series looking at using a Continuous Bag of Words model to create word embeddings. Here are the other posts.</p>
<ul class="org-ul">
<li><a href="posts/nlp/word-embeddings-build-a-model/">Introduction</a></li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/">Loading the Data</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/">Building and Training the CBOW Model</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-training-the-model/">Word Embeddings: Training the Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-training-the-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-13T14:42:07-08:00" itemprop="datePublished" title="2020-12-13 14:42">2020-12-13 14:42</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org986f45c">Building and Training the Model</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgaa81b3f">Imports</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org8dc2457">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org1b2d688">Middle</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgad79579">Initializing the model</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org8b8ca1f">Softmax</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orge700c7e">The Implementation</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org3223630">Forward propagation</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org1be6091">Test the function</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org2b000cb">Pack Index with Frequency</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org0356cea">Vector Generator</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org25b9ad7">Batch Generator</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgae21cba">Cost function</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org25dc24e">Test the function</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org3ee5986">Training the Model - Backpropagation</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgab7da20">Test the function</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org059e46b">Gradient Descent</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgb2616b9">Test Your Function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org43d696d">End</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgccfb6ac">Bundling It Up</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org2468a12">Imports</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org1a2d415">Enum Setup</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgd9a05e6">Named Tuples</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgddc8337">The CBOW Model</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org523b283">Batch Generator</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgdf5bd77">The Trainer</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org61b16e9">Testing It</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgcb06669">Forward Propagation</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org092b1a9">Cross Entropy Loss</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org2920311">Back Propagation</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgd418f8c">Putting Some Stuff Together</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org9401058">The Batches</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org27ec8e3">Gradient Descent</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org32d7b00">Gradient Re-do</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org5ee93f1">Troubleshooting the Batches</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org986f45c">
<h2 id="org986f45c">Building and Training the Model</h2>
<div class="outline-text-2" id="text-org986f45c">
<p>In the <a href="posts/nlp/word-embeddings-shakespeare-data/">previous post</a> we did some preliminary set up and data pre-processing. Now we're going to build and train a Continuous Bag of Words (CBOW) model.</p>
</div>
<div class="outline-3" id="outline-container-orgaa81b3f">
<h3 id="orgaa81b3f">Imports</h3>
<div class="outline-text-3" id="text-orgaa81b3f">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">unique</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">contain_exactly</span><span class="p">,</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">DataCleaner</span><span class="p">,</span> <span class="n">MetaData</span>

<span class="c1"># my other stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8dc2457">
<h3 id="org8dc2457">Set Up</h3>
<div class="outline-text-3" id="text-org8dc2457">
<p>Code from the previous post.</p>
<div class="highlight">
<pre><span></span><span class="n">cleaner</span> <span class="o">=</span> <span class="n">DataCleaner</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">MetaData</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">speak</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="s2">"files/posts/nlp/word-embeddings-training-the-model"</span><span class="p">)</span>
<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
<p>Something to help remember what the numpy <code>axis</code> argument is.</p>
<div class="highlight">
<pre><span></span><span class="nd">@unique</span>
<span class="k">class</span> <span class="nc">Axis</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1b2d688">
<h2 id="org1b2d688">Middle</h2>
<div class="outline-text-2" id="text-org1b2d688"></div>
<div class="outline-3" id="outline-container-orgad79579">
<h3 id="orgad79579">Initializing the model</h3>
<div class="outline-text-3" id="text-orgad79579">
<p>You will now initialize two matrices and two vectors.</p>
<ul class="org-ul">
<li>The first matrix (\(W_1\)) is of dimension \(N \times V\), where <i>V</i> is the number of words in your vocabulary and <i>N</i> is the dimension of your word vector.</li>
<li>The second matrix (\(W_2\)) is of dimension \(V \times N\).</li>
<li>Vector \(b_1\) has dimensions \(N\times 1\)</li>
<li>Vector \(b_2\) has dimensions \(V\times 1\).</li>
<li>\(b_1\) and \(b_2\) are the bias vectors of the linear layers from matrices \(W_1\) and \(W_2\).</li>
</ul>
<p>At this stage we are just initializing the parameters.</p>
<p>Please use <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html">numpy.random.rand</a> to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: initialize_model</span>
<span class="k">def</span> <span class="nf">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span><span class="n">V</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">random_seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Initialize the matrices with random values</span>

<span class="sd">    Args: </span>
<span class="sd">       N:  dimension of hidden vector </span>
<span class="sd">       V:  dimension of vocabulary</span>
<span class="sd">       random_seed: random seed for consistent results in the unit tests</span>
<span class="sd">     Returns: </span>
<span class="sd">       W1, W2, b1, b2: initialized weights and biases</span>
<span class="sd">    """</span>

    <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>
    <span class="c1"># W1 has shape (N,V)</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
    <span class="c1"># W2 has shape (V,N)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="c1"># b1 has shape (N,1)</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># b2 has shape (V,1)</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span>
</pre></div>
<p>Test your function example.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_V</span><span class="p">,</span><span class="n">tmp_N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape: </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape: </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape: </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape: </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
tmp_W1.shape: (4, 10)
tmp_W2.shape: (10, 4)
tmp_b1.shape: (4, 1)
tmp_b2.shape: (10, 1)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org8b8ca1f">
<h3 id="org8b8ca1f">Softmax</h3>
<div class="outline-text-3" id="text-org8b8ca1f">
<p>Before we can start training the model, we need to implement the softmax function as defined in equation 5:</p>
<p>\[ \text{softmax}(z_i) = \frac{e^{z_i} }{\sum_{i=0}^{V-1} e^{z_i} } \tag{5} \]</p>
<ul class="org-ul">
<li>Array indexing in code starts at 0.</li>
<li><i>V</i> is the number of words in the vocabulary (which is also the number of rows of <i>z</i>).</li>
<li><i>i</i> goes from 0 to |V| - 1.</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orge700c7e">
<h4 id="orge700c7e">The Implementation</h4>
<div class="outline-text-4" id="text-orge700c7e">
<ul class="org-ul">
<li>Assume that the input <i>z</i> to <code>softmax</code> is a 2D array</li>
<li>Each training example is represented by a column of shape (V, 1) in this 2D array.</li>
<li>There may be more than one column, in the 2D array, because you can put in a batch of examples to increase efficiency. Let's call the batch size lowercase <i>m</i>, so the <i>z</i> array has shape (V, m)</li>
<li>When taking the sum from \(i=1 \cdots V-1\), take the sum for each column (each example) separately.</li>
</ul>
<p>Please use</p>
<ul class="org-ul">
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">numpy.exp</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html">numpy.sum</a> (set the axis so that you take the sum of each column in z)</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: softmax</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate the softmax</span>

<span class="sd">    Args: </span>
<span class="sd">       z: output scores from the hidden layer</span>
<span class="sd">    Returns: </span>
<span class="sd">       yhat: prediction (estimate of y)</span>
<span class="sd">    """</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>

    <span class="c1"># Calculate yhat (softmax)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">/</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">ROWS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">yhat</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1"># Test the function</span>
<span class="n">tmp</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
                   <span class="p">])</span>
<span class="n">tmp_sm</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.73105858</span><span class="p">,</span> <span class="mf">0.88079708</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.26894142</span><span class="p">,</span> <span class="mf">0.11920292</span><span class="p">]])</span>


<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.5        0.73105858 0.88079708]
 [0.5        0.26894142 0.11920292]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org3223630">
<h3 id="org3223630">Forward propagation</h3>
<div class="outline-text-3" id="text-org3223630">
<p>We're going to implement the forward propagation <i>z</i> according to equations (1) to (3).</p>
\begin{align} h &amp;= W_1 \ X + b_1 \tag{1} \\ a &amp;= ReLU(h) \tag{2} \\ z &amp;= W_2 \ a + b_2 \tag{3} \\ \end{align}
<p>For that, you will use as activation the Rectified Linear Unit (ReLU) given by:</p>
<p>\[ f(h)=\max (0,h) \tag{6} \]</p>
<p><b>Hints:</b></p>
<ul class="org-ul">
<li>You can use <a href="https://numpy.org/doc/stable/reference/generated/numpy.maximum.html">numpy.maximum(x1,x2)</a> to get the maximum of two values</li>
<li>Use <a href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html">numpy.dot(A,B)</a> to matrix multiply A and B</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: forward_prop</span>
<span class="k">def</span> <span class="nf">forward_prop</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">W1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">b1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Pass the data through the network</span>

<span class="sd">    Args: </span>
<span class="sd">       x:  average one hot vector for the context </span>
<span class="sd">       W1, W2, b1, b2:  matrices and biases to be learned</span>
<span class="sd">    Returns: </span>
<span class="sd">       z:  output score vector</span>
<span class="sd">    """</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>

    <span class="c1"># Calculate h</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>

    <span class="c1"># Apply the relu on h (store result in h)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Calculate z</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">h</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org1be6091">
<h4 id="org1be6091">Test the function</h4>
<div class="outline-text-4" id="text-org1be6091">
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">tmp_x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>

<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">V</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"x has shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"N is </span><span class="si">{</span><span class="n">tmp_N</span><span class="si">}</span><span class="s2"> and vocabulary size V is </span><span class="si">{</span><span class="n">tmp_V</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"call forward_prop"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"z has shape </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"z has values:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"h has shape </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"h has values:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.55379268</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.58960774</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.50722933</span><span class="p">]]</span>
<span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.92477674</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.02487333</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
x has shape (3, 1)
N is 2 and vocabulary size V is 3
call forward_prop

z has shape (3, 1)
z has values:
[[0.55379268]
 [1.58960774]
 [1.50722933]]

h has shape (2, 1)
h has values:
[[0.92477674]
 [1.02487333]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2b000cb">
<h3 id="org2b000cb">Pack Index with Frequency</h3>
<div class="outline-text-3" id="text-org2b000cb">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">index_with_frequency</span><span class="p">(</span><span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                              <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""combines indexes and frequency counts-dict</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to get the indices for</span>
<span class="sd">     word_to_index: mapping of word to index</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of (word-index, word-count) tuples built from context_words</span>
<span class="sd">    """</span>
    <span class="n">frequency_dict</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="n">packed</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)):</span>
        <span class="n">word_index</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">frequency</span> <span class="o">=</span> <span class="n">frequency_dict</span><span class="p">[</span><span class="n">context_words</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span>
        <span class="n">packed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">packed</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0356cea">
<h3 id="org0356cea">Vector Generator</h3>
<div class="outline-text-3" id="text-org0356cea">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">vectors</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">"""Generates vectors of fraction of context words each word represents</span>

<span class="sd">    Args:</span>
<span class="sd">     data: source of the vectors</span>
<span class="sd">     word_to_index: mapping of word to index in the vocabulary</span>
<span class="sd">     half_window: number of tokens on either side of the word to keep</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of x, y </span>
<span class="sd">    """</span>
    <span class="n">location</span> <span class="o">=</span> <span class="n">half_window</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">location</span><span class="p">]</span>
        <span class="n">y</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">):</span> <span class="n">location</span><span class="p">]</span>
                         <span class="o">+</span> <span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">location</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

        <span class="k">for</span> <span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span> <span class="ow">in</span> <span class="n">index_with_frequency</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">frequency</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
        <span class="n">location</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">location</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"location in data is being set to 0"</span><span class="p">)</span>
            <span class="n">location</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org25b9ad7">
<h3 id="org25b9ad7">Batch Generator</h3>
<div class="outline-text-3" id="text-org25b9ad7">
<p>This uses a not so common form of the <a href="https://docs.python.org/3/reference/compound_stmts.html#while">while</a> loop. Whenever you run a loop and it reaches the end (so you didn't break it) then it will run the <code>else</code> clause.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                    <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">original</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">"""Generate batches of vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the training data</span>
<span class="sd">     word_to_index: map of word to vocabulary index</span>
<span class="sd">     half_window: number of tokens to take from either side of word</span>
<span class="sd">     batch_size: Number of vectors to put in each training batch</span>
<span class="sd">     original: run the original buggy code</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of X, Y batches</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                        <span class="n">word_to_index</span><span class="p">,</span>
                        <span class="n">half_window</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">original</span><span class="p">:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
                <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span>
</pre></div>
<p>So every time <code>batch_x</code> reaches the <code>batch_size</code> it yields the tuple and then creates a new batch before continuing the outer for-loop.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgae21cba">
<h3 id="orgae21cba">Cost function</h3>
<div class="outline-text-3" id="text-orgae21cba">
<p>The cross-entropy loss function.</p>
<ul class="org-ul">
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html">numpy.squeeze</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.multiply.html">numpy.multiply</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.log.html">numpy.log</a></li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the cross-entropy loss</span>

<span class="sd">    Args:</span>
<span class="sd">     y: array with the actual words labeled</span>
<span class="sd">     y_hat: our model's guesses for the words</span>
<span class="sd">     batch_size: the number of examples per training run</span>
<span class="sd">    """</span>
    <span class="n">log_probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
                         <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org25dc24e">
<h4 id="org25dc24e">Test the function</h4>
<div class="outline-text-4" id="text-org25dc24e">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_Ind2word</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_x.shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_y.shape </span><span class="si">{</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_z.shape: </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_h.shape: </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"call compute_cost"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_cost </span><span class="si">{</span><span class="n">tmp_cost</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">tmp_cost</span><span class="p">,</span> <span class="mf">9.9560</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
tmp_x.shape (5778, 4)
tmp_y.shape (5778, 4)
tmp_W1.shape (50, 5778)
tmp_W2.shape (5778, 50)
tmp_b1.shape (50, 1)
tmp_b2.shape (5778, 1)
tmp_z.shape: (5778, 4)
tmp_h.shape: (50, 4)
tmp_yhat.shape: (5778, 4)
call compute_cost
tmp_cost 9.9560
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org3ee5986">
<h3 id="org3ee5986">Training the Model - Backpropagation</h3>
<div class="outline-text-3" id="text-org3ee5986">
<p>Now that you have understood how the CBOW model works, you will train it. You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors.</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: back_prop</span>
<span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">yhat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">h</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">W1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">W2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">b1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">b2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Calculates the gradients</span>

<span class="sd">    Args: </span>
<span class="sd">       x:  average one hot vector for the context </span>
<span class="sd">       yhat: prediction (estimate of y)</span>
<span class="sd">       y:  target vector</span>
<span class="sd">       h:  hidden vector (see eq. 1)</span>
<span class="sd">       W1, W2, b1, b2:  matrices and biases  </span>
<span class="sd">       batch_size: batch size </span>

<span class="sd">     Returns: </span>
<span class="sd">       grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   </span>
<span class="sd">    """</span>
    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># Compute l1 as W2^T (Yhat - Y)</span>
    <span class="c1"># Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Apply relu to l1</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Compute the gradient of W1</span>
    <span class="n">grad_W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of W2</span>
    <span class="n">grad_W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of b1</span>
    <span class="n">grad_b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of b2</span>
    <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgab7da20">
<h4 id="orgab7da20">Test the function</h4>
<div class="outline-text-4" id="text-orgab7da20">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_Ind2word</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="c1"># get a batch of data</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"get a batch of data"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_x.shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_y.shape </span><span class="si">{</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Initialize weights and biases"</span><span class="p">)</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Forwad prop to get z and h"</span><span class="p">)</span>
<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_z.shape: </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_h.shape: </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Get yhat by calling softmax"</span><span class="p">)</span>
<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_m</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">tmp_C</span><span class="p">)</span>
<span class="n">tmp_grad_W1</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">,</span> <span class="n">tmp_grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_h</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"call back_prop"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_W1.shape </span><span class="si">{</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_W2.shape </span><span class="si">{</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_b1.shape </span><span class="si">{</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_b2.shape </span><span class="si">{</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>


<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example">
get a batch of data
tmp_x.shape (5778, 4)
tmp_y.shape (5778, 4)

Initialize weights and biases
tmp_W1.shape (50, 5778)
tmp_W2.shape (5778, 50)
tmp_b1.shape (50, 1)
tmp_b2.shape (5778, 1)

Forwad prop to get z and h
tmp_z.shape: (5778, 4)
tmp_h.shape: (50, 4)

Get yhat by calling softmax
tmp_yhat.shape: (5778, 4)

call back_prop
tmp_grad_W1.shape (50, 5778)
tmp_grad_W2.shape (5778, 50)
tmp_grad_b1.shape (50, 1)
tmp_grad_b2.shape (5778, 1)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org059e46b">
<h3 id="org059e46b">Gradient Descent</h3>
<div class="outline-text-3" id="text-org059e46b">
<p>Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set.</p>
<p><b>Hint:</b> For that, you will use <code>initialize_model</code> and the <code>back_prop</code> functions which you just created (and the <code>compute_cost</code> function). You can also use the provided <code>get_batches</code> helper function:</p>
<p>Also: print the cost after each batch is processed (use batch size = 128).</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: gradient_descent</span>
<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="nb">int</span> <span class="p">,</span>
                     <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.03</span><span class="p">):</span>    
    <span class="sd">"""</span>
<span class="sd">    This is the gradient_descent function</span>

<span class="sd">    Args: </span>
<span class="sd">       data:      text</span>
<span class="sd">       word2Ind:  words to Indices</span>
<span class="sd">       N:         dimension of hidden vector  </span>
<span class="sd">       V:         dimension of vocabulary </span>
<span class="sd">       num_iters: number of iterations  </span>

<span class="sd">    Returns: </span>
<span class="sd">       W1, W2, b1, b2:  updated matrices and biases   </span>
<span class="sd">    """</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">282</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>
        <span class="c1"># Get z and h</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
        <span class="c1"># Get yhat</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># Get cost</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span> <span class="p">(</span><span class="n">iters</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"iters: </span><span class="si">{</span><span class="n">iters</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2"> cost: </span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="c1"># Get gradients</span>
        <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                                                       <span class="n">yhat</span><span class="p">,</span>
                                                       <span class="n">y</span><span class="p">,</span>
                                                       <span class="n">h</span><span class="p">,</span>
                                                       <span class="n">W1</span><span class="p">,</span>
                                                       <span class="n">W2</span><span class="p">,</span>
                                                       <span class="n">b1</span><span class="p">,</span>
                                                       <span class="n">b2</span><span class="p">,</span>
                                                       <span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Update weights and biases</span>
        <span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
        <span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
        <span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
        <span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>

        <span class="c1">### END CODE HERE ###</span>

        <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span> 
        <span class="k">if</span> <span class="n">iters</span> <span class="o">==</span> <span class="n">num_iters</span><span class="p">:</span> 
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">iters</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">*=</span> <span class="mf">0.66</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgb2616b9">
<h4 id="orgb2616b9">Test Your Function</h4>
<div class="outline-text-4" id="text-orgb2616b9">
<div class="highlight">
<pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">num_iters</span> <span class="o">=</span> <span class="mi">150</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Call gradient_descent"</span><span class="p">)</span>
<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">)</span>
</pre></div>
<pre class="example">
Call gradient_descent
iters: 10 cost: 0.789141
iters: 20 cost: 0.105543
iters: 30 cost: 0.056008
iters: 40 cost: 0.038101
iters: 50 cost: 0.028868
iters: 60 cost: 0.023237
iters: 70 cost: 0.019444
iters: 80 cost: 0.016716
iters: 90 cost: 0.014660
iters: 100 cost: 0.013054
iters: 110 cost: 0.012133
iters: 120 cost: 0.011370
iters: 130 cost: 0.010698
iters: 140 cost: 0.010100
iters: 150 cost: 0.009566
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org43d696d">
<h2 id="org43d696d">End</h2>
<div class="outline-text-2" id="text-org43d696d">
<p>The <a href="posts/nlp/word-embeddings-visualizing-the-embeddings/">next post</a> is one on extracting and visualizing the embeddings using Principal Component Analysis.</p>
</div>
<div class="outline-3" id="outline-container-orgccfb6ac">
<h3 id="orgccfb6ac">Bundling It Up</h3>
<div class="outline-text-3" id="text-orgccfb6ac"></div>
<div class="outline-4" id="outline-container-org2468a12">
<h4 id="org2468a12">Imports</h4>
<div class="outline-text-4" id="text-org2468a12">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">unique</span>

<span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1a2d415">
<h4 id="org1a2d415">Enum Setup</h4>
<div class="outline-text-4" id="text-org1a2d415">
<div class="highlight">
<pre><span></span><span class="nd">@unique</span>
<span class="k">class</span> <span class="nc">Axis</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgd9a05e6">
<h4 id="orgd9a05e6">Named Tuples</h4>
<div class="outline-text-4" id="text-orgd9a05e6">
<div class="highlight">
<pre><span></span><span class="n">Gradients</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Gradients"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"input_weights"</span><span class="p">,</span> <span class="s2">"hidden_weights"</span><span class="p">,</span> <span class="s2">"input_bias"</span><span class="p">,</span> <span class="s2">"hidden_bias"</span><span class="p">])</span>

<span class="n">Weights</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Weights"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"input_weights"</span><span class="p">,</span> <span class="s2">"hidden_weights"</span><span class="p">,</span> <span class="s2">"input_bias"</span><span class="p">,</span> <span class="s2">"hidden_bias"</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgddc8337">
<h4 id="orgddc8337">The CBOW Model</h4>
<div class="outline-text-4" id="text-orgddc8337">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">CBOW</span><span class="p">:</span>
    <span class="sd">"""A continuous bag of words model builder</span>

<span class="sd">    Args:</span>
<span class="sd">     hidden: number of rows in the hidden layer</span>
<span class="sd">     vocabulary_size: number of tokens in the vocabulary</span>
<span class="sd">     learning_rate: learning rate for back-propagation updates</span>
<span class="sd">     random_seed: int</span>
<span class="sd">    """</span>
    <span class="n">hidden</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.03</span>
    <span class="n">random_seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span>    
    <span class="n">_random_generator</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PCG64</span><span class="o">=</span><span class="kc">None</span>

    <span class="c1"># layer one</span>
    <span class="n">_input_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_input_bias</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>

    <span class="c1"># hidden layer</span>
    <span class="n">_hidden_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_hidden_bias</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgffd815e"></a>The Random Generator<br>
<div class="outline-text-5" id="text-orgffd815e">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">random_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PCG64</span><span class="p">:</span>
    <span class="sd">"""The random number generator"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span>
</pre></div>
</div>
</li>
<li><a id="org9910356"></a>First Layer Weights<br>
<div class="outline-text-5" id="text-org9910356">
<p>These are initialized using numpy's new generator. I originally using their standard-normal version by mistake and the model did horrible. Using the <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.random.html#numpy.random.Generator.random">Generator.random</a> gives you a uniform distribution which seems to be what you're supposed to use.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Weights for the first layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span>
</pre></div>
</div>
</li>
<li><a id="orgd84c090"></a>First Layer Bias<br>
<div class="outline-text-5" id="text-orgd84c090">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Bias for the input layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span>
</pre></div>
</div>
</li>
<li><a id="org0e28ebd"></a>Hidden Layer Weights<br>
<div class="outline-text-5" id="text-org0e28ebd">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">hidden_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""The weights for the hidden layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span>
</pre></div>
</div>
</li>
<li><a id="orgcc10437"></a>Hidden Layer Bias<br>
<div class="outline-text-5" id="text-orgcc10437">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">hidden_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Bias for the hidden layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span>
</pre></div>
</div>
</li>
<li><a id="orge01737a"></a>Softmax<br>
<div class="outline-text-5" id="text-orge01737a">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate the softmax</span>

<span class="sd">    Args: </span>
<span class="sd">       scores: output scores from the hidden layer</span>
<span class="sd">    Returns: </span>
<span class="sd">       yhat: prediction (estimate of y)"""</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">/</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">ROWS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="org76fffdc"></a>Forward Propagation<br>
<div class="outline-text-5" id="text-org76fffdc">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""makes a model prediction</span>

<span class="sd">    Args:</span>
<span class="sd">     data: x-values to train on</span>

<span class="sd">    Returns:</span>
<span class="sd">     output, first-layer output</span>
<span class="sd">    """</span>
    <span class="n">first_layer_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_weights</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
                                  <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">second_layer_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_weights</span><span class="p">,</span> <span class="n">first_layer_output</span><span class="p">)</span>
                   <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">second_layer_output</span><span class="p">,</span> <span class="n">first_layer_output</span>
</pre></div>
</div>
</li>
<li><a id="org6bf3a0b"></a>Gradients<br>
<div class="outline-text-5" id="text-org6bf3a0b">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">hidden_input</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Gradients</span><span class="p">:</span>
    <span class="sd">"""does the gradient calculation for back-propagation</span>

<span class="sd">    This is broken out to be able to troubleshoot/compare it</span>

<span class="sd">   Args:</span>
<span class="sd">     data: the input x value</span>
<span class="sd">     predicted: what our model predicted the labels for the data should be</span>
<span class="sd">     actual: what the actual labels should have been</span>
<span class="sd">     hidden_input: the input to the hidden layer</span>
<span class="sd">    Returns:</span>
<span class="sd">     Gradients for input_weight, hidden_weight, input_bias, hidden_bias</span>
<span class="sd">    """</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">predicted</span> <span class="o">-</span> <span class="n">actual</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">difference</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">difference</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">input_weights_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">hidden_weights_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">difference</span><span class="p">,</span> <span class="n">hidden_input</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">input_bias_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span>
                                    <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                    <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">hidden_bias_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">difference</span><span class="p">,</span>
                                     <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                     <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">Gradients</span><span class="p">(</span><span class="n">input_weights</span><span class="o">=</span><span class="n">input_weights_gradient</span><span class="p">,</span>
                     <span class="n">hidden_weights</span><span class="o">=</span><span class="n">hidden_weights_gradient</span><span class="p">,</span>
                     <span class="n">input_bias</span><span class="o">=</span><span class="n">input_bias_gradient</span><span class="p">,</span>
                     <span class="n">hidden_bias</span><span class="o">=</span><span class="n">hidden_bias_gradient</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="org6086360"></a>Backward Propagation<br>
<div class="outline-text-5" id="text-org6086360">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">hidden_input</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Does back-propagation to update the weights</span>

<span class="sd">   Arg:s</span>
<span class="sd">     data: the input x value</span>
<span class="sd">     predicted: what our model predicted the labels for the data should be</span>
<span class="sd">     actual: what the actual labels should have been</span>
<span class="sd">     hidden_input: the input to the hidden layer</span>
<span class="sd">    """</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                               <span class="n">predicted</span><span class="o">=</span><span class="n">predicted</span><span class="p">,</span>
                               <span class="n">actual</span><span class="o">=</span><span class="n">actual</span><span class="p">,</span>
                               <span class="n">hidden_input</span><span class="o">=</span><span class="n">hidden_input</span><span class="p">)</span>
    <span class="c1"># I don't have setters for the properties so use the private variables</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">input_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">hidden_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">input_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">hidden_bias</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="org2140eb5"></a>Call<br>
<div class="outline-text-5" id="text-org2140eb5">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""makes a prediction on the data</span>

<span class="sd">    Args:</span>
<span class="sd">     data: input data for the prediction</span>

<span class="sd">    Returns:</span>
<span class="sd">     softmax of model output</span>
<span class="sd">    """</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org523b283">
<h4 id="org523b283">Batch Generator</h4>
<div class="outline-text-4" id="text-org523b283">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Batches</span><span class="p">:</span>
    <span class="sd">"""Generates batches of data</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the source of the data to generate (training data)</span>
<span class="sd">     word_to_index: dict mapping the word to the vocabulary index</span>
<span class="sd">     half_window: number of tokens on either side of word to grab</span>
<span class="sd">     batch_size: the number of entries per batch</span>
<span class="sd">     batches: number of batches to generate before quitting</span>
<span class="sd">     verbose: whether to emit messages</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span>
    <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">batches</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">repetitions</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>    
    <span class="n">_vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vectors</span><span class="p">:</span> <span class="nb">object</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org12ac156"></a>Vocabulary Size<br>
<div class="outline-text-5" id="text-org12ac156">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vocabulary_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Number of tokens in the vocabulary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span>
</pre></div>
</div>
</li>
<li><a id="orgeba99a8"></a>Vectors<br>
<div class="outline-text-5" id="text-orgeba99a8">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""our vector-generator started up"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_generator</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span>
</pre></div>
</div>
</li>
<li><a id="org1d232e4"></a>Indices and Frequencies<br>
<div class="outline-text-5" id="text-org1d232e4">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">indices_and_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""combines word-indexes and frequency counts-dict</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to get the indices for</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of (word-index, word-count) tuples built from context_words</span>
<span class="sd">    """</span>
    <span class="n">frequencies</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">frequencies</span><span class="p">[</span><span class="n">context_words</span><span class="p">[</span><span class="n">index</span><span class="p">]])</span>
            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">))]</span>
</pre></div>
</div>
</li>
<li><a id="orgb6a00a5"></a>Vectors<br>
<div class="outline-text-5" id="text-orgb6a00a5">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">vector_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Generates vectors infinitely</span>

<span class="sd">    x: fraction of context words represented by word</span>
<span class="sd">    y: array with 1 where center word is in the vocabulary and 0 elsewhere</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of x, y </span>
<span class="sd">    """</span>
    <span class="n">location</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">location</span><span class="p">]</span>
        <span class="n">y</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span><span class="p">):</span> <span class="n">location</span><span class="p">]</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">location</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

        <span class="k">for</span> <span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_and_frequencies</span><span class="p">(</span><span class="n">context_words</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">frequency</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
        <span class="n">location</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">location</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"location in data is being set to 0"</span><span class="p">)</span>
            <span class="n">location</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="org6911dcb"></a>Iterator Method<br>
<div class="outline-text-5" id="text-org6911dcb">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""makes this into an iterator"""</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>
</div>
</li>
<li><a id="org98ddc5b"></a>Next Method<br>
<div class="outline-text-5" id="text-org98ddc5b">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Creates the batches and returns them</span>

<span class="sd">    Returns:</span>
<span class="sd">     x, y batches</span>
<span class="sd">    """</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">repetitions</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">StopIteration</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">repetitions</span> <span class="o">+=</span> <span class="mi">1</span>    
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgdf5bd77">
<h4 id="orgdf5bd77">The Trainer</h4>
<div class="outline-text-4" id="text-orgdf5bd77">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TheTrainer</span><span class="p">:</span>
    <span class="sd">"""Something to train the model</span>

<span class="sd">    Args:</span>
<span class="sd">     model: thing to train</span>
<span class="sd">     batches: batch generator</span>
<span class="sd">     learning_impairment: rate to slow the model's learning</span>
<span class="sd">     impairment_point: how frequently to impair the learner</span>
<span class="sd">     emit_point: how frequently to emit messages</span>
<span class="sd">     verbose: whether to emit messages</span>
<span class="sd">    """</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">CBOW</span>
    <span class="n">batches</span><span class="p">:</span> <span class="n">Batches</span>
    <span class="n">learning_impairment</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.66</span>
    <span class="n">impairment_point</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span>
    <span class="n">emit_point</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>
    <span class="n">_losses</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org43b79c2"></a>Losses<br>
<div class="outline-text-5" id="text-org43b79c2">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">losses</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""Holder for the training losses"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span>
</pre></div>
</div>
</li>
<li><a id="org780af3f"></a>Gradient Descent<br>
<div class="outline-text-5" id="text-org780af3f">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>    
    <span class="sd">"""Trains the model using gradient descent</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">x_y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x_y</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_weights</span> <span class="o">=</span> <span class="n">Weights</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                            <span class="n">hidden_input</span><span class="o">=</span><span class="n">hidden_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">repetitions</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">impairment_point</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_impairment</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"new learning rate: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learning_rate</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="p">((</span><span class="n">repetitions</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">emit_point</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">repetitions</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">: loss=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="n">repetitions</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> 
</pre></div>
</div>
</li>
<li><a id="orgb872843"></a>Cross-Entropy-Loss<br>
<div class="outline-text-5" id="text-orgb872843">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                       <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the cross-entropy loss</span>

<span class="sd">    Args:</span>
<span class="sd">     predicted: array with the model's guesses</span>
<span class="sd">     actual: array with the actual labels</span>

<span class="sd">    Returns:</span>
<span class="sd">     the cross-entropy loss</span>
<span class="sd">    """</span>
    <span class="n">log_probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predicted</span><span class="p">),</span> <span class="n">actual</span><span class="p">)</span>
                         <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">actual</span><span class="p">))</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org61b16e9">
<h3 id="org61b16e9">Testing It</h3>
<div class="outline-text-3" id="text-org61b16e9">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">Batches</span><span class="p">,</span> <span class="n">CBOW</span><span class="p">,</span> <span class="n">TheTrainer</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">V</span><span class="p">)</span>


<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="n">tmp</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
                   <span class="p">])</span>
<span class="n">tmp_sm</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.73105858</span><span class="p">,</span> <span class="mf">0.88079708</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.26894142</span><span class="p">,</span> <span class="mf">0.11920292</span><span class="p">]])</span>


<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgcb06669">
<h4 id="orgcb06669">Forward Propagation</h4>
<div class="outline-text-4" id="text-orgcb06669">
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">tmp_x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">V</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.55379268</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.58960774</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.50722933</span><span class="p">]]</span>
<span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.92477674</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.02487333</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org092b1a9">
<h4 id="org092b1a9">Cross Entropy Loss</h4>
<div class="outline-text-4" id="text-org092b1a9">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tmp_cost</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">tmp_yhat</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">tmp_cost</span><span class="p">,</span> <span class="mf">9.9560</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org2920311">
<h4 id="org2920311">Back Propagation</h4>
<div class="outline-text-4" id="text-org2920311">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># get a batch of data</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>
<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>
<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">gradients</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">hidden_input</span><span class="o">=</span><span class="n">tmp_h</span><span class="p">)</span>
<span class="n">tmp_grad_W1</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">,</span> <span class="n">tmp_grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_h</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">input_weights</span><span class="p">,</span> <span class="n">tmp_grad_W1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">hidden_weights</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">input_bias</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">hidden_bias</span><span class="p">,</span> <span class="n">tmp_grad_b2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgd418f8c">
<h4 id="orgd418f8c">Putting Some Stuff Together</h4>
<div class="outline-text-4" id="text-orgd418f8c">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">hidden_layers</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="c1"># using their initial weights</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
</pre></div>
<pre class="example">
11.871189103548419
11.871189103548419
9.956016099656951
9.956016099656951
</pre>
<p>I changed the weights to use the uniform distribution which seems to work better, but weirdly it still does a little worse initially. The random-seed seems to be different for the old numpy random and their new generator.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org9401058">
<h4 id="org9401058">The Batches</h4>
<div class="outline-text-4" id="text-org9401058">
<p>The original batch-generator had a couple of bugs in it. To avoid them pass in <code>original=True</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>


<span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span>
                                <span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">original</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">old_x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">old_y</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>


<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="c1">#expect(numpy.allclose(tmp_x, old_x)).to(be_true)</span>
<span class="c1">#expect(numpy.allclose(tmp_y, old_y)).to(be_true)</span>

<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org27ec8e3">
<h4 id="org27ec8e3">Gradient Descent</h4>
<div class="outline-text-4" id="text-org27ec8e3">
<div class="highlight">
<pre><span></span><span class="n">hidden_layers</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train</span><span class="p">()</span>
</pre></div>
<pre class="example">
10: loss=12.949165499168524
20: loss=7.1739091478289225
30: loss=13.431976455238479
40: loss=4.0062314323745545
50: loss=11.595407087927406
60: loss=10.41983077447342
70: loss=7.843047289924249
80: loss=12.529314536141994
90: loss=14.122707806423126
new learning rate: 0.0198
100: loss=10.80530164111974
110: loss=4.624869443165228
120: loss=5.552813055551899
130: loss=8.483428176366933
140: loss=9.047299388851195
150: loss=4.841072955589429
</pre></div>
</div>
<div class="outline-4" id="outline-container-org32d7b00">
<h4 id="org32d7b00">Gradient Re-do</h4>
<div class="outline-text-4" id="text-org32d7b00">
<p>Something's wrong with the trainer's gradient descent so I'm going to try and update the original function to do it.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">CBOW</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batches</span><span class="p">:</span> <span class="n">Batches</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.03</span><span class="p">):</span>
    <span class="sd">"""This is the gradient_descent function</span>

<span class="sd">    Args: </span>
<span class="sd">       data:      text</span>
<span class="sd">       word2Ind:  words to Indices</span>
<span class="sd">       N:         dimension of hidden vector  </span>
<span class="sd">       V:         dimension of vocabulary </span>
<span class="sd">       num_iters: number of iterations  </span>

<span class="sd">    Returns: </span>
<span class="sd">       W1, W2, b1, b2:  updated matrices and biases   </span>
<span class="sd">    """</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Get yhat</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># Get cost</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">iters</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"iters: </span><span class="si">{</span><span class="n">iters</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2"> cost: </span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                                                             <span class="n">yhat</span><span class="p">,</span>
                                                             <span class="n">y</span><span class="p">,</span>
                                                             <span class="n">h</span><span class="p">)</span>

        <span class="c1"># Update weights and biases</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">-=</span>  <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">-=</span>  <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>

        <span class="c1">### END CODE HERE ###</span>

        <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span> 
        <span class="k">if</span> <span class="n">iters</span> <span class="o">==</span> <span class="n">num_iters</span><span class="p">:</span> 
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">iters</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">*=</span> <span class="mf">0.66</span>

    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="c1"># batch_generator(data, word2Ind, C, batch_size)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example">
iters: 10 cost: 12.949165
iters: 20 cost: 7.173909
iters: 30 cost: 13.431976
iters: 40 cost: 4.006231
iters: 50 cost: 11.595407
iters: 60 cost: 10.419831
iters: 70 cost: 7.843047
iters: 80 cost: 12.529315
iters: 90 cost: 14.122708
iters: 100 cost: 10.805302
iters: 110 cost: 4.624869
iters: 120 cost: 5.552813
iters: 130 cost: 8.483428
iters: 140 cost: 9.047299
iters: 150 cost: 4.841073
</pre>
<p>So, something's wrong with the gradient descent.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="c1">#batches = Batches(data=cleaner.processed, word_to_index=meta.word_to_index,</span>
<span class="c1">#                  half_window=half_window, batch_size=batch_size, batches=repetitions)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example">
iters: 10 cost: 0.407862
iters: 20 cost: 0.090807
iters: 30 cost: 0.050924
iters: 40 cost: 0.035379
iters: 50 cost: 0.027105
iters: 60 cost: 0.021969
iters: 70 cost: 0.018470
iters: 80 cost: 0.015932
iters: 90 cost: 0.014008
iters: 100 cost: 0.012499
iters: 110 cost: 0.011631
iters: 120 cost: 0.010911
iters: 130 cost: 0.010274
iters: 140 cost: 0.009708
iters: 150 cost: 0.009201
</pre>
<p>It looks like it's the batches.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org5ee93f1">
<h4 id="org5ee93f1">Troubleshooting the Batches</h4>
<div class="outline-text-4" id="text-org5ee93f1">
<div class="highlight">
<pre><span></span><span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span><span class="p">]</span> <span class="o">+</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">[</span><span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">packed_1</span> <span class="o">=</span> <span class="n">index_with_frequency</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span>
<span class="n">packed_2</span> <span class="o">=</span> <span class="n">batches</span><span class="o">.</span><span class="n">indices_and_frequencies</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">packed_1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">contain_exactly</span><span class="p">(</span><span class="o">*</span><span class="n">packed_2</span><span class="p">))</span>
</pre></div>
<p>So the indices and frequencies is okay.</p>
<div class="highlight">
<pre><span></span><span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">vectors</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">half_window</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">repetition</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">batches</span><span class="o">.</span><span class="n">vectors</span><span class="p">):</span>
    <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<p>And the vectors look okay.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">repetition</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># batch = next(batches)</span>
<span class="k">for</span> <span class="n">old</span> <span class="ow">in</span> <span class="n">old_generator</span><span class="p">:</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batches</span><span class="o">.</span><span class="n">vectors</span><span class="p">:</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batches</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">newx</span><span class="p">,</span> <span class="n">newy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">newx</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
            <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
                <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="k">break</span>
</pre></div>
<p>So, weirdly, rolling the <code>__next__=</code> by hand seems to work.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">repetition</span><span class="p">,</span> <span class="n">repetitions</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">150</span>
<span class="k">for</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">old_generator</span><span class="p">,</span> <span class="n">batches</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">repetition</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<pre class="example">
1
</pre>
<p>But not the batches.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">repetition</span><span class="p">,</span> <span class="n">repetitions</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">150</span>
<span class="k">for</span> <span class="n">old</span> <span class="ow">in</span> <span class="n">old_generator</span><span class="p">:</span>
    <span class="n">new</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">repetition</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<p>Actually, it looks like the old generator might be broken.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="c1">#batches = Batches(data=cleaner.processed, word_to_index=meta.word_to_index,</span>
<span class="c1">#                  half_window=half_window, batch_size=batch_size, batches=repetitions)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example">
iters: 10 cost: 12.949165
iters: 20 cost: 7.173909
iters: 30 cost: 13.431976
iters: 40 cost: 4.006231
iters: 50 cost: 11.595407
iters: 60 cost: 10.419831
iters: 70 cost: 7.843047
iters: 80 cost: 12.529315
iters: 90 cost: 14.122708
iters: 100 cost: 10.805302
iters: 110 cost: 4.624869
iters: 120 cost: 5.552813
iters: 130 cost: 8.483428
iters: 140 cost: 9.047299
iters: 150 cost: 4.841073
</pre>
<p>The old generator wasn't creating new lists every time so it was just fitting the same batch of data every time… in fact it had a while loop instead of a conditional so it was just creating one batch with the same x and y lists repeated over and over so it should really be the worse performance, not the really good performance the original generator gave. I didn't re-run the ones above but this next set is being run after fixing my implementation.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="p">,</span> <span class="n">emit_point</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">trainer</span><span class="p">()</span>
</pre></div>
<pre class="example">
2020-12-16 14:15:54,530 graeae.timers.timer start: Started: 2020-12-16 14:15:54.530779
2020-12-16 14:16:18,600 graeae.timers.timer end: Ended: 2020-12-16 14:16:18.600880
2020-12-16 14:16:18,602 graeae.timers.timer end: Elapsed: 0:00:24.070101
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
11.99601105791401 8.827228045367379
</pre>
<p>Not a huge improvement, but it didn't run for a long time either.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="p">,</span> <span class="n">emit_point</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">trainer</span><span class="p">()</span>
</pre></div>
<pre class="example">
2020-12-16 14:40:13,275 graeae.timers.timer start: Started: 2020-12-16 14:40:13.275964
new learning rate: 0.0198
100: loss=9.138356897918037
new learning rate: 0.013068000000000001
200: loss=9.077599951734605
new learning rate: 0.008624880000000001
300: loss=8.827228045367379
new learning rate: 0.005692420800000001
400: loss=8.556788482755191
new learning rate: 0.003756997728000001
500: loss=8.92744766914796
new learning rate: 0.002479618500480001
600: loss=9.052677036205138
new learning rate: 0.0016365482103168007
700: loss=8.914532962726918
new learning rate: 0.0010801218188090885
800: loss=8.885698480310062
new learning rate: 0.0007128804004139984
900: loss=9.042620463323736
2020-12-16 14:41:33,457 graeae.timers.timer end: Ended: 2020-12-16 14:41:33.457065
2020-12-16 14:41:33,458 graeae.timers.timer end: Elapsed: 0:01:20.181101
new learning rate: 0.000470501064273239
1000: loss=9.239992952104755
</pre>
<p>Hmm… doesn't seem to be improving.</p>
<div class="highlight">
<pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">VLine</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">idxmin</span><span class="p">())</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">time_series</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">hvplot</span><span class="p">()</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Loss per Repetition"</span><span class="p">,</span>
                                   <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
                                   <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">tan</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">time_series</span> <span class="o">*</span> <span class="n">line</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"training_1000"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/word-embeddings-training-the-model/training_1000.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Since the losses are in a Series we can use its <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.idxmin.html">idxmin</a> method to see when the losses bottomed out.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">idxmin</span><span class="p">())</span>
</pre></div>
<pre class="example">
247
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">247</span><span class="p">],</span> <span class="n">losses</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
8.186490214727549 9.239992952104755
</pre>
<p>So it did the best at 247 and then got a little worse as we went along.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
<pre class="example">
45.140625
</pre>
<p>We exhausted our data after 45 batches so I guess it's overfitting after a while.</p>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-shakespeare-data/">Word Embeddings: Shakespeare Data</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-shakespeare-data/" rel="bookmark"><time class="published dt-published" datetime="2020-12-13T12:44:32-08:00" itemprop="datePublished" title="2020-12-13 12:44">2020-12-13 12:44</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#orgba714c5">Beginning</a>
<ul>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org5f43baa">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#orgb80c910">Middle</a>
<ul>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#orga0d62d2">A Little Cleaning</a>
<ul>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org0be7449">Imports</a></li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#orgf88e6ef">The Cleaner</a></li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org23a8dc3">The Counter</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#orge6c8239">The Cleaned</a></li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org418fe8d">The Data Data</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org3b7e54d">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgba714c5">
<h2 id="orgba714c5">Beginning</h2>
<div class="outline-text-2" id="text-orgba714c5">
<p>This is the first part of as series on building word embeddings using a Continuous Bag of Words. There's an <a href="posts/nlp/word-embeddings-build-a-model/">overview post</a> that has links to all the posts in the series.</p>
</div>
<div class="outline-3" id="outline-container-org5f43baa">
<h3 id="org5f43baa">Imports</h3>
<div class="outline-text-3" id="text-org5f43baa">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgb80c910">
<h2 id="orgb80c910">Middle</h2>
<div class="outline-text-2" id="text-orgb80c910">
<p>We're going to be using the same dataset that we used in building the <a href="posts/nlp/autocorrect-the-system/">autocorrect system</a>.</p>
</div>
<div class="outline-3" id="outline-container-orga0d62d2">
<h3 id="orga0d62d2">A Little Cleaning</h3>
<div class="outline-text-3" id="text-orga0d62d2"></div>
<div class="outline-4" id="outline-container-org0be7449">
<h4 id="org0be7449">Imports</h4>
<div class="outline-text-4" id="text-org0be7449">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">nltk</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf88e6ef">
<h4 id="orgf88e6ef">The Cleaner</h4>
<div class="outline-text-4" id="text-orgf88e6ef">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">DataCleaner</span><span class="p">:</span>
    <span class="sd">"""A cleaner for the word-embeddings data</span>

<span class="sd">    Args:</span>
<span class="sd">     key: environment key with path to the data file</span>
<span class="sd">     env_path: path to the .env file</span>
<span class="sd">    """</span>
    <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"SHAKESPEARE"</span>
    <span class="n">env_path</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"posts/nlp/.env"</span>
    <span class="n">stop</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"."</span>
    <span class="n">_data_path</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_data</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_unpunctuated</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_punctuation</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_tokens</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_processed</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgc810121"></a>The Path To the Data<br>
<div class="outline-text-5" id="text-orgc810121">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">data_path</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
    <span class="sd">"""The path to the data file"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">load_dotenv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_path</span>
</pre></div>
</div>
</li>
<li><a id="org3958bc4"></a>The Data<br>
<div class="outline-text-5" id="text-org3958bc4">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""The data-file read in as a string"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="o">.</span><span class="n">open</span><span class="p">()</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span>
</pre></div>
</div>
</li>
<li><a id="org06724b4"></a>The Punctuation Expression<br>
<div class="outline-text-5" id="text-org06724b4">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">punctuation</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span><span class="p">:</span>
    <span class="sd">"""The regular expression to find punctuation"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_punctuation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_punctuation</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">"[,!?;-]"</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_punctuation</span>
</pre></div>
</div>
</li>
<li><a id="orgd5c6a6f"></a>The Un-Punctuated<br>
<div class="outline-text-5" id="text-orgd5c6a6f">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">unpunctuated</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""The data with punctuation replaced by stop"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unpunctuated</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unpunctuated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">punctuation</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unpunctuated</span>
</pre></div>
</div>
</li>
<li><a id="org235e4de"></a>The Tokens<br>
<div class="outline-text-5" id="text-org235e4de">
<p>We're going to use NLTK's <a href="https://www.nltk.org/api/nltk.tokenize.html?highlight=word_tokenize#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize">word_tokenize</a> function to tokenize the string.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The tokenized data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unpunctuated</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokens</span>
</pre></div>
</div>
</li>
<li><a id="org8acbbda"></a>The Processed Tokens<br>
<div class="outline-text-5" id="text-org8acbbda">
<p>The final processed data will be all lowercased words and periods only.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">processed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The final processed tokens"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_processed</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokens</span>
                           <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()</span> <span class="ow">or</span> <span class="n">token</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processed</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org23a8dc3">
<h4 id="org23a8dc3">The Counter</h4>
<div class="outline-text-4" id="text-org23a8dc3">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">MetaData</span><span class="p">:</span>
    <span class="sd">"""Compile some basic data about the data</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the cleaned and tokenized data</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">_distribution</span><span class="p">:</span> <span class="n">nltk</span><span class="o">.</span><span class="n">probability</span><span class="o">.</span><span class="n">FreqDist</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vocabulary</span><span class="p">:</span> <span class="nb">tuple</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org29aefbe"></a>The Frequency Distribution<br>
<div class="outline-text-5" id="text-org29aefbe">
<p>According to the doc-string, the <a href="https://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist">FreqDist</a> is meant to hold outcomes from experiments. It looks like a <a href="https://docs.python.org/3/library/collections.html#collections.Counter">Counter</a> with extra methods added.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nltk</span><span class="o">.</span><span class="n">probability</span><span class="o">.</span><span class="n">FreqDist</span><span class="p">:</span>
    <span class="sd">"""The Token Frequency Distribution"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_distribution</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">FreqDist</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distribution</span>
</pre></div>
</div>
</li>
<li><a id="org975f128"></a>The Vocabulary<br>
<div class="outline-text-5" id="text-org975f128">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""The sorted unique tokens in the data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span>
</pre></div>
</div>
</li>
<li><a id="org70666cb"></a>The Word-To-Index Mapping<br>
<div class="outline-text-5" id="text-org70666cb">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">word_to_index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""Maps words to their index in the vocabulary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span>
                               <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)}</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orge6c8239">
<h3 id="orge6c8239">The Cleaned</h3>
<div class="outline-text-3" id="text-orge6c8239">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">DataCleaner</span>
<span class="n">cleaner</span> <span class="o">=</span> <span class="n">DataCleaner</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">unpunctuated</span><span class="p">[:</span><span class="mi">50</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">tokens</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Tokens: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
O for a Muse of fire. that would ascend
The bright
['O', 'for', 'a', 'Muse', 'of', 'fire', '.', 'that', 'would', 'ascend']
['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend']
Tokens: 60,996
</pre></div>
</div>
<div class="outline-3" id="outline-container-org418fe8d">
<h3 id="org418fe8d">The Data Data</h3>
<div class="outline-text-3" id="text-org418fe8d">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">MetaData</span>
<span class="n">counter</span> <span class="o">=</span> <span class="n">MetaData</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">distribution</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">counter</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" - </span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">distribution</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of the Vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">index</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">word</span> <span class="o">=</span> <span class="n">counter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
<span class="n">expect</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]))</span>
</pre></div>
<pre class="example">
Size of vocabulary: 5,778
 - ('.', 9630)
 - ('the', 1521)
 - ('and', 1394)
 - ('i', 1257)
 - ('to', 1159)
 - ('of', 1093)
 - ('my', 857)
 - ('that', 781)
 - ('in', 770)
 - ('a', 752)
 - ('you', 748)
 - ('is', 630)
 - ('not', 559)
 - ('for', 467)
 - ('it', 460)
 - ('with', 441)
 - ('his', 434)
 - ('but', 417)
 - ('me', 417)
 - ('your', 397)
Size of the Vocabulary: 5,778
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org3b7e54d">
<h2 id="org3b7e54d">End</h2>
<div class="outline-text-2" id="text-org3b7e54d">
<p>Now that we have the data setup its time to <a href="posts/nlp/word-embeddings-training-the-model/">build and train the model</a>.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-build-a-model/">Word Embeddings: Build a Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-build-a-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-12T17:07:05-08:00" itemprop="datePublished" title="2020-12-12 17:07">2020-12-12 17:07</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-build-a-model/#org9d9d089">Introduction</a></li>
<li><a href="posts/nlp/word-embeddings-build-a-model/#orgfd9cda0">The Continuous Bag Of Words Model (CBOW)</a></li>
<li><a href="posts/nlp/word-embeddings-build-a-model/#orge66e17f">The Parts</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org9d9d089">
<h2 id="org9d9d089">Introduction</h2>
<div class="outline-text-2" id="text-org9d9d089">
<p>This is and introduction to a series of posts that look at how to create word embeddings using a Continuous Bag Of Words (CBOW) model.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgfd9cda0">
<h2 id="orgfd9cda0">The Continuous Bag Of Words Model (CBOW)</h2>
<div class="outline-text-2" id="text-orgfd9cda0">
<p>Let's take a look at the following sentence: <b>'I am happy because I am learning'</b>.</p>
<ul class="org-ul">
<li>In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).</li>
<li>For example, if you were to choose a context half-size of say <i>C = 2</i>, then you would try to predict the word <b>happy</b> given the context that includes 2 words before and 2 words after the center word:
<ul class="org-ul">
<li><i>C</i> words before: <code>[I, am]</code></li>
<li><i>C</i> words after: <code>[because, I]</code></li>
</ul>
</li>
<li>In other words:</li>
</ul>
<pre class="example">
context = [I,am, because, I]
target = happy
</pre>
<p>The model will be a three-layer one. The input layer (\(\bar x\)) is the average of all the one hot vectors of the context words. There will be one hidden layer, and the output layer (\(hat y\)) will be the softmax layer.</p>
<p>The architecture you will be implementing is as follows:</p>
\begin{align} h &amp;= W_1 \ X + b_1 \tag{1} \\ a &amp;= ReLU(h) \tag{2} \\ z &amp;= W_2 \ a + b_2 \tag{3} \\ \hat y &amp;= softmax(z) \tag{4} \\ \end{align}</div>
</div>
<div class="outline-2" id="outline-container-orge66e17f">
<h2 id="orge66e17f">The Parts</h2>
<div class="outline-text-2" id="text-orge66e17f">
<p>This is just and introductory post, the following are the posts in the series where things will actually be implemented.</p>
<ul class="org-ul">
<li><a href="posts/nlp/word-embeddings-shakespeare-data/">Loading the Data</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/">Building and Training the CBOW Model</a></li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/">Visualizing the Embeddings</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/extracting-word-embeddings/">Extracting Word Embeddings</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/extracting-word-embeddings/" rel="bookmark"><time class="published dt-published" datetime="2020-12-11T16:42:38-08:00" itemprop="datePublished" title="2020-12-11 16:42">2020-12-11 16:42</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/extracting-word-embeddings/#orge71b171">Introduction and Preliminaries</a>
<ul>
<li><a href="posts/nlp/extracting-word-embeddings/#org55d2fc1">Imports</a></li>
<li><a href="posts/nlp/extracting-word-embeddings/#org80517a9">Preliminary Setup</a></li>
</ul>
</li>
<li><a href="posts/nlp/extracting-word-embeddings/#orgd9ac8a6">Extracting word embedding vectors</a>
<ul>
<li><a href="posts/nlp/extracting-word-embeddings/#org46b175d">Option 1: Extract embedding vectors from \(\mathbf{W_1}\)</a></li>
<li><a href="posts/nlp/extracting-word-embeddings/#org2b8eea3">Option 2: Extract embedding vectors from \(\mathbf{W_2}\)</a></li>
<li><a href="posts/nlp/extracting-word-embeddings/#orgcce76a3">Option 3: extract embedding vectors from \(\mathbf{W_1}\) and \(\mathbf{W_2}\)</a></li>
</ul>
</li>
<li><a href="posts/nlp/extracting-word-embeddings/#org794e34f">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orge71b171">
<h2 id="orge71b171">Introduction and Preliminaries</h2>
<div class="outline-text-2" id="text-orge71b171">
<p>In the <a href="posts/nlp/training-the-cbow-model/">previous post</a> we trained the CBOW model, now in this post we'll look at how to extract word embedding vectors from a model.</p>
</div>
<div class="outline-3" id="outline-container-org55d2fc1">
<h3 id="org55d2fc1">Imports</h3>
<div class="outline-text-3" id="text-org55d2fc1">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">expect</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org80517a9">
<h3 id="org80517a9">Preliminary Setup</h3>
<div class="outline-text-3" id="text-org80517a9">
<p>Before moving on, you will be provided with some variables needed for further procedures, which should be familiar by now. Also a trained CBOW model will be simulated, the corresponding weights and biases are provided:</p>
<p>Define the tokenized version of the corpus.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">]</span>
</pre></div>
<p>Define V. Remember this is the size of the vocabulary.</p>
<div class="highlight">
<pre><span></span><span class="n">vocabulary</span> <span class="o">=</span>  <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
</pre></div>
<p>Get the <code>word_to_index</code> and <code>index_to_word</code> dictionaries for the tokenized corpus.</p>
<div class="highlight">
<pre><span></span><span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)}</span>
<span class="n">index_to_word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))</span>
</pre></div>
<p>Define first matrix of weights</p>
<div class="highlight">
<pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.41687358</span><span class="p">,</span>  <span class="mf">0.08854191</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23495225</span><span class="p">,</span>  <span class="mf">0.28320538</span><span class="p">,</span>  <span class="mf">0.41800106</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.32735501</span><span class="p">,</span>  <span class="mf">0.22795148</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23951958</span><span class="p">,</span>  <span class="mf">0.4117634</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.23924344</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.26637602</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23846886</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.37770863</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11399446</span><span class="p">,</span>  <span class="mf">0.34008124</span><span class="p">]])</span>
</pre></div>
<p>Define second matrix of weights.</p>
<div class="highlight">
<pre><span></span><span class="n">W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.22182064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43008631</span><span class="p">,</span>  <span class="mf">0.13310965</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.08476603</span><span class="p">,</span>  <span class="mf">0.08123194</span><span class="p">,</span>  <span class="mf">0.1772054</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.1871551</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.06107263</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1790735</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.07055222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02015138</span><span class="p">,</span>  <span class="mf">0.36107434</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.33480474</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.39423389</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43959196</span><span class="p">]])</span>
</pre></div>
<p>Define first vector of biases.</p>
<div class="highlight">
<pre><span></span><span class="n">b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.09688219</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.29239497</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.27364426</span><span class="p">]])</span>
</pre></div>
<p>Define second vector of biases.</p>
<div class="highlight">
<pre><span></span><span class="n">b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.0352008</span> <span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.36393384</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.12775555</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.34802326</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.07017815</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd9ac8a6">
<h2 id="orgd9ac8a6">Extracting word embedding vectors</h2>
<div class="outline-text-2" id="text-orgd9ac8a6">
<p>Once you have finished training the neural network, you have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \(\mathbf{W_1}\) and/or \(\mathbf{W_2}\).</p>
</div>
<div class="outline-3" id="outline-container-org46b175d">
<h3 id="org46b175d">Option 1: Extract embedding vectors from \(\mathbf{W_1}\)</h3>
<div class="outline-text-3" id="text-org46b175d">
<p>The first option is to take the columns of \(\mathbf{W_1}\) as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.</p>
<p><b>Note:</b> in this practice notebooks the values of the word embedding vectors are meaningless since we only trained for a single iteration with just one training example, but here's how you would proceed after the training process is complete.</p>
<p>For example \(\mathbf{W_1}\) is this matrix:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]
 [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]
 [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]
</pre>
<p>The first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.</p>
<p>These are the words corresponding to the columns.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" - </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
- am
- because
- happy
- i
- learning
</pre>
<p>And the word embedding vectors corresponding to each word are:</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">word_embedding_vector</span> <span class="o">=</span> <span class="n">W1</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">:    </span><span class="se">\t</span><span class="si">{</span><span class="n">word_embedding_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
am:     [0.41687358 0.32735501 0.26637602]
because:        [ 0.08854191  0.22795148 -0.23846886]
happy:          [-0.23495225 -0.23951958 -0.37770863]
i:      [ 0.28320538  0.4117634  -0.11399446]
learning:       [ 0.41800106 -0.23924344  0.34008124]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org2b8eea3">
<h3 id="org2b8eea3">Option 2: Extract embedding vectors from \(\mathbf{W_2}\)</h3>
<div class="outline-text-3" id="text-org2b8eea3">
<p>The second option is to transpose \(\mathbf{W_2}\) and take the columns of this transposed matrix as the word embedding vectors just like you did for \(\mathbf{W_1}\).</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[-0.22182064  0.08476603  0.1871551   0.07055222  0.33480474]
 [-0.43008631  0.08123194 -0.06107263 -0.02015138 -0.39423389]
 [ 0.13310965  0.1772054  -0.1790735   0.36107434 -0.43959196]]
</pre>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">word_embedding_vector</span> <span class="o">=</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">:    </span><span class="se">\t</span><span class="si">{</span><span class="n">word_embedding_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
am:     [-0.22182064 -0.43008631  0.13310965]
because:        [0.08476603 0.08123194 0.1772054 ]
happy:          [ 0.1871551  -0.06107263 -0.1790735 ]
i:      [ 0.07055222 -0.02015138  0.36107434]
learning:       [ 0.33480474 -0.39423389 -0.43959196]
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgcce76a3">
<h3 id="orgcce76a3">Option 3: extract embedding vectors from \(\mathbf{W_1}\) and \(\mathbf{W_2}\)</h3>
<div class="outline-text-3" id="text-orgcce76a3">
<p>The third option, which is the one you will use in this week's assignment, uses the average of \(\mathbf{W_1}\) and \(\mathbf{W_2^\intercal}\).</p>
<p><b>Calculate the average of \(\mathbf{W_1}\) and \(\mathbf{W_2^\intercal}\), and store the result in <code>W3</code>.</b></p>
<div class="highlight">
<pre><span></span><span class="n">W3</span> <span class="o">=</span> <span class="p">(</span><span class="n">W1</span> <span class="o">+</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W3</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.09752647</span><span class="p">,</span>  <span class="mf">0.08665397</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02389858</span><span class="p">,</span>  <span class="mf">0.1768788</span> <span class="p">,</span>  <span class="mf">0.3764029</span> <span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.05136565</span><span class="p">,</span>  <span class="mf">0.15459171</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15029611</span><span class="p">,</span>  <span class="mf">0.19580601</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.31673866</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.19974284</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03063173</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.27839106</span><span class="p">,</span>  <span class="mf">0.12353994</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04975536</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.09752647  0.08665397 -0.02389858  0.1768788   0.3764029 ]
 [-0.05136565  0.15459171 -0.15029611  0.19580601 -0.31673866]
 [ 0.19974284 -0.03063173 -0.27839106  0.12353994 -0.04975536]]
</pre>
<p>Extracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you've just created.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">word_embedding_vector</span> <span class="o">=</span> <span class="n">W3</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">:    </span><span class="se">\t</span><span class="si">{</span><span class="n">word_embedding_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
am:     [ 0.09752647 -0.05136565  0.19974284]
because:        [ 0.08665397  0.15459171 -0.03063173]
happy:          [-0.02389858 -0.15029611 -0.27839106]
i:      [0.1768788  0.19580601 0.12353994]
learning:       [ 0.3764029  -0.31673866 -0.04975536]
</pre>
<p>Now you know 3 different options to get the word embedding vectors from a model.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org794e34f">
<h2 id="org794e34f">End</h2>
<div class="outline-text-2" id="text-org794e34f">
<p>Now we've gone through the process of training a CBOW model in order to create word embeddings. The steps were:</p>
<ul class="org-ul">
<li><a href="posts/nlp/word-embeddings-data-preparation/">preparing the data</a></li>
<li><a href="posts/nlp/introducing-the-cbow-model/">creating the CBOW model</a></li>
<li><a href="posts/nlp/training-the-cbow-model/">training the model</a></li>
<li>Extracting the word embedding vectors from the model.</li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/training-the-cbow-model/">Training the CBOW Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/training-the-cbow-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-09T18:34:27-08:00" itemprop="datePublished" title="2020-12-09 18:34">2020-12-09 18:34</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org80c300e">Beginning</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org5126391">Imports</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org069baa5">Functions from Previous Posts</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#orgfa2f344">Data Preparation Functions</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org4219a3f">Activation Functions</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/training-the-cbow-model/#org7d7f8b2">Word Embeddings: Training the CBOW model</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org93ab497">Neural Network Initialization</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org3ce8d2b">Initialization of the weights and biases</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#orge8d692e">Define the first matrix of weights</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org42d4226">Define the second matrix of weights</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#orge366336">Define the first vector of biases</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org0b60557">Define the second vector of biases</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#orgcf7339f">Define the tokenized version of the corpus</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org11a1223">Get 'word_to_index' and 'Ind2word' dictionaries for the tokenized corpus</a></li>
</ul>
</li>
<li><a href="posts/nlp/training-the-cbow-model/#org9f9ce10">The First Training Example</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org47aa186">Forward Propagation</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#orgda8737c">The Hidden Layer</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#orgf4fe83b">The Output Layer</a></li>
</ul>
</li>
<li><a href="posts/nlp/training-the-cbow-model/#org7789a92">Cross-Entropy Loss</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#orga03c441">Backpropagation</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org1c2ad3f">Gradient descent</a></li>
</ul>
</li>
<li><a href="posts/nlp/training-the-cbow-model/#orgacbf6ca">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org80c300e">
<h2 id="org80c300e">Beginning</h2>
<div class="outline-text-2" id="text-org80c300e">
<p>Previously we looked at <a href="posts/nlp/word-embeddings-data-preparation/">preparing the data</a> and how to set up the <a href="posts/nlp/introducing-the-cbow-model/">CBOW Model</a>, now we'll look at training the model.</p>
</div>
<div class="outline-3" id="outline-container-org5126391">
<h3 id="org5126391">Imports</h3>
<div class="outline-text-3" id="text-org5126391">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">be_true</span><span class="p">,</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org069baa5">
<h3 id="org069baa5">Functions from Previous Posts</h3>
<div class="outline-text-3" id="text-org069baa5"></div>
<div class="outline-4" id="outline-container-orgfa2f344">
<h4 id="orgfa2f344">Data Preparation Functions</h4>
<div class="outline-text-4" id="text-orgfa2f344">
<p>These were previously defined in <a href="posts/nlp/word-embeddings-data-preparation/">Word Embeddings: Data Preparation</a> post.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">window_generator</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">"""Generates windows of words</span>

<span class="sd">    Args:</span>
<span class="sd">     words: cleaned tokens</span>
<span class="sd">     half_window: number of words in the half-window</span>

<span class="sd">    Yields:</span>
<span class="sd">     the next window</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">center_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">half_window</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">center_index</span><span class="p">]</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">)</span> <span class="p">:</span> <span class="n">center_index</span><span class="p">]</span>
                         <span class="o">+</span> <span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):(</span><span class="n">center_index</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>
        <span class="k">yield</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">index_word_maps</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Creates index to word mappings</span>

<span class="sd">    The index is based on sorted unique tokens in the data</span>

<span class="sd">    Args:</span>
<span class="sd">       data: the data you want to pull from</span>

<span class="sd">    Returns:</span>
<span class="sd">       word2Ind: returns dictionary mapping the word to its index</span>
<span class="sd">       Ind2Word: returns dictionary mapping the index to its word</span>
<span class="sd">    """</span>
    <span class="n">words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

    <span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="n">index_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_word</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create a one-hot-encoded vector</span>

<span class="sd">    Args:</span>
<span class="sd">     word: the word from the corpus that we're encoding</span>
<span class="sd">     word_to_index: map of the word to the index</span>
<span class="sd">     vocabulary_size: the size of the vocabulary</span>

<span class="sd">    Returns:</span>
<span class="sd">     vector with all zeros except where the word is</span>
<span class="sd">    """</span>
    <span class="n">one_hot_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
    <span class="n">one_hot_vector</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">one_hot_vector</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">def</span> <span class="nf">context_words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                            <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create vector with the mean of the one-hot-vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to covert to one-hot vectors</span>
<span class="sd">     word_to_index: dict mapping word to index</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="n">context_words_vectors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROWS</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">training_example_generator</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="sd">"""generates training examples</span>

<span class="sd">    Args:</span>
<span class="sd">     words: source of words</span>
<span class="sd">     half_window: half the window size</span>
<span class="sd">     word_to_index: dict with word to index mapping</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="n">window_generator</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">context_words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">),</span>
               <span class="n">word_to_one_hot_vector</span><span class="p">(</span>
                   <span class="n">center_word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">))</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4219a3f">
<h4 id="org4219a3f">Activation Functions</h4>
<div class="outline-text-4" id="text-org4219a3f">
<p>These functions were defined in the <a href="posts/nlp/introducing-the-cbow-model/">Introducing the CBOW Model</a> post.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Get the ReLU for the input array</span>

<span class="sd">    Args:</span>
<span class="sd">     z: an array of numbers</span>

<span class="sd">    Returns:</span>
<span class="sd">     ReLU of z</span>
<span class="sd">    """</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">result</span><span class="p">[</span><span class="n">result</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate Softmax for the input</span>

<span class="sd">    Args:</span>
<span class="sd">     v: array of values</span>

<span class="sd">    Returns:</span>
<span class="sd">     array of probabilities</span>
<span class="sd">    """</span>
    <span class="n">e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">sum_e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">e_z</span> <span class="o">/</span> <span class="n">sum_e_z</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7d7f8b2">
<h2 id="org7d7f8b2">Word Embeddings: Training the CBOW model</h2>
<div class="outline-text-2" id="text-org7d7f8b2">
<p>In previous lecture notebooks you saw how to prepare data before feeding it to a continuous bag-of-words model, the model itself, its architecture and activation functions. This notebook will walk you through:</p>
<ul class="org-ul">
<li>Forward propagation.</li>
<li>Cross-entropy loss.</li>
<li>Backpropagation.</li>
<li>Gradient descent.</li>
</ul>
<p>Which are concepts necessary to understand how the training of the model works.</p>
</div>
<div class="outline-3" id="outline-container-org93ab497">
<h3 id="org93ab497">Neural Network Initialization</h3>
<div class="outline-text-3" id="text-org93ab497">
<p>Let's dive into the neural network itself, which is shown below with all the dimensions and formulas you'll need.</p>
<p>Set <i>N</i> equal to 3. Remember that <i>N</i> is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.</p>
<p>Also set <i>V</i> equal to 5, which is the size of the vocabulary we have used so far.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Define the size of the word embedding vectors and save it in the variable 'N'</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Define V. Remember this was the size of the vocabulary in the previous lecture notebooks</span>
<span class="n">V</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org3ce8d2b">
<h3 id="org3ce8d2b">Initialization of the weights and biases</h3>
<div class="outline-text-3" id="text-org3ce8d2b">
<p>Before you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.</p>
<p>In the assignment you will implement a function to do this yourself using <code>numpy.random.rand</code>. In this notebook, we've pre-populated these matrices and vectors for you.</p>
</div>
<div class="outline-4" id="outline-container-orge8d692e">
<h4 id="orge8d692e">Define the first matrix of weights</h4>
<div class="outline-text-4" id="text-orge8d692e">
<div class="highlight">
<pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.41687358</span><span class="p">,</span>  <span class="mf">0.08854191</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23495225</span><span class="p">,</span>  <span class="mf">0.28320538</span><span class="p">,</span>  <span class="mf">0.41800106</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.32735501</span><span class="p">,</span>  <span class="mf">0.22795148</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23951958</span><span class="p">,</span>  <span class="mf">0.4117634</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.23924344</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.26637602</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23846886</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.37770863</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11399446</span><span class="p">,</span>  <span class="mf">0.34008124</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org42d4226">
<h4 id="org42d4226">Define the second matrix of weights</h4>
<div class="outline-text-4" id="text-org42d4226">
<div class="highlight">
<pre><span></span><span class="n">W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.22182064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43008631</span><span class="p">,</span>  <span class="mf">0.13310965</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.08476603</span><span class="p">,</span>  <span class="mf">0.08123194</span><span class="p">,</span>  <span class="mf">0.1772054</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.1871551</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.06107263</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1790735</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.07055222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02015138</span><span class="p">,</span>  <span class="mf">0.36107434</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.33480474</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.39423389</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43959196</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge366336">
<h4 id="orge366336">Define the first vector of biases</h4>
<div class="outline-text-4" id="text-orge366336">
<div class="highlight">
<pre><span></span><span class="n">b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.09688219</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.29239497</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.27364426</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org0b60557">
<h4 id="org0b60557">Define the second vector of biases</h4>
<div class="outline-text-4" id="text-org0b60557">
<div class="highlight">
<pre><span></span><span class="n">b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.0352008</span> <span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.36393384</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.12775555</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.34802326</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.07017815</span><span class="p">]])</span>
</pre></div>
<p><b>Check that the dimensions of these matrices are correct.</b></p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'V (vocabulary size): </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'N (embedding size / size of the hidden layer): </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of W1: </span><span class="si">{</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (NxV)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of b1: </span><span class="si">{</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Nx1)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of W2: </span><span class="si">{</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (VxN)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of b2: </span><span class="si">{</span><span class="n">b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Vx1)'</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example">
V (vocabulary size): 5
N (embedding size / size of the hidden layer): 3
size of W1: (3, 5) (NxV)
size of b1: (3, 1) (Nx1)
size of W2: (5, 3) (VxN)
size of b2: (5, 1) (Vx1)
</pre>
<p>Before moving forward, you will need some functions and variables defined in previous notebooks. They can be found next. Be sure you understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgcf7339f">
<h4 id="orgcf7339f">Define the tokenized version of the corpus</h4>
<div class="outline-text-4" id="text-orgcf7339f">
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org11a1223">
<h4 id="org11a1223">Get 'word_to_index' and 'Ind2word' dictionaries for the tokenized corpus</h4>
<div class="outline-text-4" id="text-org11a1223">
<div class="highlight">
<pre><span></span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_word</span> <span class="o">=</span> <span class="n">index_word_maps</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org9f9ce10">
<h3 id="org9f9ce10">The First Training Example</h3>
<div class="outline-text-3" id="text-org9f9ce10">
<p>Run the next cells to get the first training example, made of the vector representing the context words "i am because i", and the target which is the one-hot vector representing the center word "happy".</p>
<div class="highlight">
<pre><span></span><span class="n">training_examples</span> <span class="o">=</span> <span class="n">training_example_generator</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span>
<span class="n">x_array</span><span class="p">,</span> <span class="n">y_array</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">training_examples</span><span class="p">)</span>
</pre></div>
<p>In this notebook <code>next</code> is used because you will only be performing one iteration of training. In this week's assignment with the full training over several iterations you'll use regular <code>for</code> loops with the iterator that supplies the training examples.</p>
<p>The vector representing the context words, which will be fed into the neural network, is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_array</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.25 0.25 0.   0.5  0.  ]
</pre>
<p>The one-hot vector representing the center word to be predicted is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_array</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 1. 0. 0.]
</pre>
<p>Now convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook.</p>
<div class="highlight">
<pre><span></span> <span class="c1"># Copy vector</span>
 <span class="n">x</span> <span class="o">=</span> <span class="n">x_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

 <span class="c1"># Reshape it</span>
 <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

 <span class="c1"># Print it</span>
 <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'x:</span><span class="se">\n</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

 <span class="c1"># Copy vector</span>
 <span class="n">y</span> <span class="o">=</span> <span class="n">y_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

 <span class="c1"># Reshape it</span>
 <span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

 <span class="c1"># Print it</span>
 <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'y:</span><span class="se">\n</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
x:
[[0.25]
 [0.25]
 [0.  ]
 [0.5 ]
 [0.  ]]

y:
[[0.]
 [0.]
 [1.]
 [0.]
 [0.]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org47aa186">
<h3 id="org47aa186">Forward Propagation</h3>
<div class="outline-text-3" id="text-org47aa186"></div>
<div class="outline-4" id="outline-container-orgda8737c">
<h4 id="orgda8737c">The Hidden Layer</h4>
<div class="outline-text-4" id="text-orgda8737c">
<p>Now that you have initialized all the variables that you need for forward propagation, you can calculate the values of the hidden layer using the following formulas:</p>
\begin{align} \mathbf{z_1} = \mathbf{W_1}\mathbf{x} + \mathbf{b_1} \tag{1} \\ \mathbf{h} = \mathrm{ReLU}(\mathbf{z_1}) \tag{2} \\ \end{align}
<p>First, you can calculate the value of \(\mathbf{z_1}\).</p>
<p>Compute z1 (values of first hidden layer before applying the ReLU function)</p>
<div class="highlight">
<pre><span></span><span class="n">z1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
</pre></div>
<p>As expected you get an \(N\) by 1 matrix, or column vector with <i>N</i> elements, where <i>N</i> is equal to the embedding size, which is 3 in this example.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.36483875]
 [ 0.63710329]
 [-0.3236647 ]]
</pre>
<p>You can now take the ReLU of \(\mathbf{z_1}\) to get \(\mathbf{h}\), the vector with the values of the hidden layer.</p>
<p>Compute h (z1 after applying ReLU function)</p>
<div class="highlight">
<pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.36483875]
 [0.63710329]
 [0.        ]]
</pre>
<p>Applying ReLU means that the negative element of \(\mathbf{z_1}\) has been replaced with a zero.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgf4fe83b">
<h4 id="orgf4fe83b">The Output Layer</h4>
<div class="outline-text-4" id="text-orgf4fe83b">
<p>Here are the formulas you need to calculate the values of the output layer, represented by the vector \(\mathbf{\hat y}\):</p>
\begin{align} \mathbf{z_2} &amp;= \mathbf{W_2}\mathbf{h} + \mathbf{b_2} \tag{3} \\ \mathbf{\hat y} &amp;= \mathrm{softmax}(\mathbf{z_2}) \tag{4} \\ \end{align}
<p><b>First, calculate \(\mathbf{z_2}\).</b></p>
<p>Compute z2 (values of the output layer before applying the softmax function)</p>
<div class="highlight">
<pre><span></span><span class="n">z2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.31973737</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.28125477</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.09838369</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.33512159</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.19919612</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[-0.31973737]
 [-0.28125477]
 [-0.09838369]
 [-0.33512159]
 [-0.19919612]]
</pre>
<p>This is a <i>V</i> by 1 matrix, where <i>V</i> is the size of the vocabulary, which is 5 in this example.</p>
<p><b>Now calculate the value of \(\mathbf{\hat y}\).</b></p>
<p>Compute y_hat (z2 after applying softmax function)</p>
<div class="highlight">
<pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.18519074</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.19245626</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.23107446</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.18236353</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.20891502</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.18519074]
 [0.19245626]
 [0.23107446]
 [0.18236353]
 [0.20891502]]
</pre>
<p>As you've performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.</p>
<p><b>That being said, what word did the neural network predict?</b></p>
<div class="highlight">
<pre><span></span><span class="n">prediction</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The predicted word at index </span><span class="si">{</span><span class="n">prediction</span><span class="si">}</span><span class="s2"> is '</span><span class="si">{</span><span class="n">index_to_word</span><span class="p">[</span><span class="n">prediction</span><span class="p">]</span><span class="si">}</span><span class="s2">'."</span><span class="p">)</span>
</pre></div>
<pre class="example">
The predicted word at index 2 is 'happy'.
</pre>
<p>The neural network predicted the word "happy": the largest element of \(\mathbf{\hat y}\) is the third one, and the third word of the vocabulary is "happy".</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7789a92">
<h3 id="org7789a92">Cross-Entropy Loss</h3>
<div class="outline-text-3" id="text-org7789a92">
<p>Now that you have the network's prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.</p>
<p>Remember that you are working on a single training example, not on a batch of examples, which is why you are using <b>loss</b> and not <b>cost</b>, which is the generalized form of loss.</p>
<p>First let's recall what the prediction was.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.18519074]
 [0.19245626]
 [0.23107446]
 [0.18236353]
 [0.20891502]]
</pre>
<p>And the actual target value is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.]
 [0.]
 [1.]
 [0.]
 [0.]]
</pre>
<p>The formula for cross-entropy loss is:</p>
<p>\[ J=-\sum\limits_{k=1}^{V}y_k\log{\hat{y}_k} \tag{6} \]</p>
<p><b>Try implementing the cross-entropy loss function so you get more familiar working with numpy.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                       <span class="n">y_actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate cross-entropy loss  for the prediction</span>

<span class="sd">    Args:</span>
<span class="sd">     y_predicted: what our model predicted</span>
<span class="sd">     y_actual: the known labels</span>

<span class="sd">    Returns:</span>
<span class="sd">     cross-entropy loss for y_predicted</span>
<span class="sd">    """</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_actual</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
<p>Hint 1:</p>
<p>To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, you can simply use the &lt;code&gt;*&lt;/code&gt; operator.</p>
<p>Hint 2:</p>
<p>Once you have a vector equal to the element-wise multiplication of <code>y</code> and <code>y_hat</code>, you can use <code>numpy.sum</code> to calculate the sum of the elements of this vector.</p>
<p><b>Now use this function to calculate the loss with the actual values of \(\mathbf{y}\) and \(\mathbf{\hat y}\).</b></p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">1.4650152923611106</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
1.465
</pre>
<p>This value is neither good nor bad, which is expected as the neural network hasn't learned anything yet.</p>
<p>The actual learning will start during the next phase: backpropagation.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orga03c441">
<h3 id="orga03c441">Backpropagation</h3>
<div class="outline-text-3" id="text-orga03c441">
<p>The formulas that you will implement for backpropagation are the following.</p>
\begin{align} \frac{\partial J}{\partial \mathbf{W_1}} &amp;= \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right )\mathbf{x}^\top \tag{7}\\ \frac{\partial J}{\partial \mathbf{W_2}} &amp;= (\mathbf{\hat{y}} - \mathbf{y})\mathbf{h^\top} \tag{8}\\ \frac{\partial J}{\partial \mathbf{b_1}} &amp;= \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right ) \tag{9}\\ \frac{\partial J}{\partial \mathbf{b_2}} &amp;= \mathbf{\hat{y}} - \mathbf{y} \tag{10} \end{align}
<p><b>*Note:</b> these formulas are slightly simplified compared to the ones in the lecture as you're working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you'll be implementing the latter.</p>
<p>Let's start with an easy one.</p>
<p><b>Calculate the partial derivative of the loss function with respect to \(\mathbf{b_2}\), and store the result in <code>grad_b2</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{b_2}} = \mathbf{\hat{y}} - \mathbf{y} \tag{10} \]</p>
<p>Compute vector with partial derivatives of loss function with respect to b2</p>
<div class="highlight">
<pre><span></span><span class="n">grad_b2</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_b2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.18519074</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.19245626</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.76892554</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.18236353</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.20891502</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_b2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.18519074]
 [ 0.19245626]
 [-0.76892554]
 [ 0.18236353]
 [ 0.20891502]]
</pre>
<p><b>Next, calculate the partial derivative of the loss function with respect to \(\mathbf{W_2}\), and store the result in <code>grad_W2</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{W_2}} = (\mathbf{\hat{y}} - \mathbf{y})\mathbf{h^\top} \tag{8} \]</p>
<p>Hint: use <code>.T</code> to get a transposed matrix, e.g. <code>h.T</code> returns \(\mathbf{h^\top}\).</p>
<p>Compute matrix with partial derivatives of loss function with respect to W2.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_W2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.06756476</span><span class="p">,</span>  <span class="mf">0.11798563</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.0702155</span> <span class="p">,</span>  <span class="mf">0.12261452</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.28053384</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.48988499</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.06653328</span><span class="p">,</span>  <span class="mf">0.1161844</span> <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.07622029</span><span class="p">,</span>  <span class="mf">0.13310045</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_W2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.06756476  0.11798563  0.        ]
 [ 0.0702155   0.12261452  0.        ]
 [-0.28053384 -0.48988499  0.        ]
 [ 0.06653328  0.1161844   0.        ]
 [ 0.07622029  0.13310045  0.        ]]
</pre>
<p><b>Now calculate the partial derivative with respect to \(\mathbf{b_1}\) and store the result in <code>grad_b1</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{b_1}} = \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right ) \tag{9} \]</p>
<p>Compute vector with partial derivatives of loss function with respect to b1.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_b1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_b1</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.17045858</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_b1</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.        ]
 [0.        ]
 [0.17045858]]
</pre>
<p><b>Finally, calculate the partial derivative of the loss with respect to \(\mathbf{W_1}\), and store it in <code>grad_W1</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{W_1}} = \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right )\mathbf{x}^\top \tag{7} \] Compute matrix with partial derivatives of loss function with respect to W1.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">relu</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)),</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_W1</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.04261464</span><span class="p">,</span> <span class="mf">0.04261464</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.08522929</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_W1</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.04261464 0.04261464 0.         0.08522929 0.        ]]
</pre>
<p>Before moving on to gradient descent, double-check that all the matrices have the expected dimensions.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'V (vocabulary size): </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'N (embedding size / size of the hidden layer): </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_W1: </span><span class="si">{</span><span class="n">grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (NxV)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_b1: </span><span class="si">{</span><span class="n">grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Nx1)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_W2: </span><span class="si">{</span><span class="n">grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (VxN)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_b2: </span><span class="si">{</span><span class="n">grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Vx1)'</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example">
V (vocabulary size): 5
N (embedding size / size of the hidden layer): 3
size of grad_W1: (3, 5) (NxV)
size of grad_b1: (3, 1) (Nx1)
size of grad_W2: (5, 3) (VxN)
size of grad_b2: (5, 1) (Vx1)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org1c2ad3f">
<h3 id="org1c2ad3f">Gradient descent</h3>
<div class="outline-text-3" id="text-org1c2ad3f">
<p>During the gradient descent phase, you will update the weights and biases by subtracting \(\alpha\) times the gradient from the original matrices and vectors, using the following formulas.</p>
\begin{align} \mathbf{W_1} &amp;\gets \mathbf{W_1} - \alpha \frac{\partial J}{\partial \mathbf{W_1}} \tag{11}\\ \mathbf{W_2} &amp;\gets \mathbf{W_2} - \alpha \frac{\partial J}{\partial \mathbf{W_2}} \tag{12}\\ \mathbf{b_1} &amp;\gets \mathbf{b_1} - \alpha \frac{\partial J}{\partial \mathbf{b_1}} \tag{13}\\ \mathbf{b_2} &amp;\gets \mathbf{b_2} - \alpha \frac{\partial J}{\partial \mathbf{b_2}} \tag{14}\\ \end{align}
<p>First, let set a value for \(\alpha\).</p>
<div class="highlight">
<pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.03</span>
</pre></div>
<p>The updated weight matrix \(\mathbf{W_1}\) will be:</p>
<div class="highlight">
<pre><span></span><span class="n">W1_new</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
</pre></div>
<p>Let's compare the previous and new values of \(\mathbf{W_1}\):</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'old value of W1:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'new value of W1:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W1_new</span><span class="p">)</span>
</pre></div>
<pre class="example">
old value of W1:
[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]
 [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]
 [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]

new value of W1:
[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]
 [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]
 [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]
</pre>
<p>The difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.</p>
<p><b>Now calculate the new values of \(\mathbf{W_2}\) (to be stored in <code>W2_new</code>), \(\mathbf{b_1}\) (in <code>b1_new</code>), and \(\mathbf{b_2}\) (in <code>b2_new</code>).</b></p>
\begin{align} \mathbf{W_2} &amp;\gets \mathbf{W_2} - \alpha \frac{\partial J}{\partial \mathbf{W_2}} \tag{12}\\ \mathbf{b_1} &amp;\gets \mathbf{b_1} - \alpha \frac{\partial J}{\partial \mathbf{b_1}} \tag{13}\\ \mathbf{b_2} &amp;\gets \mathbf{b_2} - \alpha \frac{\partial J}{\partial \mathbf{b_2}} \tag{14}\\ \end{align}
<p>Compute updated W2.</p>
<div class="highlight">
<pre><span></span><span class="n">W2_new</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
</pre></div>
<p>Compute updated b1.</p>
<div class="highlight">
<pre><span></span><span class="n">b1_new</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
</pre></div>
<p>Compute updated b2.</p>
<div class="highlight">
<pre><span></span><span class="n">b2_new</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'W2_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W2_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b1_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b1_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b2_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b2_new</span><span class="p">)</span>

<span class="n">w2_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span><span class="o">-</span><span class="mf">0.22384758</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43362588</span><span class="p">,</span>  <span class="mf">0.13310965</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.08265956</span><span class="p">,</span>  <span class="mf">0.0775535</span> <span class="p">,</span>  <span class="mf">0.1772054</span> <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.19557112</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04637608</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1790735</span> <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.06855622</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02363691</span><span class="p">,</span>  <span class="mf">0.36107434</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.33251813</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3982269</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.43959196</span><span class="p">]])</span>

<span class="n">b1_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span> <span class="mf">0.09688219</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.29239497</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.27875802</span><span class="p">]])</span>

<span class="n">b2_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span> <span class="mf">0.02964508</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.36970753</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.10468778</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.35349417</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.0764456</span> <span class="p">]]</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">actual</span><span class="p">,</span> <span class="n">expected</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">((</span><span class="n">W2_new</span><span class="p">,</span> <span class="n">b1_new</span><span class="p">,</span> <span class="n">b2_new</span><span class="p">),</span> <span class="p">(</span><span class="n">w2_expected</span><span class="p">,</span> <span class="n">b1_expected</span><span class="p">,</span> <span class="n">b2_expected</span><span class="p">)):</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
W2_new
[[-0.22384758 -0.43362588  0.13310965]
 [ 0.08265956  0.0775535   0.1772054 ]
 [ 0.19557112 -0.04637608 -0.1790735 ]
 [ 0.06855622 -0.02363691  0.36107434]
 [ 0.33251813 -0.3982269  -0.43959196]]

b1_new
[[ 0.09688219]
 [ 0.29239497]
 [-0.27875802]]

b2_new
[[ 0.02964508]
 [-0.36970753]
 [-0.10468778]
 [-0.35349417]
 [-0.0764456 ]]
</pre>
<p>Congratulations, you have completed one iteration of training using one training example!</p>
<p>You'll need many more iterations to fully train the neural network, and you can optimize the learning process by training on batches of examples, as described in the lecture. You will get to do this during this week's assignment.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgacbf6ca">
<h2 id="orgacbf6ca">End</h2>
<div class="outline-text-2" id="text-orgacbf6ca">
<p>Now that we know how to train the CBOW Model, we'll move on to <a href="posts/nlp/extracting-word-embeddings/">extracting word embeddings</a> from the model.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/introducing-the-cbow-model/">Introducing the CBOW Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/introducing-the-cbow-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-09T17:02:44-08:00" itemprop="datePublished" title="2020-12-09 17:02">2020-12-09 17:02</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/introducing-the-cbow-model/#orgad6ed44">The Continuous Bag-Of-Words (CBOW) Model</a>
<ul>
<li><a href="posts/nlp/introducing-the-cbow-model/#org16ab585">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/introducing-the-cbow-model/#org00398c0">Activation Functions</a>
<ul>
<li><a href="posts/nlp/introducing-the-cbow-model/#org88a6261">ReLU</a></li>
<li><a href="posts/nlp/introducing-the-cbow-model/#orge461234">SoftMax</a></li>
<li><a href="posts/nlp/introducing-the-cbow-model/#org538b14a">Dimensions: 1-D arrays vs 2-D column vectors</a></li>
</ul>
</li>
<li><a href="posts/nlp/introducing-the-cbow-model/#org014739c">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgad6ed44">
<h2 id="orgad6ed44">The Continuous Bag-Of-Words (CBOW) Model</h2>
<div class="outline-text-2" id="text-orgad6ed44">
<p>In the <a href="posts/nlp/word-embeddings-data-preparation/">previous post</a> we prepared our data, now we'll look at how the CBOW model is constructed.</p>
</div>
<div class="outline-3" id="outline-container-org16ab585">
<h3 id="org16ab585">Imports</h3>
<div class="outline-text-3" id="text-org16ab585">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">be_true</span><span class="p">,</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org00398c0">
<h2 id="org00398c0">Activation Functions</h2>
<div class="outline-text-2" id="text-org00398c0">
<p>Let's start by implementing the activation functions, ReLU and softmax.</p>
</div>
<div class="outline-3" id="outline-container-org88a6261">
<h3 id="org88a6261">ReLU</h3>
<div class="outline-text-3" id="text-org88a6261">
<p>ReLU is used to calculate the values of the hidden layer, in the following formulas:</p>
\begin{align} \mathbf{z_1} &amp;= \mathbf{W_1}\mathbf{x} + \mathbf{b_1} \tag{1} \\ \mathbf{h} &amp;= \mathrm{ReLU}(\mathbf{z_1}) \tag{2} \\ \end{align}
<p>Let's fix a value for \(\mathbf{z_1}\) as a working example.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Define a 5X1 column vector using numpy</span>
<span class="n">z_1</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">5</span>

<span class="c1"># Print the vector</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z_1</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 2.71320643]
 [-4.79248051]
 [ 1.33648235]
 [ 2.48803883]
 [-0.01492988]]
</pre>
<p>Notice that using numpy's <code>random.rand</code> function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then 5 is subtracted from them.</p>
<p>To get the ReLU of this vector, you want all the negative values to become zeros.</p>
<p>First create a copy of this vector.</p>
<div class="highlight">
<pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">z_1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>
<p>Now determine which of its values are negative.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">h</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[False]
 [ True]
 [False]
 [False]
 [ True]]
</pre>
<p>You can now simply set all of the values which are negative to 0.</p>
<div class="highlight">
<pre><span></span><span class="n">h</span><span class="p">[</span><span class="n">h</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
<p>And that's it: you have the ReLU of \(\mathbf{z_1}\).</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[2.71320643]
 [0.        ]
 [1.33648235]
 [2.48803883]
 [0.        ]]
</pre>
<p><b>Now implement ReLU as a function.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Get the ReLU for the input array</span>

<span class="sd">    Args:</span>
<span class="sd">     z: an array of numbers</span>

<span class="sd">    Returns:</span>
<span class="sd">     ReLU of z</span>
<span class="sd">    """</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">result</span><span class="p">[</span><span class="n">result</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
<p><b>And check that it's working.</b></p>
<div class="highlight">
<pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.25459881</span><span class="p">],</span>
              <span class="p">[</span> <span class="mf">4.50714306</span><span class="p">],</span>
              <span class="p">[</span> <span class="mf">2.31993942</span><span class="p">],</span>
              <span class="p">[</span> <span class="mf">0.98658484</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">3.4398136</span> <span class="p">]])</span>

<span class="c1"># Apply ReLU to it</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span>        <span class="p">],</span>
                        <span class="p">[</span><span class="mf">4.50714306</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">2.31993942</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">0.98658484</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">0.</span>        <span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.        ]
 [4.50714306]
 [2.31993942]
 [0.98658484]
 [0.        ]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-orge461234">
<h3 id="orge461234">SoftMax</h3>
<div class="outline-text-3" id="text-orge461234">
<p>The second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:</p>
\begin{align} \mathbf{z_2} &amp;= \mathbf{W_2}\mathbf{h} + \mathbf{b_2} \tag{3} \\ \mathbf{\hat y} &amp;= \mathrm{softmax}(\mathbf{z_2}) \tag{4} \\ \end{align}
<p>To calculate softmax of a vector \(\mathbf{z}\), the <i>i</i>-th component of the resulting vector is given by:</p>
<p>\[ \textrm{softmax}(\textbf{z})_i = \frac{e^{z_i} }{\sum\limits_{j=1}^{V} e^{z_j} } \tag{5} \]</p>
<p>Let's work through an example.</p>
<div class="highlight">
<pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
<pre class="example">
[ 9.   8.  11.  10.   8.5]
</pre>
<p>You'll need to calculate the exponentials of each element, both for the numerator and for the denominator.</p>
<div class="highlight">
<pre><span></span><span class="n">e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
</pre></div>
<pre class="example">
[ 8103.08392758  2980.95798704 59874.1417152  22026.46579481
  4914.7688403 ]
</pre>
<p>The denominator is equal to the sum of these exponentials.</p>
<div class="highlight">
<pre><span></span><span class="n">sum_e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">sum_e_z</span><span class="si">:</span><span class="s2">,.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
97,899.42
</pre>
<p>And the value of the first element of \(\textrm{softmax}(\textbf{z})\) is given by:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">e_z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">sum_e_z</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
0.0828
</pre>
<p>This is for one element. You can use numpy's vectorized operations to calculate the values of all the elements of the \(\textrm{softmax}(\textbf{z})\) vector in one go.</p>
<p><b>Implement the softmax function.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate Softmax for the input</span>

<span class="sd">    Args:</span>
<span class="sd">     v: array of values</span>

<span class="sd">    Returns:</span>
<span class="sd">     array of probabilities</span>
<span class="sd">    """</span>
    <span class="n">e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">sum_e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">e_z</span> <span class="o">/</span> <span class="n">sum_e_z</span>
</pre></div>
<p><b>Now check that it works.</b></p>
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.08276948</span><span class="p">,</span>
                        <span class="mf">0.03044919</span><span class="p">,</span>
                        <span class="mf">0.61158833</span><span class="p">,</span>
                        <span class="mf">0.22499077</span><span class="p">,</span>
                        <span class="mf">0.05020223</span><span class="p">])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.08276948 0.03044919 0.61158833 0.22499077 0.05020223]
</pre>
<p>Notice that the sum of all these values is equal to 1.</p>
<div class="highlight">
<pre><span></span><span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">softmax</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">])))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org538b14a">
<h3 id="org538b14a">Dimensions: 1-D arrays vs 2-D column vectors</h3>
<div class="outline-text-3" id="text-org538b14a">
<p>Before moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let's have a look at the dimensions of the vectors you've been handling until now.</p>
<p>Create a vector of length <i>V</i> filled with zeros.</p>
<p>Define V. Remember this was the size of the vocabulary in the previous lecture notebook</p>
<div class="highlight">
<pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
<p>Define vector of length V filled with zeros</p>
<div class="highlight">
<pre><span></span><span class="n">x_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_array</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 0. 0. 0.]
</pre>
<p>This is a 1-dimensional array, as revealed by the <code>.shape</code> property of the array.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_array</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(5,)
</pre>
<p>To perform matrix multiplication in the next steps, you actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.</p>
<p>The easiest way to convert a 1D vector to a 2D column matrix is to set its `.shape` property to the number of rows and one column, as shown in the next cell.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Copy vector</span>
<span class="n">x_column_vector</span> <span class="o">=</span> <span class="n">x_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Reshape copy of vector</span>
<span class="n">x_column_vector</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># alternatively ... = (x_array.shape[0], 1)</span>

<span class="c1"># Print vector</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_column_vector</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
</pre>
<p>The shape of the resulting "vector" is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_column_vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(5, 1)
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org014739c">
<h2 id="org014739c">End</h2>
<div class="outline-text-2" id="text-org014739c">
<p>Now that we have the basics of the model we can move on to <a href="posts/nlp/training-the-cbow-model/">training the model</a>.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-data-preparation/">Word Embeddings: Data Preparation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-data-preparation/" rel="bookmark"><time class="published dt-published" datetime="2020-12-08T18:29:17-08:00" itemprop="datePublished" title="2020-12-08 18:29">2020-12-08 18:29</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-data-preparation/#orgff73531">Beginning</a></li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#org98eef07">Middle</a>
<ul>
<li><a href="posts/nlp/word-embeddings-data-preparation/#orgaa18d21">Cleaning and tokenization</a></li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#org9bd69af">Tokenize</a></li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#org5d6e3a4">More Processing</a></li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#org04ef953">Make It a Function</a></li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#orgdc7ea87">Sliding Window of Words</a></li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#org21a4307">Transforming words into vectors for the training set</a>
<ul>
<li><a href="posts/nlp/word-embeddings-data-preparation/#org3f97153">Mapping words to indices and indices to words</a></li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#orgc4289d6">Getting one-hot word vectors</a></li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#orgd65f99f">Getting context word vectors</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-data-preparation/#orgf4b0edd">Building the training set</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgff73531">
<h2 id="orgff73531">Beginning</h2>
<div class="outline-text-2" id="text-orgff73531">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">emoji</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org98eef07">
<h2 id="org98eef07">Middle</h2>
<div class="outline-text-2" id="text-org98eef07"></div>
<div class="outline-3" id="outline-container-orgaa18d21">
<h3 id="orgaa18d21">Cleaning and tokenization</h3>
<div class="outline-text-3" id="text-orgaa18d21">
<p>To demonstrate the cleaning and tokenization process, consider a corpus that contains emojis and various punctuation signs. Let's define a corpus:</p>
<div class="highlight">
<pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="s1">'Who ❤️ "word embeddings" in 2020? I do!!!'</span>
</pre></div>
<p>First, replace all interrupting punctuation signs — such as commas and exclamation marks — with periods.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Corpus:  </span><span class="si">{</span><span class="n">corpus</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Do the substitution</span>
<span class="n">ONE_OR_MORE</span> <span class="o">=</span> <span class="s2">"+"</span>
<span class="n">PERIOD</span> <span class="o">=</span> <span class="s2">"."</span>
<span class="n">PUNCTUATION</span> <span class="o">=</span> <span class="s2">",!?;-"</span>
<span class="n">CHARACTERS</span> <span class="o">=</span> <span class="s2">"[</span><span class="si">{}</span><span class="s2">]"</span>
<span class="n">EXPRESSION</span> <span class="o">=</span> <span class="n">CHARACTERS</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">PUNCTUATION</span><span class="p">)</span> <span class="o">+</span> <span class="n">ONE_OR_MORE</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">EXPRESSION</span><span class="p">,</span> <span class="n">PERIOD</span><span class="p">,</span> <span class="n">corpus</span><span class="p">)</span>
<span class="c1"># Print cleaned corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"After cleaning punctuation:  '</span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Corpus:  Who ❤️ "word embeddings" in 2020? I do!!!
After cleaning punctuation:  'Who ❤️ "word embeddings" in 2020. I do.'
</pre></div>
</div>
<div class="outline-3" id="outline-container-org9bd69af">
<h3 id="org9bd69af">Tokenize</h3>
<div class="outline-text-3" id="text-org9bd69af">
<p>Next, use NLTK's tokenization engine to split the corpus into individual tokens.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Print cleaned corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Initial string:  </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Tokenize the cleaned corpus</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Print the tokenized version of the corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'After tokenization:  </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
Initial string:  Who ❤️ "word embeddings" in 2020. I do.
After tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', "''", 'in', '2020', '.', 'I', 'do', '.']
</pre></div>
</div>
<div class="outline-3" id="outline-container-org5d6e3a4">
<h3 id="org5d6e3a4">More Processing</h3>
<div class="outline-text-3" id="text-org5d6e3a4">
<p>Finally, as you saw in the lecture, get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Print the tokenized version of the corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Initial list of tokens:  </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Filter tokenized corpus using list comprehension</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span> <span class="n">character</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">character</span> <span class="ow">in</span> <span class="n">data</span>
         <span class="k">if</span> <span class="nb">any</span><span class="p">((</span><span class="n">character</span><span class="o">.</span><span class="n">isalpha</span><span class="p">(),</span>
                 <span class="n">character</span><span class="o">==</span> <span class="s1">'.'</span><span class="p">,</span>
                 <span class="n">emoji</span><span class="o">.</span><span class="n">get_emoji_regexp</span><span class="p">()</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">character</span><span class="p">)))</span>
       <span class="p">]</span>

<span class="c1"># Print the tokenized and filtered version of the corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"After cleaning:  '</span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Initial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', "''", 'in', '2020', '.', 'I', 'do', '.']
After cleaning:  '['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']'
</pre>
<p>Note that the heart emoji is considered a token just like any normal word.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org04ef953">
<h3 id="org04ef953">Make It a Function</h3>
<div class="outline-text-3" id="text-org04ef953">
<p>Now let's streamline the cleaning and tokenization process by wrapping the previous steps in a function.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">corpus</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""clean and tokenize the corpus</span>

<span class="sd">    Args:</span>
<span class="sd">     corpus: original source text</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of cleaned tokens from the corpus</span>
<span class="sd">    """</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">EXPRESSION</span><span class="p">,</span> <span class="n">PERIOD</span><span class="p">,</span> <span class="n">corpus</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span> <span class="n">character</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">character</span> <span class="ow">in</span> <span class="n">data</span>
             <span class="k">if</span> <span class="nb">any</span><span class="p">((</span><span class="n">character</span><span class="o">.</span><span class="n">isalpha</span><span class="p">(),</span>
                     <span class="n">character</span><span class="o">==</span> <span class="s1">'.'</span><span class="p">,</span>
                     <span class="n">emoji</span><span class="o">.</span><span class="n">get_emoji_regexp</span><span class="p">()</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">character</span><span class="p">)))</span>
       <span class="p">]</span>    
    <span class="k">return</span> <span class="n">data</span>
</pre></div>
<p>Apply this function to the corpus that you'll be working on in the rest of this notebook: "I am happy because I am learning"</p>
<div class="highlight">
<pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="s1">'I am happy because I am learning'</span>

<span class="c1"># Print new corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Corpus:  </span><span class="si">{</span><span class="n">corpus</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># Save tokenized version of corpus into 'words' variable</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

<span class="c1"># Print the tokenized version of the corpus</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Words (tokens):  </span><span class="si">{</span><span class="n">words</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
Corpus:  I am happy because I am learning
Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']
</pre>
<p>An alternative sentence.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">"I'm tired of being a token! Where's all the other cheese-sniffing Gnomish at? I bet theres' at least 2 of us out there, or maybe more..."</span><span class="p">))</span>
</pre></div>
<pre class="example">
['i', 'tired', 'of', 'being', 'a', 'token', '.', 'where', 'all', 'the', 'other', 'gnomish', 'at', '.', 'i', 'bet', 'theres', 'at', 'least', 'of', 'us', 'out', 'there', '.', 'or', 'maybe', 'more']
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgdc7ea87">
<h3 id="orgdc7ea87">Sliding Window of Words</h3>
<div class="outline-text-3" id="text-orgdc7ea87">
<p>Now that you have transformed the corpus into a list of clean tokens, you can slide a window of words across this list. For each window you can extract a center word and the context words.</p>
<p>The <code>get_windows</code> function in the next cell was introduced in the lecture.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_windows</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">"""Generates windows of words</span>

<span class="sd">    Args:</span>
<span class="sd">     words: cleaned tokens</span>
<span class="sd">     half_window: number of words in the half-window</span>

<span class="sd">    Yields:</span>
<span class="sd">     the next window</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">center_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">half_window</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">center_index</span><span class="p">]</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">)</span> <span class="p">:</span> <span class="n">center_index</span><span class="p">]</span>
                         <span class="o">+</span> <span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):(</span><span class="n">center_index</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>
        <span class="k">yield</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span>
    <span class="k">return</span>
</pre></div>
<p>The first argument of this function is a list of words (or tokens). The second argument, <code>C</code>, is the context half-size. Recall that for a given center word, the context words are made of <code>C</code> words to the left and <code>C</code> words to the right of the center word.</p>
<p>Here is how you can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that you will use to train the CBOW model.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">get_windows</span><span class="p">([</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">],</span> <span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
['i', 'am', 'because', 'i']     happy
['am', 'happy', 'i', 'am']      because
['happy', 'because', 'am', 'learning']  i
</pre>
<p>The first example of the training set is made of:</p>
<ul class="org-ul">
<li>the context words "i", "am", "because", "i",</li>
<li>and the center word to be predicted: "happy".</li>
</ul>
<p><b>Now try it out yourself. In the next cell, you can change both the sentence and the context half-size.</b></p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">get_windows</span><span class="p">(</span><span class="n">tokenize</span><span class="p">(</span><span class="s2">"My baloney has a first name, it's Gerald."</span><span class="p">),</span> <span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
['my', 'baloney', 'a', 'first'] has
['baloney', 'has', 'first', 'name']     a
['has', 'a', 'name', '.']       first
['a', 'first', '.', 'it']       name
['first', 'name', 'it', 'gerald']       .
['name', '.', 'gerald', '.']    it
</pre></div>
</div>
<div class="outline-3" id="outline-container-org21a4307">
<h3 id="org21a4307">Transforming words into vectors for the training set</h3>
<div class="outline-text-3" id="text-org21a4307">
<p>To finish preparing the training set, you need to transform the context words and center words into vectors.</p>
</div>
<div class="outline-4" id="outline-container-org3f97153">
<h4 id="org3f97153">Mapping words to indices and indices to words</h4>
<div class="outline-text-4" id="text-org3f97153">
<p>The center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.</p>
<p>To create one-hot word vectors, you can start by mapping each unique word to a unique integer (or index). We have provided a helper function, <code>get_dict</code>, that creates a Python dictionary that maps words to integers and back.</p>
<p>Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_dict</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Creates index to word mappings</span>

<span class="sd">    The index is based on sorted unique tokens in the data</span>

<span class="sd">    Args:</span>
<span class="sd">       data: the data you want to pull from</span>

<span class="sd">    Returns:</span>
<span class="sd">       word2Ind: returns dictionary mapping the word to its index</span>
<span class="sd">       Ind2Word: returns dictionary mapping the index to its word</span>
<span class="sd">    """</span>
    <span class="n">words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

    <span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="n">index_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_word</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">word2Ind</span><span class="p">,</span> <span class="n">Ind2word</span> <span class="o">=</span> <span class="n">get_dict</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">word2Ind</span><span class="p">)</span>
</pre></div>
<pre class="example">
{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}
</pre>
<p>You can use this dictionary to get the index of a word.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Index of the word 'i':  </span><span class="si">{</span><span class="n">word2Ind</span><span class="p">[</span><span class="s1">'i'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Index of the word 'i':  3
</pre>
<p>And conversely, here's the dictionary that maps indices to words.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">Ind2word</span><span class="p">)</span>
</pre></div>
<pre class="example">
{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Word which has index 2:  '</span><span class="si">{</span><span class="n">Ind2word</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Word which has index 2:  'happy'
</pre>
<p>Finally, get the length of either of these dictionaries to get the size of the vocabulary of your corpus, in other words the number of different words making up the corpus.</p>
<div class="highlight">
<pre><span></span><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2Ind</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of vocabulary: </span><span class="si">{</span><span class="n">vocabulary_size</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Size of vocabulary: 5
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgc4289d6">
<h4 id="orgc4289d6">Getting one-hot word vectors</h4>
<div class="outline-text-4" id="text-orgc4289d6">
<p>Recall from the lecture that you can easily convert an integer, <i>n</i>, into a one-hot vector.</p>
<p>Consider the word "happy". First, retrieve its numeric index.</p>
<div class="highlight">
<pre><span></span><span class="n">happy_index</span> <span class="o">=</span> <span class="n">word2Ind</span><span class="p">[</span><span class="s1">'happy'</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">happy_index</span><span class="p">)</span>
</pre></div>
<pre class="example">
2
</pre>
<p>Now create a vector with the size of the vocabulary, and fill it with zeros. Create vector with the same length as the vocabulary, filled with zeros</p>
<div class="highlight">
<pre><span></span><span class="n">center_word_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">center_word_vector</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">center_word_vector</span><span class="p">)</span> <span class="o">==</span> <span class="n">vocabulary_size</span>
</pre></div>
<pre class="example">
[0. 0. 0. 0. 0.]
</pre>
<p>Next, replace the 0 of the $n$-th element with a 1.</p>
<p>Replace element number 'n' with a 1</p>
<div class="highlight">
<pre><span></span><span class="n">center_word_vector</span><span class="p">[</span><span class="n">happy_index</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
<p>And you have your one-hot word vector.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">center_word_vector</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 1. 0. 0.]
</pre>
<p><b>You can now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                           <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                           <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create a one-hot-vector with a 1 where the word is</span>


<span class="sd">    Args:</span>
<span class="sd">     word: known token to add to the vector</span>
<span class="sd">     word_to_index: dict mapping word: index</span>
<span class="sd">     vocabulary_size: how long to make the vector</span>

<span class="sd">    Returns:</span>
<span class="sd">     vector with zeros everywhere except where the word is</span>
<span class="sd">    """</span>
    <span class="n">one_hot_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
    <span class="n">one_hot_vector</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">one_hot_vector</span>
</pre></div>
<p>Check that it works as intended.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="s1">'happy'</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">))</span>
</pre></div>
<pre class="example">
[0. 0. 1. 0. 0.]
</pre>
<p><b>What is the word vector for "learning"?</b></p>
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="s1">'learning'</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">actual</span> <span class="o">==</span> <span class="n">expected</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 0. 0. 1.]
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgd65f99f">
<h4 id="orgd65f99f">Getting context word vectors</h4>
<div class="outline-text-4" id="text-orgd65f99f">
<p>To create the vectors that represent context words, you will calculate the average of the one-hot vectors representing the individual words.</p>
<p>Let's start with a list of context words.</p>
<div class="highlight">
<pre><span></span><span class="n">context_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">]</span>
</pre></div>
<p>Using Python's list comprehension construct and the <code>word_to_one_hot_vector</code> function that you created in the previous section, you can create a list of one-hot vectors representing each of the context words.</p>
<p>Create one-hot vectors for each context word using list comprehension</p>
<div class="highlight">
<pre><span></span><span class="n">context_words_vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">)</span>
</pre></div>
<pre class="example">
[array([0., 0., 0., 1., 0.]),
 array([1., 0., 0., 0., 0.]),
 array([0., 1., 0., 0., 0.]),
 array([0., 0., 0., 1., 0.])]
</pre>
<p>And you can now simply get the average of these vectors using numpy's <code>mean</code> function, to get the vector representation of the context words.</p>
<div class="highlight">
<pre><span></span><span class="n">ROWS</span><span class="p">,</span> <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">first</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROWS</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.25 0.25 0.   0.5  0.  ]
</pre>
<p><b>Now create the <code>context_words_to_vector</code> function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">context_words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                            <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create vector with the mean of the one-hot-vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to covert to one-hot vectors</span>
<span class="sd">     word_to_index: dict mapping word to index</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="n">context_words_vectors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROWS</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">words_to_vector</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">context_words_to_vector</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">word2Ind</span><span class="p">)</span>
<span class="n">second</span> <span class="o">=</span> <span class="n">words_to_vector</span><span class="p">([</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">second</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">first</span><span class="o">==</span><span class="n">second</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.25 0.25 0.   0.5  0.  ]
</pre>
<p><b>What is the vector representation of the context words "am happy i am"?</b></p>
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">words_to_vector</span><span class="p">([</span><span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span> <span class="p">,</span> <span class="mf">0.</span>  <span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.</span>  <span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">actual</span> <span class="o">==</span> <span class="n">expected</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.5  0.   0.25 0.25 0.  ]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf4b0edd">
<h3 id="orgf4b0edd">Building the training set</h3>
<div class="outline-text-3" id="text-orgf4b0edd">
<p>You can now combine the functions that you created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
<pre class="example">
['i', 'am', 'happy', 'because', 'i', 'am', 'learning']
</pre>
<p>To do this you need to use the sliding window function (<code>get_windows</code>) to extract the context words and center words, and you then convert these sets of words into a basic vector representation using <code>word_to_one_hot_vector</code> and <code>context_words_to_vector</code>.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="n">get_windows</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">half_window</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Context words:  </span><span class="si">{</span><span class="n">context_words</span><span class="si">}</span><span class="s1"> -&gt; </span><span class="si">{</span><span class="n">words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Center word:  </span><span class="si">{</span><span class="n">center_word</span><span class="si">}</span><span class="s2"> -&gt; "</span>
          <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">center_word</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
<pre class="example">
Context words:  ['i', 'am', 'because', 'i'] -&gt; [0.25 0.25 0.   0.5  0.  ]
Center word:  happy -&gt; [0. 0. 1. 0. 0.]

Context words:  ['am', 'happy', 'i', 'am'] -&gt; [0.5  0.   0.25 0.25 0.  ]
Center word:  because -&gt; [0. 1. 0. 0. 0.]

Context words:  ['happy', 'because', 'am', 'learning'] -&gt; [0.25 0.25 0.25 0.   0.25]
Center word:  i -&gt; [0. 0. 0. 1. 0.]

</pre>
<p>In this practice notebook you'll be performing a single iteration of training using a single example, but in this week's assignment you'll train the CBOW model using several iterations and batches of example. Here is how you would use a Python generator function (remember the `yield` keyword from the lecture?) to make it easier to iterate over a set of examples.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_training_example</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="sd">"""generates training examples</span>

<span class="sd">    Args:</span>
<span class="sd">     words: source of words</span>
<span class="sd">     half_window: half the window size</span>
<span class="sd">     word_to_index: dict with word to index mapping</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="n">get_windows</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">),</span> <span class="n">word_to_one_hot_vector</span><span class="p">(</span>
            <span class="n">center_word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span>
            <span class="n">vocabulary_size</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<p>The output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">context_words_vector</span><span class="p">,</span> <span class="n">center_word_vector</span> <span class="ow">in</span> <span class="n">get_training_example</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Context words vector:  </span><span class="si">{</span><span class="n">context_words_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Center word vector:  </span><span class="si">{</span><span class="n">center_word_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
<pre class="example">
Context words vector:  [0.25 0.25 0.   0.5  0.  ]
Center word vector:  [0. 0. 1. 0. 0.]

Context words vector:  [0.5  0.   0.25 0.25 0.  ]
Center word vector:  [0. 1. 0. 0. 0.]

Context words vector:  [0.25 0.25 0.25 0.   0.25]
Center word vector:  [0. 0. 0. 1. 0.]

</pre>
<p>Now that the training set is ready, we can move on to the CBOW model itself which will be covered in the <a href="posts/nlp/introducing-the-cbow-model/">next post</a>.</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/auto-complete-building-the-auto-complete-system/">Auto-Complete: Building the Auto-Complete System</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/auto-complete-building-the-auto-complete-system/" rel="bookmark"><time class="published dt-published" datetime="2020-12-04T15:21:47-08:00" itemprop="datePublished" title="2020-12-04 15:21">2020-12-04 15:21</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#orgf04d340">Build the Auto-Complete System</a>
<ul>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#orgc414a29">Imports</a></li>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#orge8581eb">Set Up</a>
<ul>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org913c045">The Environment</a></li>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org5966256">The Data</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org590261a">Middle</a>
<ul>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org2eaf863">Probabilities Again</a></li>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org19085db">Suggest-a-Word</a>
<ul>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org5a71451">Hints</a></li>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#orgf6d6999">Test It Out</a></li>
</ul>
</li>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org68dbbdb">Multiple Suggestions</a>
<ul>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org39faccf">Test It</a></li>
</ul>
</li>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#org281f23b">Multiple Word Suggestions</a></li>
</ul>
</li>
<li><a href="posts/nlp/auto-complete-building-the-auto-complete-system/#orgbeddc3a">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf04d340">
<h2 id="orgf04d340">Build the Auto-Complete System</h2>
<div class="outline-text-2" id="text-orgf04d340">
<p>In the <a href="posts/nlp/auto-complete-perplexity/">previous post</a> we tested the perplexity of our N-Gram Language model. In this, the final post in the series that we began <a href="posts/nlp/auto-complete/">with this post</a>, we'll implement the final system.</p>
</div>
<div class="outline-3" id="outline-container-orgc414a29">
<h3 id="orgc414a29">Imports</h3>
<div class="outline-text-3" id="text-orgc414a29">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>
<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.autocomplete</span> <span class="kn">import</span> <span class="n">NGrams</span><span class="p">,</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">TrainTestSplit</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge8581eb">
<h3 id="orge8581eb">Set Up</h3>
<div class="outline-text-3" id="text-orge8581eb"></div>
<div class="outline-4" id="outline-container-org913c045">
<h4 id="org913c045">The Environment</h4>
<div class="outline-text-4" id="text-org913c045">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5966256">
<h4 id="org5966256">The Data</h4>
<div class="outline-text-4" id="text-org5966256">
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_AUTOCOMPLETE"</span><span class="p">]</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org590261a">
<h2 id="org590261a">Middle</h2>
<div class="outline-text-2" id="text-org590261a"></div>
<div class="outline-3" id="outline-container-org2eaf863">
<h3 id="org2eaf863">Probabilities Again</h3>
<div class="outline-text-3" id="text-org2eaf863">
<p>Once again the function we're defining here expects this probability function so I'm going to have to paste it in here.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">estimate_probability</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                         <span class="n">previous_n_gram</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> 
                         <span class="n">n_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">n_plus1_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span>

<span class="sd">    Args:</span>
<span class="sd">       word: next word</span>
<span class="sd">       previous_n_gram: A sequence of words of length n</span>
<span class="sd">       n_gram_counts: Dictionary of counts of n-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary_size: number of words in the vocabulary</span>
<span class="sd">       k: positive constant, smoothing parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">       A probability</span>
<span class="sd">    """</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>
    <span class="n">previous_n_gram_count</span> <span class="o">=</span> <span class="n">n_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">n_plus1_gram</span> <span class="o">=</span> <span class="n">previous_n_gram</span> <span class="o">+</span> <span class="p">(</span><span class="n">word</span><span class="p">,)</span>  
    <span class="n">n_plus1_gram_count</span> <span class="o">=</span> <span class="n">n_plus1_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n_plus1_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>       
    <span class="k">return</span> <span class="p">(</span><span class="n">n_plus1_gram_count</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">previous_n_gram_count</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">vocabulary_size</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">estimate_probabilities</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Estimate the probabilities of next words using the n-gram counts with k-smoothing</span>

<span class="sd">    Args:</span>
<span class="sd">       previous_n_gram: A sequence of words of length n</span>
<span class="sd">       n_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary: List of words</span>
<span class="sd">       k: positive constant, smoothing parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">       A dictionary mapping from next words to the probability.</span>
<span class="sd">    """</span>

    <span class="c1"># convert list to tuple to use it as a dictionary key</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>

    <span class="c1"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span>
    <span class="c1"># &lt;s&gt; is not needed since it should not appear as the next word</span>
    <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">vocabulary</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">,</span> <span class="s2">"&lt;unk&gt;"</span><span class="p">]</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>

    <span class="n">probabilities</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
        <span class="n">probability</span> <span class="o">=</span> <span class="n">estimate_probability</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">previous_n_gram</span><span class="p">,</span> 
                                           <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span> 
                                           <span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="n">probabilities</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">probability</span>

    <span class="k">return</span> <span class="n">probabilities</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org19085db">
<h3 id="org19085db">Suggest-a-Word</h3>
<div class="outline-text-3" id="text-org19085db">
<p>Compute probabilities for all possible next words and suggest the most likely one.</p>
<ul class="org-ul">
<li>This function also take an optional argument `start_with`, which specifies the first few letters of the next words.</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org5a71451">
<h4 id="org5a71451">Hints</h4>
<div class="outline-text-4" id="text-org5a71451">
<ul class="org-ul">
<li><code>estimate_probabilities</code> returns a dictionary where the key is a word and the value is the word's probability.</li>
<li>Use <code>str1.startswith(str2)</code> to determine if a string starts with the letters of another string. For example, <code>'learning'.startswith('lea')</code> returns True, whereas <code>'learning'.startswith('ear')</code> returns False. There are two additional parameters in <code>str.startswith()</code>, but you can use the default values for those parameters in this case.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: suggest_a_word</span>
<span class="k">def</span> <span class="nf">suggest_a_word</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Get suggestion for the next word</span>

<span class="sd">    Args:</span>
<span class="sd">       previous_tokens: The sentence you input where each token is a word. Must have length &gt; n </span>
<span class="sd">       n_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary: List of words</span>
<span class="sd">       k: positive constant, smoothing parameter</span>
<span class="sd">       start_with: If not None, specifies the first few letters of the next word</span>

<span class="sd">    Returns:</span>
<span class="sd">       A tuple of </span>
<span class="sd">         - string of the most likely next word</span>
<span class="sd">         - corresponding probability</span>
<span class="sd">    """</span>

    <span class="c1"># length of previous words</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">n_gram_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span> 

    <span class="c1"># From the words that the user already typed</span>
    <span class="c1"># get the most recent 'n' words as the previous n-gram</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="n">previous_tokens</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="p">:]</span>

    <span class="c1"># Estimate the probabilities that each word in the vocabulary</span>
    <span class="c1"># is the next word,</span>
    <span class="c1"># given the previous n-gram, the dictionary of n-gram counts,</span>
    <span class="c1"># the dictionary of n plus 1 gram counts, and the smoothing constant</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">estimate_probabilities</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span>
                                           <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span>
                                           <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="c1"># Initialize suggested word to None</span>
    <span class="c1"># This will be set to the word with highest probability</span>
    <span class="n">suggestion</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Initialize the highest word probability to 0</span>
    <span class="c1"># this will be set to the highest probability </span>
    <span class="c1"># of all words to be suggested</span>
    <span class="n">max_prob</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># For each word and its probability in the probabilities dictionary:</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> <span class="c1"># complete this line</span>

        <span class="c1"># If the optional start_with string is set</span>
        <span class="k">if</span> <span class="n">start_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="c1"># complete this line</span>

            <span class="c1"># Check if the beginning of word does not match with the letters in 'start_with'</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">word</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">start_with</span><span class="p">):</span> <span class="c1"># complete this line</span>

                <span class="c1"># if they don't match, skip this word (move onto the next word)</span>
                <span class="k">continue</span> <span class="c1"># complete this line</span>

        <span class="c1"># Check if this word's probability</span>
        <span class="c1"># is greater than the current maximum probability</span>
        <span class="k">if</span> <span class="n">prob</span> <span class="o">&gt;</span> <span class="n">max_prob</span><span class="p">:</span> <span class="c1"># complete this line</span>

            <span class="c1"># If so, save this word as the best suggestion (so far)</span>
            <span class="n">suggestion</span> <span class="o">=</span> <span class="n">word</span>

            <span class="c1"># Save the new maximum probability</span>
            <span class="n">max_prob</span> <span class="o">=</span> <span class="n">prob</span>

    <span class="c1">### END CODE HERE</span>

    <span class="k">return</span> <span class="n">suggestion</span><span class="p">,</span> <span class="n">max_prob</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf6d6999">
<h4 id="orgf6d6999">Test It Out</h4>
<div class="outline-text-4" id="text-orgf6d6999">
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">unigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>

<span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">"like"</span><span class="p">]</span>
<span class="n">word</span><span class="p">,</span> <span class="n">probability</span> <span class="o">=</span> <span class="n">suggest_a_word</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are 'i like',</span><span class="se">\n\t</span><span class="s2">and the suggested word is `</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">` with a probability of </span><span class="si">{</span><span class="n">probability</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected_word</span><span class="p">,</span> <span class="n">expected_probability</span> <span class="o">=</span> <span class="s2">"a"</span><span class="p">,</span> <span class="mf">0.2727</span>
<span class="n">expect</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected_word</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">probability</span><span class="p">,</span> <span class="n">expected_probability</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="c1"># test your code when setting the starts_with</span>
<span class="n">tmp_starts_with</span> <span class="o">=</span> <span class="s1">'c'</span>
<span class="n">word</span><span class="p">,</span> <span class="n">probability</span> <span class="o">=</span> <span class="n">suggest_a_word</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="n">tmp_starts_with</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are 'i like', the suggestion must start with `</span><span class="si">{</span><span class="n">tmp_starts_with</span><span class="si">}</span><span class="s2">`</span><span class="se">\n\t</span><span class="s2">and the suggested word is `</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">` with a probability of </span><span class="si">{</span><span class="n">probability</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expected_word</span><span class="p">,</span> <span class="n">expected_probability</span> <span class="o">=</span> <span class="s2">"cat"</span><span class="p">,</span> <span class="mf">0.0909</span>
<span class="n">expect</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected_word</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">probability</span><span class="p">,</span> <span class="n">expected_probability</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are 'i like',
        and the suggested word is `a` with a probability of 0.2727

The previous words are 'i like', the suggestion must start with `c`
        and the suggested word is `cat` with a probability of 0.0909
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org68dbbdb">
<h3 id="org68dbbdb">Multiple Suggestions</h3>
<div class="outline-text-3" id="text-org68dbbdb">
<p>The function defined below loops over various n-gram models to get multiple suggestions.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">model_counts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_gram_counts_list</span><span class="p">)</span>
    <span class="n">suggestions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_counts</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">n_gram_counts</span> <span class="o">=</span> <span class="n">n_gram_counts_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">n_plus1_gram_counts</span> <span class="o">=</span> <span class="n">n_gram_counts_list</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">suggestion</span> <span class="o">=</span> <span class="n">suggest_a_word</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts</span><span class="p">,</span>
                                    <span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span>
                                    <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="n">start_with</span><span class="p">)</span>
        <span class="n">suggestions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">suggestion</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">suggestions</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org39faccf">
<h4 id="org39faccf">Test It</h4>
<div class="outline-text-4" id="text-org39faccf">
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">unigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">trigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">quadgram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">qintgram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>

<span class="n">n_gram_counts_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="n">trigram_counts</span><span class="p">,</span> <span class="n">quadgram_counts</span><span class="p">,</span> <span class="n">qintgram_counts</span><span class="p">]</span>
<span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">"like"</span><span class="p">]</span>
<span class="n">tmp_suggest3</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are 'i like', the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest3</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are 'i like', the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">a</td>
<td class="org-right">0.2727272727272727</td>
</tr>
<tr>
<td class="org-left">a</td>
<td class="org-right">0.2</td>
</tr>
<tr>
<td class="org-left">like</td>
<td class="org-right">0.1111111111111111</td>
</tr>
<tr>
<td class="org-left">like</td>
<td class="org-right">0.1111111111111111</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org281f23b">
<h3 id="org281f23b">Multiple Word Suggestions</h3>
<div class="outline-text-3" id="text-org281f23b">
<div class="highlight">
<pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">splitter</span> <span class="o">=</span> <span class="n">TrainTestSplit</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenized</span><span class="p">)</span>
<span class="n">train_data_processed</span> <span class="o">=</span> <span class="n">splitter</span><span class="o">.</span><span class="n">training</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">n_gram_counts_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">NGrams</span><span class="p">(</span><span class="n">train_data_processed</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">train_data_processed</span><span class="p">)))</span>
<span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">"am"</span><span class="p">,</span> <span class="s2">"to"</span><span class="p">]</span>
<span class="n">tmp_suggest4</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest4</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['i', 'am', 'to'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">be</td>
<td class="org-right">0.015552924847940564</td>
</tr>
<tr>
<td class="org-left">please</td>
<td class="org-right">5.4935999560512006e-05</td>
</tr>
<tr>
<td class="org-left">please</td>
<td class="org-right">5.494354550699157e-05</td>
</tr>
<tr>
<td class="org-left">sucks</td>
<td class="org-right">2.747403703500192e-05</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">"want"</span><span class="p">,</span> <span class="s2">"to"</span><span class="p">,</span> <span class="s2">"go"</span><span class="p">]</span>
<span class="n">tmp_suggest5</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest5</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['i', 'want', 'to', 'go'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">to</td>
<td class="org-right">0.006007762241480142</td>
</tr>
<tr>
<td class="org-left">to</td>
<td class="org-right">0.0019077728115120462</td>
</tr>
<tr>
<td class="org-left">to</td>
<td class="org-right">0.00030196552102778083</td>
</tr>
<tr>
<td class="org-left">home</td>
<td class="org-right">0.00016478989288656962</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"hey"</span><span class="p">,</span> <span class="s2">"how"</span><span class="p">,</span> <span class="s2">"are"</span><span class="p">]</span>
<span class="n">tmp_suggest6</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest6</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['hey', 'how', 'are'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">you</td>
<td class="org-right">0.010055522861602231</td>
</tr>
<tr>
<td class="org-left">you</td>
<td class="org-right">0.0014810345300458024</td>
</tr>
<tr>
<td class="org-left">you</td>
<td class="org-right">5.494656446605676e-05</td>
</tr>
<tr>
<td class="org-left">sucks</td>
<td class="org-right">2.747403703500192e-05</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"hey"</span><span class="p">,</span> <span class="s2">"how"</span><span class="p">,</span> <span class="s2">"are"</span><span class="p">,</span> <span class="s2">"you"</span><span class="p">]</span>
<span class="n">tmp_suggest7</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest7</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">'re</td>
<td class="org-right">0.012929170630459223</td>
</tr>
<tr>
<td class="org-left">?</td>
<td class="org-right">0.0011416145691764065</td>
</tr>
<tr>
<td class="org-left">?</td>
<td class="org-right">0.0007132863295931524</td>
</tr>
<tr>
<td class="org-left">&lt;e&gt;</td>
<td class="org-right">5.494656446605676e-05</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"hey"</span><span class="p">,</span> <span class="s2">"how"</span><span class="p">,</span> <span class="s2">"are"</span><span class="p">,</span> <span class="s2">"you"</span><span class="p">]</span>
<span class="n">tmp_suggest8</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="s2">"d"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest8</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">do</td>
<td class="org-right">0.004734930381388913</td>
</tr>
<tr>
<td class="org-left">doing</td>
<td class="org-right">0.000679532481652623</td>
</tr>
<tr>
<td class="org-left">doing</td>
<td class="org-right">0.0001646045375984198</td>
</tr>
<tr>
<td class="org-left">deserving</td>
<td class="org-right">2.747328223302838e-05</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgbeddc3a">
<h2 id="orgbeddc3a">End</h2>
<div class="outline-text-2" id="text-orgbeddc3a">
<p>So, now we have our system. Here are all the prior posts in this series.</p>
<ul class="org-ul">
<li><a href="posts/nlp/auto-complete/">Overview</a></li>
<li><a href="posts/nlp/auto-complete-pre-process-the-data-i/">Pre-Processing I</a></li>
<li><a href="posts/nlp/auto-complete-pre-process-the-data-ii/">Pre-Processing II</a></li>
<li><a href="posts/nlp/auto-complete-the-n-gram-model/">The N-Gram Model</a></li>
<li><a href="posts/nlp/auto-complete-perplexity/">Perplexity</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/auto-complete-perplexity/">Auto-Complete: Perplexity</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/auto-complete-perplexity/" rel="bookmark"><time class="published dt-published" datetime="2020-12-04T15:19:33-08:00" itemprop="datePublished" title="2020-12-04 15:19">2020-12-04 15:19</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/auto-complete-perplexity/#org9ce0895">Perplexity</a>
<ul>
<li><a href="posts/nlp/auto-complete-perplexity/#org931daa9">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/auto-complete-perplexity/#org4f8deba">Middle</a>
<ul>
<li><a href="posts/nlp/auto-complete-perplexity/#org50599ea">The Probability Function</a></li>
<li><a href="posts/nlp/auto-complete-perplexity/#org82779db">Calculating the Perplexity</a>
<ul>
<li><a href="posts/nlp/auto-complete-perplexity/#orgff8f821">Test It</a></li>
</ul>
</li>
<li><a href="posts/nlp/auto-complete-perplexity/#orgce5f59e">Using the Class-Based Version</a></li>
</ul>
</li>
<li><a href="posts/nlp/auto-complete-perplexity/#orgda88a0c">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org9ce0895">
<h2 id="org9ce0895">Perplexity</h2>
<div class="outline-text-2" id="text-org9ce0895">
<p>In the <a href="posts/nlp/auto-complete-the-n-gram-model/">previous post</a> we implemented the N-Gram Language Model for the auto-complete system that we began <a href="posts/nlp/auto-complete/">here</a>.</p>
<p>In this section, you will generate the perplexity score to evaluate your model on the test set.</p>
<ul class="org-ul">
<li>You will also use back-off when needed.</li>
<li>Perplexity is used as an evaluation metric of your language model.</li>
<li>To calculate the the perplexity score of the test set on an n-gram model, use:</li>
</ul>
<p>\[ PP(W) =\sqrt[N]{ \prod_{t=n+1}^N \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4} \]</p>
<ul class="org-ul">
<li>where <i>N</i> is the length of the sentence.</li>
<li><i>n</i> is the number of words in the n-gram (e.g. 2 for a bigram).</li>
<li>In math, the numbering starts at one and not zero.</li>
</ul>
<p>In code, array indexing starts at zero, so the code will use ranges for <i>t</i> according to this formula:</p>
<p>\[ PP(W) =\sqrt[N]{ \prod_{t=n}^{N-1} \frac{1}{P(w_t | w_{t-n} \cdots w_{t-1})} } \tag{4.1} \]</p>
<p>The higher the probabilities are, the lower the perplexity will be.</p>
<ul class="org-ul">
<li>The more the n-grams tell us about the sentence, the lower the perplexity score will be.</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org931daa9">
<h3 id="org931daa9">Imports</h3>
<div class="outline-text-3" id="text-org931daa9">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">expect</span><span class="p">,</span> <span class="n">be_true</span>

<span class="kn">import</span> <span class="nn">attr</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.autocomplete</span> <span class="kn">import</span> <span class="n">NGrams</span><span class="p">,</span><span class="n">NGramProbability</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4f8deba">
<h2 id="org4f8deba">Middle</h2>
<div class="outline-text-2" id="text-org4f8deba"></div>
<div class="outline-3" id="outline-container-org50599ea">
<h3 id="org50599ea">The Probability Function</h3>
<div class="outline-text-3" id="text-org50599ea">
<p>This was already defined in the previous post, but the function following it assumes its existence so I'm temporarily re-defining it here.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">estimate_probability</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                         <span class="n">previous_n_gram</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> 
                         <span class="n">n_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">n_plus1_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span>

<span class="sd">    Args:</span>
<span class="sd">       word: next word</span>
<span class="sd">       previous_n_gram: A sequence of words of length n</span>
<span class="sd">       n_gram_counts: Dictionary of counts of n-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary_size: number of words in the vocabulary</span>
<span class="sd">       k: positive constant, smoothing parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">       A probability</span>
<span class="sd">    """</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>
    <span class="n">previous_n_gram_count</span> <span class="o">=</span> <span class="n">n_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">n_plus1_gram</span> <span class="o">=</span> <span class="n">previous_n_gram</span> <span class="o">+</span> <span class="p">(</span><span class="n">word</span><span class="p">,)</span>  
    <span class="n">n_plus1_gram_count</span> <span class="o">=</span> <span class="n">n_plus1_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n_plus1_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>       
    <span class="k">return</span> <span class="p">(</span><span class="n">n_plus1_gram_count</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">previous_n_gram_count</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">vocabulary_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org82779db">
<h3 id="org82779db">Calculating the Perplexity</h3>
<div class="outline-text-3" id="text-org82779db">
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: calculate_perplexity</span>
<span class="k">def</span> <span class="nf">calculate_perplexity</span><span class="p">(</span><span class="n">sentence</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                         <span class="n">n_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">n_plus1_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Calculate perplexity for a list of sentences</span>

<span class="sd">    Args:</span>
<span class="sd">       sentence: List of strings</span>
<span class="sd">       n_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary_size: number of unique words in the vocabulary</span>
<span class="sd">       k: Positive smoothing constant</span>

<span class="sd">    Returns:</span>
<span class="sd">       Perplexity score</span>
<span class="sd">    """</span>
    <span class="c1"># length of previous words</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">n_gram_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span> 

    <span class="c1"># prepend &lt;s&gt; and append &lt;e&gt;</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"&lt;s&gt;"</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">]</span>

    <span class="c1"># Cast the sentence from a list to a tuple</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

    <span class="c1"># length of sentence (after adding &lt;s&gt; and &lt;e&gt; tokens)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

    <span class="c1"># The variable p will hold the product</span>
    <span class="c1"># that is calculated inside the n-root</span>
    <span class="c1"># Update this in the code below</span>
    <span class="n">product_pi</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># Index t ranges from n to N - 1, inclusive on both ends</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span> <span class="c1"># complete this line</span>

        <span class="c1"># get the n-gram preceding the word at position t</span>
        <span class="n">n_gram</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">t</span> <span class="o">-</span> <span class="n">n</span><span class="p">:</span> <span class="n">t</span><span class="p">]</span>

        <span class="c1"># get the word at position t</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

        <span class="c1"># Estimate the probability of the word given the n-gram</span>
        <span class="c1"># using the n-gram counts, n-plus1-gram counts,</span>
        <span class="c1"># vocabulary size, and smoothing constant</span>
        <span class="n">probability</span> <span class="o">=</span> <span class="n">estimate_probability</span><span class="p">(</span>
            <span class="n">word</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">previous_n_gram</span><span class="o">=</span><span class="n">n_gram</span><span class="p">,</span>
            <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">,</span>
            <span class="n">n_gram_counts</span><span class="o">=</span><span class="n">n_gram_counts</span><span class="p">,</span>
            <span class="n">n_plus1_gram_counts</span><span class="o">=</span><span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

        <span class="c1"># Update the product of the probabilities</span>
        <span class="c1"># This 'product_pi' is a cumulative product </span>
        <span class="c1"># of the (1/P) factors that are calculated in the loop</span>
        <span class="n">product_pi</span> <span class="o">*=</span> <span class="mi">1</span><span class="o">/</span><span class="n">probability</span>

    <span class="c1"># Take the Nth root of the product</span>
    <span class="n">perplexity</span> <span class="o">=</span> <span class="n">product_pi</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>

    <span class="c1">### END CODE HERE ### </span>
    <span class="k">return</span> <span class="n">perplexity</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgff8f821">
<h4 id="orgff8f821">Test It</h4>
<div class="outline-text-4" id="text-orgff8f821">
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
                 <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">unigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>


<span class="n">perplexity_train1</span> <span class="o">=</span> <span class="n">calculate_perplexity</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                         <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span>
                                         <span class="nb">len</span><span class="p">(</span><span class="n">unique_words</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">2.8040</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Perplexity for first train sample: </span><span class="si">{</span><span class="n">perplexity_train1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">perplexity_train1</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">test_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">]</span>
<span class="n">perplexity_test</span> <span class="o">=</span> <span class="n">calculate_perplexity</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">,</span>
                                       <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span>
                                       <span class="nb">len</span><span class="p">(</span><span class="n">unique_words</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Perplexity for test sample: </span><span class="si">{</span><span class="n">perplexity_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">3.9654</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">perplexity_test</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
Perplexity for first train sample: 2.8040
Perplexity for test sample: 3.9654
</pre>
<p><b>Note:</b> If your sentence is really long, there will be underflow when multiplying many fractions.</p>
<ul class="org-ul">
<li>To handle longer sentences, modify your implementation to take the sum of the log of the probabilities.</li>
</ul>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgce5f59e">
<h3 id="orgce5f59e">Using the Class-Based Version</h3>
<div class="outline-text-3" id="text-orgce5f59e">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Perplexity</span><span class="p">:</span>
    <span class="sd">"""Calculate perplexity</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the tokenized training input</span>
<span class="sd">     n: the size of the n-grams</span>
<span class="sd">     augment_vocabulary: whether to augment the vocabulary for toy examples</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">augment_vocabulary</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>
    <span class="n">_probabilifier</span><span class="p">:</span> <span class="n">NGramProbability</span><span class="o">=</span><span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">probabilifier</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NGramProbability</span><span class="p">:</span>
        <span class="sd">"""Probability Calculator"""</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_probabilifier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_probabilifier</span> <span class="o">=</span> <span class="n">NGramProbability</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span>
                <span class="n">augment_vocabulary</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">augment_vocabulary</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_probabilifier</span>

    <span class="k">def</span> <span class="nf">perplexity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="sd">"""Calculates the perplexity for the sentence"""</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="s2">"&lt;s&gt;"</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">])</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

        <span class="n">n_grams</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentence</span><span class="p">[</span><span class="n">position</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">:</span> <span class="n">position</span><span class="p">]</span>
                   <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">(</span><span class="n">sentence</span><span class="p">[</span><span class="n">position</span><span class="p">]</span>
                 <span class="k">for</span> <span class="n">position</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="n">words_n_grams</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">n_grams</span><span class="p">)</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">probabilifier</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">n_gram</span><span class="p">)</span>
                         <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">n_gram</span> <span class="ow">in</span> <span class="n">words_n_grams</span><span class="p">)</span>
        <span class="n">product</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">prod</span><span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">probability</span> <span class="k">for</span> <span class="n">probability</span> <span class="ow">in</span> <span class="n">probabilities</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">product</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">N</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
                 <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Perplexity</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">augment_vocabulary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">perplexity</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">expected</span> <span class="o">=</span> <span class="mf">2.8040</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Perplexity for first train sample: </span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">test_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">]</span>
<span class="n">model</span>
<span class="n">perplexity_test</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">perplexity</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Perplexity for test sample: </span><span class="si">{</span><span class="n">perplexity_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">3.9654</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">perplexity_test</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgda88a0c">
<h2 id="orgda88a0c">End</h2>
<div class="outline-text-2" id="text-orgda88a0c">
<p>In the next part we'll build our <a href="posts/nlp/auto-complete-building-the-auto-complete-system/">completed auto-complete system</a>.</p>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="next"><a href="index-15.html" rel="next">Older posts</a></li>
</ul>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script><!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
