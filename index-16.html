<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neurotic Networking (old posts, page 16) | Neurotic Networking</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/index-16.html" rel="canonical">
<link href="index-17.html" rel="prev" type="text/html">
<link href="index-15.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="."><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/sentiment-analysis-deep-learning-model/">Sentiment Analysis: Deep Learning Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/sentiment-analysis-deep-learning-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-23T15:14:07-08:00" itemprop="datePublished" title="2020-12-23 15:14">2020-12-23 15:14</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#org841880f">Beginning</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#org8f6f00f">Imports</a></li>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#org953b653">Set Up</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#orge1e0d1c">The Random Seed</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#orge7d172d">Middle</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#org3a4c67e">Trax Review</a>
<ul>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#org41121b3">JAX Arrays</a></li>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#org509ead5">Squaring</a></li>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#org9fba742">Gradients</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/sentiment-analysis-deep-learning-model/#org5b8d7d3">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org841880f">
<h2 id="org841880f">Beginning</h2>
<div class="outline-text-2" id="text-org841880f">
<p>Previously we created sentiment analysis models using the Logistic Regression and Naive Bayes algorithms. However if we were to give those models an example like:</p>
<blockquote>
<p>This movie was almost good.</p>
</blockquote>
<p>The model would have predicted a positive sentiment for that review. That sentence, however, is expressing the negative sentiment that the movie was not good. To solve those kinds of misclassifications we will write a program that uses deep neural networks to identify sentiment in text.</p>
<p>This model will follow a similar structure to the Continuous Bag of Words Model (<a href="posts/nlp/introducing-the-cbow-model/">Introducing the CBOW Model</a>) that we looked at previously - indeed most of the deep nets have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Although we looked at <a href="https://github.com/google/trax">Trax</a> and <a href="https://jax.readthedocs.io/en/latest/index.html">JAX</a> in a previous post (<a href="posts/nlp/introducing-trax/">Introducing Trax</a>) we'll start off with a review of some of their features and then in future posts we'll implement the actual model. These are the other posts.</p>
<ul class="org-ul">
<li><a href="posts/nlp/sentiment-analysis-pre-processing-the-data/">Loading the Data</a></li>
<li><a href="posts/nlp/sentiment-analysis-defining-the-model/">Defining the Model</a></li>
<li><a href="posts/nlp/sentiment-analysis-training-the-model/">Training the Model</a></li>
<li><a href="posts/nlp/sentiment-analysis-testing-the-model/">Testing the Model</a></li>
</ul>
</div>
<div class="outline-3" id="outline-container-org8f6f00f">
<h3 id="org8f6f00f">Imports</h3>
<div class="outline-text-3" id="text-org8f6f00f">
<div class="highlight">
<pre><span></span><span class="c1"># from python</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">import</span> <span class="nn">trax</span>
<span class="kn">import</span> <span class="nn">trax.fastmath.numpy</span> <span class="k">as</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org953b653">
<h3 id="org953b653">Set Up</h3>
<div class="outline-text-3" id="text-org953b653"></div>
<div class="outline-4" id="outline-container-orge1e0d1c">
<h4 id="orge1e0d1c">The Random Seed</h4>
<div class="outline-text-4" id="text-orge1e0d1c">
<div class="highlight">
<pre><span></span><span class="n">trax</span><span class="o">.</span><span class="n">supervised</span><span class="o">.</span><span class="n">trainer_lib</span><span class="o">.</span><span class="n">init_random_number_generators</span><span class="p">(</span><span class="mi">31</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orge7d172d">
<h2 id="orge7d172d">Middle</h2>
<div class="outline-text-2" id="text-orge7d172d"></div>
<div class="outline-3" id="outline-container-org3a4c67e">
<h3 id="org3a4c67e">Trax Review</h3>
<div class="outline-text-3" id="text-org3a4c67e"></div>
<div class="outline-4" id="outline-container-org41121b3">
<h4 id="org41121b3">JAX Arrays</h4>
<div class="outline-text-4" id="text-org41121b3">
<p>First, the JAX reimplementation of numpy (from <a href="https://trax-ml.readthedocs.io/en/latest/trax.fastmath.html">Trax.fastmath</a>).</p>
<div class="highlight">
<pre><span></span><span class="n">an_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">5.0</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">an_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">an_array</span><span class="p">))</span>
</pre></div>
<pre class="example">
DeviceArray(5., dtype=float32)
&lt;class 'jax.interpreters.xla._DeviceArray'&gt;
</pre>
<p><b>Note:</b> the trax library is strict about the typing so <code>5</code> won't work, it has to be a float.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org509ead5">
<h4 id="org509ead5">Squaring</h4>
<div class="outline-text-4" id="text-org509ead5">
<p>Now we'll create a function to square the array.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"f(</span><span class="si">{</span><span class="n">an_array</span><span class="si">}</span><span class="s2">) -&gt; </span><span class="si">{</span><span class="n">square</span><span class="p">(</span><span class="n">an_array</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
f(5.0) -&gt; 25.0
</pre></div>
</div>
<div class="outline-4" id="outline-container-org9fba742">
<h4 id="org9fba742">Gradients</h4>
<div class="outline-text-4" id="text-org9fba742">
<p>The gradient (derivative) of function <code>f</code> with respect to its input <code>x</code> is the derivative of \(x^2\).</p>
<ul class="org-ul">
<li>The derivative of \(x^2\) is \(2x\).</li>
<li>When <i>x</i> is <i>5</i>, then <i>2x=10</i>.</li>
</ul>
<p>You can calculate the gradient of a function by using <code>trax.fastmath.grad(fun=)</code> and passing in the name of the function.</p>
<ul class="org-ul">
<li>In this case the function you want to take the gradient of is <code>square</code>.</li>
<li>The object returned (saved in <code>square_gradient</code> in this example) is a function that can calculate the gradient of <code>square</code> for a given <code>trax.fastmath.numpy</code> array.</li>
</ul>
<p>Use <code>trax.fastmath.grad</code> to calculate the gradient (derivative) of the function.</p>
<div class="highlight">
<pre><span></span><span class="n">square_gradient</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">fastmath</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">square</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">square_gradient</span><span class="p">))</span>
</pre></div>
<pre class="example">
&lt;class 'function'&gt;
</pre>
<p>Call the newly created function and pass in a value for x (the DeviceArray stored in 'a')</p>
<div class="highlight">
<pre><span></span><span class="n">gradient_calculation</span> <span class="o">=</span> <span class="n">square_gradient</span><span class="p">(</span><span class="n">an_array</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">gradient_calculation</span><span class="p">)</span>
</pre></div>
<pre class="example">
DeviceArray(10., dtype=float32)
</pre>
<p>The function returned by <code>trax.fastmath.grad</code> takes in <i>x=5</i> and calculates the gradient of <code>square</code>, which is <i>2x</i>, which equals <i>10</i>. The value is also stored as a DeviceArray from the jax library.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org5b8d7d3">
<h2 id="org5b8d7d3">End</h2>
<div class="outline-text-2" id="text-org5b8d7d3">
<p>Now that we've had a brief review of Trax let's move on to <a href="posts/nlp/sentiment-analysis-pre-processing-the-data/">loading the data</a>.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/data-generators/">Data Generators</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/data-generators/" rel="bookmark"><time class="published dt-published" datetime="2020-12-23T12:50:22-08:00" itemprop="datePublished" title="2020-12-23 12:50">2020-12-23 12:50</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/data-generators/#org105c5c8">Data generators</a>
<ul>
<li><a href="posts/nlp/data-generators/#orga1d288f">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/data-generators/#org66e47cc">Examples</a>
<ul>
<li><a href="posts/nlp/data-generators/#org4f03e41">An Example of a Circular List</a></li>
<li><a href="posts/nlp/data-generators/#orgc1f027d">Shuffling the data order</a></li>
<li><a href="posts/nlp/data-generators/#org805177d">Data Generator Function</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org105c5c8">
<h2 id="org105c5c8">Data generators</h2>
<div class="outline-text-2" id="text-org105c5c8">
<p>In Python, a <a href="https://wiki.python.org/moin/Generators">generator</a> is a function that behaves like an iterator. It will return the next item. In many AI applications, it is advantageous to have a data generator to handle loading and transforming data for different applications.</p>
<p>In the following example, we use a set of samples <code>a</code>, to derive a new set of samples, with more elements than the original set.</p>
<p><b>Note:</b> Pay attention to the use of list <code>lines_index</code> and variable <code>index</code> to traverse the original list.</p>
</div>
<div class="outline-3" id="outline-container-orga1d288f">
<h3 id="orga1d288f">Imports</h3>
<div class="outline-text-3" id="text-orga1d288f">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">cycle</span>

<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">expect</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org66e47cc">
<h2 id="org66e47cc">Examples</h2>
<div class="outline-text-2" id="text-org66e47cc"></div>
<div class="outline-3" id="outline-container-org4f03e41">
<h3 id="org4f03e41">An Example of a Circular List</h3>
<div class="outline-text-3" id="text-org4f03e41">
<p>This is sort of a fake generator that uses indices to make it look like it's infinite.</p>
<div class="highlight">
<pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">a_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>                      <span class="c1"># similar to index in data_generator below</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>        <span class="c1"># `b` is longer than `a` forcing a wrap   </span>
    <span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">","</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">a_size</span>    
</pre></div>
<pre class="example">
1,2,3,4,1,2,3,4,1,2,
</pre>
<p>There's a python built-in that's equivalent to this called <a href="https://docs.python.org/3/library/itertools.html#itertools.cycle">cycle</a>.</p>
<div class="highlight">
<pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">cycle</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">","</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">end</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>    
</pre></div>
<pre class="example">
1,2,3,4,1,2,3,4,1,2,
</pre>
<p>And if you wanted to make your own generator version you could use the yield keyword.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">infinite</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="sd">"""Generates elements infinitely</span>

<span class="sd">    Args:</span>
<span class="sd">     a: list</span>

<span class="sd">    Yields:</span>
<span class="sd">     elements of a</span>
<span class="sd">    """</span>
    <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">end</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">end</span>
    <span class="k">return</span>

<span class="n">a_infinite</span> <span class="o">=</span> <span class="n">infinite</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">a_infinite</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">end</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">","</span><span class="p">)</span>
</pre></div>
<pre class="example">
1,2,3,4,1,2,3,4,1,2,
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgc1f027d">
<h3 id="orgc1f027d">Shuffling the data order</h3>
<div class="outline-text-3" id="text-orgc1f027d">
<p>In the next example, we will do the same as before, but shuffling the order of the elements in the output list. Note that here, our strategy of traversing using <code>lines_index</code> and <code>index</code> becomes very important, because we can simulate a shuffle in the input data, without doing that in reality.</p>
<div class="highlight">
<pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">a_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">data_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">a_size</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Original order of indices: </span><span class="si">{</span><span class="n">data_indices</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Original order of indices: [0, 1, 2, 3]
</pre>
<p>If we shuffle the index_list we can change the order of our circular list without modifying the order or our original data.</p>
<div class="highlight">
<pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span> <span class="c1"># Shuffle the order</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Shuffled order of indices: </span><span class="si">{</span><span class="n">data_indices</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Shuffled order of indices: [3, 0, 1, 2]
</pre>
<p>Now we create a list of random values from a that is larger than a.</p>
<div class="highlight">
<pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">data_indices</span><span class="p">]</span>
<span class="n">b_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"New value order for first batch: </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">batch_counter</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">b_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">b_size</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">batch_counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">data_indices</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Shuffled Indexes for Batch No. </span><span class="si">{</span><span class="n">batch_counter</span><span class="si">}</span><span class="s2"> :</span><span class="si">{</span><span class="n">data_indices</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Values for Batch No.</span><span class="si">{</span><span class="n">batch_counter</span><span class="si">}</span><span class="s2"> :</span><span class="si">{</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">data_indices</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">b</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">data_indices</span><span class="p">[</span><span class="n">data_index</span><span class="p">]])</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">a_size</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Final value of b: </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="si">}</span><span class="s2"> items"</span><span class="p">)</span>
</pre></div>
<pre class="example">
New value order for first batch: [1, 3, 4, 2]

Shuffled Indexes for Batch No. 2 :[1, 3, 2, 0]
Values for Batch No.2 :[2, 4, 3, 1]

Shuffled Indexes for Batch No. 3 :[0, 3, 2, 1]
Values for Batch No.3 :[1, 4, 3, 2]

Final value of b: [1, 3, 4, 2, 2, 4, 3, 1, 1, 4] with 10 items
</pre>
<p><b>Note:</b> We call an epoch each time that an algorithm passes over all the training examples. Shuffling the examples for each epoch is known to reduce variance, making the models more general and overfit less.</p>
<p>Using sample. instead.</p>
<div class="highlight">
<pre><span></span><span class="n">data_indices</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">a_size</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="n">a_size</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">data_indices</span><span class="p">]</span>
<span class="n">b_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"New value order for first batch: </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">batch_counter</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">data_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">b_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="n">b_size</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">data_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">batch_counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">data_indices</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">data_indices</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">a_size</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Shuffled Indexes for Batch No. </span><span class="si">{</span><span class="n">batch_counter</span><span class="si">}</span><span class="s2"> :</span><span class="si">{</span><span class="n">data_indices</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Values for Batch No.</span><span class="si">{</span><span class="n">batch_counter</span><span class="si">}</span><span class="s2"> :</span><span class="si">{</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">data_indices</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="n">b</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">data_indices</span><span class="p">[</span><span class="n">data_index</span><span class="p">]])</span>
    <span class="n">data_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">a_size</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Final value of b: </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="si">}</span><span class="s2"> items"</span><span class="p">)</span>
</pre></div>
<pre class="example">
New value order for first batch: [1, 4, 3, 2]

Shuffled Indexes for Batch No. 2 :[3, 0, 1, 2]
Values for Batch No.2 :[4, 1, 2, 3]

Shuffled Indexes for Batch No. 3 :[2, 0, 1, 3]
Values for Batch No.3 :[3, 1, 2, 4]

Final value of b: [1, 4, 3, 2, 4, 1, 2, 3, 3, 1] with 10 items
</pre></div>
</div>
<div class="outline-3" id="outline-container-org805177d">
<h3 id="org805177d">Data Generator Function</h3>
<div class="outline-text-3" id="text-org805177d">
<p>This will be a data generator function that takes in <code>batch_size, x, y shuffle</code> where x could be a large list of samples, and y is a list of the tags associated with those samples. Return a subset of those inputs in a tuple of two arrays <code>(X,Y)</code>. Each is an array of dimension (<code>batch_size</code>). If <code>shuffle=True</code>, the data will be traversed in a random form.</p>
<p>Which runs continuously in the fashion of generators, pausing when yielding the next values. We will generate a <code>batch_size</code> output on each pass of this loop.</p>
<p>It has an inner loop that stores the data samples in temporary lists <code>(X, Y)</code> which will be included in the next batch.</p>
<p>There are three slightly out-of-the-ordinary features to this function.</p>
<ol class="org-ol">
<li>The first is the use of a list of a predefined size to store the data for each batch. Using a predefined size list reduces the computation time if the elements in the array are of a fixed size, like numbers. If the elements are of different sizes, it is better to use an empty array and append one element at a time during the loop.</li>
<li>The second is tracking the current location in the incoming lists of samples. Generators variables hold their values between invocations, so we create an <code>index</code> variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the <code>index</code> to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.</li>
<li>The third also relates to wrapping. Because <code>batch_size</code> and the length of the input lists are not aligned, gathering a <code>batch_size</code> group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the <code>index</code> to 0. We can re-shuffle the list of indexes to produce different batches each time.</li>
</ol>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">data_generator</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">data_x</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">data_y</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">"""Infinite batch generator</span>

<span class="sd">      Args: </span>
<span class="sd">       batch_size: the size to make batches</span>
<span class="sd">       data_x: list containing samples</span>
<span class="sd">       data_y: list containing labels</span>
<span class="sd">       shuffle: Shuffle the data order</span>

<span class="sd">      Yields:</span>
<span class="sd">       a tuple containing 2 elements:</span>
<span class="sd">       X - list of dim (batch_size) of samples</span>
<span class="sd">       Y - list of dim (batch_size) of labels</span>
<span class="sd">    """</span>
    <span class="n">amount_of_data</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_x</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">amount_of_data</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_y</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">re_shuffle</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="n">shuffler</span> <span class="o">=</span> <span class="n">re_shuffle</span> <span class="k">if</span> <span class="n">shuffle</span> <span class="k">else</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
    <span class="n">source_indices</span> <span class="o">=</span> <span class="n">shuffler</span><span class="p">(</span><span class="n">data_x</span><span class="p">)</span>

    <span class="n">source_location</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">batch_location</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>                            
            <span class="n">X</span><span class="p">[</span><span class="n">batch_location</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_x</span><span class="p">[</span><span class="n">source_indices</span><span class="p">[</span><span class="n">source_location</span><span class="p">]]</span>
            <span class="n">Y</span><span class="p">[</span><span class="n">batch_location</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_y</span><span class="p">[</span><span class="n">source_indices</span><span class="p">[</span><span class="n">source_location</span><span class="p">]]</span>
            <span class="n">source_location</span> <span class="o">=</span> <span class="p">(</span><span class="n">source_location</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">amount_of_data</span>
            <span class="n">source_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">shuffler</span><span class="p">(</span><span class="n">data_x</span><span class="p">)</span> <span class="k">if</span> <span class="n">source_location</span> <span class="o">==</span> <span class="mi">0</span>
                              <span class="k">else</span> <span class="n">source_indices</span><span class="p">)</span>            
        <span class="k">yield</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_data_generator</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Tests the un-shuffled version of the generator</span>

<span class="sd">    Raises:</span>
<span class="sd">     AssertionError: some value didn't match.</span>
<span class="sd">    """</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">xi</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>

    <span class="n">generator</span> <span class="o">=</span> <span class="n">data_generator</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">expected</span> <span class="ow">in</span> <span class="p">(([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">]),</span>
                     <span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span>
                     <span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                     <span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">16</span><span class="p">])):</span>
        <span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">generator</span><span class="p">),</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
    <span class="k">return</span>
<span class="n">test_data_generator</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/introducing-trax/">Introducing Trax</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/introducing-trax/" rel="bookmark"><time class="published dt-published" datetime="2020-12-17T17:19:06-08:00" itemprop="datePublished" title="2020-12-17 17:19">2020-12-17 17:19</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/introducing-trax/#orgf447e0b">Background</a>
<ul>
<li><a href="posts/nlp/introducing-trax/#org09f090a">Why Trax and not TensorFlow or PyTorch?</a></li>
<li><a href="posts/nlp/introducing-trax/#org97ba895">Why not Keras then?</a></li>
<li><a href="posts/nlp/introducing-trax/#org4b32e47">How to Code in Trax</a></li>
<li><a href="posts/nlp/introducing-trax/#org413f5cd">Trax, JAX, TensorFlow and Tensor2Tensor</a></li>
<li><a href="posts/nlp/introducing-trax/#org7caaf79">Installing Trax</a></li>
<li><a href="posts/nlp/introducing-trax/#org4414ea7">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/introducing-trax/#orgbf2610a">Middle</a>
<ul>
<li><a href="posts/nlp/introducing-trax/#org935d644">Layers</a>
<ul>
<li><a href="posts/nlp/introducing-trax/#orgc9d5903">Relu Layer</a></li>
<li><a href="posts/nlp/introducing-trax/#orge960c49">Concatenate Layer</a></li>
<li><a href="posts/nlp/introducing-trax/#orga6124f3">Configuring Layers</a></li>
<li><a href="posts/nlp/introducing-trax/#org2649efd">Layer Weights</a></li>
<li><a href="posts/nlp/introducing-trax/#org2b8ebba">Custom Layers</a></li>
</ul>
</li>
<li><a href="posts/nlp/introducing-trax/#org0551d27">Combinators</a>
<ul>
<li><a href="posts/nlp/introducing-trax/#org6630e1f">Serial Combinator</a></li>
</ul>
</li>
<li><a href="posts/nlp/introducing-trax/#orga86c419">JAX</a></li>
</ul>
</li>
<li><a href="posts/nlp/introducing-trax/#org14935be">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf447e0b">
<h2 id="orgf447e0b">Background</h2>
<div class="outline-text-2" id="text-orgf447e0b">
<p>This is going to be a first look at <a href="https://github.com/google/trax">Trax</a> a Deep Learning framework built by the Google Brain team.</p>
</div>
<div class="outline-3" id="outline-container-org09f090a">
<h3 id="org09f090a">Why Trax and not TensorFlow or PyTorch?</h3>
<div class="outline-text-3" id="text-org09f090a">
<p>TensorFlow and PyTorch are both extensive frameworks that can do almost anything in deep learning. They offer a lot of flexibility, but that often means verbosity of syntax and extra time to code.</p>
<p>Trax is much more concise. It runs on a TensorFlow backend but allows you to train models with 1 line commands. Trax also runs end to end, allowing you to get data, model and train all with a single terse statement. This means you can focus on learning, instead of spending hours on the idiosyncrasies of a big framework's implementation.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org97ba895">
<h3 id="org97ba895">Why not Keras then?</h3>
<div class="outline-text-3" id="text-org97ba895">
<p>Keras is now part of Tensorflow itself from 2.0 onwards. Also, trax is good for implementing new state of the art algorithms like Transformers, Reformers, BERT because it is actively maintained by Google Brain Team for advanced deep learning tasks. It runs smoothly on CPUs,GPUs and TPUs as well with comparatively lesser modifications in code.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org4b32e47">
<h3 id="org4b32e47">How to Code in Trax</h3>
<div class="outline-text-3" id="text-org4b32e47">
<p>Building models in Trax relies on 2 key concepts:- <b>layers</b> and <b>combinators</b>. Trax layers are simple objects that process data and perform computations. They can be chained together into composite layers using Trax combinators, allowing you to build layers and models of any complexity.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org413f5cd">
<h3 id="org413f5cd">Trax, JAX, TensorFlow and Tensor2Tensor</h3>
<div class="outline-text-3" id="text-org413f5cd">
<p>You already know that Trax uses Tensorflow as a backend, but it also uses the <a href="https://github.com/google/jax">JAX</a> library to speed up computation too. You can view JAX as an enhanced and optimized version of numpy.</p>
<p>You import their version of numpy using <code>import trax.fastmath.numpy</code>. If you see this line, remember that when calling <code>numpy</code> you are really calling Trax’s version of numpy that is compatible with JAX.**</p>
<p>As a result of this, where you used to encounter the type <code>numpy.ndarray</code> now you will find the type <code>jax.interpreters.xla.DeviceArray</code>. The documentation for JAX is <a href="https://jax.readthedocs.io/en/latest/index.html">here</a> and specifically they have a page <a href="https://jax.readthedocs.io/en/latest/jax.numpy.html">with the numpy functions implemented so far</a>.</p>
<p><a href="https://tensorflow.github.io/tensor2tensor/">Tensor2Tensor</a> is another name you might have heard. It started as an end to end solution much like how Trax is designed, but it grew unwieldy and complicated. So you can view Trax as the new improved version that operates much faster and simpler.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org7caaf79">
<h3 id="org7caaf79">Installing Trax</h3>
<div class="outline-text-3" id="text-org7caaf79">
<p>Note that there is another library called <a href="https://trax.readthedocs.io/en/latest/">TraX</a> which is something different.</p>
<p>We're going to use Trax version 1.3.1 here, so to install it with pip:</p>
<div class="highlight">
<pre><span></span>pip install trax==1.3.1
</pre></div>
<p>Note the <code>==</code> for the version, not <code>=</code>. This is a very big install so maybe take a break after you run it. You aren't going to get the full benefit of JAX if you don't have CUDA set up can use TPUs so make sure to set up CUDA if you're not using google colab. I also had to install <code>cmake</code> to get <code>trax</code> to install.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org4414ea7">
<h3 id="org4414ea7">Imports</h3>
<div class="outline-text-3" id="text-org4414ea7">
<div class="highlight">
<pre><span></span><span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">shapes</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">fastmath</span>
</pre></div>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html">Layers</a> are the basic building blocks for Trax</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.html#module-trax.shapes">shapes</a> are used for data handling</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.fastmath.html">fastmath</a> is the JAX version of numpy that can run on GPUs and TPUs</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgbf2610a">
<h2 id="orgbf2610a">Middle</h2>
<div class="outline-text-2" id="text-orgbf2610a"></div>
<div class="outline-3" id="outline-container-org935d644">
<h3 id="org935d644">Layers</h3>
<div class="outline-text-3" id="text-org935d644">
<p>Layers are the core building blocks in Trax - they are the base classes. They take inputs, compute functions/custom calculations and return outputs.</p>
</div>
<div class="outline-4" id="outline-container-orgc9d5903">
<h4 id="orgc9d5903">Relu Layer</h4>
<div class="outline-text-4" id="text-orgc9d5903">
<p>First we'll build a ReLU activation function as a layer. A layer like this is one of the simplest types. Notice there is no object initialization so it works just like a math function.</p>
<p><b>Note: Activation functions are also layers in Trax, which might look odd if you have been using other frameworks for a longer time.</b></p>
<div class="highlight">
<pre><span></span><span class="n">relu</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">()</span>
</pre></div>
<p>You can inspect the properties of a layer:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">relu</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">relu</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">relu</span><span class="o">.</span><span class="n">n_out</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : Relu
expected inputs : 1
promised outputs : 1 

</pre>
<p>We'll make an input the layer using numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x :"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x : [-2 -1  0  1  2] 

</pre>
<p>And see what it puts out.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
-- Outputs --
y : [0 0 0 1 2]
</pre>
<p>I don't know why but JAX doesn't thing I have a GPU, even though tensorflow does. This whole thing is a little messed up right now because the current release of tensorflow doesn't work on Ubuntu 20.10. I'm running it with the nightly build (2.5) but I have to install all the Trax dependencies one at a time or it will clobber the tensorflow installation with the older version (the one that doesn't work) so there's a lot of places for error.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orge960c49">
<h4 id="orge960c49">Concatenate Layer</h4>
<div class="outline-text-4" id="text-orge960c49">
<p>Now a layer that takes 2 inputs. Notice the change in the expected inputs property from 1 to 2.</p>
<p>First create a concatenate trax layer and check out its properties.</p>
<div class="highlight">
<pre><span></span><span class="n">concatenate</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">concatenate</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">concatenate</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">concatenate</span><span class="o">.</span><span class="n">n_out</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : Concatenate
expected inputs : 2
promised outputs : 1 

</pre>
<p>Now create the two inputs.</p>
<div class="highlight">
<pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">])</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">/</span> <span class="o">-</span><span class="mi">10</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x1 :"</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x2 :"</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x1 : [-10 -20 -30]
x2 : [1. 2. 3.] 

</pre>
<p>And now feed the inputs through the concatenate layer.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Outputs --
y : [-10. -20. -30.   1.   2.   3.]
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga6124f3">
<h4 id="orga6124f3">Configuring Layers</h4>
<div class="outline-text-4" id="text-orga6124f3">
<p>You can change the default settings of layers. For example, you can change the expected inputs for a concatenate layer from 2 to 3 using the optional parameter <code>n_items</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">concatenate_three</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">(</span><span class="n">n_items</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">concatenate_three</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">concatenate_three</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">concatenate_three</span><span class="o">.</span><span class="n">n_out</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : Concatenate
expected inputs : 3
promised outputs : 1 

</pre>
<p>Create some inputs.</p>
<div class="highlight">
<pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">])</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">/</span> <span class="o">-</span><span class="mi">10</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">*</span> <span class="mf">0.99</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x1 :"</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x2 :"</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x3 :"</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x1 : [-10 -20 -30]
x2 : [1. 2. 3.]
x3 : [0.99 1.98 2.97] 

</pre>
<p>And now do the concatenation.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">concatenate_three</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Outputs --
y : [-10.   -20.   -30.     1.     2.     3.     0.99   1.98   2.97]
</pre></div>
</div>
<div class="outline-4" id="outline-container-org2649efd">
<h4 id="org2649efd">Layer Weights</h4>
<div class="outline-text-4" id="text-org2649efd">
<p>Some layer types include mutable weights and biases that are used in computation and training. Layers of this type require initialization before use.</p>
<p>For example the <code>LayerNorm</code> layer calculates normalized data, that is also scaled by weights and biases. During initialization you pass the data shape and data type of the inputs, so the layer can initialize compatible arrays of weights and biases.</p>
<p>Initialize it.</p>
<div class="highlight">
<pre><span></span><span class="n">norm</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">()</span>
</pre></div>
<p>Now some input data.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">"float"</span><span class="p">)</span>
</pre></div>
<p>Use the input data signature to get the shape and type for the initializing weights and biases. We need to convert the input datatype from the usual ndarray to a trax ShapeDtype</p>
<div class="highlight">
<pre><span></span><span class="n">norm</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> 
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Normal shape:"</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">"Data Type:"</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Shapes Trax:"</span><span class="p">,</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="s2">"Data Type:"</span><span class="p">,</span><span class="nb">type</span><span class="p">(</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</pre></div>
<pre class="example">
Normal shape: (4,) Data Type: &lt;class 'tuple'&gt;
Shapes Trax: ShapeDtype{shape:(4,), dtype:float64} Data Type: &lt;class 'trax.shapes.ShapeDtype'&gt;
</pre>
<p>Here are its properties.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">n_out</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : LayerNorm
expected inputs : 1
promised outputs : 1
</pre>
<p>And the weights and biases.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"weights :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"biases :"</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>
</pre></div>
<pre class="example">
weights : [1. 1. 1. 1.]
biases : [0. 0. 0. 0.]
</pre>
<p>We have our input array.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x :"</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x : [0. 1. 2. 3.]
</pre>
<p>So we can inspect what the layer did to it.</p>
<div class="highlight">
<pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Outputs --
y : [-1.3416404  -0.44721344  0.44721344  1.3416404 ]
</pre>
<p>If you look at it you can see that the positives cancel out the negatives, giving us a sum of 0. I don't know why that's the norm, but maybe it'll become obvious later.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org2b8ebba">
<h4 id="org2b8ebba">Custom Layers</h4>
<div class="outline-text-4" id="text-org2b8ebba">
<p>You can create your own custom layers too and define custom functions for computations by using <code>layers.Fn</code>. Let me show you how.</p>
<div class="highlight">
<pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org54ac6b0">
Help on function Fn in module trax.layers.base:

Fn(name, f, n_out=1)
    Returns a layer with no weights that applies the function `f`.
    
    `f` can take and return any number of arguments, and takes only positional
    arguments -- no default or keyword arguments. It often uses JAX-numpy (`jnp`).
    The following, for example, would create a layer that takes two inputs and
    returns two outputs -- element-wise sums and maxima:
    
        `Fn('SumAndMax', lambda x0, x1: (x0 + x1, jnp.maximum(x0, x1)), n_out=2)`
    
    The layer's number of inputs (`n_in`) is automatically set to number of
    positional arguments in `f`, but you must explicitly set the number of
    outputs (`n_out`) whenever it's not the default value 1.
    
    Args:
      name: Class-like name for the resulting layer; for use in debugging.
      f: Pure function from input tensors to output tensors, where each input
          tensor is a separate positional arg, e.g., `f(x0, x1) --&gt; x0 + x1`.
          Output tensors must be packaged as specified in the `Layer` class
          docstring.
      n_out: Number of outputs promised by the layer; default value 1.
    
    Returns:
      Layer executing the function `f`.
</pre></div>
<ul class="org-ul">
<li><a id="org08e4d47"></a>Define a custom layer<br>
<div class="outline-text-5" id="text-org08e4d47">
<p>In this example we'll create a layer to calculate the input times 2.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">double_it</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">:</span>
    <span class="sd">"""A custom layer function that doubles any inputs</span>


<span class="sd">    Returns:</span>
<span class="sd">     a custom function that takes one numeric argument and doubles it</span>
<span class="sd">    """</span>
    <span class="n">layer_name</span> <span class="o">=</span> <span class="s2">"TimesTwo"</span>

    <span class="c1"># Custom function for the custom layer</span>
    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>

    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">(</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="orgee07e31"></a>Test it<br>
<div class="outline-text-5" id="text-orgee07e31">
<div class="highlight">
<pre><span></span><span class="n">double</span> <span class="o">=</span> <span class="n">double_it</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">double</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">double</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">double</span><span class="o">.</span><span class="n">n_out</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Properties --
name : TimesTwo
expected inputs : 1
promised outputs : 1
</pre>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x :"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">double</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x : [1 2 3] 

-- Outputs --
y : [2 4 6]
</pre></div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0551d27">
<h3 id="org0551d27">Combinators</h3>
<div class="outline-text-3" id="text-org0551d27">
<p>You can combine layers to build more complex layers. Trax provides a set of objects named combinator layers to make this happen. Combinators are themselves layers, so behavior commutes.</p>
</div>
<div class="outline-4" id="outline-container-org6630e1f">
<h4 id="org6630e1f">Serial Combinator</h4>
<div class="outline-text-4" id="text-org6630e1f">
<p>This is the most common and easiest to use. You could, for example, build a simple neural network by combining layers into a single layer using the <code>Serial</code> combinator. This new layer then acts just like a single layer, so you can inspect intputs, outputs and weights. Or even combine it into another layer! Combinators can then be used as trainable models. <i>Try adding more layers.</i></p>
<p><b>Note:As you must have guessed, if there is serial combinator, there must be a parallel combinator as well. Do try to explore about combinators and other layers from the trax documentation and look at the repo to understand how these layers are written.</b></p>
<div class="highlight">
<pre><span></span><span class="n">serial</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Relu</span><span class="p">(),</span>
    <span class="n">double</span><span class="p">,</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_units</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span> 
<span class="p">)</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org2b697ed"></a>Initialization<br>
<div class="outline-text-5" id="text-org2b697ed">
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="c1">#input</span>
<span class="n">serial</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">shapes</span><span class="o">.</span><span class="n">signature</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Serial Model --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">serial</span><span class="p">,</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Properties --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"name :"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"sublayers :"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"expected inputs :"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">n_in</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"promised outputs :"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">n_out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"weights & biases:"</span><span class="p">,</span> <span class="n">serial</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org1604a4b">
-- Serial Model --
Serial[
  LayerNorm
  Relu
  TimesTwo
  Dense_2
  Dense_1
  LogSoftmax
] 

-- Properties --
name : Serial
sublayers : [LayerNorm, Relu, TimesTwo, Dense_2, Dense_1, LogSoftmax]
expected inputs : 1
promised outputs : 1
weights & biases: [(DeviceArray([1, 1, 1, 1, 1], dtype=int32), DeviceArray([0, 0, 0, 0, 0], dtype=int32)), (), (), (DeviceArray([[ 0.19178385,  0.1832077 ],
             [-0.36949775, -0.03924937],
             [ 0.43800744,  0.788491  ],
             [ 0.43107533, -0.3623491 ],
             [ 0.6186575 ,  0.04764405]], dtype=float32), DeviceArray([-3.0051979e-06,  1.4359505e-06], dtype=float32)), (DeviceArray([[-0.6747592],
             [-0.8550365]], dtype=float32), DeviceArray([-8.9325863e-07], dtype=float32)), ()] 
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"-- Inputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"x :"</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">serial</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"-- Outputs --"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"y :"</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
-- Inputs --
x : [-2 -1  0  1  2] 

-- Outputs --
y : [0.]
</pre></div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orga86c419">
<h3 id="orga86c419">JAX</h3>
<div class="outline-text-3" id="text-orga86c419">
<p>Just remember to lookout for which numpy you are using, the regular numpy or Trax's JAX compatible numpy. Watch those import blocks. Numpy and fastmath.numpy have different data types.</p>
<p>Regular numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">x_numpy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"good old numpy : "</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_numpy</span><span class="p">),</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
good old numpy :  &lt;class 'numpy.ndarray'&gt; 

</pre>
<p>Fastmath and jax numpy.</p>
<div class="highlight">
<pre><span></span><span class="n">x_jax</span> <span class="o">=</span> <span class="n">fastmath</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"jax trax numpy : "</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">x_jax</span><span class="p">))</span>
</pre></div>
<pre class="example">
jax trax numpy :  &lt;class 'jax.interpreters.xla._DeviceArray'&gt;
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org14935be">
<h2 id="org14935be">End</h2>
<div class="outline-text-2" id="text-org14935be">
<ul class="org-ul">
<li>Trax is a concise framework, built on TensorFlow, for end to end machine learning. The key building blocks are layers and combinators.</li>
<li>This was a lab that was part of coursera's <b>Natural Language Processing with Sequence Models</b> course put up by DeepLearning.AI.</li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-visualizing-the-embeddings/">Word Embeddings: Visualizing the Embeddings</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/" rel="bookmark"><time class="published dt-published" datetime="2020-12-16T15:44:25-08:00" itemprop="datePublished" title="2020-12-16 15:44">2020-12-16 15:44</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#orge00b1cd">Extracting and Visualizing the Embeddings</a>
<ul>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#org6486d1f">Imports</a></li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#orgb0350eb">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#orgd65418a">Middle</a>
<ul>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#orge6936d6">Set It Up</a></li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#org9350d85">Visualizing</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/#org6a34607">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orge00b1cd">
<h2 id="orge00b1cd">Extracting and Visualizing the Embeddings</h2>
<div class="outline-text-2" id="text-orge00b1cd">
<p>In the <a href="posts/nlp/word-embeddings-training-the-model/">previous post</a> we built a Continuous Bag of Words model to predict a word based on the fraction of words each word surrounding it made up within a window (e.g. the fraction of the four words surrounding the word that each word made up). Now we're going to use the weights of the model as word embeddings and see if we can visualize them.</p>
</div>
<div class="outline-3" id="outline-container-org6486d1f">
<h3 id="org6486d1f">Imports</h3>
<div class="outline-text-3" id="text-org6486d1f">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Batches</span><span class="p">,</span>
    <span class="n">CBOW</span><span class="p">,</span>
    <span class="n">DataCleaner</span><span class="p">,</span>
    <span class="n">MetaData</span><span class="p">,</span>
    <span class="n">TheTrainer</span><span class="p">,</span>
    <span class="p">)</span>
<span class="c1"># my other stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb0350eb">
<h3 id="orgb0350eb">Set Up</h3>
<div class="outline-text-3" id="text-orgb0350eb">
<div class="highlight">
<pre><span></span><span class="n">cleaner</span> <span class="o">=</span> <span class="n">DataCleaner</span><span class="p">()</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">MetaData</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">)</span>
<span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">speak</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"word-embeddings-visualizing-the-embeddings"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layer</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="p">,</span> <span class="n">emit_point</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">trainer</span><span class="p">()</span>
</pre></div>
<pre class="example">
2020-12-16 16:32:17,189 graeae.timers.timer start: Started: 2020-12-16 16:32:17.189213
50: loss=9.88889093658385
new learning rate: 0.0198
100: loss=9.138356897918037
150: loss=9.149555378031549
new learning rate: 0.013068000000000001
200: loss=9.077599951734605
2020-12-16 16:32:37,403 graeae.timers.timer end: Ended: 2020-12-16 16:32:37.403860
2020-12-16 16:32:37,405 graeae.timers.timer end: Elapsed: 0:00:20.214647
250: loss=8.607763835003631
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">best_loss</span><span class="p">)</span>
</pre></div>
<pre class="example">
8.186490214727549
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd65418a">
<h2 id="orgd65418a">Middle</h2>
<div class="outline-text-2" id="text-orgd65418a"></div>
<div class="outline-3" id="outline-container-orge6936d6">
<h3 id="orge6936d6">Set It Up</h3>
<div class="outline-text-3" id="text-orge6936d6">
<p>We're going to use the method of averaging the weights of the two layers to form the embeddings.</p>
<div class="highlight">
<pre><span></span><span class="n">embeddings</span> <span class="o">=</span> <span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">best_weights</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">T</span>
              <span class="o">+</span> <span class="n">trainer</span><span class="o">.</span><span class="n">best_weights</span><span class="o">.</span><span class="n">hidden_weights</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
</pre></div>
<p>And now our words.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"queen"</span><span class="p">,</span><span class="s2">"lord"</span><span class="p">,</span><span class="s2">"man"</span><span class="p">,</span> <span class="s2">"woman"</span><span class="p">,</span><span class="s2">"dog"</span><span class="p">,</span><span class="s2">"wolf"</span><span class="p">,</span>
         <span class="s2">"rich"</span><span class="p">,</span><span class="s2">"happy"</span><span class="p">,</span><span class="s2">"sad"</span><span class="p">]</span>
</pre></div>
<p>Now we need to translate the words into their indices so we can grab the rows in the mebedding that match.</p>
<div class="highlight">
<pre><span></span><span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">indices</span><span class="p">,</span> <span class="p">:]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span> 
</pre></div>
<pre class="example">
(10, 50) [2745, 3951, 2961, 3023, 5675, 1452, 5674, 4191, 2316, 4278]
</pre>
<p>There are 10 rows to match our ten words and 50 columns to match the number chosen for the hidden layer.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org9350d85">
<h3 id="org9350d85">Visualizing</h3>
<div class="outline-text-3" id="text-org9350d85">
<p>We're going to use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn's PCA</a> for Principal Component Analysis. The <code>n_components</code> argument is the number of components it will keep - we'll keep 2.</p>
<div class="highlight">
<pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">pca_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">reduced</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">"X"</span><span class="p">,</span> <span class="s2">"Y"</span><span class="p">])</span>

<span class="n">pca_data</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">points</span> <span class="o">=</span> <span class="n">pca_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span>
                                 <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">pca_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">,</span> <span class="n">text_baseline</span><span class="o">=</span><span class="s2">"top"</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">points</span> <span class="o">*</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"PCA Embeddings"</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"embeddings_pca"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/word-embeddings-visualizing-the-embeddings/embeddings_pca.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Well, that's pretty horrible. Might need work.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org6a34607">
<h2 id="org6a34607">End</h2>
<div class="outline-text-2" id="text-org6a34607">
<p>This is the final post in the series looking at using a Continuous Bag of Words model to create word embeddings. Here are the other posts.</p>
<ul class="org-ul">
<li><a href="posts/nlp/word-embeddings-build-a-model/">Introduction</a></li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/">Loading the Data</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/">Building and Training the CBOW Model</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-training-the-model/">Word Embeddings: Training the Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-training-the-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-13T14:42:07-08:00" itemprop="datePublished" title="2020-12-13 14:42">2020-12-13 14:42</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgcc84c12">Building and Training the Model</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgecd038d">Imports</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org57e467d">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org7b9ed03">Middle</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgf498642">Initializing the model</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org7f65f9e">Softmax</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org6c35267">The Implementation</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org182faad">Forward propagation</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orge0c47ca">Test the function</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgb96225d">Pack Index with Frequency</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgc9c5613">Vector Generator</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org77731cf">Batch Generator</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgbbd8908">Cost function</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org36ea798">Test the function</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgeb46942">Training the Model - Backpropagation</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgd59302b">Test the function</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org7cc0242">Gradient Descent</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgc360b08">Test Your Function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org162bcf3">End</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orge5faf6b">Bundling It Up</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgb0587fa">Imports</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org4593295">Enum Setup</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org273fbac">Named Tuples</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org8beee33">The CBOW Model</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org351a26c">Batch Generator</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgbcea38d">The Trainer</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org9ef1da1">Testing It</a>
<ul>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org0a5e623">Forward Propagation</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgcc245d1">Cross Entropy Loss</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgc38c9a2">Back Propagation</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgcc2297f">Putting Some Stuff Together</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgd62eadc">The Batches</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org7e12b1a">Gradient Descent</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#org495d876">Gradient Re-do</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/#orgdc4e0de">Troubleshooting the Batches</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgcc84c12">
<h2 id="orgcc84c12">Building and Training the Model</h2>
<div class="outline-text-2" id="text-orgcc84c12">
<p>In the <a href="posts/nlp/word-embeddings-shakespeare-data/">previous post</a> we did some preliminary set up and data pre-processing. Now we're going to build and train a Continuous Bag of Words (CBOW) model.</p>
</div>
<div class="outline-3" id="outline-container-orgecd038d">
<h3 id="orgecd038d">Imports</h3>
<div class="outline-text-3" id="text-orgecd038d">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">unique</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">contain_exactly</span><span class="p">,</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">DataCleaner</span><span class="p">,</span> <span class="n">MetaData</span>

<span class="c1"># my other stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org57e467d">
<h3 id="org57e467d">Set Up</h3>
<div class="outline-text-3" id="text-org57e467d">
<p>Code from the previous post.</p>
<div class="highlight">
<pre><span></span><span class="n">cleaner</span> <span class="o">=</span> <span class="n">DataCleaner</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">MetaData</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">speak</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="s2">"files/posts/nlp/word-embeddings-training-the-model"</span><span class="p">)</span>
<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
<p>Something to help remember what the numpy <code>axis</code> argument is.</p>
<div class="highlight">
<pre><span></span><span class="nd">@unique</span>
<span class="k">class</span> <span class="nc">Axis</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7b9ed03">
<h2 id="org7b9ed03">Middle</h2>
<div class="outline-text-2" id="text-org7b9ed03"></div>
<div class="outline-3" id="outline-container-orgf498642">
<h3 id="orgf498642">Initializing the model</h3>
<div class="outline-text-3" id="text-orgf498642">
<p>You will now initialize two matrices and two vectors.</p>
<ul class="org-ul">
<li>The first matrix (\(W_1\)) is of dimension \(N \times V\), where <i>V</i> is the number of words in your vocabulary and <i>N</i> is the dimension of your word vector.</li>
<li>The second matrix (\(W_2\)) is of dimension \(V \times N\).</li>
<li>Vector \(b_1\) has dimensions \(N\times 1\)</li>
<li>Vector \(b_2\) has dimensions \(V\times 1\).</li>
<li>\(b_1\) and \(b_2\) are the bias vectors of the linear layers from matrices \(W_1\) and \(W_2\).</li>
</ul>
<p>At this stage we are just initializing the parameters.</p>
<p>Please use <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html">numpy.random.rand</a> to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: initialize_model</span>
<span class="k">def</span> <span class="nf">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span><span class="n">V</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">random_seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Initialize the matrices with random values</span>

<span class="sd">    Args: </span>
<span class="sd">       N:  dimension of hidden vector </span>
<span class="sd">       V:  dimension of vocabulary</span>
<span class="sd">       random_seed: random seed for consistent results in the unit tests</span>
<span class="sd">     Returns: </span>
<span class="sd">       W1, W2, b1, b2: initialized weights and biases</span>
<span class="sd">    """</span>

    <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>
    <span class="c1"># W1 has shape (N,V)</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
    <span class="c1"># W2 has shape (V,N)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="c1"># b1 has shape (N,1)</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># b2 has shape (V,1)</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span>
</pre></div>
<p>Test your function example.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_V</span><span class="p">,</span><span class="n">tmp_N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape: </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape: </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape: </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape: </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
tmp_W1.shape: (4, 10)
tmp_W2.shape: (10, 4)
tmp_b1.shape: (4, 1)
tmp_b2.shape: (10, 1)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org7f65f9e">
<h3 id="org7f65f9e">Softmax</h3>
<div class="outline-text-3" id="text-org7f65f9e">
<p>Before we can start training the model, we need to implement the softmax function as defined in equation 5:</p>
<p>\[ \text{softmax}(z_i) = \frac{e^{z_i} }{\sum_{i=0}^{V-1} e^{z_i} } \tag{5} \]</p>
<ul class="org-ul">
<li>Array indexing in code starts at 0.</li>
<li><i>V</i> is the number of words in the vocabulary (which is also the number of rows of <i>z</i>).</li>
<li><i>i</i> goes from 0 to |V| - 1.</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org6c35267">
<h4 id="org6c35267">The Implementation</h4>
<div class="outline-text-4" id="text-org6c35267">
<ul class="org-ul">
<li>Assume that the input <i>z</i> to <code>softmax</code> is a 2D array</li>
<li>Each training example is represented by a column of shape (V, 1) in this 2D array.</li>
<li>There may be more than one column, in the 2D array, because you can put in a batch of examples to increase efficiency. Let's call the batch size lowercase <i>m</i>, so the <i>z</i> array has shape (V, m)</li>
<li>When taking the sum from \(i=1 \cdots V-1\), take the sum for each column (each example) separately.</li>
</ul>
<p>Please use</p>
<ul class="org-ul">
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">numpy.exp</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html">numpy.sum</a> (set the axis so that you take the sum of each column in z)</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: softmax</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate the softmax</span>

<span class="sd">    Args: </span>
<span class="sd">       z: output scores from the hidden layer</span>
<span class="sd">    Returns: </span>
<span class="sd">       yhat: prediction (estimate of y)</span>
<span class="sd">    """</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>

    <span class="c1"># Calculate yhat (softmax)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">/</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">ROWS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">yhat</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1"># Test the function</span>
<span class="n">tmp</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
                   <span class="p">])</span>
<span class="n">tmp_sm</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.73105858</span><span class="p">,</span> <span class="mf">0.88079708</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.26894142</span><span class="p">,</span> <span class="mf">0.11920292</span><span class="p">]])</span>


<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.5        0.73105858 0.88079708]
 [0.5        0.26894142 0.11920292]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org182faad">
<h3 id="org182faad">Forward propagation</h3>
<div class="outline-text-3" id="text-org182faad">
<p>We're going to implement the forward propagation <i>z</i> according to equations (1) to (3).</p>
\begin{align} h &amp;= W_1 \ X + b_1 \tag{1} \\ a &amp;= ReLU(h) \tag{2} \\ z &amp;= W_2 \ a + b_2 \tag{3} \\ \end{align}
<p>For that, you will use as activation the Rectified Linear Unit (ReLU) given by:</p>
<p>\[ f(h)=\max (0,h) \tag{6} \]</p>
<p><b>Hints:</b></p>
<ul class="org-ul">
<li>You can use <a href="https://numpy.org/doc/stable/reference/generated/numpy.maximum.html">numpy.maximum(x1,x2)</a> to get the maximum of two values</li>
<li>Use <a href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html">numpy.dot(A,B)</a> to matrix multiply A and B</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: forward_prop</span>
<span class="k">def</span> <span class="nf">forward_prop</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">W1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">b1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Pass the data through the network</span>

<span class="sd">    Args: </span>
<span class="sd">       x:  average one hot vector for the context </span>
<span class="sd">       W1, W2, b1, b2:  matrices and biases to be learned</span>
<span class="sd">    Returns: </span>
<span class="sd">       z:  output score vector</span>
<span class="sd">    """</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>

    <span class="c1"># Calculate h</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>

    <span class="c1"># Apply the relu on h (store result in h)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Calculate z</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">h</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orge0c47ca">
<h4 id="orge0c47ca">Test the function</h4>
<div class="outline-text-4" id="text-orge0c47ca">
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">tmp_x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>

<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">V</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"x has shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"N is </span><span class="si">{</span><span class="n">tmp_N</span><span class="si">}</span><span class="s2"> and vocabulary size V is </span><span class="si">{</span><span class="n">tmp_V</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"call forward_prop"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"z has shape </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"z has values:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"h has shape </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"h has values:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.55379268</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.58960774</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.50722933</span><span class="p">]]</span>
<span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.92477674</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.02487333</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org4de721b">
x has shape (3, 1)
N is 2 and vocabulary size V is 3
call forward_prop

z has shape (3, 1)
z has values:
[[0.55379268]
 [1.58960774]
 [1.50722933]]

h has shape (2, 1)
h has values:
[[0.92477674]
 [1.02487333]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb96225d">
<h3 id="orgb96225d">Pack Index with Frequency</h3>
<div class="outline-text-3" id="text-orgb96225d">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">index_with_frequency</span><span class="p">(</span><span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                              <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""combines indexes and frequency counts-dict</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to get the indices for</span>
<span class="sd">     word_to_index: mapping of word to index</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of (word-index, word-count) tuples built from context_words</span>
<span class="sd">    """</span>
    <span class="n">frequency_dict</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="n">packed</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)):</span>
        <span class="n">word_index</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">frequency</span> <span class="o">=</span> <span class="n">frequency_dict</span><span class="p">[</span><span class="n">context_words</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span>
        <span class="n">packed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">packed</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc9c5613">
<h3 id="orgc9c5613">Vector Generator</h3>
<div class="outline-text-3" id="text-orgc9c5613">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">vectors</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">"""Generates vectors of fraction of context words each word represents</span>

<span class="sd">    Args:</span>
<span class="sd">     data: source of the vectors</span>
<span class="sd">     word_to_index: mapping of word to index in the vocabulary</span>
<span class="sd">     half_window: number of tokens on either side of the word to keep</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of x, y </span>
<span class="sd">    """</span>
    <span class="n">location</span> <span class="o">=</span> <span class="n">half_window</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">location</span><span class="p">]</span>
        <span class="n">y</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">):</span> <span class="n">location</span><span class="p">]</span>
                         <span class="o">+</span> <span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">location</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

        <span class="k">for</span> <span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span> <span class="ow">in</span> <span class="n">index_with_frequency</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">frequency</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
        <span class="n">location</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">location</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"location in data is being set to 0"</span><span class="p">)</span>
            <span class="n">location</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org77731cf">
<h3 id="org77731cf">Batch Generator</h3>
<div class="outline-text-3" id="text-org77731cf">
<p>This uses a not so common form of the <a href="https://docs.python.org/3/reference/compound_stmts.html#while">while</a> loop. Whenever you run a loop and it reaches the end (so you didn't break it) then it will run the <code>else</code> clause.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                    <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">original</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">"""Generate batches of vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the training data</span>
<span class="sd">     word_to_index: map of word to vocabulary index</span>
<span class="sd">     half_window: number of tokens to take from either side of word</span>
<span class="sd">     batch_size: Number of vectors to put in each training batch</span>
<span class="sd">     original: run the original buggy code</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of X, Y batches</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                        <span class="n">word_to_index</span><span class="p">,</span>
                        <span class="n">half_window</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">original</span><span class="p">:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
                <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span>
</pre></div>
<p>So every time <code>batch_x</code> reaches the <code>batch_size</code> it yields the tuple and then creates a new batch before continuing the outer for-loop.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgbbd8908">
<h3 id="orgbbd8908">Cost function</h3>
<div class="outline-text-3" id="text-orgbbd8908">
<p>The cross-entropy loss function.</p>
<ul class="org-ul">
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html">numpy.squeeze</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.multiply.html">numpy.multiply</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.log.html">numpy.log</a></li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the cross-entropy loss</span>

<span class="sd">    Args:</span>
<span class="sd">     y: array with the actual words labeled</span>
<span class="sd">     y_hat: our model's guesses for the words</span>
<span class="sd">     batch_size: the number of examples per training run</span>
<span class="sd">    """</span>
    <span class="n">log_probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
                         <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org36ea798">
<h4 id="org36ea798">Test the function</h4>
<div class="outline-text-4" id="text-org36ea798">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_Ind2word</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_x.shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_y.shape </span><span class="si">{</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_z.shape: </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_h.shape: </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"call compute_cost"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_cost </span><span class="si">{</span><span class="n">tmp_cost</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">tmp_cost</span><span class="p">,</span> <span class="mf">9.9560</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orga58a758">
tmp_x.shape (5778, 4)
tmp_y.shape (5778, 4)
tmp_W1.shape (50, 5778)
tmp_W2.shape (5778, 50)
tmp_b1.shape (50, 1)
tmp_b2.shape (5778, 1)
tmp_z.shape: (5778, 4)
tmp_h.shape: (50, 4)
tmp_yhat.shape: (5778, 4)
call compute_cost
tmp_cost 9.9560
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgeb46942">
<h3 id="orgeb46942">Training the Model - Backpropagation</h3>
<div class="outline-text-3" id="text-orgeb46942">
<p>Now that you have understood how the CBOW model works, you will train it. You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors.</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: back_prop</span>
<span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">yhat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">h</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">W1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">W2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">b1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">b2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Calculates the gradients</span>

<span class="sd">    Args: </span>
<span class="sd">       x:  average one hot vector for the context </span>
<span class="sd">       yhat: prediction (estimate of y)</span>
<span class="sd">       y:  target vector</span>
<span class="sd">       h:  hidden vector (see eq. 1)</span>
<span class="sd">       W1, W2, b1, b2:  matrices and biases  </span>
<span class="sd">       batch_size: batch size </span>

<span class="sd">     Returns: </span>
<span class="sd">       grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   </span>
<span class="sd">    """</span>
    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># Compute l1 as W2^T (Yhat - Y)</span>
    <span class="c1"># Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Apply relu to l1</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Compute the gradient of W1</span>
    <span class="n">grad_W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of W2</span>
    <span class="n">grad_W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of b1</span>
    <span class="n">grad_b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of b2</span>
    <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgd59302b">
<h4 id="orgd59302b">Test the function</h4>
<div class="outline-text-4" id="text-orgd59302b">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_Ind2word</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="c1"># get a batch of data</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"get a batch of data"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_x.shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_y.shape </span><span class="si">{</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Initialize weights and biases"</span><span class="p">)</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Forwad prop to get z and h"</span><span class="p">)</span>
<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_z.shape: </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_h.shape: </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Get yhat by calling softmax"</span><span class="p">)</span>
<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_m</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">tmp_C</span><span class="p">)</span>
<span class="n">tmp_grad_W1</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">,</span> <span class="n">tmp_grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_h</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"call back_prop"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_W1.shape </span><span class="si">{</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_W2.shape </span><span class="si">{</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_b1.shape </span><span class="si">{</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_b2.shape </span><span class="si">{</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>


<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example" id="orgb769135">
get a batch of data
tmp_x.shape (5778, 4)
tmp_y.shape (5778, 4)

Initialize weights and biases
tmp_W1.shape (50, 5778)
tmp_W2.shape (5778, 50)
tmp_b1.shape (50, 1)
tmp_b2.shape (5778, 1)

Forwad prop to get z and h
tmp_z.shape: (5778, 4)
tmp_h.shape: (50, 4)

Get yhat by calling softmax
tmp_yhat.shape: (5778, 4)

call back_prop
tmp_grad_W1.shape (50, 5778)
tmp_grad_W2.shape (5778, 50)
tmp_grad_b1.shape (50, 1)
tmp_grad_b2.shape (5778, 1)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7cc0242">
<h3 id="org7cc0242">Gradient Descent</h3>
<div class="outline-text-3" id="text-org7cc0242">
<p>Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set.</p>
<p><b>Hint:</b> For that, you will use <code>initialize_model</code> and the <code>back_prop</code> functions which you just created (and the <code>compute_cost</code> function). You can also use the provided <code>get_batches</code> helper function:</p>
<p>Also: print the cost after each batch is processed (use batch size = 128).</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: gradient_descent</span>
<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="nb">int</span> <span class="p">,</span>
                     <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.03</span><span class="p">):</span>    
    <span class="sd">"""</span>
<span class="sd">    This is the gradient_descent function</span>

<span class="sd">    Args: </span>
<span class="sd">       data:      text</span>
<span class="sd">       word2Ind:  words to Indices</span>
<span class="sd">       N:         dimension of hidden vector  </span>
<span class="sd">       V:         dimension of vocabulary </span>
<span class="sd">       num_iters: number of iterations  </span>

<span class="sd">    Returns: </span>
<span class="sd">       W1, W2, b1, b2:  updated matrices and biases   </span>
<span class="sd">    """</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">282</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>
        <span class="c1"># Get z and h</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
        <span class="c1"># Get yhat</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># Get cost</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span> <span class="p">(</span><span class="n">iters</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"iters: </span><span class="si">{</span><span class="n">iters</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2"> cost: </span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="c1"># Get gradients</span>
        <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                                                       <span class="n">yhat</span><span class="p">,</span>
                                                       <span class="n">y</span><span class="p">,</span>
                                                       <span class="n">h</span><span class="p">,</span>
                                                       <span class="n">W1</span><span class="p">,</span>
                                                       <span class="n">W2</span><span class="p">,</span>
                                                       <span class="n">b1</span><span class="p">,</span>
                                                       <span class="n">b2</span><span class="p">,</span>
                                                       <span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Update weights and biases</span>
        <span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
        <span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
        <span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
        <span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>

        <span class="c1">### END CODE HERE ###</span>

        <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span> 
        <span class="k">if</span> <span class="n">iters</span> <span class="o">==</span> <span class="n">num_iters</span><span class="p">:</span> 
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">iters</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">*=</span> <span class="mf">0.66</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgc360b08">
<h4 id="orgc360b08">Test Your Function</h4>
<div class="outline-text-4" id="text-orgc360b08">
<div class="highlight">
<pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">num_iters</span> <span class="o">=</span> <span class="mi">150</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Call gradient_descent"</span><span class="p">)</span>
<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orgdfb25cc">
Call gradient_descent
iters: 10 cost: 0.789141
iters: 20 cost: 0.105543
iters: 30 cost: 0.056008
iters: 40 cost: 0.038101
iters: 50 cost: 0.028868
iters: 60 cost: 0.023237
iters: 70 cost: 0.019444
iters: 80 cost: 0.016716
iters: 90 cost: 0.014660
iters: 100 cost: 0.013054
iters: 110 cost: 0.012133
iters: 120 cost: 0.011370
iters: 130 cost: 0.010698
iters: 140 cost: 0.010100
iters: 150 cost: 0.009566
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org162bcf3">
<h2 id="org162bcf3">End</h2>
<div class="outline-text-2" id="text-org162bcf3">
<p>The <a href="posts/nlp/word-embeddings-visualizing-the-embeddings/">next post</a> is one on extracting and visualizing the embeddings using Principal Component Analysis.</p>
</div>
<div class="outline-3" id="outline-container-orge5faf6b">
<h3 id="orge5faf6b">Bundling It Up</h3>
<div class="outline-text-3" id="text-orge5faf6b"></div>
<div class="outline-4" id="outline-container-orgb0587fa">
<h4 id="orgb0587fa">Imports</h4>
<div class="outline-text-4" id="text-orgb0587fa">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">unique</span>

<span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4593295">
<h4 id="org4593295">Enum Setup</h4>
<div class="outline-text-4" id="text-org4593295">
<div class="highlight">
<pre><span></span><span class="nd">@unique</span>
<span class="k">class</span> <span class="nc">Axis</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org273fbac">
<h4 id="org273fbac">Named Tuples</h4>
<div class="outline-text-4" id="text-org273fbac">
<div class="highlight">
<pre><span></span><span class="n">Gradients</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Gradients"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"input_weights"</span><span class="p">,</span> <span class="s2">"hidden_weights"</span><span class="p">,</span> <span class="s2">"input_bias"</span><span class="p">,</span> <span class="s2">"hidden_bias"</span><span class="p">])</span>

<span class="n">Weights</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Weights"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"input_weights"</span><span class="p">,</span> <span class="s2">"hidden_weights"</span><span class="p">,</span> <span class="s2">"input_bias"</span><span class="p">,</span> <span class="s2">"hidden_bias"</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8beee33">
<h4 id="org8beee33">The CBOW Model</h4>
<div class="outline-text-4" id="text-org8beee33">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">CBOW</span><span class="p">:</span>
    <span class="sd">"""A continuous bag of words model builder</span>

<span class="sd">    Args:</span>
<span class="sd">     hidden: number of rows in the hidden layer</span>
<span class="sd">     vocabulary_size: number of tokens in the vocabulary</span>
<span class="sd">     learning_rate: learning rate for back-propagation updates</span>
<span class="sd">     random_seed: int</span>
<span class="sd">    """</span>
    <span class="n">hidden</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.03</span>
    <span class="n">random_seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span>    
    <span class="n">_random_generator</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PCG64</span><span class="o">=</span><span class="kc">None</span>

    <span class="c1"># layer one</span>
    <span class="n">_input_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_input_bias</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>

    <span class="c1"># hidden layer</span>
    <span class="n">_hidden_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_hidden_bias</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgaa55b95"></a>The Random Generator<br>
<div class="outline-text-5" id="text-orgaa55b95">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">random_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PCG64</span><span class="p">:</span>
    <span class="sd">"""The random number generator"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span>
</pre></div>
</div>
</li>
<li><a id="org07c3847"></a>First Layer Weights<br>
<div class="outline-text-5" id="text-org07c3847">
<p>These are initialized using numpy's new generator. I originally using their standard-normal version by mistake and the model did horrible. Using the <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.random.html#numpy.random.Generator.random">Generator.random</a> gives you a uniform distribution which seems to be what you're supposed to use.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Weights for the first layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span>
</pre></div>
</div>
</li>
<li><a id="orgc4f28a9"></a>First Layer Bias<br>
<div class="outline-text-5" id="text-orgc4f28a9">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Bias for the input layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span>
</pre></div>
</div>
</li>
<li><a id="org19feb02"></a>Hidden Layer Weights<br>
<div class="outline-text-5" id="text-org19feb02">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">hidden_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""The weights for the hidden layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span>
</pre></div>
</div>
</li>
<li><a id="org2cd4722"></a>Hidden Layer Bias<br>
<div class="outline-text-5" id="text-org2cd4722">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">hidden_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Bias for the hidden layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span>
</pre></div>
</div>
</li>
<li><a id="orge963aaa"></a>Softmax<br>
<div class="outline-text-5" id="text-orge963aaa">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate the softmax</span>

<span class="sd">    Args: </span>
<span class="sd">       scores: output scores from the hidden layer</span>
<span class="sd">    Returns: </span>
<span class="sd">       yhat: prediction (estimate of y)"""</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">/</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">ROWS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="org4b725a8"></a>Forward Propagation<br>
<div class="outline-text-5" id="text-org4b725a8">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""makes a model prediction</span>

<span class="sd">    Args:</span>
<span class="sd">     data: x-values to train on</span>

<span class="sd">    Returns:</span>
<span class="sd">     output, first-layer output</span>
<span class="sd">    """</span>
    <span class="n">first_layer_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_weights</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
                                  <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">second_layer_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_weights</span><span class="p">,</span> <span class="n">first_layer_output</span><span class="p">)</span>
                   <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">second_layer_output</span><span class="p">,</span> <span class="n">first_layer_output</span>
</pre></div>
</div>
</li>
<li><a id="org0019722"></a>Gradients<br>
<div class="outline-text-5" id="text-org0019722">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">hidden_input</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Gradients</span><span class="p">:</span>
    <span class="sd">"""does the gradient calculation for back-propagation</span>

<span class="sd">    This is broken out to be able to troubleshoot/compare it</span>

<span class="sd">   Args:</span>
<span class="sd">     data: the input x value</span>
<span class="sd">     predicted: what our model predicted the labels for the data should be</span>
<span class="sd">     actual: what the actual labels should have been</span>
<span class="sd">     hidden_input: the input to the hidden layer</span>
<span class="sd">    Returns:</span>
<span class="sd">     Gradients for input_weight, hidden_weight, input_bias, hidden_bias</span>
<span class="sd">    """</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">predicted</span> <span class="o">-</span> <span class="n">actual</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">difference</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">difference</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">input_weights_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">hidden_weights_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">difference</span><span class="p">,</span> <span class="n">hidden_input</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">input_bias_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span>
                                    <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                    <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">hidden_bias_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">difference</span><span class="p">,</span>
                                     <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                     <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">Gradients</span><span class="p">(</span><span class="n">input_weights</span><span class="o">=</span><span class="n">input_weights_gradient</span><span class="p">,</span>
                     <span class="n">hidden_weights</span><span class="o">=</span><span class="n">hidden_weights_gradient</span><span class="p">,</span>
                     <span class="n">input_bias</span><span class="o">=</span><span class="n">input_bias_gradient</span><span class="p">,</span>
                     <span class="n">hidden_bias</span><span class="o">=</span><span class="n">hidden_bias_gradient</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="orga06aafa"></a>Backward Propagation<br>
<div class="outline-text-5" id="text-orga06aafa">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">hidden_input</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Does back-propagation to update the weights</span>

<span class="sd">   Arg:s</span>
<span class="sd">     data: the input x value</span>
<span class="sd">     predicted: what our model predicted the labels for the data should be</span>
<span class="sd">     actual: what the actual labels should have been</span>
<span class="sd">     hidden_input: the input to the hidden layer</span>
<span class="sd">    """</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                               <span class="n">predicted</span><span class="o">=</span><span class="n">predicted</span><span class="p">,</span>
                               <span class="n">actual</span><span class="o">=</span><span class="n">actual</span><span class="p">,</span>
                               <span class="n">hidden_input</span><span class="o">=</span><span class="n">hidden_input</span><span class="p">)</span>
    <span class="c1"># I don't have setters for the properties so use the private variables</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">input_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">hidden_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">input_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">hidden_bias</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="org6b322fb"></a>Call<br>
<div class="outline-text-5" id="text-org6b322fb">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""makes a prediction on the data</span>

<span class="sd">    Args:</span>
<span class="sd">     data: input data for the prediction</span>

<span class="sd">    Returns:</span>
<span class="sd">     softmax of model output</span>
<span class="sd">    """</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org351a26c">
<h4 id="org351a26c">Batch Generator</h4>
<div class="outline-text-4" id="text-org351a26c">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Batches</span><span class="p">:</span>
    <span class="sd">"""Generates batches of data</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the source of the data to generate (training data)</span>
<span class="sd">     word_to_index: dict mapping the word to the vocabulary index</span>
<span class="sd">     half_window: number of tokens on either side of word to grab</span>
<span class="sd">     batch_size: the number of entries per batch</span>
<span class="sd">     batches: number of batches to generate before quitting</span>
<span class="sd">     verbose: whether to emit messages</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span>
    <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">batches</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">repetitions</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>    
    <span class="n">_vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vectors</span><span class="p">:</span> <span class="nb">object</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org4c96abb"></a>Vocabulary Size<br>
<div class="outline-text-5" id="text-org4c96abb">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vocabulary_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">"""Number of tokens in the vocabulary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span>
</pre></div>
</div>
</li>
<li><a id="orgeae8cc9"></a>Vectors<br>
<div class="outline-text-5" id="text-orgeae8cc9">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""our vector-generator started up"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_generator</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span>
</pre></div>
</div>
</li>
<li><a id="orge6eb27d"></a>Indices and Frequencies<br>
<div class="outline-text-5" id="text-orge6eb27d">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">indices_and_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""combines word-indexes and frequency counts-dict</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to get the indices for</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of (word-index, word-count) tuples built from context_words</span>
<span class="sd">    """</span>
    <span class="n">frequencies</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">frequencies</span><span class="p">[</span><span class="n">context_words</span><span class="p">[</span><span class="n">index</span><span class="p">]])</span>
            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">))]</span>
</pre></div>
</div>
</li>
<li><a id="orgb8e52fc"></a>Vectors<br>
<div class="outline-text-5" id="text-orgb8e52fc">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">vector_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Generates vectors infinitely</span>

<span class="sd">    x: fraction of context words represented by word</span>
<span class="sd">    y: array with 1 where center word is in the vocabulary and 0 elsewhere</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of x, y </span>
<span class="sd">    """</span>
    <span class="n">location</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">location</span><span class="p">]</span>
        <span class="n">y</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span><span class="p">):</span> <span class="n">location</span><span class="p">]</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">location</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

        <span class="k">for</span> <span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_and_frequencies</span><span class="p">(</span><span class="n">context_words</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">frequency</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
        <span class="n">location</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">location</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"location in data is being set to 0"</span><span class="p">)</span>
            <span class="n">location</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="orgd421562"></a>Iterator Method<br>
<div class="outline-text-5" id="text-orgd421562">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""makes this into an iterator"""</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>
</div>
</li>
<li><a id="org9bdade8"></a>Next Method<br>
<div class="outline-text-5" id="text-org9bdade8">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Creates the batches and returns them</span>

<span class="sd">    Returns:</span>
<span class="sd">     x, y batches</span>
<span class="sd">    """</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">repetitions</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">StopIteration</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">repetitions</span> <span class="o">+=</span> <span class="mi">1</span>    
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgbcea38d">
<h4 id="orgbcea38d">The Trainer</h4>
<div class="outline-text-4" id="text-orgbcea38d">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TheTrainer</span><span class="p">:</span>
    <span class="sd">"""Something to train the model</span>

<span class="sd">    Args:</span>
<span class="sd">     model: thing to train</span>
<span class="sd">     batches: batch generator</span>
<span class="sd">     learning_impairment: rate to slow the model's learning</span>
<span class="sd">     impairment_point: how frequently to impair the learner</span>
<span class="sd">     emit_point: how frequently to emit messages</span>
<span class="sd">     verbose: whether to emit messages</span>
<span class="sd">    """</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">CBOW</span>
    <span class="n">batches</span><span class="p">:</span> <span class="n">Batches</span>
    <span class="n">learning_impairment</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.66</span>
    <span class="n">impairment_point</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span>
    <span class="n">emit_point</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>
    <span class="n">_losses</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgd73d30e"></a>Losses<br>
<div class="outline-text-5" id="text-orgd73d30e">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">losses</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""Holder for the training losses"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span>
</pre></div>
</div>
</li>
<li><a id="orgd9dd3c0"></a>Gradient Descent<br>
<div class="outline-text-5" id="text-orgd9dd3c0">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>    
    <span class="sd">"""Trains the model using gradient descent</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">x_y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x_y</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_weights</span> <span class="o">=</span> <span class="n">Weights</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                            <span class="n">hidden_input</span><span class="o">=</span><span class="n">hidden_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">repetitions</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">impairment_point</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_impairment</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"new learning rate: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learning_rate</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="p">((</span><span class="n">repetitions</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">emit_point</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">repetitions</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">: loss=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="n">repetitions</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> 
</pre></div>
</div>
</li>
<li><a id="org501a5c0"></a>Cross-Entropy-Loss<br>
<div class="outline-text-5" id="text-org501a5c0">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                       <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the cross-entropy loss</span>

<span class="sd">    Args:</span>
<span class="sd">     predicted: array with the model's guesses</span>
<span class="sd">     actual: array with the actual labels</span>

<span class="sd">    Returns:</span>
<span class="sd">     the cross-entropy loss</span>
<span class="sd">    """</span>
    <span class="n">log_probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predicted</span><span class="p">),</span> <span class="n">actual</span><span class="p">)</span>
                         <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">actual</span><span class="p">))</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org9ef1da1">
<h3 id="org9ef1da1">Testing It</h3>
<div class="outline-text-3" id="text-org9ef1da1">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">Batches</span><span class="p">,</span> <span class="n">CBOW</span><span class="p">,</span> <span class="n">TheTrainer</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">V</span><span class="p">)</span>


<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="n">tmp</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
                   <span class="p">])</span>
<span class="n">tmp_sm</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.73105858</span><span class="p">,</span> <span class="mf">0.88079708</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.26894142</span><span class="p">,</span> <span class="mf">0.11920292</span><span class="p">]])</span>


<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org0a5e623">
<h4 id="org0a5e623">Forward Propagation</h4>
<div class="outline-text-4" id="text-org0a5e623">
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">tmp_x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">V</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.55379268</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.58960774</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.50722933</span><span class="p">]]</span>
<span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.92477674</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.02487333</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgcc245d1">
<h4 id="orgcc245d1">Cross Entropy Loss</h4>
<div class="outline-text-4" id="text-orgcc245d1">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tmp_cost</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">tmp_yhat</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">tmp_cost</span><span class="p">,</span> <span class="mf">9.9560</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc38c9a2">
<h4 id="orgc38c9a2">Back Propagation</h4>
<div class="outline-text-4" id="text-orgc38c9a2">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># get a batch of data</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>
<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>
<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">gradients</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">hidden_input</span><span class="o">=</span><span class="n">tmp_h</span><span class="p">)</span>
<span class="n">tmp_grad_W1</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">,</span> <span class="n">tmp_grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_h</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">input_weights</span><span class="p">,</span> <span class="n">tmp_grad_W1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">hidden_weights</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">input_bias</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">hidden_bias</span><span class="p">,</span> <span class="n">tmp_grad_b2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgcc2297f">
<h4 id="orgcc2297f">Putting Some Stuff Together</h4>
<div class="outline-text-4" id="text-orgcc2297f">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">hidden_layers</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="c1"># using their initial weights</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
</pre></div>
<pre class="example">
11.871189103548419
11.871189103548419
9.956016099656951
9.956016099656951
</pre>
<p>I changed the weights to use the uniform distribution which seems to work better, but weirdly it still does a little worse initially. The random-seed seems to be different for the old numpy random and their new generator.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgd62eadc">
<h4 id="orgd62eadc">The Batches</h4>
<div class="outline-text-4" id="text-orgd62eadc">
<p>The original batch-generator had a couple of bugs in it. To avoid them pass in <code>original=True</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>


<span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span>
                                <span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">original</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">old_x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">old_y</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>


<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="c1">#expect(numpy.allclose(tmp_x, old_x)).to(be_true)</span>
<span class="c1">#expect(numpy.allclose(tmp_y, old_y)).to(be_true)</span>

<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org7e12b1a">
<h4 id="org7e12b1a">Gradient Descent</h4>
<div class="outline-text-4" id="text-org7e12b1a">
<div class="highlight">
<pre><span></span><span class="n">hidden_layers</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train</span><span class="p">()</span>
</pre></div>
<pre class="example" id="org58d9ebd">
10: loss=12.949165499168524
20: loss=7.1739091478289225
30: loss=13.431976455238479
40: loss=4.0062314323745545
50: loss=11.595407087927406
60: loss=10.41983077447342
70: loss=7.843047289924249
80: loss=12.529314536141994
90: loss=14.122707806423126
new learning rate: 0.0198
100: loss=10.80530164111974
110: loss=4.624869443165228
120: loss=5.552813055551899
130: loss=8.483428176366933
140: loss=9.047299388851195
150: loss=4.841072955589429
</pre></div>
</div>
<div class="outline-4" id="outline-container-org495d876">
<h4 id="org495d876">Gradient Re-do</h4>
<div class="outline-text-4" id="text-org495d876">
<p>Something's wrong with the trainer's gradient descent so I'm going to try and update the original function to do it.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">CBOW</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batches</span><span class="p">:</span> <span class="n">Batches</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.03</span><span class="p">):</span>
    <span class="sd">"""This is the gradient_descent function</span>

<span class="sd">    Args: </span>
<span class="sd">       data:      text</span>
<span class="sd">       word2Ind:  words to Indices</span>
<span class="sd">       N:         dimension of hidden vector  </span>
<span class="sd">       V:         dimension of vocabulary </span>
<span class="sd">       num_iters: number of iterations  </span>

<span class="sd">    Returns: </span>
<span class="sd">       W1, W2, b1, b2:  updated matrices and biases   </span>
<span class="sd">    """</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Get yhat</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># Get cost</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">iters</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"iters: </span><span class="si">{</span><span class="n">iters</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2"> cost: </span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                                                             <span class="n">yhat</span><span class="p">,</span>
                                                             <span class="n">y</span><span class="p">,</span>
                                                             <span class="n">h</span><span class="p">)</span>

        <span class="c1"># Update weights and biases</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">-=</span>  <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">-=</span>  <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>

        <span class="c1">### END CODE HERE ###</span>

        <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span> 
        <span class="k">if</span> <span class="n">iters</span> <span class="o">==</span> <span class="n">num_iters</span><span class="p">:</span> 
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">iters</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">*=</span> <span class="mf">0.66</span>

    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="c1"># batch_generator(data, word2Ind, C, batch_size)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org5239e5e">
iters: 10 cost: 12.949165
iters: 20 cost: 7.173909
iters: 30 cost: 13.431976
iters: 40 cost: 4.006231
iters: 50 cost: 11.595407
iters: 60 cost: 10.419831
iters: 70 cost: 7.843047
iters: 80 cost: 12.529315
iters: 90 cost: 14.122708
iters: 100 cost: 10.805302
iters: 110 cost: 4.624869
iters: 120 cost: 5.552813
iters: 130 cost: 8.483428
iters: 140 cost: 9.047299
iters: 150 cost: 4.841073
</pre>
<p>So, something's wrong with the gradient descent.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="c1">#batches = Batches(data=cleaner.processed, word_to_index=meta.word_to_index,</span>
<span class="c1">#                  half_window=half_window, batch_size=batch_size, batches=repetitions)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org80596cd">
iters: 10 cost: 0.407862
iters: 20 cost: 0.090807
iters: 30 cost: 0.050924
iters: 40 cost: 0.035379
iters: 50 cost: 0.027105
iters: 60 cost: 0.021969
iters: 70 cost: 0.018470
iters: 80 cost: 0.015932
iters: 90 cost: 0.014008
iters: 100 cost: 0.012499
iters: 110 cost: 0.011631
iters: 120 cost: 0.010911
iters: 130 cost: 0.010274
iters: 140 cost: 0.009708
iters: 150 cost: 0.009201
</pre>
<p>It looks like it's the batches.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgdc4e0de">
<h4 id="orgdc4e0de">Troubleshooting the Batches</h4>
<div class="outline-text-4" id="text-orgdc4e0de">
<div class="highlight">
<pre><span></span><span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span><span class="p">]</span> <span class="o">+</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">[</span><span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">packed_1</span> <span class="o">=</span> <span class="n">index_with_frequency</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span>
<span class="n">packed_2</span> <span class="o">=</span> <span class="n">batches</span><span class="o">.</span><span class="n">indices_and_frequencies</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">packed_1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">contain_exactly</span><span class="p">(</span><span class="o">*</span><span class="n">packed_2</span><span class="p">))</span>
</pre></div>
<p>So the indices and frequencies is okay.</p>
<div class="highlight">
<pre><span></span><span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">vectors</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">half_window</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">repetition</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">batches</span><span class="o">.</span><span class="n">vectors</span><span class="p">):</span>
    <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<p>And the vectors look okay.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">repetition</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># batch = next(batches)</span>
<span class="k">for</span> <span class="n">old</span> <span class="ow">in</span> <span class="n">old_generator</span><span class="p">:</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batches</span><span class="o">.</span><span class="n">vectors</span><span class="p">:</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batches</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">newx</span><span class="p">,</span> <span class="n">newy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">newx</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
            <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
                <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="k">break</span>
</pre></div>
<p>So, weirdly, rolling the <code>__next__=</code> by hand seems to work.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">repetition</span><span class="p">,</span> <span class="n">repetitions</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">150</span>
<span class="k">for</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">old_generator</span><span class="p">,</span> <span class="n">batches</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">repetition</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<pre class="example">
1
</pre>
<p>But not the batches.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">repetition</span><span class="p">,</span> <span class="n">repetitions</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">150</span>
<span class="k">for</span> <span class="n">old</span> <span class="ow">in</span> <span class="n">old_generator</span><span class="p">:</span>
    <span class="n">new</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">repetition</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<p>Actually, it looks like the old generator might be broken.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="c1">#batches = Batches(data=cleaner.processed, word_to_index=meta.word_to_index,</span>
<span class="c1">#                  half_window=half_window, batch_size=batch_size, batches=repetitions)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org89ae4ee">
iters: 10 cost: 12.949165
iters: 20 cost: 7.173909
iters: 30 cost: 13.431976
iters: 40 cost: 4.006231
iters: 50 cost: 11.595407
iters: 60 cost: 10.419831
iters: 70 cost: 7.843047
iters: 80 cost: 12.529315
iters: 90 cost: 14.122708
iters: 100 cost: 10.805302
iters: 110 cost: 4.624869
iters: 120 cost: 5.552813
iters: 130 cost: 8.483428
iters: 140 cost: 9.047299
iters: 150 cost: 4.841073
</pre>
<p>The old generator wasn't creating new lists every time so it was just fitting the same batch of data every time… in fact it had a while loop instead of a conditional so it was just creating one batch with the same x and y lists repeated over and over so it should really be the worse performance, not the really good performance the original generator gave. I didn't re-run the ones above but this next set is being run after fixing my implementation.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="p">,</span> <span class="n">emit_point</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">trainer</span><span class="p">()</span>
</pre></div>
<pre class="example">
2020-12-16 14:15:54,530 graeae.timers.timer start: Started: 2020-12-16 14:15:54.530779
2020-12-16 14:16:18,600 graeae.timers.timer end: Ended: 2020-12-16 14:16:18.600880
2020-12-16 14:16:18,602 graeae.timers.timer end: Elapsed: 0:00:24.070101
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
11.99601105791401 8.827228045367379
</pre>
<p>Not a huge improvement, but it didn't run for a long time either.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="p">,</span> <span class="n">emit_point</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">trainer</span><span class="p">()</span>
</pre></div>
<pre class="example" id="org42eedf8">
2020-12-16 14:40:13,275 graeae.timers.timer start: Started: 2020-12-16 14:40:13.275964
new learning rate: 0.0198
100: loss=9.138356897918037
new learning rate: 0.013068000000000001
200: loss=9.077599951734605
new learning rate: 0.008624880000000001
300: loss=8.827228045367379
new learning rate: 0.005692420800000001
400: loss=8.556788482755191
new learning rate: 0.003756997728000001
500: loss=8.92744766914796
new learning rate: 0.002479618500480001
600: loss=9.052677036205138
new learning rate: 0.0016365482103168007
700: loss=8.914532962726918
new learning rate: 0.0010801218188090885
800: loss=8.885698480310062
new learning rate: 0.0007128804004139984
900: loss=9.042620463323736
2020-12-16 14:41:33,457 graeae.timers.timer end: Ended: 2020-12-16 14:41:33.457065
2020-12-16 14:41:33,458 graeae.timers.timer end: Elapsed: 0:01:20.181101
new learning rate: 0.000470501064273239
1000: loss=9.239992952104755
</pre>
<p>Hmm… doesn't seem to be improving.</p>
<div class="highlight">
<pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">VLine</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">idxmin</span><span class="p">())</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">time_series</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">hvplot</span><span class="p">()</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Loss per Repetition"</span><span class="p">,</span>
                                   <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
                                   <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">tan</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">time_series</span> <span class="o">*</span> <span class="n">line</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"training_1000"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/word-embeddings-training-the-model/training_1000.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Since the losses are in a Series we can use its <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.idxmin.html">idxmin</a> method to see when the losses bottomed out.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">idxmin</span><span class="p">())</span>
</pre></div>
<pre class="example">
247
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">247</span><span class="p">],</span> <span class="n">losses</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
8.186490214727549 9.239992952104755
</pre>
<p>So it did the best at 247 and then got a little worse as we went along.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
<pre class="example">
45.140625
</pre>
<p>We exhausted our data after 45 batches so I guess it's overfitting after a while.</p>
</div>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-shakespeare-data/">Word Embeddings: Shakespeare Data</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-shakespeare-data/" rel="bookmark"><time class="published dt-published" datetime="2020-12-13T12:44:32-08:00" itemprop="datePublished" title="2020-12-13 12:44">2020-12-13 12:44</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org14a77c5">Beginning</a>
<ul>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org45eb0c2">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org8eedda3">Middle</a>
<ul>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org2681a87">A Little Cleaning</a>
<ul>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org673bd26">Imports</a></li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#orgbf12cdd">The Cleaner</a></li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org1dca3a5">The Counter</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#org56ed936">The Cleaned</a></li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#orgea389fc">The Data Data</a></li>
</ul>
</li>
<li><a href="posts/nlp/word-embeddings-shakespeare-data/#orgcd996e6">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org14a77c5">
<h2 id="org14a77c5">Beginning</h2>
<div class="outline-text-2" id="text-org14a77c5">
<p>This is the first part of as series on building word embeddings using a Continuous Bag of Words. There's an <a href="posts/nlp/word-embeddings-build-a-model/">overview post</a> that has links to all the posts in the series.</p>
</div>
<div class="outline-3" id="outline-container-org45eb0c2">
<h3 id="org45eb0c2">Imports</h3>
<div class="outline-text-3" id="text-org45eb0c2">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org8eedda3">
<h2 id="org8eedda3">Middle</h2>
<div class="outline-text-2" id="text-org8eedda3">
<p>We're going to be using the same dataset that we used in building the <a href="posts/nlp/autocorrect-the-system/">autocorrect system</a>.</p>
</div>
<div class="outline-3" id="outline-container-org2681a87">
<h3 id="org2681a87">A Little Cleaning</h3>
<div class="outline-text-3" id="text-org2681a87"></div>
<div class="outline-4" id="outline-container-org673bd26">
<h4 id="org673bd26">Imports</h4>
<div class="outline-text-4" id="text-org673bd26">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>

<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">nltk</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgbf12cdd">
<h4 id="orgbf12cdd">The Cleaner</h4>
<div class="outline-text-4" id="text-orgbf12cdd">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">DataCleaner</span><span class="p">:</span>
    <span class="sd">"""A cleaner for the word-embeddings data</span>

<span class="sd">    Args:</span>
<span class="sd">     key: environment key with path to the data file</span>
<span class="sd">     env_path: path to the .env file</span>
<span class="sd">    """</span>
    <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"SHAKESPEARE"</span>
    <span class="n">env_path</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"posts/nlp/.env"</span>
    <span class="n">stop</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"."</span>
    <span class="n">_data_path</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_data</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_unpunctuated</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_punctuation</span><span class="p">:</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_tokens</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_processed</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orga0e69e2"></a>The Path To the Data<br>
<div class="outline-text-5" id="text-orga0e69e2">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">data_path</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span><span class="p">:</span>
    <span class="sd">"""The path to the data file"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">load_dotenv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">])</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data_path</span>
</pre></div>
</div>
</li>
<li><a id="org9daafdb"></a>The Data<br>
<div class="outline-text-5" id="text-org9daafdb">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""The data-file read in as a string"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_path</span><span class="o">.</span><span class="n">open</span><span class="p">()</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span>
</pre></div>
</div>
</li>
<li><a id="org3d5a9b1"></a>The Punctuation Expression<br>
<div class="outline-text-5" id="text-org3d5a9b1">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">punctuation</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">re</span><span class="o">.</span><span class="n">Pattern</span><span class="p">:</span>
    <span class="sd">"""The regular expression to find punctuation"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_punctuation</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_punctuation</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s2">"[,!?;-]"</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_punctuation</span>
</pre></div>
</div>
</li>
<li><a id="orgcc72f13"></a>The Un-Punctuated<br>
<div class="outline-text-5" id="text-orgcc72f13">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">unpunctuated</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""The data with punctuation replaced by stop"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unpunctuated</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unpunctuated</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">punctuation</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unpunctuated</span>
</pre></div>
</div>
</li>
<li><a id="org06c1b71"></a>The Tokens<br>
<div class="outline-text-5" id="text-org06c1b71">
<p>We're going to use NLTK's <a href="https://www.nltk.org/api/nltk.tokenize.html?highlight=word_tokenize#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize">word_tokenize</a> function to tokenize the string.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The tokenized data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unpunctuated</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokens</span>
</pre></div>
</div>
</li>
<li><a id="orgd36f132"></a>The Processed Tokens<br>
<div class="outline-text-5" id="text-orgd36f132">
<p>The final processed data will be all lowercased words and periods only.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">processed</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""The final processed tokens"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_processed</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokens</span>
                           <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()</span> <span class="ow">or</span> <span class="n">token</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_processed</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org1dca3a5">
<h4 id="org1dca3a5">The Counter</h4>
<div class="outline-text-4" id="text-org1dca3a5">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">MetaData</span><span class="p">:</span>
    <span class="sd">"""Compile some basic data about the data</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the cleaned and tokenized data</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">_distribution</span><span class="p">:</span> <span class="n">nltk</span><span class="o">.</span><span class="n">probability</span><span class="o">.</span><span class="n">FreqDist</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vocabulary</span><span class="p">:</span> <span class="nb">tuple</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org071fc12"></a>The Frequency Distribution<br>
<div class="outline-text-5" id="text-org071fc12">
<p>According to the doc-string, the <a href="https://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist">FreqDist</a> is meant to hold outcomes from experiments. It looks like a <a href="https://docs.python.org/3/library/collections.html#collections.Counter">Counter</a> with extra methods added.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">distribution</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nltk</span><span class="o">.</span><span class="n">probability</span><span class="o">.</span><span class="n">FreqDist</span><span class="p">:</span>
    <span class="sd">"""The Token Frequency Distribution"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distribution</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_distribution</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">FreqDist</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_distribution</span>
</pre></div>
</div>
</li>
<li><a id="orge988067"></a>The Vocabulary<br>
<div class="outline-text-5" id="text-orge988067">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""The sorted unique tokens in the data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span>
</pre></div>
</div>
</li>
<li><a id="orgdf86a00"></a>The Word-To-Index Mapping<br>
<div class="outline-text-5" id="text-orgdf86a00">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">word_to_index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="sd">"""Maps words to their index in the vocabulary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span>
                               <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)}</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_word_to_index</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org56ed936">
<h3 id="org56ed936">The Cleaned</h3>
<div class="outline-text-3" id="text-org56ed936">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">DataCleaner</span>
<span class="n">cleaner</span> <span class="o">=</span> <span class="n">DataCleaner</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">unpunctuated</span><span class="p">[:</span><span class="mi">50</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">tokens</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Tokens: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
O for a Muse of fire. that would ascend
The bright
['O', 'for', 'a', 'Muse', 'of', 'fire', '.', 'that', 'would', 'ascend']
['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend']
Tokens: 60,996
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgea389fc">
<h3 id="orgea389fc">The Data Data</h3>
<div class="outline-text-3" id="text-orgea389fc">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">MetaData</span>
<span class="n">counter</span> <span class="o">=</span> <span class="n">MetaData</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">distribution</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">counter</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" - </span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">distribution</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Size of the Vocabulary: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">index</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">word</span> <span class="o">=</span> <span class="n">counter</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
<span class="n">expect</span><span class="p">(</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">counter</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]))</span>
</pre></div>
<pre class="example" id="orgf6757d3">
Size of vocabulary: 5,778
 - ('.', 9630)
 - ('the', 1521)
 - ('and', 1394)
 - ('i', 1257)
 - ('to', 1159)
 - ('of', 1093)
 - ('my', 857)
 - ('that', 781)
 - ('in', 770)
 - ('a', 752)
 - ('you', 748)
 - ('is', 630)
 - ('not', 559)
 - ('for', 467)
 - ('it', 460)
 - ('with', 441)
 - ('his', 434)
 - ('but', 417)
 - ('me', 417)
 - ('your', 397)
Size of the Vocabulary: 5,778
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orgcd996e6">
<h2 id="orgcd996e6">End</h2>
<div class="outline-text-2" id="text-orgcd996e6">
<p>Now that we have the data setup its time to <a href="posts/nlp/word-embeddings-training-the-model/">build and train the model</a>.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/word-embeddings-build-a-model/">Word Embeddings: Build a Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/word-embeddings-build-a-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-12T17:07:05-08:00" itemprop="datePublished" title="2020-12-12 17:07">2020-12-12 17:07</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/word-embeddings-build-a-model/#org6d86b05">Introduction</a></li>
<li><a href="posts/nlp/word-embeddings-build-a-model/#org7c5fb03">The Continuous Bag Of Words Model (CBOW)</a></li>
<li><a href="posts/nlp/word-embeddings-build-a-model/#orgcc06519">The Parts</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org6d86b05">
<h2 id="org6d86b05">Introduction</h2>
<div class="outline-text-2" id="text-org6d86b05">
<p>This is and introduction to a series of posts that look at how to create word embeddings using a Continuous Bag Of Words (CBOW) model.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org7c5fb03">
<h2 id="org7c5fb03">The Continuous Bag Of Words Model (CBOW)</h2>
<div class="outline-text-2" id="text-org7c5fb03">
<p>Let's take a look at the following sentence: <b>'I am happy because I am learning'</b>.</p>
<ul class="org-ul">
<li>In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).</li>
<li>For example, if you were to choose a context half-size of say <i>C = 2</i>, then you would try to predict the word <b>happy</b> given the context that includes 2 words before and 2 words after the center word:
<ul class="org-ul">
<li><i>C</i> words before: <code>[I, am]</code></li>
<li><i>C</i> words after: <code>[because, I]</code></li>
</ul>
</li>
<li>In other words:</li>
</ul>
<pre class="example" id="orgd076778">
context = [I,am, because, I]
target = happy
</pre>
<p>The model will be a three-layer one. The input layer (\(\bar x\)) is the average of all the one hot vectors of the context words. There will be one hidden layer, and the output layer (\(hat y\)) will be the softmax layer.</p>
<p>The architecture you will be implementing is as follows:</p>
\begin{align} h &amp;= W_1 \ X + b_1 \tag{1} \\ a &amp;= ReLU(h) \tag{2} \\ z &amp;= W_2 \ a + b_2 \tag{3} \\ \hat y &amp;= softmax(z) \tag{4} \\ \end{align}</div>
</div>
<div class="outline-2" id="outline-container-orgcc06519">
<h2 id="orgcc06519">The Parts</h2>
<div class="outline-text-2" id="text-orgcc06519">
<p>This is just and introductory post, the following are the posts in the series where things will actually be implemented.</p>
<ul class="org-ul">
<li><a href="posts/nlp/word-embeddings-shakespeare-data/">Loading the Data</a></li>
<li><a href="posts/nlp/word-embeddings-training-the-model/">Building and Training the CBOW Model</a></li>
<li><a href="posts/nlp/word-embeddings-visualizing-the-embeddings/">Visualizing the Embeddings</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/extracting-word-embeddings/">Extracting Word Embeddings</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/extracting-word-embeddings/" rel="bookmark"><time class="published dt-published" datetime="2020-12-11T16:42:38-08:00" itemprop="datePublished" title="2020-12-11 16:42">2020-12-11 16:42</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/extracting-word-embeddings/#orga898500">Introduction and Preliminaries</a>
<ul>
<li><a href="posts/nlp/extracting-word-embeddings/#org8709d7c">Imports</a></li>
<li><a href="posts/nlp/extracting-word-embeddings/#org64fa93c">Preliminary Setup</a></li>
</ul>
</li>
<li><a href="posts/nlp/extracting-word-embeddings/#orgcdca8ea">Extracting word embedding vectors</a>
<ul>
<li><a href="posts/nlp/extracting-word-embeddings/#orga5cc5c3">Option 1: Extract embedding vectors from \(\mathbf{W_1}\)</a></li>
<li><a href="posts/nlp/extracting-word-embeddings/#org12c9993">Option 2: Extract embedding vectors from \(\mathbf{W_2}\)</a></li>
<li><a href="posts/nlp/extracting-word-embeddings/#org88e27a2">Option 3: extract embedding vectors from \(\mathbf{W_1}\) and \(\mathbf{W_2}\)</a></li>
</ul>
</li>
<li><a href="posts/nlp/extracting-word-embeddings/#orga237a0a">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orga898500">
<h2 id="orga898500">Introduction and Preliminaries</h2>
<div class="outline-text-2" id="text-orga898500">
<p>In the <a href="posts/nlp/training-the-cbow-model/">previous post</a> we trained the CBOW model, now in this post we'll look at how to extract word embedding vectors from a model.</p>
</div>
<div class="outline-3" id="outline-container-org8709d7c">
<h3 id="org8709d7c">Imports</h3>
<div class="outline-text-3" id="text-org8709d7c">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">expect</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org64fa93c">
<h3 id="org64fa93c">Preliminary Setup</h3>
<div class="outline-text-3" id="text-org64fa93c">
<p>Before moving on, you will be provided with some variables needed for further procedures, which should be familiar by now. Also a trained CBOW model will be simulated, the corresponding weights and biases are provided:</p>
<p>Define the tokenized version of the corpus.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">]</span>
</pre></div>
<p>Define V. Remember this is the size of the vocabulary.</p>
<div class="highlight">
<pre><span></span><span class="n">vocabulary</span> <span class="o">=</span>  <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
<span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
</pre></div>
<p>Get the <code>word_to_index</code> and <code>index_to_word</code> dictionaries for the tokenized corpus.</p>
<div class="highlight">
<pre><span></span><span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)}</span>
<span class="n">index_to_word</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))</span>
</pre></div>
<p>Define first matrix of weights</p>
<div class="highlight">
<pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.41687358</span><span class="p">,</span>  <span class="mf">0.08854191</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23495225</span><span class="p">,</span>  <span class="mf">0.28320538</span><span class="p">,</span>  <span class="mf">0.41800106</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.32735501</span><span class="p">,</span>  <span class="mf">0.22795148</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23951958</span><span class="p">,</span>  <span class="mf">0.4117634</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.23924344</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.26637602</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23846886</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.37770863</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11399446</span><span class="p">,</span>  <span class="mf">0.34008124</span><span class="p">]])</span>
</pre></div>
<p>Define second matrix of weights.</p>
<div class="highlight">
<pre><span></span><span class="n">W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.22182064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43008631</span><span class="p">,</span>  <span class="mf">0.13310965</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.08476603</span><span class="p">,</span>  <span class="mf">0.08123194</span><span class="p">,</span>  <span class="mf">0.1772054</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.1871551</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.06107263</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1790735</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.07055222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02015138</span><span class="p">,</span>  <span class="mf">0.36107434</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.33480474</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.39423389</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43959196</span><span class="p">]])</span>
</pre></div>
<p>Define first vector of biases.</p>
<div class="highlight">
<pre><span></span><span class="n">b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.09688219</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.29239497</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.27364426</span><span class="p">]])</span>
</pre></div>
<p>Define second vector of biases.</p>
<div class="highlight">
<pre><span></span><span class="n">b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.0352008</span> <span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.36393384</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.12775555</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.34802326</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.07017815</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgcdca8ea">
<h2 id="orgcdca8ea">Extracting word embedding vectors</h2>
<div class="outline-text-2" id="text-orgcdca8ea">
<p>Once you have finished training the neural network, you have three options to get word embedding vectors for the words of your vocabulary, based on the weight matrices \(\mathbf{W_1}\) and/or \(\mathbf{W_2}\).</p>
</div>
<div class="outline-3" id="outline-container-orga5cc5c3">
<h3 id="orga5cc5c3">Option 1: Extract embedding vectors from \(\mathbf{W_1}\)</h3>
<div class="outline-text-3" id="text-orga5cc5c3">
<p>The first option is to take the columns of \(\mathbf{W_1}\) as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.</p>
<p><b>Note:</b> in this practice notebooks the values of the word embedding vectors are meaningless since we only trained for a single iteration with just one training example, but here's how you would proceed after the training process is complete.</p>
<p>For example \(\mathbf{W_1}\) is this matrix:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]
 [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]
 [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]
</pre>
<p>The first column, which is a 3-element vector, is the embedding vector of the first word of your vocabulary. The second column is the word embedding vector for the second word, and so on.</p>
<p>These are the words corresponding to the columns.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">" - </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
- am
- because
- happy
- i
- learning
</pre>
<p>And the word embedding vectors corresponding to each word are:</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">word_embedding_vector</span> <span class="o">=</span> <span class="n">W1</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">:    </span><span class="se">\t</span><span class="si">{</span><span class="n">word_embedding_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
am:     [0.41687358 0.32735501 0.26637602]
because:        [ 0.08854191  0.22795148 -0.23846886]
happy:          [-0.23495225 -0.23951958 -0.37770863]
i:      [ 0.28320538  0.4117634  -0.11399446]
learning:       [ 0.41800106 -0.23924344  0.34008124]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org12c9993">
<h3 id="org12c9993">Option 2: Extract embedding vectors from \(\mathbf{W_2}\)</h3>
<div class="outline-text-3" id="text-org12c9993">
<p>The second option is to transpose \(\mathbf{W_2}\) and take the columns of this transposed matrix as the word embedding vectors just like you did for \(\mathbf{W_1}\).</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[-0.22182064  0.08476603  0.1871551   0.07055222  0.33480474]
 [-0.43008631  0.08123194 -0.06107263 -0.02015138 -0.39423389]
 [ 0.13310965  0.1772054  -0.1790735   0.36107434 -0.43959196]]
</pre>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">word_embedding_vector</span> <span class="o">=</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">:    </span><span class="se">\t</span><span class="si">{</span><span class="n">word_embedding_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
am:     [-0.22182064 -0.43008631  0.13310965]
because:        [0.08476603 0.08123194 0.1772054 ]
happy:          [ 0.1871551  -0.06107263 -0.1790735 ]
i:      [ 0.07055222 -0.02015138  0.36107434]
learning:       [ 0.33480474 -0.39423389 -0.43959196]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org88e27a2">
<h3 id="org88e27a2">Option 3: extract embedding vectors from \(\mathbf{W_1}\) and \(\mathbf{W_2}\)</h3>
<div class="outline-text-3" id="text-org88e27a2">
<p>The third option, which is the one you will use in this week's assignment, uses the average of \(\mathbf{W_1}\) and \(\mathbf{W_2^\intercal}\).</p>
<p><b>Calculate the average of \(\mathbf{W_1}\) and \(\mathbf{W_2^\intercal}\), and store the result in <code>W3</code>.</b></p>
<div class="highlight">
<pre><span></span><span class="n">W3</span> <span class="o">=</span> <span class="p">(</span><span class="n">W1</span> <span class="o">+</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W3</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.09752647</span><span class="p">,</span>  <span class="mf">0.08665397</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02389858</span><span class="p">,</span>  <span class="mf">0.1768788</span> <span class="p">,</span>  <span class="mf">0.3764029</span> <span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.05136565</span><span class="p">,</span>  <span class="mf">0.15459171</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.15029611</span><span class="p">,</span>  <span class="mf">0.19580601</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.31673866</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.19974284</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.03063173</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.27839106</span><span class="p">,</span>  <span class="mf">0.12353994</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04975536</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.09752647  0.08665397 -0.02389858  0.1768788   0.3764029 ]
 [-0.05136565  0.15459171 -0.15029611  0.19580601 -0.31673866]
 [ 0.19974284 -0.03063173 -0.27839106  0.12353994 -0.04975536]]
</pre>
<p>Extracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix you've just created.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">word_embedding_vector</span> <span class="o">=</span> <span class="n">W3</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s1">:    </span><span class="se">\t</span><span class="si">{</span><span class="n">word_embedding_vector</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
am:     [ 0.09752647 -0.05136565  0.19974284]
because:        [ 0.08665397  0.15459171 -0.03063173]
happy:          [-0.02389858 -0.15029611 -0.27839106]
i:      [0.1768788  0.19580601 0.12353994]
learning:       [ 0.3764029  -0.31673866 -0.04975536]
</pre>
<p>Now you know 3 different options to get the word embedding vectors from a model.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orga237a0a">
<h2 id="orga237a0a">End</h2>
<div class="outline-text-2" id="text-orga237a0a">
<p>Now we've gone through the process of training a CBOW model in order to create word embeddings. The steps were:</p>
<ul class="org-ul">
<li><a href="posts/nlp/word-embeddings-data-preparation/">preparing the data</a></li>
<li><a href="posts/nlp/introducing-the-cbow-model/">creating the CBOW model</a></li>
<li><a href="posts/nlp/training-the-cbow-model/">training the model</a></li>
<li>Extracting the word embedding vectors from the model.</li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/training-the-cbow-model/">Training the CBOW Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/training-the-cbow-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-09T18:34:27-08:00" itemprop="datePublished" title="2020-12-09 18:34">2020-12-09 18:34</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org85abd66">Beginning</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org966750e">Imports</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org4479336">Functions from Previous Posts</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org2016ad7">Data Preparation Functions</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org645d019">Activation Functions</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/nlp/training-the-cbow-model/#orgc30c903">Word Embeddings: Training the CBOW model</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org81c6749">Neural Network Initialization</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#orga71ae4c">Initialization of the weights and biases</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org2a757c9">Define the first matrix of weights</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#orgcd5dc6f">Define the second matrix of weights</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org3f78d05">Define the first vector of biases</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org7d65447">Define the second vector of biases</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org8f2c605">Define the tokenized version of the corpus</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org49c3062">Get 'word_to_index' and 'Ind2word' dictionaries for the tokenized corpus</a></li>
</ul>
</li>
<li><a href="posts/nlp/training-the-cbow-model/#org58275d2">The First Training Example</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org5ef2648">Forward Propagation</a>
<ul>
<li><a href="posts/nlp/training-the-cbow-model/#org1413bba">The Hidden Layer</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org20dbb49">The Output Layer</a></li>
</ul>
</li>
<li><a href="posts/nlp/training-the-cbow-model/#org31e7b7f">Cross-Entropy Loss</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org56eae7d">Backpropagation</a></li>
<li><a href="posts/nlp/training-the-cbow-model/#org561b4cd">Gradient descent</a></li>
</ul>
</li>
<li><a href="posts/nlp/training-the-cbow-model/#org4ce7a7a">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org85abd66">
<h2 id="org85abd66">Beginning</h2>
<div class="outline-text-2" id="text-org85abd66">
<p>Previously we looked at <a href="posts/nlp/word-embeddings-data-preparation/">preparing the data</a> and how to set up the <a href="posts/nlp/introducing-the-cbow-model/">CBOW Model</a>, now we'll look at training the model.</p>
</div>
<div class="outline-3" id="outline-container-org966750e">
<h3 id="org966750e">Imports</h3>
<div class="outline-text-3" id="text-org966750e">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">be_true</span><span class="p">,</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org4479336">
<h3 id="org4479336">Functions from Previous Posts</h3>
<div class="outline-text-3" id="text-org4479336"></div>
<div class="outline-4" id="outline-container-org2016ad7">
<h4 id="org2016ad7">Data Preparation Functions</h4>
<div class="outline-text-4" id="text-org2016ad7">
<p>These were previously defined in <a href="posts/nlp/word-embeddings-data-preparation/">Word Embeddings: Data Preparation</a> post.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">window_generator</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">"""Generates windows of words</span>

<span class="sd">    Args:</span>
<span class="sd">     words: cleaned tokens</span>
<span class="sd">     half_window: number of words in the half-window</span>

<span class="sd">    Yields:</span>
<span class="sd">     the next window</span>
<span class="sd">    """</span>
    <span class="k">for</span> <span class="n">center_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">half_window</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">center_index</span><span class="p">]</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">)</span> <span class="p">:</span> <span class="n">center_index</span><span class="p">]</span>
                         <span class="o">+</span> <span class="n">words</span><span class="p">[(</span><span class="n">center_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):(</span><span class="n">center_index</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>
        <span class="k">yield</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">index_word_maps</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Creates index to word mappings</span>

<span class="sd">    The index is based on sorted unique tokens in the data</span>

<span class="sd">    Args:</span>
<span class="sd">       data: the data you want to pull from</span>

<span class="sd">    Returns:</span>
<span class="sd">       word2Ind: returns dictionary mapping the word to its index</span>
<span class="sd">       Ind2Word: returns dictionary mapping the index to its word</span>
<span class="sd">    """</span>
    <span class="n">words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

    <span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="n">index_to_word</span> <span class="o">=</span> <span class="p">{</span><span class="n">index</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_word</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create a one-hot-encoded vector</span>

<span class="sd">    Args:</span>
<span class="sd">     word: the word from the corpus that we're encoding</span>
<span class="sd">     word_to_index: map of the word to the index</span>
<span class="sd">     vocabulary_size: the size of the vocabulary</span>

<span class="sd">    Returns:</span>
<span class="sd">     vector with all zeros except where the word is</span>
<span class="sd">    """</span>
    <span class="n">one_hot_vector</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
    <span class="n">one_hot_vector</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">one_hot_vector</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">def</span> <span class="nf">context_words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                            <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create vector with the mean of the one-hot-vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to covert to one-hot vectors</span>
<span class="sd">     word_to_index: dict mapping word to index</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="n">context_words_vectors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">word_to_one_hot_vector</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">context_words_vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROWS</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">training_example_generator</span><span class="p">(</span><span class="n">words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="sd">"""generates training examples</span>

<span class="sd">    Args:</span>
<span class="sd">     words: source of words</span>
<span class="sd">     half_window: half the window size</span>
<span class="sd">     word_to_index: dict with word to index mapping</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">center_word</span> <span class="ow">in</span> <span class="n">window_generator</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">half_window</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">(</span><span class="n">context_words_to_vector</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">),</span>
               <span class="n">word_to_one_hot_vector</span><span class="p">(</span>
                   <span class="n">center_word</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">))</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org645d019">
<h4 id="org645d019">Activation Functions</h4>
<div class="outline-text-4" id="text-org645d019">
<p>These functions were defined in the <a href="posts/nlp/introducing-the-cbow-model/">Introducing the CBOW Model</a> post.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Get the ReLU for the input array</span>

<span class="sd">    Args:</span>
<span class="sd">     z: an array of numbers</span>

<span class="sd">    Returns:</span>
<span class="sd">     ReLU of z</span>
<span class="sd">    """</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">result</span><span class="p">[</span><span class="n">result</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate Softmax for the input</span>

<span class="sd">    Args:</span>
<span class="sd">     v: array of values</span>

<span class="sd">    Returns:</span>
<span class="sd">     array of probabilities</span>
<span class="sd">    """</span>
    <span class="n">e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">sum_e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">e_z</span> <span class="o">/</span> <span class="n">sum_e_z</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc30c903">
<h2 id="orgc30c903">Word Embeddings: Training the CBOW model</h2>
<div class="outline-text-2" id="text-orgc30c903">
<p>In previous lecture notebooks you saw how to prepare data before feeding it to a continuous bag-of-words model, the model itself, its architecture and activation functions. This notebook will walk you through:</p>
<ul class="org-ul">
<li>Forward propagation.</li>
<li>Cross-entropy loss.</li>
<li>Backpropagation.</li>
<li>Gradient descent.</li>
</ul>
<p>Which are concepts necessary to understand how the training of the model works.</p>
</div>
<div class="outline-3" id="outline-container-org81c6749">
<h3 id="org81c6749">Neural Network Initialization</h3>
<div class="outline-text-3" id="text-org81c6749">
<p>Let's dive into the neural network itself, which is shown below with all the dimensions and formulas you'll need.</p>
<p>Set <i>N</i> equal to 3. Remember that <i>N</i> is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer.</p>
<p>Also set <i>V</i> equal to 5, which is the size of the vocabulary we have used so far.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Define the size of the word embedding vectors and save it in the variable 'N'</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Define V. Remember this was the size of the vocabulary in the previous lecture notebooks</span>
<span class="n">V</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orga71ae4c">
<h3 id="orga71ae4c">Initialization of the weights and biases</h3>
<div class="outline-text-3" id="text-orga71ae4c">
<p>Before you start training the neural network, you need to initialize the weight matrices and bias vectors with random values.</p>
<p>In the assignment you will implement a function to do this yourself using <code>numpy.random.rand</code>. In this notebook, we've pre-populated these matrices and vectors for you.</p>
</div>
<div class="outline-4" id="outline-container-org2a757c9">
<h4 id="org2a757c9">Define the first matrix of weights</h4>
<div class="outline-text-4" id="text-org2a757c9">
<div class="highlight">
<pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.41687358</span><span class="p">,</span>  <span class="mf">0.08854191</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23495225</span><span class="p">,</span>  <span class="mf">0.28320538</span><span class="p">,</span>  <span class="mf">0.41800106</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.32735501</span><span class="p">,</span>  <span class="mf">0.22795148</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23951958</span><span class="p">,</span>  <span class="mf">0.4117634</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.23924344</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.26637602</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.23846886</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.37770863</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.11399446</span><span class="p">,</span>  <span class="mf">0.34008124</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgcd5dc6f">
<h4 id="orgcd5dc6f">Define the second matrix of weights</h4>
<div class="outline-text-4" id="text-orgcd5dc6f">
<div class="highlight">
<pre><span></span><span class="n">W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.22182064</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43008631</span><span class="p">,</span>  <span class="mf">0.13310965</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.08476603</span><span class="p">,</span>  <span class="mf">0.08123194</span><span class="p">,</span>  <span class="mf">0.1772054</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.1871551</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.06107263</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1790735</span> <span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.07055222</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02015138</span><span class="p">,</span>  <span class="mf">0.36107434</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.33480474</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.39423389</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43959196</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3f78d05">
<h4 id="org3f78d05">Define the first vector of biases</h4>
<div class="outline-text-4" id="text-org3f78d05">
<div class="highlight">
<pre><span></span><span class="n">b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.09688219</span><span class="p">],</span>
                  <span class="p">[</span> <span class="mf">0.29239497</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.27364426</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org7d65447">
<h4 id="org7d65447">Define the second vector of biases</h4>
<div class="outline-text-4" id="text-org7d65447">
<div class="highlight">
<pre><span></span><span class="n">b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.0352008</span> <span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.36393384</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.12775555</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.34802326</span><span class="p">],</span>
                  <span class="p">[</span><span class="o">-</span><span class="mf">0.07017815</span><span class="p">]])</span>
</pre></div>
<p><b>Check that the dimensions of these matrices are correct.</b></p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'V (vocabulary size): </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'N (embedding size / size of the hidden layer): </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of W1: </span><span class="si">{</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (NxV)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of b1: </span><span class="si">{</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Nx1)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of W2: </span><span class="si">{</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (VxN)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of b2: </span><span class="si">{</span><span class="n">b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Vx1)'</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example">
V (vocabulary size): 5
N (embedding size / size of the hidden layer): 3
size of W1: (3, 5) (NxV)
size of b1: (3, 1) (Nx1)
size of W2: (5, 3) (VxN)
size of b2: (5, 1) (Vx1)
</pre>
<p>Before moving forward, you will need some functions and variables defined in previous notebooks. They can be found next. Be sure you understand everything that is going on in the next cell, if not consider doing a refresh of the first lecture notebook.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org8f2c605">
<h4 id="org8f2c605">Define the tokenized version of the corpus</h4>
<div class="outline-text-4" id="text-org8f2c605">
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'because'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">,</span> <span class="s1">'am'</span><span class="p">,</span> <span class="s1">'learning'</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org49c3062">
<h4 id="org49c3062">Get 'word_to_index' and 'Ind2word' dictionaries for the tokenized corpus</h4>
<div class="outline-text-4" id="text-org49c3062">
<div class="highlight">
<pre><span></span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">index_to_word</span> <span class="o">=</span> <span class="n">index_word_maps</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org58275d2">
<h3 id="org58275d2">The First Training Example</h3>
<div class="outline-text-3" id="text-org58275d2">
<p>Run the next cells to get the first training example, made of the vector representing the context words "i am because i", and the target which is the one-hot vector representing the center word "happy".</p>
<div class="highlight">
<pre><span></span><span class="n">training_examples</span> <span class="o">=</span> <span class="n">training_example_generator</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span>
<span class="n">x_array</span><span class="p">,</span> <span class="n">y_array</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">training_examples</span><span class="p">)</span>
</pre></div>
<p>In this notebook <code>next</code> is used because you will only be performing one iteration of training. In this week's assignment with the full training over several iterations you'll use regular <code>for</code> loops with the iterator that supplies the training examples.</p>
<p>The vector representing the context words, which will be fed into the neural network, is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_array</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.25 0.25 0.   0.5  0.  ]
</pre>
<p>The one-hot vector representing the center word to be predicted is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_array</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 1. 0. 0.]
</pre>
<p>Now convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects, as explained in a previous notebook.</p>
<div class="highlight">
<pre><span></span> <span class="c1"># Copy vector</span>
 <span class="n">x</span> <span class="o">=</span> <span class="n">x_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

 <span class="c1"># Reshape it</span>
 <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

 <span class="c1"># Print it</span>
 <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'x:</span><span class="se">\n</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>

 <span class="c1"># Copy vector</span>
 <span class="n">y</span> <span class="o">=</span> <span class="n">y_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

 <span class="c1"># Reshape it</span>
 <span class="n">y</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

 <span class="c1"># Print it</span>
 <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'y:</span><span class="se">\n</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
<pre class="example">
x:
[[0.25]
 [0.25]
 [0.  ]
 [0.5 ]
 [0.  ]]

y:
[[0.]
 [0.]
 [1.]
 [0.]
 [0.]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org5ef2648">
<h3 id="org5ef2648">Forward Propagation</h3>
<div class="outline-text-3" id="text-org5ef2648"></div>
<div class="outline-4" id="outline-container-org1413bba">
<h4 id="org1413bba">The Hidden Layer</h4>
<div class="outline-text-4" id="text-org1413bba">
<p>Now that you have initialized all the variables that you need for forward propagation, you can calculate the values of the hidden layer using the following formulas:</p>
\begin{align} \mathbf{z_1} = \mathbf{W_1}\mathbf{x} + \mathbf{b_1} \tag{1} \\ \mathbf{h} = \mathrm{ReLU}(\mathbf{z_1}) \tag{2} \\ \end{align}
<p>First, you can calculate the value of \(\mathbf{z_1}\).</p>
<p>Compute z1 (values of first hidden layer before applying the ReLU function)</p>
<div class="highlight">
<pre><span></span><span class="n">z1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
</pre></div>
<p>As expected you get an \(N\) by 1 matrix, or column vector with <i>N</i> elements, where <i>N</i> is equal to the embedding size, which is 3 in this example.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.36483875]
 [ 0.63710329]
 [-0.3236647 ]]
</pre>
<p>You can now take the ReLU of \(\mathbf{z_1}\) to get \(\mathbf{h}\), the vector with the values of the hidden layer.</p>
<p>Compute h (z1 after applying ReLU function)</p>
<div class="highlight">
<pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">z1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.36483875]
 [0.63710329]
 [0.        ]]
</pre>
<p>Applying ReLU means that the negative element of \(\mathbf{z_1}\) has been replaced with a zero.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org20dbb49">
<h4 id="org20dbb49">The Output Layer</h4>
<div class="outline-text-4" id="text-org20dbb49">
<p>Here are the formulas you need to calculate the values of the output layer, represented by the vector \(\mathbf{\hat y}\):</p>
\begin{align} \mathbf{z_2} &amp;= \mathbf{W_2}\mathbf{h} + \mathbf{b_2} \tag{3} \\ \mathbf{\hat y} &amp;= \mathrm{softmax}(\mathbf{z_2}) \tag{4} \\ \end{align}
<p><b>First, calculate \(\mathbf{z_2}\).</b></p>
<p>Compute z2 (values of the output layer before applying the softmax function)</p>
<div class="highlight">
<pre><span></span><span class="n">z2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.31973737</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.28125477</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.09838369</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.33512159</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.19919612</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">z2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[-0.31973737]
 [-0.28125477]
 [-0.09838369]
 [-0.33512159]
 [-0.19919612]]
</pre>
<p>This is a <i>V</i> by 1 matrix, where <i>V</i> is the size of the vocabulary, which is 5 in this example.</p>
<p><b>Now calculate the value of \(\mathbf{\hat y}\).</b></p>
<p>Compute y_hat (z2 after applying softmax function)</p>
<div class="highlight">
<pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.18519074</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.19245626</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.23107446</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.18236353</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.20891502</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.18519074]
 [0.19245626]
 [0.23107446]
 [0.18236353]
 [0.20891502]]
</pre>
<p>As you've performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better.</p>
<p><b>That being said, what word did the neural network predict?</b></p>
<div class="highlight">
<pre><span></span><span class="n">prediction</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The predicted word at index </span><span class="si">{</span><span class="n">prediction</span><span class="si">}</span><span class="s2"> is '</span><span class="si">{</span><span class="n">index_to_word</span><span class="p">[</span><span class="n">prediction</span><span class="p">]</span><span class="si">}</span><span class="s2">'."</span><span class="p">)</span>
</pre></div>
<pre class="example">
The predicted word at index 2 is 'happy'.
</pre>
<p>The neural network predicted the word "happy": the largest element of \(\mathbf{\hat y}\) is the third one, and the third word of the vocabulary is "happy".</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org31e7b7f">
<h3 id="org31e7b7f">Cross-Entropy Loss</h3>
<div class="outline-text-3" id="text-org31e7b7f">
<p>Now that you have the network's prediction, you can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.</p>
<p>Remember that you are working on a single training example, not on a batch of examples, which is why you are using <b>loss</b> and not <b>cost</b>, which is the generalized form of loss.</p>
<p>First let's recall what the prediction was.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.18519074]
 [0.19245626]
 [0.23107446]
 [0.18236353]
 [0.20891502]]
</pre>
<p>And the actual target value is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.]
 [0.]
 [1.]
 [0.]
 [0.]]
</pre>
<p>The formula for cross-entropy loss is:</p>
<p>\[ J=-\sum\limits_{k=1}^{V}y_k\log{\hat{y}_k} \tag{6} \]</p>
<p><b>Try implementing the cross-entropy loss function so you get more familiar working with numpy.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                       <span class="n">y_actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate cross-entropy loss  for the prediction</span>

<span class="sd">    Args:</span>
<span class="sd">     y_predicted: what our model predicted</span>
<span class="sd">     y_actual: the known labels</span>

<span class="sd">    Returns:</span>
<span class="sd">     cross-entropy loss for y_predicted</span>
<span class="sd">    """</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_actual</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_predicted</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
<p>Hint 1:</p>
<p>To multiply two numpy matrices (such as &lt;code&gt;y&lt;/code&gt; and &lt;code&gt;y_hat&lt;/code&gt;) element-wise, you can simply use the &lt;code&gt;*&lt;/code&gt; operator.</p>
<p>Hint 2:</p>
<p>Once you have a vector equal to the element-wise multiplication of <code>y</code> and <code>y_hat</code>, you can use <code>numpy.sum</code> to calculate the sum of the elements of this vector.</p>
<p><b>Now use this function to calculate the loss with the actual values of \(\mathbf{y}\) and \(\mathbf{\hat y}\).</b></p>
<div class="highlight">
<pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">1.4650152923611106</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
1.465
</pre>
<p>This value is neither good nor bad, which is expected as the neural network hasn't learned anything yet.</p>
<p>The actual learning will start during the next phase: backpropagation.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org56eae7d">
<h3 id="org56eae7d">Backpropagation</h3>
<div class="outline-text-3" id="text-org56eae7d">
<p>The formulas that you will implement for backpropagation are the following.</p>
\begin{align} \frac{\partial J}{\partial \mathbf{W_1}} &amp;= \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right )\mathbf{x}^\top \tag{7}\\ \frac{\partial J}{\partial \mathbf{W_2}} &amp;= (\mathbf{\hat{y}} - \mathbf{y})\mathbf{h^\top} \tag{8}\\ \frac{\partial J}{\partial \mathbf{b_1}} &amp;= \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right ) \tag{9}\\ \frac{\partial J}{\partial \mathbf{b_2}} &amp;= \mathbf{\hat{y}} - \mathbf{y} \tag{10} \end{align}
<p><b>*Note:</b> these formulas are slightly simplified compared to the ones in the lecture as you're working on a single training example, whereas the lecture provided the formulas for a batch of examples. In the assignment you'll be implementing the latter.</p>
<p>Let's start with an easy one.</p>
<p><b>Calculate the partial derivative of the loss function with respect to \(\mathbf{b_2}\), and store the result in <code>grad_b2</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{b_2}} = \mathbf{\hat{y}} - \mathbf{y} \tag{10} \]</p>
<p>Compute vector with partial derivatives of loss function with respect to b2</p>
<div class="highlight">
<pre><span></span><span class="n">grad_b2</span> <span class="o">=</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_b2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="mf">0.18519074</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.19245626</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.76892554</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.18236353</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.20891502</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_b2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.18519074]
 [ 0.19245626]
 [-0.76892554]
 [ 0.18236353]
 [ 0.20891502]]
</pre>
<p><b>Next, calculate the partial derivative of the loss function with respect to \(\mathbf{W_2}\), and store the result in <code>grad_W2</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{W_2}} = (\mathbf{\hat{y}} - \mathbf{y})\mathbf{h^\top} \tag{8} \]</p>
<p>Hint: use <code>.T</code> to get a transposed matrix, e.g. <code>h.T</code> returns \(\mathbf{h^\top}\).</p>
<p>Compute matrix with partial derivatives of loss function with respect to W2.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_W2</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.06756476</span><span class="p">,</span>  <span class="mf">0.11798563</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.0702155</span> <span class="p">,</span>  <span class="mf">0.12261452</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.28053384</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.48988499</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.06653328</span><span class="p">,</span>  <span class="mf">0.1161844</span> <span class="p">,</span>  <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.07622029</span><span class="p">,</span>  <span class="mf">0.13310045</span><span class="p">,</span>  <span class="mf">0.</span>        <span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_W2</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 0.06756476  0.11798563  0.        ]
 [ 0.0702155   0.12261452  0.        ]
 [-0.28053384 -0.48988499  0.        ]
 [ 0.06653328  0.1161844   0.        ]
 [ 0.07622029  0.13310045  0.        ]]
</pre>
<p><b>Now calculate the partial derivative with respect to \(\mathbf{b_1}\) and store the result in <code>grad_b1</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{b_1}} = \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right ) \tag{9} \]</p>
<p>Compute vector with partial derivatives of loss function with respect to b1.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_b1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_b1</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.17045858</span><span class="p">]])</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_b1</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.        ]
 [0.        ]
 [0.17045858]]
</pre>
<p><b>Finally, calculate the partial derivative of the loss with respect to \(\mathbf{W_1}\), and store it in <code>grad_W1</code>.</b></p>
<p>\[ \frac{\partial J}{\partial \mathbf{W_1}} = \rm{ReLU}\left ( \mathbf{W_2^\top} (\mathbf{\hat{y}} - \mathbf{y})\right )\mathbf{x}^\top \tag{7} \] Compute matrix with partial derivatives of loss function with respect to W1.</p>
<div class="highlight">
<pre><span></span><span class="n">grad_W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">relu</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)),</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_W1</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.</span>        <span class="p">],</span>
    <span class="p">[</span><span class="mf">0.04261464</span><span class="p">,</span> <span class="mf">0.04261464</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">,</span> <span class="mf">0.08522929</span><span class="p">,</span> <span class="mf">0.</span>        <span class="p">]])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">grad_W1</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.         0.         0.         0.         0.        ]
 [0.         0.         0.         0.         0.        ]
 [0.04261464 0.04261464 0.         0.08522929 0.        ]]
</pre>
<p>Before moving on to gradient descent, double-check that all the matrices have the expected dimensions.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'V (vocabulary size): </span><span class="si">{</span><span class="n">V</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'N (embedding size / size of the hidden layer): </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_W1: </span><span class="si">{</span><span class="n">grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (NxV)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_b1: </span><span class="si">{</span><span class="n">grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Nx1)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_W2: </span><span class="si">{</span><span class="n">grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (VxN)'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'size of grad_b2: </span><span class="si">{</span><span class="n">grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1"> (Vx1)'</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example">
V (vocabulary size): 5
N (embedding size / size of the hidden layer): 3
size of grad_W1: (3, 5) (NxV)
size of grad_b1: (3, 1) (Nx1)
size of grad_W2: (5, 3) (VxN)
size of grad_b2: (5, 1) (Vx1)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org561b4cd">
<h3 id="org561b4cd">Gradient descent</h3>
<div class="outline-text-3" id="text-org561b4cd">
<p>During the gradient descent phase, you will update the weights and biases by subtracting \(\alpha\) times the gradient from the original matrices and vectors, using the following formulas.</p>
\begin{align} \mathbf{W_1} &amp;\gets \mathbf{W_1} - \alpha \frac{\partial J}{\partial \mathbf{W_1}} \tag{11}\\ \mathbf{W_2} &amp;\gets \mathbf{W_2} - \alpha \frac{\partial J}{\partial \mathbf{W_2}} \tag{12}\\ \mathbf{b_1} &amp;\gets \mathbf{b_1} - \alpha \frac{\partial J}{\partial \mathbf{b_1}} \tag{13}\\ \mathbf{b_2} &amp;\gets \mathbf{b_2} - \alpha \frac{\partial J}{\partial \mathbf{b_2}} \tag{14}\\ \end{align}
<p>First, let set a value for \(\alpha\).</p>
<div class="highlight">
<pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.03</span>
</pre></div>
<p>The updated weight matrix \(\mathbf{W_1}\) will be:</p>
<div class="highlight">
<pre><span></span><span class="n">W1_new</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
</pre></div>
<p>Let's compare the previous and new values of \(\mathbf{W_1}\):</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'old value of W1:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'new value of W1:'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W1_new</span><span class="p">)</span>
</pre></div>
<pre class="example">
old value of W1:
[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]
 [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]
 [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]

new value of W1:
[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]
 [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]
 [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]
</pre>
<p>The difference is very subtle (hint: take a closer look at the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.</p>
<p><b>Now calculate the new values of \(\mathbf{W_2}\) (to be stored in <code>W2_new</code>), \(\mathbf{b_1}\) (in <code>b1_new</code>), and \(\mathbf{b_2}\) (in <code>b2_new</code>).</b></p>
\begin{align} \mathbf{W_2} &amp;\gets \mathbf{W_2} - \alpha \frac{\partial J}{\partial \mathbf{W_2}} \tag{12}\\ \mathbf{b_1} &amp;\gets \mathbf{b_1} - \alpha \frac{\partial J}{\partial \mathbf{b_1}} \tag{13}\\ \mathbf{b_2} &amp;\gets \mathbf{b_2} - \alpha \frac{\partial J}{\partial \mathbf{b_2}} \tag{14}\\ \end{align}
<p>Compute updated W2.</p>
<div class="highlight">
<pre><span></span><span class="n">W2_new</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
</pre></div>
<p>Compute updated b1.</p>
<div class="highlight">
<pre><span></span><span class="n">b1_new</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
</pre></div>
<p>Compute updated b2.</p>
<div class="highlight">
<pre><span></span><span class="n">b2_new</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'W2_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W2_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b1_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b1_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'b2_new'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b2_new</span><span class="p">)</span>

<span class="n">w2_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span><span class="o">-</span><span class="mf">0.22384758</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.43362588</span><span class="p">,</span>  <span class="mf">0.13310965</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.08265956</span><span class="p">,</span>  <span class="mf">0.0775535</span> <span class="p">,</span>  <span class="mf">0.1772054</span> <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.19557112</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04637608</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1790735</span> <span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.06855622</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02363691</span><span class="p">,</span>  <span class="mf">0.36107434</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.33251813</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3982269</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.43959196</span><span class="p">]])</span>

<span class="n">b1_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span> <span class="mf">0.09688219</span><span class="p">],</span>
    <span class="p">[</span> <span class="mf">0.29239497</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.27875802</span><span class="p">]])</span>

<span class="n">b2_expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
   <span class="p">[[</span> <span class="mf">0.02964508</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.36970753</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.10468778</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.35349417</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mf">0.0764456</span> <span class="p">]]</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">actual</span><span class="p">,</span> <span class="n">expected</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">((</span><span class="n">W2_new</span><span class="p">,</span> <span class="n">b1_new</span><span class="p">,</span> <span class="n">b2_new</span><span class="p">),</span> <span class="p">(</span><span class="n">w2_expected</span><span class="p">,</span> <span class="n">b1_expected</span><span class="p">,</span> <span class="n">b2_expected</span><span class="p">)):</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
W2_new
[[-0.22384758 -0.43362588  0.13310965]
 [ 0.08265956  0.0775535   0.1772054 ]
 [ 0.19557112 -0.04637608 -0.1790735 ]
 [ 0.06855622 -0.02363691  0.36107434]
 [ 0.33251813 -0.3982269  -0.43959196]]

b1_new
[[ 0.09688219]
 [ 0.29239497]
 [-0.27875802]]

b2_new
[[ 0.02964508]
 [-0.36970753]
 [-0.10468778]
 [-0.35349417]
 [-0.0764456 ]]
</pre>
<p>Congratulations, you have completed one iteration of training using one training example!</p>
<p>You'll need many more iterations to fully train the neural network, and you can optimize the learning process by training on batches of examples, as described in the lecture. You will get to do this during this week's assignment.</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4ce7a7a">
<h2 id="org4ce7a7a">End</h2>
<div class="outline-text-2" id="text-org4ce7a7a">
<p>Now that we know how to train the CBOW Model, we'll move on to <a href="posts/nlp/extracting-word-embeddings/">extracting word embeddings</a> from the model.</p>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/introducing-the-cbow-model/">Introducing the CBOW Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/introducing-the-cbow-model/" rel="bookmark"><time class="published dt-published" datetime="2020-12-09T17:02:44-08:00" itemprop="datePublished" title="2020-12-09 17:02">2020-12-09 17:02</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/introducing-the-cbow-model/#orgf07e9b1">The Continuous Bag-Of-Words (CBOW) Model</a>
<ul>
<li><a href="posts/nlp/introducing-the-cbow-model/#orgc86bce6">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/introducing-the-cbow-model/#org1366384">Activation Functions</a>
<ul>
<li><a href="posts/nlp/introducing-the-cbow-model/#org591c1bf">ReLU</a></li>
<li><a href="posts/nlp/introducing-the-cbow-model/#org3a7b8e4">SoftMax</a></li>
<li><a href="posts/nlp/introducing-the-cbow-model/#org7de9825">Dimensions: 1-D arrays vs 2-D column vectors</a></li>
</ul>
</li>
<li><a href="posts/nlp/introducing-the-cbow-model/#org859f38a">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf07e9b1">
<h2 id="orgf07e9b1">The Continuous Bag-Of-Words (CBOW) Model</h2>
<div class="outline-text-2" id="text-orgf07e9b1">
<p>In the <a href="posts/nlp/word-embeddings-data-preparation/">previous post</a> we prepared our data, now we'll look at how the CBOW model is constructed.</p>
</div>
<div class="outline-3" id="outline-container-orgc86bce6">
<h3 id="orgc86bce6">Imports</h3>
<div class="outline-text-3" id="text-orgc86bce6">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">be_true</span><span class="p">,</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1366384">
<h2 id="org1366384">Activation Functions</h2>
<div class="outline-text-2" id="text-org1366384">
<p>Let's start by implementing the activation functions, ReLU and softmax.</p>
</div>
<div class="outline-3" id="outline-container-org591c1bf">
<h3 id="org591c1bf">ReLU</h3>
<div class="outline-text-3" id="text-org591c1bf">
<p>ReLU is used to calculate the values of the hidden layer, in the following formulas:</p>
\begin{align} \mathbf{z_1} &amp;= \mathbf{W_1}\mathbf{x} + \mathbf{b_1} \tag{1} \\ \mathbf{h} &amp;= \mathrm{ReLU}(\mathbf{z_1}) \tag{2} \\ \end{align}
<p>Let's fix a value for \(\mathbf{z_1}\) as a working example.</p>
<div class="highlight">
<pre><span></span><span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Define a 5X1 column vector using numpy</span>
<span class="n">z_1</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">5</span>

<span class="c1"># Print the vector</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z_1</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[ 2.71320643]
 [-4.79248051]
 [ 1.33648235]
 [ 2.48803883]
 [-0.01492988]]
</pre>
<p>Notice that using numpy's <code>random.rand</code> function returns a numpy array filled with values taken from a uniform distribution over [0, 1). Numpy allows vectorization so each value is multiplied by 10 and then 5 is subtracted from them.</p>
<p>To get the ReLU of this vector, you want all the negative values to become zeros.</p>
<p>First create a copy of this vector.</p>
<div class="highlight">
<pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">z_1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>
<p>Now determine which of its values are negative.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">h</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[False]
 [ True]
 [False]
 [False]
 [ True]]
</pre>
<p>You can now simply set all of the values which are negative to 0.</p>
<div class="highlight">
<pre><span></span><span class="n">h</span><span class="p">[</span><span class="n">h</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
<p>And that's it: you have the ReLU of \(\mathbf{z_1}\).</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[2.71320643]
 [0.        ]
 [1.33648235]
 [2.48803883]
 [0.        ]]
</pre>
<p><b>Now implement ReLU as a function.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Get the ReLU for the input array</span>

<span class="sd">    Args:</span>
<span class="sd">     z: an array of numbers</span>

<span class="sd">    Returns:</span>
<span class="sd">     ReLU of z</span>
<span class="sd">    """</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">result</span><span class="p">[</span><span class="n">result</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">result</span>
</pre></div>
<p><b>And check that it's working.</b></p>
<div class="highlight">
<pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.25459881</span><span class="p">],</span>
              <span class="p">[</span> <span class="mf">4.50714306</span><span class="p">],</span>
              <span class="p">[</span> <span class="mf">2.31993942</span><span class="p">],</span>
              <span class="p">[</span> <span class="mf">0.98658484</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mf">3.4398136</span> <span class="p">]])</span>

<span class="c1"># Apply ReLU to it</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span>        <span class="p">],</span>
                        <span class="p">[</span><span class="mf">4.50714306</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">2.31993942</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">0.98658484</span><span class="p">],</span>
                        <span class="p">[</span><span class="mf">0.</span>        <span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.        ]
 [4.50714306]
 [2.31993942]
 [0.98658484]
 [0.        ]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org3a7b8e4">
<h3 id="org3a7b8e4">SoftMax</h3>
<div class="outline-text-3" id="text-org3a7b8e4">
<p>The second activation function that you need is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:</p>
\begin{align} \mathbf{z_2} &amp;= \mathbf{W_2}\mathbf{h} + \mathbf{b_2} \tag{3} \\ \mathbf{\hat y} &amp;= \mathrm{softmax}(\mathbf{z_2}) \tag{4} \\ \end{align}
<p>To calculate softmax of a vector \(\mathbf{z}\), the <i>i</i>-th component of the resulting vector is given by:</p>
<p>\[ \textrm{softmax}(\textbf{z})_i = \frac{e^{z_i} }{\sum\limits_{j=1}^{V} e^{z_j} } \tag{5} \]</p>
<p>Let's work through an example.</p>
<div class="highlight">
<pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
<pre class="example">
[ 9.   8.  11.  10.   8.5]
</pre>
<p>You'll need to calculate the exponentials of each element, both for the numerator and for the denominator.</p>
<div class="highlight">
<pre><span></span><span class="n">e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
</pre></div>
<pre class="example">
[ 8103.08392758  2980.95798704 59874.1417152  22026.46579481
  4914.7688403 ]
</pre>
<p>The denominator is equal to the sum of these exponentials.</p>
<div class="highlight">
<pre><span></span><span class="n">sum_e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">sum_e_z</span><span class="si">:</span><span class="s2">,.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
97,899.42
</pre>
<p>And the value of the first element of \(\textrm{softmax}(\textbf{z})\) is given by:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">e_z</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">sum_e_z</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
0.0828
</pre>
<p>This is for one element. You can use numpy's vectorized operations to calculate the values of all the elements of the \(\textrm{softmax}(\textbf{z})\) vector in one go.</p>
<p><b>Implement the softmax function.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate Softmax for the input</span>

<span class="sd">    Args:</span>
<span class="sd">     v: array of values</span>

<span class="sd">    Returns:</span>
<span class="sd">     array of probabilities</span>
<span class="sd">    """</span>
    <span class="n">e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">sum_e_z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">e_z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">e_z</span> <span class="o">/</span> <span class="n">sum_e_z</span>
</pre></div>
<p><b>Now check that it works.</b></p>
<div class="highlight">
<pre><span></span><span class="n">actual</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.08276948</span><span class="p">,</span>
                        <span class="mf">0.03044919</span><span class="p">,</span>
                        <span class="mf">0.61158833</span><span class="p">,</span>
                        <span class="mf">0.22499077</span><span class="p">,</span>
                        <span class="mf">0.05020223</span><span class="p">])</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.08276948 0.03044919 0.61158833 0.22499077 0.05020223]
</pre>
<p>Notice that the sum of all these values is equal to 1.</p>
<div class="highlight">
<pre><span></span><span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">softmax</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">])))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7de9825">
<h3 id="org7de9825">Dimensions: 1-D arrays vs 2-D column vectors</h3>
<div class="outline-text-3" id="text-org7de9825">
<p>Before moving on to implement forward propagation, backpropagation, and gradient descent in the next lecture notebook, let's have a look at the dimensions of the vectors you've been handling until now.</p>
<p>Create a vector of length <i>V</i> filled with zeros.</p>
<p>Define V. Remember this was the size of the vocabulary in the previous lecture notebook</p>
<div class="highlight">
<pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="mi">5</span>
</pre></div>
<p>Define vector of length V filled with zeros</p>
<div class="highlight">
<pre><span></span><span class="n">x_array</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_array</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0. 0. 0. 0. 0.]
</pre>
<p>This is a 1-dimensional array, as revealed by the <code>.shape</code> property of the array.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_array</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(5,)
</pre>
<p>To perform matrix multiplication in the next steps, you actually need your column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.</p>
<p>The easiest way to convert a 1D vector to a 2D column matrix is to set its `.shape` property to the number of rows and one column, as shown in the next cell.</p>
<div class="highlight">
<pre><span></span><span class="c1"># Copy vector</span>
<span class="n">x_column_vector</span> <span class="o">=</span> <span class="n">x_array</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Reshape copy of vector</span>
<span class="n">x_column_vector</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># alternatively ... = (x_array.shape[0], 1)</span>

<span class="c1"># Print vector</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_column_vector</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.]
 [0.]
 [0.]
 [0.]
 [0.]]
</pre>
<p>The shape of the resulting "vector" is:</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">x_column_vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
(5, 1)
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org859f38a">
<h2 id="org859f38a">End</h2>
<div class="outline-text-2" id="text-org859f38a">
<p>Now that we have the basics of the model we can move on to <a href="posts/nlp/training-the-cbow-model/">training the model</a>.</p>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="index-17.html" rel="prev">Newer posts</a></li>
<li class="next"><a href="index-15.html" rel="next">Older posts</a></li>
</ul>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script><!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
